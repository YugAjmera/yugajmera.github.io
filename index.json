[{"content":"Imagine being able to generate photorealistic 3D models of objects and scenes that can be viewed from any angle, with details so realistic that they are indistinguishable from reality. That\u0026rsquo;s what the Neural Radiance Fields (NeRF) is capable of doing and much more. With more than 50 papers related to NeRFs in the CVPR 2022, it is one of the most influential papers of all time.\nNeural fields A neural field is a neural network that parametrizes a signal. In our case, this signal is either a single 3D scene or an object. It is important to note that a single network needs to be trained to encode (capture) a single scene. It is worth mentioning that, unlike standard machine learning, the objective is to overfit the neural network to a particular scene. Essentially, neural fields embed the scene into the weights of the network.\n3D scenes are usually stored in computer graphics using voxel grids or polygon meshes. However, voxels are costly to store and polygon meshes are limited to hard surfaces. Neural fields are gaining popularity as they are efficient and compact representations of objects or scenes that are differentiable, continuous, and can have arbitrary dimensions and resolutions. Neural radiance fields are a special case of Neural fields that solve the view synthesis problem.\nNeural Radiance Fields (NeRFs) NeRFs as proposed by Mildenhall et al accept a single continuous 5D coordinate as input, which consists of a spatial location $(x, y, z)$ and viewing direction $(\\theta, \\phi)$. This particular point of the object/scene is fed into an MLP, which outputs the corresponding color intensities $c = (r, g, b)$ and volume density $\\sigma$.\nThe network’s weights are optimized to encode the representation of the scene so that the model can easily render novel views seen from any point in space.\nRay Marching To gain a better grasp of the different stages in NeRF training, let\u0026rsquo;s use this 3D scene as an instance.\nThe training dataset includes images ($H \\times W$) captured from $n$ different viewpoints of the same scene. Each image\u0026rsquo;s camera viewpoint is stored as $x_c, y_c, z_c$ in the world space. Since the camera is always \u0026ldquo;aimed\u0026rdquo; at the object, we only need two more rotation parameters to fully describe the pose: the inclination and azimuth angles ($\\theta$, $\\phi$).\nNow, for each camera pose, we \u0026ldquo;shoot\u0026rdquo; a ray from the camera (or viewer\u0026rsquo;s eye), through every pixel of the image, resulting in $H \\times W$ rays per pose. Each ray is described by two vectors,\n$\\mathbf{o}$ : a vector denoting the origin of the ray. $\\mathbf{d}$ : a normalized vector denoting the direction of the ray. An arbitrary point on the ray can then be defined as $r(t) = \\mathbf{o} + t * \\mathbf{d}$.\nThis process of ray tracing is known as backward tracing, as it involves tracing the path of light rays from the camera to the object, as opposed to tracing from the light source to the object.\nInput: A set of camera poses $(x_c, , y_c, , z_c,, , \\theta, , \\phi)_n$\nOutput: A bundle of rays for every pose $(r_{\\mathbf{o}, \\mathbf{d}})_{H \\times W \\times n}$\nSampling Query points You may wonder, what is done with the rays? We trace them from the camera through the scene by adjusting the parameter $t$ until they intersect with some interesting location (object) in the scene. To find these locations, we incrementally step along the ray and sample points at regular intervals.\nBy querying a trained neural network at these 3D points along the viewing ray, we can determine if they belong to the object volume and obtain their visual properties to render an image. However, sampling points along a ray is challenging, as too many non-object points won\u0026rsquo;t provide useful information, and focusing only on high-density regions may miss interesting areas.\nIn our toy example, we uniformly sample along the ray by taking $m$ samples. However, to improve performance, the authors use \u0026ldquo;hierarchical volume sampling\u0026rdquo; to allocate samples proportionally to their expected impact on the final rendering.\nInput: A bundle of rays for every pose $(r_{\\mathbf{o}, \\mathbf{d}})_{H \\times W \\times n}$\nOutput: A set of 3D query points for every ray $(x_p, , y_p, , z_p)_{m \\times H \\times W \\times n}$\nPositional Encoding Once we collect the query points for every ray, we are potentially ready to feed them into our neural network. However, the authors found that resulting renderings perform poorly at representing high-frequency variations in color and geometry that make images perceptually sharp and vivid for the human eye.\nThis observation is consistent with Rahaman et al. who show that deep networks have a tendency to learn lower-frequency functions. They claim that mapping the inputs to a higher dimensional space using high-frequency functions before passing them to the network enables better fitting of data that contains high-frequency variation.\nThe authors use the positional encoding containing sine and cosine functions of varying frequencies.\n\\begin{equation*} \\gamma(p) = [sin (2^0 \\pi p) , cos (2^0 \\pi p) ; sin (2 \\pi p) , cos (2 \\pi p) ; \u0026hellip; ; sin (2^{L-1} \\pi p) , cos (2^{L-1} \\pi p)] \\end{equation*}\nwhere $L$ is the number of dimensions in the positional encoding. The function $\\gamma(.)$ is applied separately to each of the three coordinate values in $\\mathbf{x}$ (which are normalized to lie in [−1, 1]). As viewing direction is also an input to our network, we embed it as well. The authors use $L = 10$ for embeding the 3D query points and $L = 4$ for embeding the viewing direction.\nInput: A set of 3D query points for every ray, $(\\mathbf{x} = x_p, , y_p, , z_p)_{m \\times H \\times W \\times n}$ and 3D viewing direction $(\\mathbf{d} = \\theta, \\phi, \\psi)_n$\nOutput: Embedings of query points $\\gamma(\\mathbf{x})_{m \\times H \\times W \\times n}$ and viewing direction $\\gamma(\\mathbf{d})_n$\nNeural Network inference To achieve multiview consistency in a neural network, we restrict the network to predict the volume density $\\sigma$ as a function of only the location in 3D space, while allowing the RGB color $c$ to be predicted as a function of both location and viewing direction.\nThe MLP architecture consists of 8 fully-connected layers, each with 256 channels and ReLU activations. A skip connection is included in the network that concatenates the input to the fifth layer\u0026rsquo;s activation. It takes the encoded query points $\\gamma(\\mathbf{x})$ as input and produces two outputs: the volume density $\\sigma$ and a 256-dimensional feature vector.\nThis feature vector is then concatenated with the embedded viewing direction $\\gamma(\\mathbf{d})$ and passed through another fully-connected layer with 128 channels and a ReLU activation to produce the view-dependent RGB color $c$ (tuple in the range from 0 to 1). The authors claim that a model trained without view dependence has difficulty representing specularities.\nBoth of these pieces of information combined allow us to compute the volume density profile for every ray as shown in the figure below.\nInput: Embedings of query points $\\gamma(\\mathbf{x})_{m \\times H \\times W \\times n}$ and viewing direction $\\gamma(\\mathbf{d})_n$\nOutput: RGB color and volume density for every query point $(c, , \\sigma)_{m \\times H \\times W \\times n}$\nVolume Rendering Now, it\u0026rsquo;s time to turn this volume density profile along a ray into an rgb pixel value. Once we have computed the pixel values for all the rays, we will have a full $H \\times W$ image for all the $n$ viewpoints.\nThe volume rendering process involves computing the accumulated radiance along the viewing ray as it passes through the neural radiance field. This is done by integrating the radiance values at each sampled point along the ray, weighted by the transmittance, or opacity of the medium at that point.\nThe expected color of a camera ray $r(t) = \\mathbf{o} + t * \\mathbf{d}$ with near and far bounds $t_n$ and $t_f$ is given as:\n\\begin{equation} C(r) = \\int_{t_n}^{t_f} T(t) ; \\sigma(r(t)) ; c(r(t), \\mathbf{d}) ; dt \\end{equation}\nwhere function $T(t)$ denotes the accumulated transmittance along the ray from $t_n$ to $t$, i.e., the probability that the ray travels from $t_n$ to $t$ without hitting any other particle.\n\\begin{equation*} T(t) = \\text{exp} \\left( - \\int_{t_n}^{t} \\sigma(r(s)) ; ds \\right) \\end{equation*}\nThese complex-looking integrals can be approximated via numerical quadrature. We use a stratified sampling approach where we select a random set of quadrature points ${t_i}_{i = 1}^N$ by uniformly drawing samples from $N$ evenly-spaced ray bins between $t_n$ and $t_f$.\nThe use of stratified sampling allows for a continuous representation of the scene, despite using a discrete set of samples to estimate the integral. This is because the MLP is evaluated at continuous positions during optimization.\nThe approximated color of each ray is computed as,\n\\begin{equation*} C(r) \\approx \\sum_{i = 1}^N T(t_i) ; \\alpha_i ; c_i \\end{equation*} where $\\alpha_i = (1 - \\text{exp}(-\\sigma_i , \\delta_i))$, which can be viewed as a measure of opacity and $\\delta_i = t_{i+1} - t_i$ is the distance between two quadrature points. The accumulated transmittance is then a cumulative product of all the transmittance (which is 1 - opacity) behind it,\n\\begin{equation*} T_i = \\prod_{j=1}^{i-1} (1 - \\alpha_j) \\end{equation*}\nInput: RGB color and volume density for every query point $(c, , \\sigma)_{m \\times H \\times W \\times n}$\nOutput: Rendered Images $(H \\times W)_{n}$\nComputing loss The final step is to calculate the loss between the rendered image and the ground truth for each viewpoint in the dataset. This loss function is a simple L2 loss between each pixel of the rendered image and the corresponding pixel of the ground truth image.\nCredits: While writing this blog, I referred to dtransposed\u0026rsquo;s blog post (the images are also from this post), and AI Summer\u0026rsquo;s post.\n","permalink":"https://yugajmera.github.io/posts/nerf/post/","summary":"Imagine being able to generate photorealistic 3D models of objects and scenes that can be viewed from any angle, with details so realistic that they are indistinguishable from reality. That\u0026rsquo;s what the Neural Radiance Fields (NeRF) is capable of doing and much more. With more than 50 papers related to NeRFs in the CVPR 2022, it is one of the most influential papers of all time.\nNeural fields A neural field is a neural network that parametrizes a signal.","title":"Understanding Neural Radiance Fields (NeRFs)"},{"content":"Diffusion models are a new class of state-of-the-art generative models that generate diverse high-resolution images. There are already a bunch of different diffusion models that include Open AI’s DALL-E 2 and GLIDE, Google’s Imagen, and Stability AI’s Stable Diffusion. In this blog post, we will dig our way up from the basic principles described in the most prominent one, which is the Denoising Diffusion Probabilistic Models (DDPM) as initialized by Sohl-Dickstein et al in 2015 and then improved by Ho. et al in 2020.\nThe basic idea behind diffusion models is rather simple. It takes an input image $\\mathbf{x}_0$ and gradually adds Gaussian noise to it through a series of $T$ time steps. We will call this the forward process. A network is then trained to recover the original image by reversing the noising process. By being able to model the reverse process, we can start from random noise and denoise it step-by-step to generate new data.\nForward Diffusion Process Consider an image $\\mathbf{x}_0$ sampled from the real data distribution (or the training set). The subscript denotes the number of time step. The forward process denoted by $q$ is modeled as a Markov chain, where the distribution at a particular time step depends only on the sample from the previous step. The distribution of corrupted samples can be written as, \\begin{align*} q(\\mathbf{x}_{1:T} | \\mathbf{x}_0) \u0026amp;= \\prod_{t=1}^T q(\\mathbf{x}_{t} | \\mathbf{x}_{t-1}) \\tag{1} \\end{align*}\nAt each step of the Markov chain, we add Gaussian noise to $\\mathbf{x}_{t-1}$ producing a new latent variable $\\mathbf{x}_t$. The transition distribution forms a unimodal diagonal Gaussian as, \\begin{equation*} q(\\mathbf{x}_{t} | \\mathbf{x}_{t-1}) = \\mathcal{N} (\\mathbf{x}_t; \\; \\mu_t = \\sqrt{1 - \\beta_t} \\; \\mathbf{x}_{t-1}, \\; \\Sigma_t = \\beta_t \\mathbf{I}) \\tag{2} \\end{equation*} where $\\beta_t$ is the variance of Gaussian at a time step $t$. It is a hyperparameter that follows a fixed schedule such that it increases with time and lies in the range $[0, 1]$.\nHo et al. sets a linear schedule for the variance starting from $\\beta_1 = 10^{-4}$ to $\\beta_T = 0.02$, and $T = 1000$.\nA latent variable $\\mathbf{x}_t$ can be sampled from the distribution $q(\\mathbf{x}_{t} | \\mathbf{x}_{t-1})$ by using the reparameterization trick is as, \\begin{equation*} \\mathbf{x}_{t} = \\sqrt{1 - \\beta_t} \\; \\mathbf{x}_{t-1} + \\sqrt{\\beta_t} \\; \\epsilon_t \\tag{3} \\end{equation*} where $\\epsilon_t \\sim \\mathcal{N}(0, 1)$.\nEquation 3 shows that we need to compute all the previous samples $\\mathbf{x}_{t-1}, \u0026hellip;, \\mathbf{x}_{0}$ in order to obtain $\\mathbf{x}_t$, making it expensive. To solve this problem, we define, \\begin{align*} \u0026amp;\\alpha_t = (1 - \\beta_t), \u0026amp;\\bar{\\alpha_t} = \\prod_{s=0}^T \\alpha_s \\end{align*} and rewrite equation 3 in a recursive manner, \\begin{align*} \\mathbf{x}_{t} \u0026amp;= \\sqrt{\\alpha_t} \\; \\mathbf{x}_{t-1} + \\sqrt{1 - \\alpha_t} \\; \\epsilon_t \\\\ \u0026amp;= \\sqrt{\\alpha_t} \\; \\left[ \\sqrt{\\alpha_{t-1}} \\; \\mathbf{x}_{t-2} + \\sqrt{1 - \\alpha_{t-1}} \\; \\epsilon_t \\right] + \\sqrt{1 - \\alpha_t} \\; \\epsilon_t \\\\ \u0026amp;= \\sqrt{\\alpha_t \\alpha_{t-1}} \\; \\mathbf{x}_{t-2} + \\sqrt{ (\\alpha_t)(1 - \\alpha_{t-1}) + (1 - \\alpha_t)} \\; \\epsilon_t \\\\ \u0026amp;= \\sqrt{\\alpha_t \\alpha_{t-1}} \\; \\mathbf{x}_{t-2} + \\sqrt{1 - \\alpha_{t} \\alpha_{t-1}} \\; \\epsilon_t \\\\ \u0026amp; \u0026hellip; \\\\ \u0026amp;= \\sqrt{\\alpha_t \\alpha_{t-1} \\; \u0026hellip;\\; \\alpha_{0}} \\; \\mathbf{x}_{0} + \\sqrt{1 - \\alpha_{t} \\alpha_{t-1} \\; \u0026hellip; \\; \\alpha_0} \\; \\epsilon_t \\\\ \u0026amp;= \\sqrt{\\bar{\\alpha_t}} \\; \\mathbf{x}_{0} + \\sqrt{1 - \\bar{\\alpha_{t}}} \\; \\epsilon_t \\tag{4} \\end{align*}\nThe close-form sampling at any arbitrary timestep can be carried out using the following distribution, \\begin{align*} \\mathbf{x}_{t} \\sim q(\\mathbf{x}_{t} | \\mathbf{x}_{0}) = \\mathcal{N} (\\mathbf{x}_{t}; \\; \\mu_t = \\sqrt{\\bar{\\alpha_t}} \\; \\mathbf{x}_{0}, \\; \\Sigma_t = (1 - \\bar{\\alpha_t}) \\mathbf{I}) \\tag{5} \\end{align*} Since $\\beta_t$ is a hyperparameter that is fixed beforehand, we can precompute $\\alpha_t$ and $\\bar{\\alpha_t}$ for all timesteps and use Equation 4 to sample the latent variable $\\mathbf{x}_t$ in one go.\nReverse Diffusion Process As $T \\xrightarrow{} \\infty$, $\\bar{\\alpha_t} \\xrightarrow{} 0$, the distribution $q(\\mathbf{x}_{T} | \\mathbf{x}_0) \\approx \\mathcal{N}(0, \\mathbf{I})$ (also called isotropic Gaussian distribution), losing all information about the original sample. Therefore if we manage to learn the reverse distribution, we can sample $\\mathbf{x}_T \\sim \\mathcal{N}(0, \\mathbf{I})$, and run the denoising process step-wise to generate a new sample.\nWith a small enough step size ($\\beta_t \\ll 1$), the reverse process has the same functional form as the forward process. Therefore, the reverse distribution can also be modeled as a unimodal diagonal Gaussian. Unfortunately, it is not straightforward to estimate $q(\\mathbf{x}_{t-1}| \\mathbf{x}_{t})$, as it needs to use the entire dataset (It\u0026rsquo;s intractable since it requires knowing the distribution of all possible images in order to calculate this conditional probability).\nHence, we use a network to learn this Gaussian by parameterizing the mean and variance, \\begin{equation*} p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t}) = \\mathcal{N} (\\mathbf{x}_{t-1}; \\; \\mu_\\theta(\\mathbf{x}_{t}, t), \\; \\Sigma_\\theta(\\mathbf{x}_{t}, t)) \\tag{6} \\end{equation*} Apart from the latent sample $\\mathbf{x}_{t}$, the model also takes time step $t$ as input. Different time steps are associated with different noise levels, and the model learns to undo these individually.\nLike the forward process, the reverse process can also be set up as a Markov chain. We can write the joint probability of the sequence of samples as, \\begin{equation*} p_\\theta (x_{0:T}) = p(\\mathbf{x}_{T}) \\prod_{t=1}^T p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t}) \\tag{7} \\end{equation*} Here, $p(\\mathbf{x}_{T}) = \\mathcal{N}(0, \\mathbf{I})$ as we start training with a sample from pure noise distribution.\nTraining Objective (Loss function) The forward process is fixed and it\u0026rsquo;s the reverse process that we solely focus on learning. Diffusion models can be seen as latent variable models, and are similar to variational autoencoders (VAEs), where $\\mathbf{x}_{0}$ is an observed variable and $\\mathbf{x}_{1}, \\mathbf{x}_{2}, \u0026hellip;, \\mathbf{x}_{T}$ are latent variables.\nMaximizing the variational lower bound (also called evidence lower bound ELBO) on the marginal log-likelihood forms the objective in VAEs. For an observed variable $x$ and latent variable $z$, this lower bound can be written as, \\begin{align*} \\log p_\\theta (x) \\ge \\mathbf{E}_{q(z|x)} [\\log p_\\theta (x|z)] - \\mathbf{D}_{KL} \\left( q (z|x) \\; || \\; p_\\theta(z) \\right) \\end{align*}\nRewriting it in the diffusion model framework we get, \\begin{align*} \\text{ELBO} \u0026amp;= \\mathbf{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})} [\\log p_\\theta (\\mathbf{x}_{0}|\\mathbf{x}_{1:T})] - \\mathbf{D}_{KL} \\left( q (\\mathbf{x}_{1:T}|\\mathbf{x}_{0}) \\; || \\; p_\\theta(\\mathbf{x}_{1:T}) \\right) \\\\ \u0026amp;= \\mathbf{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})} [\\log p_\\theta (\\mathbf{x}_{0}|\\mathbf{x}_{1:T})] - \\mathbf{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})} \\left[ \\log \\frac{q (\\mathbf{x}_{1:T}|\\mathbf{x}_{0})}{p_\\theta(\\mathbf{x}_{1:T})} \\right] \\\\ \u0026amp;= \\mathbf{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})} \\left[ \\log \\frac{p_\\theta (\\mathbf{x}_{0}|\\mathbf{x}_{1:T}) \\; p_\\theta(\\mathbf{x}_{1:T}) }{q (\\mathbf{x}_{1:T}|\\mathbf{x}_{0})} \\right] \\\\ \u0026amp;= \\mathbf{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})} \\left[ \\log \\frac{p_\\theta (\\mathbf{x}_{0:T})}{q (\\mathbf{x}_{1:T}|\\mathbf{x}_{0})} \\right] \\\\ \\\\ \u0026amp;\\text{Using equation 1 and 5,} \\\\ \u0026amp;= \\mathbf{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})} \\left[ \\log p(\\mathbf{x}_{T}) + \\sum_{t=1}^T \\log \\frac{p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t})}{q (\\mathbf{x}_{t}|\\mathbf{x}_{t-1})} \\right] \\\\ \\\\ \u0026amp;\\text{Taking the edge case $t=1$ out,} \\\\ \u0026amp;= \\mathbf{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})} \\left[ \\log p(\\mathbf{x}_{T}) + \\log \\frac{p_\\theta (\\mathbf{x}_{0} | \\mathbf{x}_{1})}{q (\\mathbf{x}_{1}|\\mathbf{x}_{0})} + \\sum_{t=2}^T \\log \\frac{p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t})}{q (\\mathbf{x}_{t}|\\mathbf{x}_{t-1})} \\right] \\\\ \\\\ \u0026amp;\\text{Using Markov property and then Bayes’ rule we can write,} \\\\ \u0026amp;\\text{$q (\\mathbf{x}_{t}|\\mathbf{x}_{t-1}) = q (\\mathbf{x}_{t}|\\mathbf{x}_{t-1}, \\mathbf{x}_{0}) = \\frac{ q (\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0}) \\; q (\\mathbf{x}_{t}|\\mathbf{x}_{0})}{ q (\\mathbf{x}_{t-1}, \\mathbf{x}_{0})}$ and replace,} \\\\ \\\\ \u0026amp;= \\mathbf{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})} \\left[ \\log p(\\mathbf{x}_{T}) + \\log \\frac{p_\\theta (\\mathbf{x}_{0} | \\mathbf{x}_{1})}{q (\\mathbf{x}_{1}|\\mathbf{x}_{0})} + \\sum_{t=2}^T \\log \\frac{p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t}) \\; q (\\mathbf{x}_{t-1}|\\mathbf{x}_{0})}{q (\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0}) \\; q (\\mathbf{x}_{t}|\\mathbf{x}_{0})} \\right] \\\\ \u0026amp;= \\mathbf{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})} \\left[ \\log p(\\mathbf{x}_{T}) + \\log \\frac{p_\\theta (\\mathbf{x}_{0} | \\mathbf{x}_{1})}{q (\\mathbf{x}_{1}|\\mathbf{x}_{0})} + \\sum_{t=2}^T \\log \\frac{p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t})}{q (\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0})} + \\sum_{t=2}^T \\log \\frac{q (\\mathbf{x}_{t-1}|\\mathbf{x}_{0})}{q (\\mathbf{x}_{t}|\\mathbf{x}_{0})} \\right] \\\\ \u0026amp;= \\mathbf{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})} \\left[ \\log p(\\mathbf{x}_{T}) + \\log \\frac{p_\\theta (\\mathbf{x}_{0} | \\mathbf{x}_{1})}{q (\\mathbf{x}_{1}|\\mathbf{x}_{0})} + \\sum_{t=2}^T \\log \\frac{p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t})}{q (\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0})} + \\log \\frac{ q (\\mathbf{x}_{1}|\\mathbf{x}_{0}) \\; \\cancel{q (\\mathbf{x}_{2}|\\mathbf{x}_{0})} \\; \u0026hellip; \\; \\cancel{q (\\mathbf{x}_{T-1}|\\mathbf{x}_{0})}}{ \\cancel{q (\\mathbf{x}_{2}|\\mathbf{x}_{0})} \\; q \\cancel{(\\mathbf{x}_{3}|\\mathbf{x}_{0})} \\; \u0026hellip; \\; q (\\mathbf{x}_{T}|\\mathbf{x}_{0})} \\right]\\\\ \u0026amp;= \\mathbf{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})} \\left[ \\log {\\color{blue}{p(\\mathbf{x}_{T})}} + \\log \\frac{p_\\theta (\\mathbf{x}_{0} | \\mathbf{x}_{1})}{\\color{red}q (\\mathbf{x}_{1}|\\mathbf{x}_{0})} + \\sum_{t=2}^T \\log \\frac{p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t})}{q (\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0})} + \\log \\frac{ \\color{red}{q (\\mathbf{x}_{1}|\\mathbf{x}_{0})}}{\\color{blue}{q (\\mathbf{x}_{T}|\\mathbf{x}_{0})}} \\right]\\\\ \u0026amp;= \\mathbf{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})} \\left[ \\log p_\\theta (\\mathbf{x}_{0} | \\mathbf{x}_{1}) + \\log {\\color{blue}\\frac{p(\\mathbf{x}_{T})}{q (\\mathbf{x}_{T}|\\mathbf{x}_{0})}} + \\sum_{t=2}^T \\log \\frac{p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t})}{q (\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0})} \\right] \\\\ \u0026amp;= \\mathbf{E}_{q(\\mathbf{x}_{1}|\\mathbf{x}_{0})} [\\log p_\\theta (\\mathbf{x}_{0} | \\mathbf{x}_{1})] + \\mathbf{E}_{q(\\mathbf{x}_{T}|\\mathbf{x}_{0})} \\left[ \\log {\\frac{p(\\mathbf{x}_{T})}{q (\\mathbf{x}_{T}|\\mathbf{x}_{0})}} \\right] + \\mathbf{E}_{q(\\mathbf{x}_{t-1}, \\mathbf{x}_{t}|\\mathbf{x}_{0})} \\left[ \\sum_{t=2}^T \\log \\frac{p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t})}{q (\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0})} \\right] \\\\ \u0026amp;= \\mathbf{E}_{q(\\mathbf{x}_{1}|\\mathbf{x}_{0})} [\\log p_\\theta (\\mathbf{x}_{0} | \\mathbf{x}_{1})] - \\mathbf{D}_{KL} (q (\\mathbf{x}_{T}|\\mathbf{x}_{0}) \\; || \\; p(\\mathbf{x}_{T})) - \\sum_{t\u0026gt;1} \\mathbf{E}_{q(\\mathbf{x}_{t}|\\mathbf{x}_{0})} \\left[ \\mathbf{D}_{KL} ( q (\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0}) \\; || \\; p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t})) \\right] \\end{align*}\nThe objective of maximizing this lower bound is equivalent to minimizing a loss function that is its negation, \\begin{align*} L_{vlb} = \\underbrace{\\mathbf{D}_{KL} (q (\\mathbf{x}_{T}|\\mathbf{x}_{0}) \\; || \\; p(\\mathbf{x}_{T}))}_{L_{T}} + \\sum_{t\u0026gt;1} \\mathbf{E}_{q(\\mathbf{x}_{t}|\\mathbf{x}_{0})} [ \\underbrace{\\mathbf{D}_{KL} ( q (\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0}) \\; || \\; p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t}))}_{L_{t-1}} ] \\; \\underbrace{- \\mathbf{E}_{q(\\mathbf{x}_{1}|\\mathbf{x}_{0})} \\left[ \\log p_\\theta (\\mathbf{x}_{0} | \\mathbf{x}_{1}) \\right]}_{L_0} \\tag{8} \\end{align*}\nThe term $L_T$ has no trainable parameters so it\u0026rsquo;s ignored during training, furthermore, as we have assumed a large enough $T$ such that the final distribution is Gaussian, this term effectively becomes zero.\n$L_0$ can be interpreted as a reconstruction term (similar to VAE).\nThe term $L_{t-1}$ formulates the difference between the predicted denoising steps $p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t})$ and the reverse diffusion step $q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0})$ (which is given as a target to the model). It is explicitly conditioned on the original sample $\\mathbf{x}_{0}$ in the loss function so that the distribution $q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0})$ takes the form of Gaussian. \\begin{align*} q (\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0}) = \\mathcal{N} (\\mathbf{x}_{t-1}; \\; \\tilde{\\mu}_t(\\mathbf{x}_t, \\mathbf{x}_0) \\; \\tilde{\\beta_t}) \\end{align*}\nBut why do we need it to be Gaussian?\nSince the model output, $p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t})$ is already parameterized as a Gaussian, every KL term compares two Gaussian distributions and therefore they can be computed in closed form. This makes the loss function tractable.\nIntuitively, a painter (our generative model) needs a reference image ($\\mathbf{x}_{0}$) to slowly draw (reverse diffusion step $q(\\mathbf{x}_{t-1} | \\mathbf{x}_t)$) an image. Thus, we can take a small step backward, meaning from noise to generate an image, if and only if we have $\\mathbf{x}_{0}$ as a reference.\nUsing Bayes\u0026rsquo; rule we can write, \\begin{align*} q (\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0}) \u0026amp;= q (\\mathbf{x}_{t}|\\mathbf{x}_{t-1}, \\mathbf{x}_{0}) \\; \\frac{q (\\mathbf{x}_{t-1}|\\mathbf{x}_{0})}{q (\\mathbf{x}_{t}| \\mathbf{x}_{0})} \\\\ \\\\ \u0026amp;\\text{Using equation 2 and 4,} \\\\ \u0026amp;= \\mathcal{N} (\\mathbf{x}_t; \\; \\sqrt{1 - \\beta_t} \\; \\mathbf{x}_{t-1}, \\; \\beta_t \\mathbf{I}) \\; \\frac{ \\mathcal{N} (\\mathbf{x}_{t-1}; \\; \\sqrt{\\bar{\\alpha}_{t-1}} \\; \\mathbf{x}_{0}, \\; (1 - \\bar{\\alpha}_{t-1}) \\mathbf{I})}{ \\mathcal{N} (\\mathbf{x}_{t}; \\; \\sqrt{\\bar{\\alpha}_{t}} \\; \\mathbf{x}_{0}, \\; (1 - \\bar{\\alpha}_{t}) \\mathbf{I})} \\\\ \\\\ \u0026amp;\\text{Replacing $1 - \\beta_t = \\alpha_t$,} \\\\ \u0026amp;= \\mathcal{N} (\\mathbf{x}_t; \\; \\sqrt{\\alpha_t} \\; \\mathbf{x}_{t-1}, \\; \\beta_t \\mathbf{I}) \\; \\frac{ \\mathcal{N} (\\mathbf{x}_{t-1}; \\; \\sqrt{\\bar{\\alpha}_{t-1}} \\; \\mathbf{x}_{0}, \\; (1 - \\bar{\\alpha}_{t-1}) \\mathbf{I})}{ \\mathcal{N} (\\mathbf{x}_{t}; \\; \\sqrt{\\bar{\\alpha}_{t}} \\; \\mathbf{x}_{0}, \\; (1 - \\bar{\\alpha}_{t}) \\mathbf{I})} \\\\ \u0026amp; \\propto \\text{exp} \\left[ -\\frac{1}{2} \\left( \\frac{(\\mathbf{x}_t - \\sqrt{\\alpha_t} \\; \\mathbf{x}_{t-1})^2}{\\beta_t} + \\frac{(\\mathbf{x}_{t-1} - \\sqrt{\\bar{\\alpha}_{t-1}} \\; \\mathbf{x}_{0})^2}{(1 - \\bar{\\alpha}_{t-1})} - \\frac{(\\mathbf{x}_{t} - \\sqrt{\\bar{\\alpha}_{t}} \\; \\mathbf{x}_{0})^2}{(1 - \\bar{\\alpha}_{t})} \\right) \\right] \\\\ \u0026amp;= \\text{exp} \\left[ -\\frac{1}{2} \\left( \\frac{\\mathbf{x}_t^2 + \\alpha_t \\; \\mathbf{x}_{t-1}^2 - 2 \\; \\mathbf{x}_t \\sqrt{\\alpha_t} \\; \\mathbf{x}_{t-1}}{\\beta_t} + \\frac{\\mathbf{x}_{t-1}^2 + \\bar{\\alpha}_{t-1} \\; \\mathbf{x}_{0}^2 - 2\\; \\mathbf{x}_{t-1} \\sqrt{\\bar{\\alpha}_{t-1}} \\; \\mathbf{x}_{0}}{(1 - \\bar{\\alpha}_{t-1})}- \\frac{(\\mathbf{x}_{t} - \\sqrt{\\bar{\\alpha}_{t}} \\; \\mathbf{x}_{0})^2}{(1 - \\bar{\\alpha}_{t})} \\right) \\right] \\\\ \u0026amp;= \\text{exp} \\left[ -\\frac{1}{2} \\left( \\mathbf{x}_{t-1}^2 \\left( \\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}} \\right) + \\mathbf{x}_{t-1} \\left( \\frac{- 2 \\; \\mathbf{x}_t \\sqrt{\\alpha_t}}{\\beta_t} + \\frac{- 2\\; \\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0}{1 - \\bar{\\alpha}_{t-1}} \\right) + C(\\mathbf{x}_t, \\mathbf{x}_0)) \\right) \\right] \\\\ \\\\ \u0026amp;\\text{where C is a function whose details are omitted for readability.} \\\\ \u0026amp;= \\text{exp} \\left[ -\\frac{1}{2} \\left( \\mathbf{x}_{t-1}^2 \\left( \\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}} \\right) - 2 \\; \\mathbf{x}_{t-1} \\left( \\frac{\\mathbf{x}_t \\sqrt{\\alpha_t}}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0}{1 - \\bar{\\alpha}_{t-1}} \\right) + C(\\mathbf{x}_t, \\mathbf{x}_0)) \\right) \\right] \\\\ \\\\ \u0026amp;\\text{which can be written in the form,} \\\\ \u0026amp;= \\text{exp} \\left[ -\\frac{1}{2} \\left( \\frac{(\\mathbf{x}_{t-1} - \\tilde{\\mu}_t(\\mathbf{x}_t, \\mathbf{x}_0))^2}{\\tilde{\\beta}_t} \\right) \\right] \\end{align*}\nSuch that the variance is, \\begin{align*} \\tilde{\\beta_t} \u0026amp;= \\left( \\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}} \\right)^{-1} = \\frac{(1 - \\bar{\\alpha}_{t-1}) \\; \\beta_t }{\\alpha_t - \\alpha_t \\; \\bar{\\alpha}_{t-1} + \\beta_t} = \\frac{(1 - \\bar{\\alpha}_{t-1}) \\beta_t }{1 - \\alpha_t \\; \\bar{\\alpha}_{t-1} } \\\\ \u0026amp;= \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_{t}} \\; \\beta_t \\tag{9} \\end{align*}\nand the mean is, \\begin{align*} \\tilde{\\mu}_t(\\mathbf{x}_t, \\mathbf{x}_0) \u0026amp;= \\left( \\frac{\\mathbf{x}_t \\sqrt{\\alpha_t}}{\\beta_t} + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\mathbf{x}_0}{1 - \\bar{\\alpha}_{t-1}} \\right) \\; \\left( \\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1 - \\bar{\\alpha}_{t-1}} \\right)^{-1} \\\\ \u0026amp;= \\frac{\\sqrt{\\alpha_t} \\; (1 - \\bar{\\alpha}_{t-1}) \\; \\mathbf{x}_t + \\beta_t \\; \\sqrt{\\bar{\\alpha}_{t-1}} \\; \\mathbf{x}_0 }{\\alpha_t - \\alpha_t \\; \\bar{\\alpha}_{t-1} + \\beta_t} \\\\ \u0026amp;= \\frac{\\sqrt{\\alpha_t} \\; (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_{t}} \\mathbf{x}_t + \\frac{\\beta_t \\; \\sqrt{\\bar{\\alpha}_{t-1}}}{1 - \\bar{\\alpha}_{t}} \\; \\mathbf{x}_0 \\end{align*}\nEquation 4 is $\\mathbf{x}_t = \\sqrt{\\bar{\\alpha_t}} \\; \\mathbf{x}_{0} + \\sqrt{1 - \\bar{\\alpha_{t}}} \\; \\epsilon_t$, which can be rewritten as, $\\mathbf{x}_{0} = \\frac{\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha_{t}}} \\; \\epsilon_t}{\\sqrt{\\bar{\\alpha_t}}}$. Substituting, \\begin{align*} \\tilde{\\mu}_t \u0026amp;= \\frac{\\sqrt{\\alpha_t} \\; (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_{t}} \\mathbf{x}_t + \\frac{\\beta_t \\; \\sqrt{\\bar{\\alpha}_{t-1}}}{1 - \\bar{\\alpha}_{t}} \\; \\frac{\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha_{t}}} \\; \\epsilon_t}{\\sqrt{\\bar{\\alpha_t}}} \\\\ \u0026amp;= \\frac{\\sqrt{\\alpha_t} \\; (1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_{t}} \\mathbf{x}_t + \\frac{\\beta_t}{1 - \\bar{\\alpha}_{t}} \\; \\frac{\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha_{t}}} \\; \\epsilon_t}{\\sqrt{\\alpha_t}} \\\\ \u0026amp;= \\frac{ \\left( \\alpha_t \\; (1 - \\bar{\\alpha}_{t-1}) + \\beta_t \\right) \\mathbf{x}_t - \\beta_t \\sqrt{1 - \\bar{\\alpha_{t}}} \\; \\epsilon_t }{\\sqrt{\\alpha_t} \\; (1 - \\bar{\\alpha_{t}})} \\\\ \u0026amp;= \\frac{ \\left( 1 - \\bar{\\alpha_{t}} \\right) \\mathbf{x}_t - \\beta_t \\sqrt{1 - \\bar{\\alpha_{t}}} \\; \\epsilon_t }{\\sqrt{\\alpha_t} \\; (1 - \\bar{\\alpha_{t}})} \\\\ \u0026amp;= \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_{t}}}} \\; \\epsilon_t \\right) \\tag{10} \\end{align*}\nTherefore, the term $L_{t-1}$ estimates the KL-divergence between, \\begin{align*} \\text{Predicted: } \u0026amp; p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t}) = \\mathcal{N} (\\mathbf{x}_{t-1}; \\; \\mu_\\theta(\\mathbf{x}_{t}, t), \\; \\Sigma_\\theta(\\mathbf{x}_{t}, t))\\\\ \\text{Target: } \u0026amp; q (\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0}) = \\mathcal{N} (\\mathbf{x}_{t-1}; \\; \\tilde{\\mu}_t(\\mathbf{x}_t), \\; \\tilde{\\beta_t}) \\\\ \\end{align*}\nRecall that we learn a neural network that predicts the mean and diagonal variance of the Gaussian distribution of the reverse process. Ho et al. decided to keep the predicted variances fixed to time-dependent constants because they found that learning them leads to unstable training and poorer sample quality. They set $\\Sigma_\\theta(\\mathbf{x}_{t}, t) = \\sigma_t^2 \\; \\mathbf{I}$, where $\\sigma_t^2 = \\beta_t$ or $\\tilde{\\beta}_t$ (both gave same results).\nBecause $\\mathbf{x}_t$ is available as input at training time, instead of predicting the mean (equation 10), we make it predict the noise term $\\epsilon_t$ using $\\epsilon_\\theta$. We can then write the predicted mean as, \\begin{align*} \\mu_\\theta(\\mathbf{x}_{t}, t) = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_{t}}}} \\; \\epsilon_\\theta(\\mathbf{x}_{t}, t) \\right) \\tag{11} \\end{align*}\nand the predicted de-noised sample can be written using the reparameterization trick as, \\begin{align*} \\mathbf{x}_{t-1} \u0026amp;= \\mu_\\theta(\\mathbf{x}_{t}, t) + \\sqrt{\\Sigma_\\theta(\\mathbf{x}_{t}, t)} \\; z_t \\\\ \u0026amp;= \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_{t}}}} \\; \\epsilon_\\theta(\\mathbf{x}_{t}, t) \\right) + \\sigma_t z_t \\tag{12} \\end{align*} where $z \\sim \\mathcal{N}(0, 1)$ at each time step.\nThus, the network predicts only the noise term at each time step $t$. Let\u0026rsquo;s simplify the term $L_{t-1}$, given that the variances are equal. KL-divergence b/w two Gaussians is given as:, \\begin{align*} D_{KL}(p||q) = \\frac{1}{2}\\left[\\log\\frac{|\\Sigma_q|}{|\\Sigma_p|} - k + (\\mu_p-\\mu_q)^T\\Sigma_q^{-1}(\\mu_p-\\mu_q) + tr\\left\\{\\Sigma_q^{-1}\\Sigma_p\\right\\}\\right] \\end{align*} where $k$ is number of dimensions.\n\\begin{align*} L_{t-1} \u0026amp;= \\mathbf{D}_{KL} ( q (\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0}) \\; || \\; p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_{t})) \\\\ \u0026amp;= \\frac{1}{2}\\left[- k + \\frac{ (\\tilde{\\mu}_t - \\mu_\\theta(\\mathbf{x}_{t}, t)) (\\tilde{\\mu}_t - \\mu_\\theta(\\mathbf{x}_{t}, t))^T }{\\sigma_t^2} + tr\\left\\{\\mathbf{I} \\right\\}\\right] \\\\ \u0026amp;= \\frac{1}{2}\\left[- k + \\frac{ (\\tilde{\\mu}_t - \\mu_\\theta(\\mathbf{x}_{t}, t)) (\\tilde{\\mu}_t - \\mu_\\theta(\\mathbf{x}_{t}, t))^T }{\\sigma_t^2} + k \\right] \\\\ \u0026amp;= \\frac{1}{2 \\sigma_t^2} || \\tilde{\\mu}_t - \\mu_\\theta(\\mathbf{x}_{t}, t) ||^2 \\\\ \u0026amp;= \\frac{1}{2 \\sigma_t^2} \\Vert \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_{t}}}} \\; \\epsilon_t \\right) - \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_{t}}}} \\; \\epsilon_\\theta(\\mathbf{x}_{t}, t) \\right) \\Vert^2 \\\\ \u0026amp;=\\frac{\\beta_t^2}{2 \\sigma_t^2 \\; \\alpha_t \\; (1 - \\bar{\\alpha_{t}}) } || \\epsilon_t - \\epsilon_\\theta(\\mathbf{x}_{t}, t) ||^2 \\tag{13} \\end{align*} The objective reduces to a weighted L2-loss between the noises and second term of the loss function becomes, $\\mathbf{E}_{q}[L_{t-1}] = \\mathbf{E}_{\\mathbf{x}_{t}, \\epsilon_t} [w_t \\; || \\epsilon_t - \\epsilon_\\theta(\\mathbf{x}_{t}, t) ||^2 ]$\nEmpirically, Ho et al. found that training the diffusion model works better with a simplified objective that ignores the weighting term in $L_{t-1}$. They also got rid of the term $L_0$ by altering the sampling method, such that at the end of sampling ($t$ = 1), we obtain $\\mathbf{x}_0 = \\mu_\\theta(\\mathbf{x}_0, t=1)$.\nThe simplified loss function for DDPM is given as, \\begin{align*} L_{simple} \u0026amp;= \\mathbf{E}_{\\mathbf{x}_{t}, \\epsilon_t} \\left[|| \\epsilon_t - \\epsilon_\\theta(\\mathbf{x}_{t}, t) ||^2 \\right] \\tag{14.1} \\\\ \u0026amp;= \\mathbf{E}_{\\mathbf{x}_{0}, \\epsilon_t} \\left[|| \\epsilon_t - \\epsilon_\\theta(\\sqrt{\\bar{\\alpha_t}} \\; \\mathbf{x}_{0} + \\sqrt{1 - \\bar{\\alpha_{t}}} \\; \\epsilon_t, t) ||^2 \\right] \\tag{14.2} \\end{align*}\n","permalink":"https://yugajmera.github.io/posts/diffusion-models/post/","summary":"Diffusion models are a new class of state-of-the-art generative models that generate diverse high-resolution images. There are already a bunch of different diffusion models that include Open AI’s DALL-E 2 and GLIDE, Google’s Imagen, and Stability AI’s Stable Diffusion. In this blog post, we will dig our way up from the basic principles described in the most prominent one, which is the Denoising Diffusion Probabilistic Models (DDPM) as initialized by Sohl-Dickstein et al in 2015 and then improved by Ho.","title":"Decoding Diffusion Models"},{"content":"Introduced in 2014 by Goodfellow. et al, Generative Adversarial Networks (GANs) revolutionized the field of Generative modeling. They proposed a new framework that generated very realistic synthetic data trained through a minimax two-player game.\nWith GANs, we don\u0026rsquo;t explicitly learn the distribution of the data $p_{\\text{data}}$, but we can still sample from it. Like VAEs, GANs also have two networks: a Generator and a Discriminator that are trained simultaneously.\nA latent variable is sampled from a prior $\\mathbf{z} \\sim p(\\mathbf{z})$ and passed through the Generator to obtain a fake sample $\\mathbf{x} = G(\\mathbf{z})$. Then the discriminator performs an image classification task that takes in all samples and classifies it as real ($1$) or fake ($0$). These are trained together such that while competing with each other, they make each other stronger at the same time.\nTo summarize,\nA discriminator $D$ estimates the probability of a given sample coming from the real dataset. It works as a critic and is optimized to tell the fake samples from the real ones.\nA generator $G$ outputs synthetic samples given a noise variable input $\\mathbf{z}$. It is trained to capture the real data distribution (want $p_G \\approx p_{\\text{data}}$) so that its generative samples can be as real as possible, or in other words, can trick the discriminator to output higher probabilities.\nTraining Objective We want to make sure that the Discriminator\u0026rsquo;s decision over the real data is accurate, i.e. $D(\\mathbf{x}) = 1$. This can be achieved by maximizing the log-likelihood over the data. \\begin{equation*} \\max_{\\color{blue}{D}} \\; \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\log {\\color{blue}{D}}(\\mathbf{x}) \\right] \\tag{1} \\end{equation*}\nThe Discriminator should also assign low probabilities to the fake samples generated by the Generator, i.e. $D(G(\\mathbf{z})) = 0$. In other words, minimize the log-likelihood of the fake data. \\begin{align*} \\min_{\\color{blue}{D}} \\; \\mathbb{E}_{{\\color{green}{\\mathbf{z}}} \\sim p({\\color{green}{z}})} [\\log({\\color{blue}{D}} \\left( {\\color{orange}{G}}({\\color{green}{z}})) \\right)] \\tag{2.1} \\end{align*} which can be re-written as, \\begin{align*} \u0026amp; \\max_{\\color{blue}{D}} \\; \\mathbb{E}_{{\\color{green}{\\mathbf{z}}} \\sim p({\\color{green}{z}})} [\\log(1 -{\\color{blue}{D}} \\left( {\\color{orange}{G}}({\\color{green}{z}})) \\right)] \\tag{2.2} \\end{align*}\nOn the other hand, the Generator wants to fool the Discriminator such that it classifies fake samples as real, i.e. $D(G(\\mathbf{z})) = 1$. This can be obtained by maximizing the log-likelihood over the fake data. \\begin{align*} \\max_{\\color{orange}{G}} \\; \\mathbb{E}_{{\\color{green}{\\mathbf{z}}} \\sim p({\\color{green}{z}})} [\\log({\\color{blue}{D}} \\left( {\\color{orange}{G}}({\\color{green}{z}})) \\right)] \\tag{3.1} \\end{align*} which can be re-written as, \\begin{align*} \\min_{\\color{orange}{G}} \\; \\mathbb{E}_{{\\color{green}{\\mathbf{z}}} \\sim p({\\color{green}{z}})} [\\log(1 -{\\color{blue}{D}} \\left( {\\color{orange}{G}}({\\color{green}{z}})) \\right)] \\tag{3.2} \\end{align*}\nThe full objective is given by,\n\\begin{align*} \\large{\\min_{\\color{orange}{G}} \\, \\max_{\\color{blue}{D}} \\; \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\log {\\color{blue}{D}}(\\mathbf{x}) \\right] \\; + \\; \\mathbb{E}_{{\\color{green}{\\mathbf{z}}} \\sim p({\\color{green}{z}})} [\\log(1 - {\\color{blue}{D}} \\left( {\\color{orange}{G}}({\\color{green}{z}})) \\right)]} \\tag{4} \\end{align*}\nThe objective of training GANs is therefore a minimax game: the generator $G$ is trying hard to trick the discriminator, while the critic model $D$ is trying hard not to be cheated. In this process, both networks force each other to improve their functionalities. And both are trained together with alternating gradient updates (gradient ascent on discriminator and gradient descent on generator).\nHowever, at the start of training, the generator is really bad due to which $D(G(\\mathbf{z}))$ is close to zero, creating a vanishing gradients problem for the Generator (as loss = 0). Therefore, instead of $\\min_{\\color{orange}{G}} \\; \\mathbb{E}_{{\\color{green}{\\mathbf{z}}} \\sim p({\\color{green}{z}})} [\\log(1 -{\\color{blue}{D}} \\left( {\\color{orange}{G}}({\\color{green}{z}})) \\right)]$, we use $\\min_{\\color{orange}{G}} \\; \\mathbb{E}_{{\\color{green}{\\mathbf{z}}} \\sim p({\\color{green}{z}})} - [\\log({\\color{blue}{D}} \\left( {\\color{orange}{G}}({\\color{green}{z}})) \\right)]$ or just $\\max_{\\color{orange}{G}} \\; \\mathbb{E}_{{\\color{green}{\\mathbf{z}}} \\sim p({\\color{green}{z}})} [\\log({\\color{blue}{D}} \\left( {\\color{orange}{G}}({\\color{green}{z}})) \\right)]$ (Equation 3.2) as an objective for the Generator.\nTraining Algorithm\nOptimality Let\u0026rsquo;s write Equation 4 with a change of variables (ignoring the improvement of Generator for now), \\begin{align*} \u0026amp; \\min_{\\color{orange}{G}} \\, \\max_{\\color{blue}{D}} \\; \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\log {\\color{blue}{D}}(\\mathbf{x}) \\right] \\; + \\; \\mathbb{E}_{{\\color{green}{\\mathbf{z}}} \\sim p({\\color{green}{z}})} [\\log(1 - {\\color{blue}{D}} \\left( {\\color{orange}{G}}({\\color{green}{z}})) \\right)] \\\\ \u0026amp;= \\min_{\\color{orange}{G}} \\, \\max_{\\color{blue}{D}} \\; \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\log {\\color{blue}{D}}(\\mathbf{x}) \\right] \\; + \\; \\mathbb{E}_{\\mathbf{x} \\sim p_{\\color{orange}{G}} } [\\log(1 - {\\color{blue}{D}} (\\mathbf{x}) )] \\\\ \u0026amp;= \\min_{\\color{orange}{G}} \\, \\int_{\\mathbf{x}} \\max_{\\color{blue}{D}} \\left( p_{\\text{data}}(\\mathbf{x}) \\, \\log {\\color{blue}{D}}(\\mathbf{x}) \\; + \\; p_{\\color{orange}{G}}(\\mathbf{x}) \\, \\log(1 - {\\color{blue}{D}} (\\mathbf{x}) ) \\right)\\\\ \\\\ \u0026amp;\\text{For the max function, we take the derivative wrt ${\\color{blue}{D}}$ and assign it to zero.}\\\\ \u0026amp;\\frac{p_{\\text{data}}(\\mathbf{x})}{{\\color{blue}{D}}(\\mathbf{x})} - \\frac{p_{\\color{orange}{G}}(\\mathbf{x})}{(1 - {\\color{blue}{D}} (\\mathbf{x}))} = 0 \\; \\Rightarrow {{\\color{blue}{D}}_{\\max}(\\mathbf{x})} = \\frac{p_{\\text{data}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_{\\color{orange}{G}}(\\mathbf{x})} \\text{ [Optimal Discriminator]} \\\\ \\\\ \u0026amp;= \\min_{\\color{orange}{G}} \\, \\int_{\\mathbf{x}} \\left( p_{\\text{data}}(\\mathbf{x}) \\, \\log \\frac{p_{\\text{data}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_{\\color{orange}{G}}(\\mathbf{x})} \\; + \\; p_{\\color{orange}{G}}(\\mathbf{x}) \\, \\log \\frac{p_{\\color{orange}{G}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_{\\color{orange}{G}}(\\mathbf{x})} \\right)\\\\ \u0026amp;= \\min_{\\color{orange}{G}} \\, \\left( \\mathbf{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\, \\left[ \\log \\frac{p_{\\text{data}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_{\\color{orange}{G}}(\\mathbf{x})} \\right] \\; + \\; \\mathbf{E}_{\\mathbf{x} \\sim p_{\\color{orange}{G}}} \\, \\left[ \\log \\frac{p_{\\color{orange}{G}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_{\\color{orange}{G}}(\\mathbf{x})} \\right] \\right)\\\\ \\\\ \u0026amp;\\text{Multiply and divide by 2} \\\\ \u0026amp;= \\min_{\\color{orange}{G}} \\, \\left( \\mathbf{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\, \\left[ \\log \\frac{2 \\, p_{\\text{data}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_{\\color{orange}{G}}(\\mathbf{x})} \\right] \\; + \\; \\mathbf{E}_{\\mathbf{x} \\sim p_{\\color{orange}{G}}} \\, \\left[ \\log \\frac{2 \\, p_{\\color{orange}{G}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_{\\color{orange}{G}}(\\mathbf{x})} \\right] - \\log 4 \\right) \\\\ \\\\ \u0026amp;\\text{From the definition of KL-Divergence: } \\mathbf{D}_{KL}(p || q) = \\mathbf{E}_{x \\sim p} \\left[ \\log \\frac{p(x)}{q(x)} \\right]\\\\ \u0026amp;= \\min_{\\color{orange}{G}} \\, \\left( \\mathbf{D}_{KL} \\left[ p_{\\text{data}} \\; || \\; \\frac{p_{\\text{data}} + p_{\\color{orange}{G}}}{2} \\right] \\, + \\, \\mathbf{D}_{KL} \\left[ p_{\\color{orange}{G}} \\; || \\; \\frac{p_{\\text{data}} + p_{\\color{orange}{G}}}{2} \\right] - \\log 4 \\right) \\\\ \\\\ \u0026amp;\\text{From Jensen-Shannon divergence: } \\mathbf{D}_{JS}(p || q) = \\frac{1}{2} \\mathbf{D}_{KL}(p || \\frac{p + q}{2}) + \\frac{1}{2} \\mathbf{D}_{KL}(q || \\frac{p + q}{2}) \\\\ \u0026amp;= \\min_{\\color{orange}{G}} \\, \\left( 2 \\, \\mathbf{D}_{JS} (p_{\\text{data}} \\, || \\, p_{\\color{orange}{G}}) - \\log 4 \\right)\\\\ \\end{align*}\nJS divergence is always non-negative and is zero only when the two distributions are equal. Hence, training a GAN with this objective, minimizes the JS divergence to zero, obtaining a global minimum when $p_{\\text{data}} = p_{G}$.\nThe global minimum of the minimax game happens when, \\begin{align*} \\text{Optimal Generator: } \u0026amp; p_{\\text{data}} = p_{G} \\\\ \\text{Optimal Discriminator: } \u0026amp; D_{G}^*(\\mathbf{x}) = \\frac{p_{\\text{data}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_G(\\mathbf{x})} = \\frac{1}{2} \\end{align*}\nSome prominent papers,\nUnsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (DCGAN): Architectures of Generator and Discriminator created with CNNs.\nWasserstein GAN (WGAN): Improvement over traditional GAN training.\nUnpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (CycleGAN): For image-to-image translation\n","permalink":"https://yugajmera.github.io/posts/gan/post/","summary":"Introduced in 2014 by Goodfellow. et al, Generative Adversarial Networks (GANs) revolutionized the field of Generative modeling. They proposed a new framework that generated very realistic synthetic data trained through a minimax two-player game.\nWith GANs, we don\u0026rsquo;t explicitly learn the distribution of the data $p_{\\text{data}}$, but we can still sample from it. Like VAEs, GANs also have two networks: a Generator and a Discriminator that are trained simultaneously.\nA latent variable is sampled from a prior $\\mathbf{z} \\sim p(\\mathbf{z})$ and passed through the Generator to obtain a fake sample $\\mathbf{x} = G(\\mathbf{z})$.","title":"Generative Adversarial Networks: A Two-player game"},{"content":"Till now, I have talked about supervised learning where our model is trained to learn a mapping function of data $\\mathbf{x}$ to predict labels $\\mathbf{y}$, for example, the tasks of classification, object detection, segmentation, image captioning, etc. However, supervised learning requires large datasets that are created with human annotations to train the models.\nThe other side of machine learning is called unsupervised learning where we just have data $\\mathbf{x}$, and the goal is to learn some hidden underlying structure of the data using a model. Through this approach, we aim to learn the distribution of the data $p(\\mathbf{x})$ and then sample from it to generate new data, in our case images.\nAutoencoders An autoencoder is a bottleneck structure that extracts features $\\mathbf{z}$ from the input images $\\mathbf{x}$ and uses them to reconstruct the original data $\\mathbf{x}\u0026rsquo;$, learning an identity mapping.\nEncoder: network that compresses the input into a latent-space representation (of a lower dimension) Decoder: network that reconstructs the output from this representation Since we want the autoencoder to learn to reconstruct the input data, the loss objective is a simple L2-loss between the input and the reconstructed output $\\mathbf{x}\u0026rsquo; = f_\\theta(g_\\phi(\\mathbf{x}))$. \\begin{equation*} L_\\text{AE}(\\theta, \\phi) = || \\mathbf{x} - f_\\theta(g_\\phi(\\mathbf{x})) ||_2^2 \\tag{1} \\end{equation*}\nThis approach is used for learning a lower-dimensional feature representation from the training data that captures meaningful factors of variation in it. After training, we get rid of the decoder and use the encoder to obtain features that contain useful information about the input that can be used for downstream tasks.\nAutoencoders are generally used for dimensionality reduction (or data compression) and to initialize a supervised model. Since it is not probabilistic, we cannot sample new data from the learned model, and new images cannot be generated$^*$.\nVariantional Autoencoders (VAEs) VAEs, introduced in Auto-Encoding Variational Bayes paper are a probabilistic version of autoencoder that lets you sample from the model to generate new data. Instead of mapping the input into a fixed vector, we want to map it into a distribution, denoted as $p_\\theta$.\nPrior $p_\\theta (\\mathbf{z})$: Prob distribution over the latent variables (fixed and often assumed to be a simple Gaussian) Posterior $p_\\theta (\\mathbf{x}|\\mathbf{z})$: Prob distribution over images conditioned on the latent variables Likelihood $p_\\theta (\\mathbf{z}|\\mathbf{x})$: Prob distribution over latent variables given input data Assuming that we know the real parameter $\\theta^*$ for this distribution, we can generate new samples for a real data point $\\mathbf{x}^{(i)}$ as,\nFirst, sample a $\\mathbf{z}^{(i)}$ from a true prior distribution $p_{\\theta^*}(\\mathbf{z})$. Feed it into the decoder model to obtain the posterior $p_{\\theta^*}(\\mathbf{x} \\vert \\mathbf{z} = \\mathbf{z}^{(i)})$. Then sample a new data point $\\mathbf{x\u0026rsquo;}^{(i)}$ from this true posterior. As you would be wondering a neural network cannot output a probability distribution so, instead the decoder predicts the mean $\\mu_{\\mathbf{x} \\vert \\mathbf{z}}$ and diagonal variance $\\Sigma_{\\mathbf{x} \\vert \\mathbf{z}}$ of the posterior, assuming it to be also Gaussian such that $p_\\theta (\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_{\\mathbf{x} \\vert \\mathbf{z}}, \\Sigma_{\\mathbf{x} \\vert \\mathbf{z}})$.\nThere is an inherent assumption here that the pixels of the generated image are conditionally independent given the latent variable and this is the reason why VAEs tend to generate blurry images.\nIn order to estimate the true parameter $\\theta^*$, we learn the model parameters that maximize the probability of generating real data samples (maximum likelihood estimation), \\begin{align*} \\theta^{*} \u0026amp;= \\arg\\max_\\theta \\prod_{i=1}^n p_\\theta(\\mathbf{x}^{(i)}) \\\\ \u0026amp;= \\arg\\max_\\theta \\sum_{i=1}^n \\log p_\\theta(\\mathbf{x}^{(i)}) \\tag{2} \\end{align*}\nBefore moving further, let\u0026rsquo;s see how we can obtain the distribution of the data $p_\\theta(\\mathbf{x})$.\nMarginalize out the latent variable to simplify as, \\begin{align*} p_\\theta(\\mathbf{x}) \u0026amp;= \\int p_\\theta(\\mathbf{x}, \\mathbf{z}) d\\mathbf{z} = \\int \\underbrace{p_\\theta(\\mathbf{x}\\vert\\mathbf{z})}_{\\text{posterior}} \\; \\underbrace{p_\\theta(\\mathbf{z})}_{\\text{prior}} d\\mathbf{z} \\\\ \\end{align*}\nThe terms in the integral - prior (fixed) and posterior (estimated by the decoder) are straightforward to compute. However, it is impossible to integrate over all $\\mathbf{z}$ as it is expensive to check all the possible values of $\\mathbf{z}$ and sum them up.\nAnother idea is to use Baye\u0026rsquo;s rule, \\begin{align*} p_\\theta(\\mathbf{x}) = \\frac{p_\\theta(\\mathbf{x}\\vert\\mathbf{z}) \\; p_\\theta(\\mathbf{z})}{p_\\theta(\\mathbf{z}\\vert\\mathbf{x})} \\approx \\frac{p_\\theta(\\mathbf{x}\\vert\\mathbf{z}) \\; p_\\theta(\\mathbf{z})}{q_\\phi(\\mathbf{z}\\vert\\mathbf{x})} \\end{align*}\nThe numerator has the same terms as above. The denominator has the likelihood term that can be approximated by another neural network (encoder) as $q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\approx p_\\theta (\\mathbf{z}|\\mathbf{x})$.\nAgain, we assume that is follows a Gaussian distribution as $q_\\phi (\\mathbf{z}\\vert\\mathbf{x}) = \\mathcal{N}(\\mu_{\\mathbf{z} \\vert \\mathbf{x}}, \\Sigma_{\\mathbf{z} \\vert \\mathbf{x}})$, and the encoder predicts the mean $\\mu_{\\mathbf{z} \\vert \\mathbf{x}}$ and diagonal variance $\\Sigma_{\\mathbf{z} \\vert \\mathbf{x}}$.\nThe encoder, therefore, outputs a distribution of latent variables $\\mathbf{z}$ given the input data $\\mathbf{x}$. The training process looks like,\nInput a real data point $\\mathbf{x}^{(i)}$ to the encoder to obtain the likelihood $q_\\phi (\\mathbf{z}\\vert\\mathbf{x} = \\mathbf{x}^{(i)})$. Sample a latent variable from this likelihood $\\mathbf{z}^{(i)} \\sim \\mathcal{N}(\\mu_{\\mathbf{z} \\vert \\mathbf{x}}, \\Sigma_{\\mathbf{z} \\vert \\mathbf{x}})$ Feed it into the decoder to obtain the posterior $p_{\\theta^*}(\\mathbf{x} \\vert \\mathbf{z} = \\mathbf{z}^{(i)})$. Then sample a new data point from this posterior $\\mathbf{x\u0026rsquo;}^{(i)} \\sim \\mathcal{N}(\\mu_{\\mathbf{x} \\vert \\mathbf{z}}, \\Sigma_{\\mathbf{x} \\vert \\mathbf{z}})$ Sampling is a stochastic process and therefore we cannot backpropagate the gradient. To make it trainable, the reparameterization trick is introduced such that, \\begin{align*} \\mathbf{z} \u0026amp;\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)}) = \\mathcal{N}(\\mathbf{z}; \\boldsymbol{\\mu}^{(i)}, \\boldsymbol{\\sigma}^{2(i)}\\boldsymbol{I})\\\\ \\mathbf{z} \u0026amp;= \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon} \\text{, where } \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\boldsymbol{I}) \\end{align*} where $\\odot$ refers to an element-wise product.\nDeriving ELBO Loss In order to obtain the loss objective of a VAE, we further simpify Equation 2 by first multipying $1 = \\int q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)}) \\; dz$ with $\\log p_\\theta(\\mathbf{x}^{(i)})$ and then solving, \\begin{align*} \\log p_\\theta(\\mathbf{x}^{(i)}) \u0026amp;= \\log p_\\theta(\\mathbf{x}^{(i)}) \\int q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)}) \\; dz = \\int q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)}) \\log p_\\theta(\\mathbf{x}^{(i)}) \\; dz \\\\ \u0026amp;=\\mathbf{E}_{\\mathbf{z}\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)})} [\\log p_\\theta(\\mathbf{x}^{(i)})] \\\\ \\\\ \u0026amp;\\text{applying Baye\u0026rsquo;s rule,}\\\\ \u0026amp;= \\mathbf{E}_{\\mathbf{z}\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}^{(i)}\\vert\\mathbf{z}) \\; p_\\theta(\\mathbf{z})}{p_\\theta(\\mathbf{z}\\vert\\mathbf{x}^{(i)})} \\right] \\\\ \\\\ \u0026amp;\\text{multipy \u0026amp; divide by same term}\\\\ \u0026amp;= \\mathbf{E}_{\\mathbf{z}\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x}^{(i)}\\vert\\mathbf{z}) \\; p_\\theta(\\mathbf{z})}{p_\\theta(\\mathbf{z}\\vert\\mathbf{x}^{(i)})} \\frac{q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)})}{q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)})} \\right] \\\\ \\\\ \u0026amp;\\text{rearranging}\\\\ \u0026amp;= \\mathbf{E}_{\\mathbf{z}\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)})} \\left[ \\log p_\\theta(\\mathbf{x}^{(i)}\\vert\\mathbf{z}) \\right] + \\mathbf{E}_{\\mathbf{z}\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)})} \\left[ \\frac{p_\\theta(\\mathbf{z})}{q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)})} \\right] + \\mathbf{E}_{\\mathbf{z}\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)})} \\left[ \\frac{q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)})}{p_\\theta(\\mathbf{z}\\vert\\mathbf{x}^{(i)})} \\right] \\\\ \u0026amp;= \\mathbf{E}_{\\mathbf{z}\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)})} \\left[ \\log p_\\theta(\\mathbf{x}^{(i)}\\vert\\mathbf{z}) \\right] - \\mathbf{D}_{KL}\\left( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)}) \\; || \\; p_\\theta(\\mathbf{z}) \\right) + \\underbrace{\\mathbf{D}_{KL} \\left( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)}) \\; || \\; p_\\theta(\\mathbf{z}\\vert\\mathbf{x}^{(i)}) \\right)}_{\\text{cannot be computed but KL-Divergence} \\ge 0 \\text{ always}} \\\\ \u0026amp;\\ge \\underbrace{\\mathbf{E}_{\\mathbf{z}\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)})} \\left[ \\log p_\\theta(\\mathbf{x}^{(i)}\\vert\\mathbf{z}) \\right]}_{\\text{reconstruction term}} - \\underbrace{\\mathbf{D}_{KL} \\left( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)}) \\; || \\; p_\\theta(\\mathbf{z}) \\right)}_{\\text{prior matching term}} \\end{align*}\nThe first term is the data reconstruction term that measures the reconstruction likelihood of the decoder from our variational distribution. It ensures that the learned distribution is modeling effective latents that the original data can be regenerated from.\nThe second term measures how similar the learned variational distribution is to a prior belief held over latent variables. Since both the distributions are Gaussians, KL-divergence can be computer in closed form.\nThis gives us a lower bound on the log-likelihood of the data and we jointly train both the encoder and decoder to maximize it. \\begin{align*} \\theta^{*} \u0026amp;= \\arg\\max_\\theta \\sum_{i=1}^n \\log p_\\theta(\\mathbf{x}^{(i)}) \\\\ \u0026amp;\\ge \\arg\\max_{\\theta, \\phi} \\sum_{i=1}^n \\mathbf{E}_{\\mathbf{z}\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)})} \\left[ \\log p_\\theta(\\mathbf{x}^{(i)}\\vert\\mathbf{z}) \\right]- \\mathbf{D}_{KL}(q_\\phi(\\mathbf{z}\\vert\\mathbf{x}^{(i)}) \\; || \\; p_\\theta(\\mathbf{z})) \\end{align*} It is known as the variational lower bound, or evidence lower bound (ELBO).\nThe loss function (that we minimize) is given as, \\begin{equation*} L_{\\text{VAE}}(\\theta, \\phi) = - \\mathbf{E}_{\\mathbf{z}\\sim q_\\phi(\\mathbf{z}\\vert\\mathbf{x})} \\left[ \\log p_\\theta(\\mathbf{x}\\vert\\mathbf{z}) \\right] + \\mathbf{D}_{KL}\\left( q_\\phi(\\mathbf{z}\\vert\\mathbf{x}) \\; || \\; p_\\theta(\\mathbf{z}) \\right) \\tag{3} \\end{equation*}\n$*$ - I have tried generating new images using an autoencoder by first obtaining the latent representation of two images and then performing the reconstruction on the linear interpolation between them. Below are the results on the CIFAR10 dataset, with the first two columns showing original images and the last one showing the new image.\n","permalink":"https://yugajmera.github.io/posts/variational-autoencoder/post/","summary":"Till now, I have talked about supervised learning where our model is trained to learn a mapping function of data $\\mathbf{x}$ to predict labels $\\mathbf{y}$, for example, the tasks of classification, object detection, segmentation, image captioning, etc. However, supervised learning requires large datasets that are created with human annotations to train the models.\nThe other side of machine learning is called unsupervised learning where we just have data $\\mathbf{x}$, and the goal is to learn some hidden underlying structure of the data using a model.","title":"Generative Modeling with Variational Autoencoders"},{"content":"In 2015, Batch Normalization was discovered, which heralded the progress of architectures as it was now possible to train deep networks without using any tricks - allowing us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, eliminating the need for Dropout.\nAnother major problem with deep networks was of vanishing/exploding gradients, but it was now being handled by Kaiming Initialization. With the stage set in place, experiments began on training deeper models.\nIt was noted that when deeper networks are able to start converging, a degradation problem occurs: as we increase the depth, the accuracy gets saturated (which might be unsurprising) and then degrades rapidly. Unexpectedly, such degradation is not caused by overfitting (which was the initial guess), because adding more layers to a suitably deep model lead to higher training error. It turns out, deeper networks are underfitting!\nBy intuition, it is expected that deeper models should do at least as good as shallow models, as they can emulate the shallower model by copying layers from it and setting the other extra layers to identity. Since the deeper models are underfitting, it shows that there is a problem in optimization and somehow these deeper networks are not able to learn the identity functions.\nA solution to this is to change the design of the network so that it is easy for it to learn the identity functions on unused layers. This forms the idea of a residual block shown below.\nIn order to learn the identity function, the weights of the two convolutions are set to zero. It is easier to push the residual to zero during learning than to fit an identity mapping.\nThese connections are often called skip connections and they neither add extra parameters nor computational complexity. Residual blocks can still be trained end-to-end by SGD with backpropagation (there is no need for any kind of modification). A residual network (or ResNet) is formed by stacking many such residual blocks.\nArchitecture: Similar to VGG, ResNet is divided into 4 stages with a different number of residual blocks. The design principles are fixed as,\nEach residual block has two $3 \\times 3$ convolution layers. After each stage, the number of channels is doubled (starting with 64 and going to 512) After each stage, the spatial dimension is reduced by half using a stride of 2 in the first conv of the next stage. Whenever the channel dimension is increased, the spatial dimension is decreased proportionally (was done with a max-pool layer in VGG), so as to preserve the time complexity per layer.\nWhen the stage changes, the dimension of the input to skip connection becomes different from the output. In such a case, a $1 \\times 1$ conv (with a stride of 2) is added to the input to match the dimensions. The output then becomes: \\begin{equation*} \\text{H(x) = F(x) + W x} \\end{equation*} where $W$ is a learnable parameter called projection shortcut. These are used only for changing dimensions, other shortcuts are identity.\nEach convolution is followed by a BatchNorm layer and a ReLU non-activation.\nLike GoogleNet, ResNet uses an aggressive stem network to downsample the image input 4x before applying the residual blocks. It also does not have fully-connected layers, instead uses global average pooling followed by a single linear layer (with softmax) to generate class scores.\nThe authors presented two variants, ResNet-18 and ResNet-34.\nAs the 34-layer network performed better than the 18-layer one, it was clear that adding even more layers would mean better performance. However, adding more layers would also bring more computation costs.\nTaking inspiration from GoogleNet, the authors added bottleneck layers in each residual block. The bottleneck block accepts 4 times the channel dimension as a basic block and works with lower computation costs even with an extra added layer per block.\nReplacing all the basic blocks in ResNet-34 with bottleneck blocks would give the ResNet-50 architecture. ResNet-50 is an excellent baseline architecture for many tasks today! Similarly using a different number of bottleneck blocks in different stages, the authors present ResNet-101 and ResNet-152 variants.\nTraining: The model was trained on the Cross-entropy loss function using stochastic gradient descent with a batch size of 256 examples, a momentum of 0.9, and a weight decay of 0.0001. The learning rate starts from 0.1 and is divided by 10 when the error plateaus.\nThe weights were initialized using Kaiming initialization. No dropout was used.\nResNet was the first architecture to have crossed the human error rate and won 1st place on ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in 2015.\nImportant design choices of the paper are,\n4 Stages, Bottleneck residual blocks (conv layers with skip connections) Number of channels stage-wise: 64, 128, 256, 512 Batch Normalization used Activation Function: ReLU Data pre-processing: subtract per-pixel mean Weight Initialization: Kaiming Regularization: L2 Weight decay ($\\lambda$ = 1e-4) Learning rate: 0.1 and reduced (divided by 10) when error plateaus Optimization Method: SGD + Momentum (m=0.9) with batch size of 256 Loss function: Cross-Entropy loss Link to the paper: Deep Residual Learning for Image Recognition\n","permalink":"https://yugajmera.github.io/posts/resnet/post/","summary":"In 2015, Batch Normalization was discovered, which heralded the progress of architectures as it was now possible to train deep networks without using any tricks - allowing us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, eliminating the need for Dropout.\nAnother major problem with deep networks was of vanishing/exploding gradients, but it was now being handled by Kaiming Initialization. With the stage set in place, experiments began on training deeper models.","title":"ResNet: The Revolution of Depth"},{"content":" The 2014 winner of the ImageNet challenge was the InceptionNet or GoogLeNet architecture from Google. The most straightforward way of improving the performance of deep neural networks is by increasing their size: depth (the number of layers) and width (the number of units at each layer).\nBut with such a big network comes a large number of parameters (which makes it prone to overfitting) and increased use of computation resources. In order to cater to these problems, the design choices of GoogLeNet were focused on efficiency.\nArchitecture: In order to very aggressively downsample the spatial dimensions of the input image at the beginning, GoogLeNet uses a Stem Network. This looks very similar to AlexNet or ZFNet (and local response normalization is used in this network).\nIt then includes an Inception module, a local structure with parallel branches that was repeated many times throughout the entire network (just like stages in VGGNet). In order to eliminate the kernel size as a hyperparameter, the input is passed through all the kernel sizes: $1 \\times 1$, $3 \\times 3$, $5 \\times 5$ and max-pooling $3 \\times 3$ with a stride of 1 and the outputs are concatenated together. ReLU is used as an activation function here.\nThis enables the network to learn various spatial patterns at different scales as a result of the varying conv filter sizes. The design follows the practical intuition that visual information should be processed at various scales and then aggregated so that the next stage can abstract features from the different scales simultaneously.\nNaive Inception Module\nOne big problem with such modules is that performing convolutions can be prohibitively expensive when the input has many channels. From the above image, let\u0026rsquo;s take the example of 32 $5 \\times 5$ conv filters which are applied over the $28 \\times 28 \\times 192$ input.\nThe number of multiplications: \\begin{equation*} 5 \\times 5 \\times 32 * 28 \\times 28 \\times 192 = 120422400 \\approx 120 \\text{M} \\end{equation*}\nThis problem becomes even more pronounced once pooling units are added to the mix: the number of output filters equals the number of filters in the previous stage (as pooling layers do not reduce the number of channels). The merging of the output of the pooling layer with outputs of the conv layers would lead to an inevitable increase in the number of outputs from stage to stage, leading to a computational blow-up within a few stages.\nTo solve this problem, a $1 \\times 1$ bottleneck layer is introduced to reduce channel dimensions before performing expensive spatial convolutions. Not only is it used to compute reductions, but it also introduces extra non-linearity (as they are followed by ReLU).\nInception module with dimensionality reduction\nThe bottleneck layer is implemented before the convolutions so that the computation takes place on a reduced number of channels, giving rise to reduced computation. It also included after the maxpool layer to control the number of output channels.\nThe number of multiplications: \\begin{align*} \u0026amp; (1 \\times 1 \\times 16 * 28 \\times 28 \\times 192) + (5 \\times 5 \\times 32 * 28 \\times 28 \\times 16) \\\\ \u0026amp;= 2408448 + 10035200 \\\\ \u0026amp;= 12443648 \\approx 12M. \\end{align*}\nThe computation cost of a component within the Inception Module(12M) is ten times smaller than a Naive Inception Module(120M). Such a technique allows for increasing the number of layers significantly without an uncontrolled blow-up in computational complexity at later stages.\nThe previous models used to flatten the output from the last conv layer, and pass it through the fully-connected layers to collapse the spatial dimensions. However, FC layers have a lot of parameters that increase memory usage. GoogLeNet instead uses global average pooling with a kernel size of $7 \\times 7$, followed by one single linear layer (with softmax) to generate class scores.\nRemoving FC layers means we cannot add dropout layers anymore. Given the relatively large depth of the network, the ability to propagate gradients back through all the layers became a problem. The authors had to add auxiliary classifiers at two intermediate stages to combat the vanishing gradient problem while providing regularization.\nAuxillary Classifier\nDuring training, their loss gets added to the total loss of the network with a discount weight (the losses of the auxiliary classifiers were weighted by 0.3). At inference time, these auxiliary networks are discarded.\nThe full 22-layer GoogLeNet architecture is shown below.\nTraining: The model was trained on the Cross-entropy loss function using stochastic gradient descent with a momentum of 0.9. A fixed learning rate schedule was used, where it was decreased by 4% every 8 epochs. (No other information about training is given in the paper)\nLink to the paper: Going Deeper with Convolutions\nTrivia: The name Inception probably sounds familiar, especially if you are a fan of the actor Leonardo DiCaprio or movie director, Christopher Nolan. Inception is a movie released in 2010, and the concepts of the embedded dream state (dreams within dreams) were the central premise of the film. This idea turned into a popular internet meme, which the authors cite as an inspiration for the name chosen for the architecture (as it is a network within a network).\nThe team also chose the name \u0026ldquo;GoogLeNet\u0026rdquo; as their team name in the ILSVRC14 competition, paying homage to Yann LeCuns pioneering LeNet 5 network (the earliest network that introduced convolutions)\n","permalink":"https://yugajmera.github.io/posts/inceptionnet/post/","summary":"The 2014 winner of the ImageNet challenge was the InceptionNet or GoogLeNet architecture from Google. The most straightforward way of improving the performance of deep neural networks is by increasing their size: depth (the number of layers) and width (the number of units at each layer).\nBut with such a big network comes a large number of parameters (which makes it prone to overfitting) and increased use of computation resources.","title":"InceptionNet: Google's comeback for ImageNet Challenge"},{"content":" With the advent of AlexNet, all the submissions to the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) switched over to using convolutional neural networks. In 2013, the winner of this challenge was ZFNet, a modified version of AlexNet which gave better accuracy. It was also an 8-layer network that tweaked some of the layer configurations of AlexNet by trial and error.\nZFNet used 7 $\\times$ 7 sized filters in the first layer with a stride of 2 instead of 11 $\\times$ 11 filters with a stride of 4. The intuition behind this is that we were losing a lot of pixel information (aggressively downsampling the input), which can be retained by having smaller filter sizes and small strides. The padding is removed for the first two conv layers (to match the subsequent conv layers shapes of AlexNet). With just these two changes, they are able to achieve a reasonably large increase in performance over AlexNet.\nAlexNet or ZFNet were designed in somewhat an ad-hoc manner with some arbitrary number of convolution layers, and pooling layers and the configurations of each layer were set by trial and error. This makes it very hard to scale them.\nIn 2014, the 2nd place winner of this challenge was VGGNet, which was one of the first architectures that had a principled design throughout to guide the overall configuration of the network. With such a method, they were able to create deeper networks (16-19 layers!) and achieve significant improvement over the prior configurations.\nArchitecture: VGGNet has very clean and simple design principles where the configuration of each layer is fixed as,\nAll convolution layers are fixed to have a kernel size of 3 $\\times$ 3 with a stride 1 and pad 1. All max-pooling layers have a size of 2 $\\times$ 2 with stride 2. After each pooling layer, double the number of channels. While AlexNet had 5 convolutional layers, VGGNet has 5 stages,\nStage 1: conv-conv-pool\nStage 2: conv-conv-pool\nStage 3: conv-conv-pool\nStage 4: conv-conv-conv-[conv]-pool\nStage 5: conv-conv-conv-[conv]-pool\nThe authors presented two variants, VGG-16 and VGG-19 with 16 and 19 layers respectively. VGG-19 has 4 convolutions in stages 4 and 5.\nInspirations from AlexNet: VGGNet uses ReLU non-linearities; the stack of convolution layers is followed by 3 fully-connected layers, with a softmax layer at the end; Dropout layers are added after the first two fully-connected layers (dropout ratio set to 0.5).\nA 3 $\\times$ 3 convolutions is used as it is the smallest size to capture the notion of left/right, up/down, and center. A stack of two such 3 $\\times$ 3 conv layers have an effective receptive field of 5 $\\times$ 5, and three such layers that a 7 $\\times$ 7 effective receptive field. So what have we gained by using, for instance, a stack of three 3 $\\times$ 3 conv layers instead of a single 7×7 layer?\nThree non-linearities instead of one - makes the decision function more discriminative Decrease in the number of parameters - For input and output C channels, three 3 $\\times$ 3 would have $3(3 * 3 * C^2) = 27C^2$ params, whereas a 7 $\\times$ 7 layers woudl have $7 * 7 * C^2 = 49C^2$ params. A padding of 1 is used so that the spatial dimension is preserved after convolution(\u0026lsquo;same\u0026rsquo; padding).\nNo kind of normalization is used in the network as it does not improve the performance on the dataset but instead leads to increased memory consumption and computation time.\nTraining: The training hyperparameters and choices follows AlexNet. The model was trained on the Cross-entropy loss function using mini-batch gradient descent with a batch size of 256 and momentum of 0.9. The training was regularized with an L2 weight decay of 0.0005.\nThe learning rate was initialized at 0.01 and then decreased by a factor of 10 when the validation set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning was stopped after 74 epochs.\nIn spite of a larger number of parameters and the greater depth as compared to AlexNet, VGGNet required fewer epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv filter sizes, (b) better initialization of weights and biases - they used Xavier initialization.\nOverall VGGNet highlighted the importance of depth in achieving better performance on the image classification task. Important design choices of the paper are,\n5 Stages: Each conv 3 $\\times$ 3, s=1, p=1; Each max-pool 2 $\\times$ 2, s = 2 Number of channels: 64, 128, 256, 512 No Normalization used Activation Function: ReLU Data pre-processing: subtract per-channel mean (mean RGB value from each pixel) Weight Initialization: Xavier Regularization: L2 Weight decay ($\\lambda$ = 5e-4), Dropout (p=0.5) Learning rate: 0.01 and reduced (divided by 10) when val accuracy plateaus Optimization Method: SGD + Momentum (m=0.9) with batch size of 256 Loss function: Cross-Entropy loss Link to the Papers: Visualizing and Understanding Convolutional Networks , Very Deep Convolutional Networks for Large-Scale Image Recognition\nTrivia: VGGNet is named after the Visual Geometry Group at Oxford where a grad student (Karen Simonyan) and a faculty member (Andrew Zisserman) came up with this idea and managed to come very close to GoogLeNet\u0026rsquo;s performance which was built by the whole team at Google with access to lots of resources!\n","permalink":"https://yugajmera.github.io/posts/vggnet/post/","summary":"With the advent of AlexNet, all the submissions to the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) switched over to using convolutional neural networks. In 2013, the winner of this challenge was ZFNet, a modified version of AlexNet which gave better accuracy. It was also an 8-layer network that tweaked some of the layer configurations of AlexNet by trial and error.\nZFNet used 7 $\\times$ 7 sized filters in the first layer with a stride of 2 instead of 11 $\\times$ 11 filters with a stride of 4.","title":"VGGNet: Very Deep Convolutional Networks"},{"content":"Do you wonder about how to come up with different design choices (architecture, optimization method, data manipulation, loss function, etc.) for the deep learning model so that it gives the best performance? Let\u0026rsquo;s look at the different CNN architectures that have performed well in the past on image classification tasks.\nThe ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was the huge benchmark for image classification because it held a yearly challenge from 2010 to 2017 where teams around the world would compete with their best-performing classification models. This competition uses a subset of ImageNet\u0026rsquo;s images containing 1.2 million high-resolution images with 1000 different classes and challenges researchers to achieve the lowest top-1 and top-5 error rates (top-5 error rate would be the percent of test images where the correct label is not one of the model\u0026rsquo;s five most likely labels).\nFor the first two years (2010 and 2011), the winning systems were not neural network-based at all. They used multiple layers of hand-designed feature extractors with linear neural networks on top.\nTraining CNNs were prohibitively expensive to apply on large scale to high-resolution images during that time. It was in 2012 with the introduction of GPUs paired with a highly-optimized implementation of 2D convolutions and the ImageNet dataset containing enough labeled examples, it became easy to train such models without severe overfitting.\nAlexNet was the largest convolutional neural network trained at that time and achieved the best results ever reported on the ImageNet dataset, crushing all the other competitors by a big margin. This made CNNs a mainstream topic in the field of computer vision and AlexNet can undoubtedly be called one of the most influential research works in this field (the number of citations this paper has is just crazy!).\nData Manipulation: Since the images of the dataset are of variable size, they are first downsampled to a fixed size of 256 $\\times$ 256. This is done by rescaling the shortest side of a rectangular image to a length of 256 and then cropping out the central 256 $\\times$ 256 patches. The mean image is then subtracted from the training set.\nTwo forms of data augmentation techniques are applied to the training data to reduce overfitting, which is performed on the fly on the CPU while the GPUs train the previous batch of data, so essentially is computationally free.\nThe first form consists of extracting random 224 $\\times$ 224 patches (and their horizontal reflections) from the original image, increasing the size of the training set by a factor of 2048. At test time, the network makes a prediction by extracting five 224 $\\times$ 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (ten patches in total), and averaging the predictions on them.\nThe second form is called PCA Color Augmentation (also called Fancy PCA) which consists of altering the intensities of the RGB channels in the training images. More details about this method and the code to implement it can be found here: Fancy PCA. This scheme approximately captures an important property of images that object identity is invariant to changes in the intensity and color of the illumination and reduces the top-1 error rate by over 1%.\nArchitecture: The model has 8 layers: 5 Convolution Layers and 3 Fully-connected layers. The authors mention that this depth is vital as it was found that removing any convolution layer resulted in inferior performance. The output of the last fully-connected layer is fed to a 1000-way softmax (which produces a distribution over the 1000 class labels).\nThe original paper mentions an input image shape of 224 $\\times$ 224 and uses a pad of 3 in the first convolution. In practice, we begin with an image of size 227 $\\times$ 227, and the padding is made zero, so as to encompass more information from the image (as padding of zero has no information)\nTanh non-linearity was commonly used during that time, and AlexNet was the first CNN architecture that used ReLU (called it non-saturating nonlinearity). The authors argued that CNNs train several times faster with ReLU than their equivalent tanh units because tanh saturates for large absolute values of activations killing the gradient (vanishing gradient problem). Today, ReLU is the default choice of the activation function.\nMax-pooling layers are included in the network with a kernel size of 3 and stride of 2, and they call it \u0026ldquo;overlapping pooling\u0026rdquo;. Compared to local pooling (kernel size = 2 and stride = 2), overlapping pooling reduces the top-1 and top-5 error rates by 0.4% and 0.3% respectively because they are slightly more difficult to overfit.\nDropout layers with a probability of 0.5 are also added in the first two fully-connected layers to prevent overfitting during training. Without dropout, the network exhibits substantial overfitting. It roughly doubles the number of iterations required to converge.\nAlexNet uses \u0026ldquo;Local Response Normalization\u0026rdquo; that reduces top-1 and top-5 error rates by 1.4% and 1.2% respectively. This is not used anymore so we won\u0026rsquo;t talk about it, but it was an early precursor to batch normalization.\nTraining: The model was trained on the Cross-entropy loss function using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005. It was found that this small amount of weight decay was important for the model to learn as it is not merely a regularizer, but it also reduces the model\u0026rsquo;s training error.\nThe weights for each layer were initialized from a zero-mean Gaussian distribution with a standard deviation of 0.01. The biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected layers, are initialized with the constant 1 and for the remaining layers with the constant 0.\nAn equal learning rate is used for all the layers. The learning rate was initialized at 0.01 and reduced three times prior to termination, which was divided by 10 when the validation error rate stopped improving with the current learning rate.\nAbove is a typical AlexNet architecture image that you would see almost everywhere. The model was spread across two GTX 580 3GB GPUs as the model was too big to fit on one GPU memory, and they communicated only in certain layers to save computation. However, we don\u0026rsquo;t need such a complicated scheme anymore as we can train the entire model on a single GPU (Google colab provides 12GB/16GB GPUs today).\nIt took about 90 epochs through the training set of 1.2 million images, which took five to six days on these two GPUs.\nOverall AlexNet showed the power of the convolutional neural network in achieving record-breaking results on an image classification task. Important design choices of the paper are,\n8 Layers: 5 CNN layers + 3 Fully-connected layers Activation Function: ReLU Data pre-processing: subtract mean image Heavy data augmentation Regularization: L2 Weight decay ($\\lambda$ = 5e-4), Dropout (p=0.5) Learning rate: 0.01 and reduced (divided by 10) when val accuracy plateaus Optimization Method: SGD + Momentum (m=0.9) with batch size of 128 Loss function: Cross-Entropy loss Link to the Paper: ImageNet Classification with Deep Convolutional Neural Networks\n","permalink":"https://yugajmera.github.io/posts/alexnet/post/","summary":"Do you wonder about how to come up with different design choices (architecture, optimization method, data manipulation, loss function, etc.) for the deep learning model so that it gives the best performance? Let\u0026rsquo;s look at the different CNN architectures that have performed well in the past on image classification tasks.\nThe ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was the huge benchmark for image classification because it held a yearly challenge from 2010 to 2017 where teams around the world would compete with their best-performing classification models.","title":"AlexNet: The First CNN to win ImageNet Challenge"},{"content":"Image classification is a fundamental task in computer vision that attempts to comprehend an entire image as a whole. The goal is to classify the image by assigning it to a specific class label. Typically, image classification refers to images in which only one object appears and is analyzed.\nNow that we have all the ingredients required to code up our deep learning architecture, let\u0026rsquo;s dive right into creating a model that can classify handwritten digits (MNIST Dataset) using Convolutional Neural Networks from scratch. The MNIST dataset consists of 70,000 $28 \\times 28$ black-and-white images of handwritten digits. I will be using Pytorch for this implementation. Don\u0026rsquo;t forget to change the runtime to GPU to get accelerated processing!\nThe first step is to import the relevant libraries that we will be using throughout our code.\nimport torch from torch import nn import torchvision import matplotlib.pyplot as plt Downloading and Pre-processing the Dataset\nThe dataset is downloaded into two subsets, the train set containing 60,000 images and the test set containing 10,000 images. While downloading, we apply some transformations to the images - convert them to tensors for processing them in the convolution layers and normalizing them. Each pixel value is originally [0,255] that gets divided by 255 while getting converted to tensor, resulting in a range [0,1]. We further normalize the image with a mean of 0.5 and a standard deviation of 0.5 to obtain the pixel values in the range [-1,1].\nmy_transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5,), (0.5,)) ]) # Download the dataset mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=my_transform) mnist_testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=my_transform) Loading the Dataset\nAs we discussed in this post, the test set is reserved to be used only once at the very end of our pipeline to evaluate our model. We randomly split the training data into a training set containing 80% of the images and a validation set containing 20% of the images and then build their loaders. I have defined a batch size of 64 for both the loaders but it can be set in any power of 2 (why? check out mini-batch gradient descent).\n# Split the dataset into train set(80%) and validation set(20%) and then load it len_train = int(0.8 * len(mnist_trainset)) len_val = len(mnist_trainset) - len_train train_dataset, val_dataset = torch.utils.data.random_split(mnist_trainset, [len_train, len_val]) # train_loader is the data loader containing the training samples train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 64, shuffle = True) # val_loader is the data loader containing the validation samples val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = 64, shuffle = True) Defining the Architecture\nWe will be using the Convolution Layer (link) to extract features from the images followed by Batch Norm (link) and Activation function (link). We add then add the Pooling layer (to downsample the images) and repeat this block again. Next, the features are flattened and fed into a fully-connected layer followed by an activation function. We then add a Dropout layer (link) for regularization, followed by another fully connected layer that gives 10 outputs (for 10 digits).\n# CUDA for PyTorch use_cuda = torch.cuda.is_available() device = torch.device(\u0026quot;cuda:0\u0026quot; if use_cuda else \u0026quot;cpu\u0026quot;) # Creating the architecture class DigitClassification(torch.nn.Module): def __init__(self): super(DigitClassification, self).__init__() self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(32) self.max_pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2) self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(64) self.max_pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) self.fc1 = torch.nn.Linear(7 * 7 * 64, 128) self.dropout = torch.nn.Dropout(p=0.5) self.fc2 = torch.nn.Linear(128, 10) self.relu = nn.ReLU() pass def forward(self, x): x = self.conv1(x) x = self.relu(self.bn1(x)) x = self.max_pool1(x) x = self.conv2(x) x = self.relu(self.bn2(x)) x = self.max_pool2(x) x = torch.flatten(x, 1) x = self.relu(self.fc1(x)) x = self.dropout(x) x = self.fc2(x) return x # Instantiating the network model = DigitClassification().to(device) Training our Model\nNow that we have defined our model let\u0026rsquo;s train it! I will be used the Adam optimizer and the Cross-entropy Loss function for learning the weights of our model. The training process is very similar to that defined in the post Implementing Backpropagation. Since we are using Batch Normalization that behaves differently during train and test times, we have to include model.train() and model.eval() to denote two different modes of our model while training and evaluating respectively.\nWe get the training loss for the entire epoch by adding all the losses for each batch iteration and averaging them. After that, we compare the model’s prediction with the actual labels and calculate the accuracy of the model.\nnum_epochs = 10 criterion = nn.CrossEntropyLoss() learning_rate = 0.001 optimizer = torch.optim.Adam(model.parameters(), learning_rate) train_loss, val_loss = [], [] train_acc, val_acc = [], [] for epoch in range(num_epochs): model.train() running_loss = 0. correct, total = 0, 0 for i, (image, label) in enumerate(train_loader): image = image.to(device) label = label.to(device) output = model.forward(image) optimizer.zero_grad() loss = criterion(output, label) loss.backward() optimizer.step() running_loss += loss.item() _, predicted = torch.max(output, dim=1) total += label.size(0) correct += (predicted == label).sum().item() train_loss.append(running_loss/len(train_loader)) train_acc.append(correct/total) model.eval() running_loss = 0. correct, total = 0, 0 for i, (image, label) in enumerate(val_loader): image = image.to(device) label = label.to(device) output = model.forward(image) loss = criterion(output, label) running_loss += loss.item() _, predicted = torch.max(output, dim=1) total += label.size(0) correct += (predicted == label).sum().item() val_loss.append(running_loss/len(train_loader)) val_acc.append(correct/total) print('\\nEpoch: {}/{}, Train Loss: {:.4f}, Val Loss: {:.4f}, Val Accuracy: {:.4f}'.format(epoch + 1, num_epochs, train_loss[-1], val_loss[-1], val_acc[-1])) plt.figure(1) plt.plot(list(range(num_epochs)), train_loss, label='Training Loss') plt.plot(list(range(num_epochs)), val_loss, label='Validation Loss') plt.xlabel(\u0026quot;Epochs\u0026quot;) plt.ylabel(\u0026quot;Loss\u0026quot;) plt.title('Epoch vs Loss') plt.legend() plt.figure(2) plt.plot(list(range(num_epochs)), train_acc, label='Training Accuracy') plt.plot(list(range(num_epochs)), val_acc, label='Validation Accuracy') plt.xlabel(\u0026quot;Epochs\u0026quot;) plt.ylabel(\u0026quot;Accuracy\u0026quot;) plt.title('Epoch vs Accuracy') plt.legend() plt.show() After setting the model to eval mode, we iterate over each batch from the validation data loader using the enumerate function. We do similar steps as training but we do not backpropagate the loss. The plots generated from training look something like this.\nTesting our Model\nJust like we did for the training data, let\u0026rsquo;s create a test loader first. I have kept the batch size as 1 because we aren\u0026rsquo;t doing any kind of optimization here so batches are irrelevant.\ntest_loader = torch.utils.data.DataLoader(mnist_testset, batch_size = 1) model.eval() correct, total = 0, 0 for i, (image, label) in enumerate(test_loader): image = image.to(device) label = label.to(device) output = model.forward(image) _, predicted = torch.max(output, 1) total += label.size(0) correct += (predicted == label).sum().item() print(correct/total) We enumerate through our test data loader and calculate the model\u0026rsquo;s accuracy on unseen data the same way we did with the validation loop. With this model, I got $99.2%$ accuracy on the test set by training the model for just 10 epochs!\n","permalink":"https://yugajmera.github.io/posts/img-classification-mnist/post/","summary":"Image classification is a fundamental task in computer vision that attempts to comprehend an entire image as a whole. The goal is to classify the image by assigning it to a specific class label. Typically, image classification refers to images in which only one object appears and is analyzed.\nNow that we have all the ingredients required to code up our deep learning architecture, let\u0026rsquo;s dive right into creating a model that can classify handwritten digits (MNIST Dataset) using Convolutional Neural Networks from scratch.","title":"Image Classification using CNNs: MNIST dataset"},{"content":"Linear Neural Networks that we have talked about till this point do not work when dealing with image data, as they don\u0026rsquo;t respect the spatial structure of images. When a neural network is applied to a 2D image, it flattens it out in 1D and then uses it as input. This creates a need for a new computational node that operates on images - Convolutional Neural Networks (CNNs).\nA convolution layer has a 3D image tensor (3 X H X W) as an input and a 3D filter (also called a kernel) that convolves over the image, i.e. slides over the image spatially, computing dot products. It should be noted that since it a 2D operation, the number of depth channels (here its 3 RGB) always has to match the number of depth channels in the filter. The figure below shows an example of the convolution operation.\nThe values in the filter are the weights that are learned from training, and the same filter gets applied over all the positions of the image. Here is what it would look like in 3D.\nEach 3D filter gives a 2D output called an activation map. Having only one filter does not make much sense, so a convolution layer involves convolving the input with a bank of filters (different filters with different weight values). When we convolve these filters with the input, we get one 2D activation map for each filter. We stack all the 2D activation maps together to obtain a 3D output tensor with the number of depth channels equal to the number of filters in that layer. An example is shown below,\nAs you can clearly note that the size of the activation map is not equal to the size of the image (or the input). Actually, it depends on the size of the filter. Here\u0026rsquo;s how, \\begin{align*} \\underbrace{N \\times C_{in} \\times H \\times W}_{\\text{Input size}} + \\underbrace{C_{out} \\times C_{in} \\times K_w \\times K_h}_{\\text{Filters size}} \\Rightarrow \\underbrace{N \\times C_{out} \\times H\u0026rsquo; \\times W\u0026rsquo;}_{\\text{Output size}} \\end{align*} where $N$ is the number of images in a single mini-batch, $C_{in}$ is the number of input channels and $C_{out}$ is the number of output channels or the number of filters in that convolution layer. Usually, $H = W$ (considering a square image) and $K_w = K_h = K$ (considering a square kernel), the output size $H\u0026rsquo; = W\u0026rsquo;$ is given by, \\begin{align*} W\u0026rsquo; = \\frac{(W - K + 2P)}{S} + 1 \\end{align*} where $P$ is the padding size and $S$ is the stride. Padding is a process of adding zeros around the input, applied so that the input size does not shrink after convolution operation. Stride is often used to downsample our input, where we modify the amount of movement of the filter over the input. So in a stride of 2, rather than placing our filter on every pixel of the image, we would place it on alternate pixels.\nJust like linear networks, stacking multiple convolution layers would result in one single big convolution. So, we need to have activations after each convolution. We even sometimes have $1 \\times 1$ convolution layer which gives MLP operating on each input pixel separately. Common choices:\n$P = (K-1)/2 \\hspace{1mm}$ gives \u0026ldquo;same\u0026rdquo; padding (output size = input size) $C_{in}$, $C_{out}$ = 32, 64, 128, 256 (in Powers of 2) K = 3, P = 1, S = 1 (Kernel = $3 \\times 3$) K = 5, P = 2, S = 1 (Kernel = $5 \\times 5$) K = 1, P = 0, S = 1 (Kernel = $1 \\times 1$) K = 3, P = 1, S = 2 (Downsample by 2) Pooling layers are another way (and much more preferred) to downsample a large input. The concept of the filter is the same as the convolution layer but instead of convolving, we take the maximum value of the elements in a kernel. Such a type of operation is called max-pooling and it takes only the kernel size and stride as hyperparameters. An example is shown below where the kernel size is 2 and the stride is 2.\nAnother common pooling operation is average pooling, where we take the average of all the elements in the kernel. Pooling layers do not have any learnable parameters so, during backpropagation, it simply points to the value that had maximum in the previous layer (where the value came from). It itself introduces non-linearity to our model, so any kind of activation is not required after pooling layers.\nIt also introduces translational invariance to our input, which means if a cat feature is present in the top-left corner of the filter or in the right-bottom, it will still give the same output as we are essentially performing the max over the kernel. The output remains the same even if the exact position of something in the image changes a little.\nIt should be noted that when we downsample our input using pooling or strided convolutions, the number of channels increases so that the total volume remains preserved.\n","permalink":"https://yugajmera.github.io/posts/cnn/post/","summary":"Linear Neural Networks that we have talked about till this point do not work when dealing with image data, as they don\u0026rsquo;t respect the spatial structure of images. When a neural network is applied to a 2D image, it flattens it out in 1D and then uses it as input. This creates a need for a new computational node that operates on images - Convolutional Neural Networks (CNNs).\nA convolution layer has a 3D image tensor (3 X H X W) as an input and a 3D filter (also called a kernel) that convolves over the image, i.","title":"Neural Network for Images: Convolutional Neural Networks (CNNs)"},{"content":"In the previous post, we talked about Convolution and Pooling layers. Stacking a large number of these layers (CNNs with activation functions and pooling) results in a Deep CNN architecture, which is often hard to train. It becomes very difficult to converge once they become very deep. The most common solution to this problem is Batch Normalization.\nThe idea is to normalize the outputs of a layer so that they have zero mean and unit variance. If you ask why?. The distribution of the inputs to layers deep in the network may change after each mini-batch when the weights are updated. This can cause the learning algorithm to forever chase a moving target. This change in the distribution of inputs to layers in the network is referred to as the technical name \u0026ldquo;internal covariate shift\u0026rdquo; and batch normalization helps to reduce this internal covariance shift, improving optimization.\nWe introduce batch normalization as a layer in our network that takes in inputs and normalizes them. \\begin{align*} \u0026amp; \\hat{x}_{ij} = \\frac{x_{ij} - \\mu_j}{\\sqrt{\\sigma_j^2 + \\epsilon}} \\\\ \u0026amp; y_{ij} = \\gamma_j \\hat{x}_{ij} + \\beta_j \\end{align*} Fo each feature/channel, $\\mu_j$ and $\\sigma_j$ are the mean and standard deviation across the mini-batch of inputs. Having zero mean and unit variance is a too hard constraint for our network so we have a scale and shift parameters $\\gamma$ and $\\beta$ that can be learned during training.\nA nice side-effect of changing the mean and variance of each channel (i.e. introducing randomness) is that it helps in regularization, enhancing the generalization ability of the networks.\nHowever, since we are computing $\\mu_j$ and $\\sigma_j$ across the mini-batch of inputs, these estimates depend upon the mini-batch itself. So during test time, if I have a mini-batch of say [cat, dog, frog] and another mini-batch of [cat, car, horse], and both of them have the same cat image, the outputs would be different for each one because the model would compute two different means and standard deviation for both batches. The output that our model produces for each input element of a batch depends upon every other element in the batch.\nTherefore in batch normalization, we make sure that our model behaves differently during training and testing times. During test time, the batch norm layer would not compute the mean and standard deviation over the batch, instead, we will use the running average of means and standard deviations generated during training. So during test time, the batch norm becomes a linear operation (as mean and standard deviations are constants) and can easily be fused with the previous linear or convolution layer. Therefore, the batch norm layer is often inserted after fully connected or convolutional layers, and before nonlinearity.\nTo summarize, batch-norm\nMakes deep networks much easier to train! Allows higher learning rates, faster convergence Networks become more robust to initialization Acts as regularization during training Zero overhead at test-time: can be fused with conv One of the irritating features of batch-norm is that it behaves differently during training and test time. One variant of batch-norm is called Layer Normalization which has the same behavior during training and testing. The only difference is that rather than averaging over batch dimensions, we average over the feature dimension. So, the estimates no longer depend on the elements of the batch and we have per-channel mean and standard deviation. Layer normalization is commonly used in RNNs and Transformers.\nAnother variant of this type of layer is called Instance Normalization, where we average over the spatial dimensions of the image. So, during training, we would have a per-image mean and per-image standard deviation, and this type of normalization would also behave the same during training and testing.\nWe also have one more variant to it, called the Group Normalization which is generally used for group convolutions, where we split the channel into some number of groups and then normalize over these subsets of the channel dimension. Such a type of normalization tends to work quite well in some applications such as object detection.\nThe figure below gives an intuition on these four types of normalizations,\n","permalink":"https://yugajmera.github.io/posts/batch-norm/post/","summary":"In the previous post, we talked about Convolution and Pooling layers. Stacking a large number of these layers (CNNs with activation functions and pooling) results in a Deep CNN architecture, which is often hard to train. It becomes very difficult to converge once they become very deep. The most common solution to this problem is Batch Normalization.\nThe idea is to normalize the outputs of a layer so that they have zero mean and unit variance.","title":"Understanding Batch Normalization"},{"content":"Datasets drive our choices for all the deep learning hyperparameters and peculiarities involved in making accurate predictions. We split the dataset into a training set, test set, and validation set. Our model is trained on the training set, and we choose the hyperparameters (usually by trial and error) on the validation set. We select the hyperparameters that have the highest accuracy on the validation set and then fix our model. The test set is reserved to be used only once at the very end of our pipeline to evaluate our model. Our algorithm is run exactly once on the test set and that accuracy gives us a proper estimate of our model\u0026rsquo;s performance on truly unseen data.\nNow that we have created our training set to perform the optimization process, let\u0026rsquo;s look at how manipulating this data can help us train our model.\nData Preprocessing The loss function is computed on the mini-batch of the training data at every step of the optimization algorithm. Therefore, the loss landscape on which optimization is performed depends directly on the training data. The aim of pre-processing the data is to make it more amenable to efficient training.\nWe usually normalize the data before using it, making the data cloud zero-centered and ensuring that all the features have the same variance. This makes it easier to optimize as it becomes less sensitive to small changes in weight. Note that we would use the same exact mean and standard deviation (that we used on the training set) to normalize the test set.\nSome standard practices of data pre-processing with image data include:\nSubtract the mean image (used in AlexNet) Subtract per-channel mean (mean along each channel, used in VGGNet) Subtract per-channel mean and divide by per-channel std (used in ResNet) Data Augmentation It is a commonly used technique where we perform image transformations on the training set for increasing both the size and the diversity of labeled training data. Not only is data augmentation used to increase the number of training samples, but it also acts as a form of regularization by adding noise to the training data.\nImage augmentations have become a common implicit regularization technique in computer vision tasks to combat overfitting in deep convolutional neural networks. Basic image transformations: flipping, rotating, scaling, blurring, jittering and cropping. It should be noted that only the image transformations that make sense are applied. For example, performing a vertical flip on a car image is never done (as upside-down car images are not found) but the same transformation can be applied to a white blood cell image in a blood smear.\n","permalink":"https://yugajmera.github.io/posts/data-manipulation/post/","summary":"Datasets drive our choices for all the deep learning hyperparameters and peculiarities involved in making accurate predictions. We split the dataset into a training set, test set, and validation set. Our model is trained on the training set, and we choose the hyperparameters (usually by trial and error) on the validation set. We select the hyperparameters that have the highest accuracy on the validation set and then fix our model. The test set is reserved to be used only once at the very end of our pipeline to evaluate our model.","title":"Data Manipulation for Deep Learning"},{"content":"Our motivation behind using optimization was to obtain that specific set of weights that incurs the least loss on our training data to achieve the maximum possible accuracy on the test set. But this never works in practice! (yes, you read that right). If we are trying to incur the least loss on the training data, i.e., fit the training data perfectly (called overfitting), our model might not always fit the test data perfectly. It is important to remember that the training and the test set are assumed to be sampled from a common dataset, and our aim is to ensure that our model fits this dataset. To achieve good accuracy, we must ensure that our model generalizes well to fit the test set as much as possible. We apply this in practice using a technique called regularization.\nLet\u0026rsquo;s understand this concept with a simple example. The blue points are the training data, and we fit two different models, m1 (polynomial) and m2 (linear). While m1 perfectly fits the training set (overfits!), m2 generalizes the data. Look at the third image, where the yellow points are the test set. Clearly, m2 would perform better than m1. We use regularization to make sure we choose simpler models (generalize more) and prevent the model from overfitting.\nWeight decay In this type of regularization, we prevent overfitting by zeroing out some weights of our model. An additional term is added in computation of loss over the training data as, \\begin{align*} L \u0026amp;= \\frac{1}{N} \\sum_{i = 1}^N L_i(x_i, y_i, W) + \\lambda R(W) \\end{align*} where $\\lambda$ is the hyperparameter that controls the regularization strength. Since we minimize this loss function in our optimization process, the added term decays some weights of our model. $R(W)$ for different types of weight decay can be written as, \\begin{align*} \\text{L2: } \u0026amp; R(W) = \\sum_k \\sum_l W_{k,l}^2 \\\\ \\text{L1: } \u0026amp; R(W) = \\sum_k \\sum_l |W_{k,l}| \\\\ \\text{Elastic Net (L1 + L2) : } \u0026amp; R(W) = \\sum_k \\sum_l \\beta W_{k,l}^2 + |W_{k,l}| \\\\ \\end{align*} L2 regularization shrinks weights proportionally to its size, so bigger weights gets shrunk more (since we are adding the square of them in the loss function). Whereas L1 regularization shrinks all the weights by the same amount, with the smaller weights getting zeroed out.\nL2 regularization (also called weight decay) is the most commonly used and can directly be added in our optimizer instance of Pytorch as $\\lambda$ = 1e-4\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-4) Dropout In the forward pass, we randomly set some of the neurons to zero, as shown in the figure below. The probability of dropping is a hyperparameter and is usually set to 0.5.\nSince we are randomly zeroing out neurons in each layer of our network in each forward pass, we would obtain outputs that are also random. We need to average out this randomness at test time to have a deterministic output. So, if we are randomly removing half of the nodes in the forward pass (p=0.5), then for testing, we would use the full network but shrink the outputs by half (multiply by dropout probability). A derivation of the same is shown below for a single neuron.\nBy doing this, we scale the activations so that for each neuron: output at test time = expected output at training time (average out the randomness). Having two different behaviors of our model at training and test time looks cumbersome, so in practice, we use a variant called inverted dropout. In the forward pass, we randomly mask half of the neurons and double the output of the remaining neurons, so we don\u0026rsquo;t have to change anything at the test time.\nIntroducing randomness in our network helps the model generalize better and prevent overfitting. Furthermore, it helps our optimizer to jump out of local minima by stimulating dead neurons during the training process. Another interpretation of dropouts is training a large ensemble of sub-networks that share weights.\nDropout can directly be implemented in Pytorch when defining our model as,\nmodel = torch.nn.Sequential(torch.nn.Linear(3072, 100), torch.nn.ReLU(), torch.nn.Dropout(p=0.5), torch.nn.Linear(100, 10)) Early Stopping This notion of early stopping helps us to choose the num_steps hyperparameter, i.e., how long should we train our model? When training, we look at three curves: training loss as a function of iterations (should be decaying exponentially) and training and validation accuracy as a function of iterations. These curves give some sense of the health of our network.\nEarly stopping is a fairly simple regularization technique, where we stop the training process when the validation accuracy decreases. Training further would increase training accuracy (as our model starts to overfit), but it would perform very poorly on our test set (as the validation set is a good representation of the test set).\n","permalink":"https://yugajmera.github.io/posts/regularization/post/","summary":"Our motivation behind using optimization was to obtain that specific set of weights that incurs the least loss on our training data to achieve the maximum possible accuracy on the test set. But this never works in practice! (yes, you read that right). If we are trying to incur the least loss on the training data, i.e., fit the training data perfectly (called overfitting), our model might not always fit the test data perfectly.","title":"Regularization: Weight decay, Dropout, Early stopping"},{"content":"All the variants of gradient descent, such as Momentum, Adagrad, RMSProp, and Adam, use a learning rate as a hyperparameter for global minimum search. Different learning rates produce different learning behaviors (refer to the Figure below), so it is essential to set a good learning rate, and we prefer to choose the red one.\nBut it is not always possible to come up with one \u0026ldquo;perfect\u0026rdquo; learning rate by trial and error. So what if don\u0026rsquo;t keep the learning rate fixed, and change it during the training process?\nWe can choose a high learning rate to allow our optimization to make quick progress in the initial iterations of training and then decay it over time. This would speed up our algorithm and result in better performance characteristics. This mechanism of changing the learning rates over the training process is called learning rate schedules. Let\u0026rsquo;s see some commonly used learning rate schedulers,\nStep schedule - We start with a high learning rate (like the green one). When the curve starts flattening, we reduce the learning rate and continue the same process until convergence. E.g., for ResNets, multiplying LR by 0.1 after epochs 30, 60, and 90 would result in a learning curve as shown beside. But this kind of scheduling introduces a lot of new hyperparameters - at what fixed points should we decay the LR, and to what value? Tuning them usually takes a lot of time.\nDecay functions - Instead of explicitly choosing fixed time points, we use a function that determines the learning rate at each epoch, hence it has no additional hyperparameters. We start with some initial learning rate and then decay it over the training process with a function to zero.\nCosine scheduling uses a cosine function. It is a very popularly used learning rate scheduler in computer vision problems. We can also linearly decay the learning rate over time. Such kind of scheduling is used commonly for language tasks. The Transformers paper (Attention is all you need) used an inverse square root decay function. The learning rate at epoch $t$ in each of these cases is given as, \\begin{align*} \\text{Cosine: } \u0026amp; \\eta_t = \\frac{\\eta_0}{2} ( 1 + cos \\frac{t \\pi}{T}) \\\\ \\text{Linear: } \u0026amp; \\eta_t = \\eta_0 (1 - \\frac{t}{T}) \\\\ \\text{Inverse Sqrt: } \u0026amp; \\eta_t = \\frac{\\eta_0}{ \\sqrt{t} }\\\\ \\end{align*} where $\\eta_0$ is the initial learning rate and $T$ is the total number of epochs.\nLR schedules are an optional technique that can be implemented to boost learning. It is important to note that using LR schedules with SGD + Momentum is fairly important. Still, if we are using one of the more complicated optimization method, such as Adam, then it often works well even with a constant learning rate. In fact, SGD + Momentum can outperform Adam but may require more tuning of LR and schedule.\nRecall the training block code of the previous post. Pytorch\u0026rsquo;s optim package can be used to implement these LR schedulers. After each epoch in the training loop, we use .step() function to update the learning rate as per our schedule.\nmodel = torch.nn.Sequential(torch.nn.Linear(3072, 100), torch.nn.ReLU(), torch.nn.Linear(100, 10)) optimizer = torch.optim.SGD(model.parameters(), lr=initial_learning_rate) scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_steps) mini_batches = torch.utils.data.DataLoader(data, batch_size=batch_size) for t in range(num_steps): for X_batch, y_batch in mini_batches: y_pred = model(X_batch) loss = loss_fn(y_pred, y_batch) loss.backward() optimizer.step() optimizer.zero_grad() scheduler.step() ","permalink":"https://yugajmera.github.io/posts/learning-rates/post/","summary":"All the variants of gradient descent, such as Momentum, Adagrad, RMSProp, and Adam, use a learning rate as a hyperparameter for global minimum search. Different learning rates produce different learning behaviors (refer to the Figure below), so it is essential to set a good learning rate, and we prefer to choose the red one.\nBut it is not always possible to come up with one \u0026ldquo;perfect\u0026rdquo; learning rate by trial and error.","title":"Using Learning Rate Schedules for Training"},{"content":" While we have talked about how optimization algorithms use the negative gradient of the loss function with respect to the weights to update the parameters of our model, we will now focus on how to actually compute these gradients on an arbitrary loss function. We use a backpropagation technique that creates a computation graph to perform a forward and backward pass on the model. In the forward pass, we compute outputs of each layer of our neural network sequentially. In the backward pass, we apply the chain rule for computing derivatives of each individual layer backward.\nConsider the example of a computation graph shown below.\nFirst we perform a forward pass where we have the value of inputs $x = -2, y = 5, z = -4$, \\begin{align*} q \u0026amp;= x + y = -2 + 5 = -3\\\\ f \u0026amp; = z * q = -4 * -3 = -12 \\end{align*} Now lets compute the backward pass, \\begin{align*} \u0026amp; \\frac{df}{df} = 1, \\hspace{5mm} \\frac{df}{dq} = z = -4, \\hspace{5mm} \\frac{df}{dz} = q = -3 \\\\ \u0026amp; \\frac{df}{dx} = \\frac{df}{dq} \\frac{dq}{dx} = -4 * 1 = -4, \\hspace{5mm} \\frac{df}{dy} = \\frac{df}{dq} \\frac{dq}{dy} = -4 * 1 = -4 \\\\ \\end{align*} This way, we have computed the derivative of the output with respect to the input. These gradients involved in chain rule have names, \\begin{align*} \\underbrace{\\frac{df}{dy}}_{\\text{downstream gradient}} = \\underbrace{\\frac{df}{dq}}_{\\text{upstream gradient}} \\hspace{2mm} \\underbrace{\\frac{dq}{dy}}_{\\text{local gradient}} \\end{align*}\nWith a much more complicated neural network model, we follow the same procedure but don\u0026rsquo;t worry, you don\u0026rsquo;t have to compute the chain rule manually; Pytorch comes to the rescue!\nConsider this simple neural network model on which we will apply backpropagation. It an image classification task with each image of shape (32 X 32 X 3 = 3072) input dimensions. The model with two layers outputs scores for 10 classes as shown below. [Linear -\u0026gt; ReLU -\u0026gt; Linear]\nPytorch uses the autograd package to build computational graphs out of Tensors and automatically compute gradients. Initializing weights as tensors with requires_grad = True flag enables autograd. Once we compute the loss, the gradient of this loss with respect to the weights can automatically be computed by running loss.backward(). This function implements backpropagation on all the inputs that have require grad set and accumulates the gradients in .grad() variable.\nThe torch.no_grad() function makes sure a computation graph is not created on the operations within it. It is crucial to run .zero_grad() function that zeroes out the current gradients to accumulate fresh gradients in the next step (as it does not overwrite automatically). A simple mini-batch gradient descent algorithm looks as follows,\nw1 = torch.randn(3072, 100, requires_grad=True) w2 = torch.randn(100, 10, requires_grad=True) mini_batches = split(data, batch_size) # Split data into mini-batches for t in range(num_steps): for X_batch, y_batch in mini_batches: y_pred = X_batch.mm(w1).clamp(min=0).mm(w2) # Forward Pass loss = loss_fn(y_pred, y_batch) loss.backward() # Compute gradients wrt loss with torch.no_grad(): # Tells Pytorch not to build a graph w1 -= learning_rate * w1.grad() w2 -= learning_rate* w2.grad() w1.zero_grad() # Set gradients to zero w2.zero_grad() X_batch is the input vector (mini-batch) to our model and y_batch is the corresponding label vector. y_pred is the scores vector that our model outputs. The loss function returns the average loss over a mini-batch which is then used to compute the gradients. Pytorch provides the Dataloader primitive to create mini-batches.\nIt was easy to define the model, initialize weights, and write the forward pass. But what if we had a neural network with 50 layers? Explicitly writing code for all this would be very cumbersome. Pytorch provides the nn.Sequential API to define models easily. The weights have required grad set True implicitly and are initialized using the Kaiming initialization we discussed in the last post.\nmodel = torch.nn.Sequential(torch.nn.Linear(3072, 100), torch.nn.ReLU(), torch.nn.Linear(100, 10)) mini_batches = torch.utils.data.DataLoader(data, batch_size=batch_size) for t in range(num_steps): for X_batch, y_batch in mini_batches: y_pred = model(X_batch) # Forward Pass loss = loss_fn(y_pred, y_batch) loss.backward() # Compute gradients wrt loss with torch.no_grad(): # Tells Pytorch not to build a graph for param in model.parameters(): param -= learning_rate * param.grad() model.zero_grad() # Set gradients to zero It\u0026rsquo;s annoying to write the weight update code all the time. Pytorch provides the optim package (makes life easier once again!) for implementing the standard gradient descent rules that we have discussed. In our training loop, after computing gradients, we use this optimizer to automatically make the gradient step for us (one step of update) and zero out the gradients. This is what a typical training block looks like in practice,\nmodel = torch.nn.Sequential(torch.nn.Linear(3072, 100), torch.nn.ReLU(), torch.nn.Linear(100, 10)) optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) mini_batches = torch.utils.data.DataLoader(data, batch_size=batch_size) for t in range(num_steps): for X_batch, y_batch in mini_batches: y_pred = model(X_batch) loss = loss_fn(y_pred, y_batch) loss.backward() optimizer.step() optimizer.zero_grad() ","permalink":"https://yugajmera.github.io/posts/backprop/post/","summary":"While we have talked about how optimization algorithms use the negative gradient of the loss function with respect to the weights to update the parameters of our model, we will now focus on how to actually compute these gradients on an arbitrary loss function. We use a backpropagation technique that creates a computation graph to perform a forward and backward pass on the model. In the forward pass, we compute outputs of each layer of our neural network sequentially.","title":"Implementing Backpropagation"},{"content":"In this post, we will make choices for components of our deep neural network architecture, including activation functions and how the weights of each layer get initialized to ease the optimization process. A neural network is composed of interconnected layers, with every neuron in one layer connecting to every neuron in the next layer. Such a fully connected neural network is often called Multi-layer Perceptron (MLP). Let\u0026rsquo;s dive right into defining our deep neural network architecture.\n\\begin{align*} \\text{Linear function: } \u0026amp; f = W x \\\\ \\text{2-Layer Neural Network: } \u0026amp; f = W_2 \\hspace{2mm} z(W_1 x) \\\\ \\text{3-Layer Neural Network: } \u0026amp; f = W_3 \\hspace{2mm} z_2(W_2 \\hspace{2mm} z_1(W_1 x)) \\\\ \\end{align*} The weights $W$ have 1s appended at the last column for the bias (called the bias trick). A typical deep learning model has many layers. The number of learnable weights is equal to the number of layers in the DNN (as each layer has an associated weight), and each layer is followed by an activation function $z$.\nActivation functions are super critical to the functioning of a neural network as they introduce non-linearity into our model. Without them, stacking linear layers on top of one another would result in one big linear layer ($ y = W_2 W_1 x = W x $).\nA neural network with one hidden layer is a universal approximator as it can fit any function. This is because the activation function makes complex data linearly separable in the transform (feature) space. More the number of layers (more activation functions), the more complicated the decision boundary in the input space giving rise to a perfect classification model (one that fits the training data perfectly).\nActivation functions How should one select an activation function? Here are some commonly used activation functions that one can choose from, but each one has its pros and cons,\nSigmoid - It has a nice interpretation of a probability by transforming the input data in the range [0,1], i.e., gives the probability of a particular feature being present. However, the output of the sigmoid saturates (i.e., the curve becomes parallel to the x-axis) for a large positive or large negative number. Thus, the gradient at these regions is almost zero (dw $\\approx$ 0), killing the learning process.\nThis phenomenon is called the vanishing gradient problem where the gradients of the parameters with respect to the loss function become very small as the number of layers in the network increases. This can make it difficult for the network to learn, because the gradients are used to update the parameters during training, and if the gradients are very small, the updates will also be small and the learning process will be slow.\nFurthermore, computing the exponential function is expensive, slowing our training process.\nTanh - It is shifted version of sigmoid that outputs in the range [-1,1]. It is slightly better than sigmoid as it is zero-centered, so it can produce positive and negative outputs. But it has all the other problems as sigmoid.\nReLU - Rectified Linear Unit (or ReLU) is most commonly used activation function and is often used to address the vanishing gradient problem because it has a non-saturating gradient. This means that the gradient of the ReLU function is always 1 for positive inputs and 0 for negative inputs, so the gradients do not vanish as the input increases, which can allow the network to learn more effectively. Training is also significantly faster using the ReLU function.\nHowever, ReLU does have a potential issue known as the \u0026ldquo;dying ReLU\u0026rdquo; problem, where the activation function becomes stuck at 0 for negative inputs. This can occur if the weights are not initialized properly or if the learning rate is too high, causing the weights to never be updated for negative inputs. This can lead to neurons becoming \u0026ldquo;dead\u0026rdquo; and not contributing to the output of the network, which can degrade performance.\nLeaky ReLU - It is an improved version of ReLU function to solve the dying ReLU problem as it has a small positive slope in the negative area. So even if the activations are negative, it will still receive smaller gradients and have the potential to keep learning. So it does not saturate in any region and is also computationally efficient. The value 0.1 is a hyperparameter that we need to set.\nOther advanced ReLU options include Parametric RELU (PReLU), Exponential linear unit (ELU), Scaled ELU (SELU), and many more! To summarize,\nDon\u0026rsquo;t think too hard; just use ReLU! Don\u0026rsquo;t use sigmoid/tanh. Try out advanced RELU options to push the accuracy by 0.1%. Weight Initialization Now that we have made decisions about the architecture of our network, let\u0026rsquo;s see how we can initialize the weights of each layer of this network. Each time a neural network is initialized with a different set of weights, it results in a different starting point for the optimization process, potentially resulting in a different final set of weights with different performance characteristics.\nWhat would happen if we initialize the weights to a contant value, say zero? Then the outputs would all be zero and would not depend on the input, giving zero gradients for all training examples (the network won\u0026rsquo;t train).\nOne approach can be to initialize the weights with small random numbers (gaussian with zero mean, std = 0.01). This might work for small networks but multiplying small weights ($\u0026lt; 1$) recursively in a deep network would result in smaller activations that would produce very small gradients. Our optimization algorithm would take small steps with small gradients, taking a very long time to train (or converge).\nIf we initialize the weights with high values (std = 0.05) and an activation function like sigmoid/tanh is applied, the function maps its value near to 1, where the curve saturates, giving small gradients (called the vanishing gradients problem). If we use an activation function like ReLU, higher values give rise of exploding gradients problem. We need some way of initializing weights that are not too small or too big (find a sweet spot).\nXavier Initialization - If we can make sure that the variance of the output of a layer is the same as the variance of the input (so that the scale of activations doesn\u0026rsquo;t change), then we can solve the problem of vanishing/exploding gradients. Consider a linear layer, \\begin{align*} y \u0026amp;= \\sum_{i=1}^{D_{in}} x_i w_i \\\\ Var(y) \u0026amp;= D_{in} * Var(x_i w_i) \\\\ \u0026amp;= D_{in} * Var(x_i) * Var(w_i) \\\\ Var(y) = Var(x_i) \u0026amp; \\Rightarrow Var(w_i) = 1/D_{in} \\end{align*} Instead of using the standard deviation as a hyperparameter, we set it to the inverse square root of the input dimension of the layer. The weights of a layer $l$ are initialized as,\nw_l = torch.rand(dim_l, dim_{l-1})/torch.sqrt(dim_{l-1}) This derivation depends on the choice of activation function as the linear layer would be followed by it. This will work if we use a tanh function that is symmetric around zero and matches the variance of the linear layer. However, with ReLU, negative activations are zeroed out, outputting smaller values, violating our motivation of keeping the variance constant. Xavier initialization assumes a zero-centered function and won\u0026rsquo;t work with a ReLU activation function.\nKaiming/ He Initialization - Since ReLU takes away activations that are less than zero, which is half of the expected outputs (since they are uniformly distributed around zero), all we have to do is double the variance to keep the variance constant through a layer. Therefore, we set $Var(w_i) = 2/D_{in}$.\nw_l = torch.rand(dim_l, dim_{l-1}) * torch.sqrt(2/dim_{l-1}) While weight initialization is an active research area, the above schemes work well in practice.\n","permalink":"https://yugajmera.github.io/posts/neural-networks/post/","summary":"In this post, we will make choices for components of our deep neural network architecture, including activation functions and how the weights of each layer get initialized to ease the optimization process. A neural network is composed of interconnected layers, with every neuron in one layer connecting to every neuron in the next layer. Such a fully connected neural network is often called Multi-layer Perceptron (MLP). Let\u0026rsquo;s dive right into defining our deep neural network architecture.","title":"Neural Networks: Activation functions and Weight Initialization"},{"content":"The loss function tells us how good our current classifier, with its current weights, is performing on the training data. Because we\u0026rsquo;re a bunch of greedy people, we want to find those golden set of weights that incurs the minimum loss on our training set, squeezing out every ounce of accuracy we can to ace the test set.\nWe generally initialize our model with some weights and then use the training set to search over all the possible values, aiming to arrive at the one that optimally fits our data. Optimization is the process of finding the set of parameters that minimizes the loss function.\nImagine a landscape, where every point (x,y) represents a weight value and the height at that point is the loss function. Our goal is to navigate to the bottom-most point of this terrain, where we\u0026rsquo;ll find the weights that yield the minimal loss.\nSince we don\u0026rsquo;t have the exact equation of the landscape, computing the minimum of the curve is not straightforward (it\u0026rsquo;s not a convex optimization problem). So, instead, we take iterative, small steps towards the direction of the local minimum, hoping to eventually reach at the lowest point. This direction of the local steepest descent is essentially the negative gradient of the loss function with respect to the weights.\nGradient Descent In the algorithm below, also known as the vanilla version of gradient descent, we begin by initializing the weights with some arbitary values. Then, we loop for a fixed number of iterations, where for each iteration, we compute the gradient for our current weights and take a step in the negative gradient direction to update the weight values.\n# Vanilla Gradient Descent w = initialize_weights() for t in range(num_steps): dw = compute_gradient(loss_fn, data, w) w += - learning_rate * dw The size of each step is determined by a hyperparameter known as the learning rate. This parameter controls how much we move in the negative gradient direction during each iteration of the optimization process. A higher learning rate means taking larger steps toward the negative gradient, which might lead to faster convergence but can also result in unstable behavior. On the other hand, lower learning rates are more stable but may take longer to converge. Therefore it is crucial to select an optimal learning rate.\nThe number of iterations is a also hyperparameter that determines the finite number of steps to run this algorithm for the model to converge. This value depends on the time at hand and computation resources. We usually stop when the model converges.\nJust as we averaged the losses over all images in the training data to compute the overall loss, we also take the average of individual gradients when calculating the overall gradient.\n\\begin{align*} L \u0026amp;= \\frac{1}{N} \\sum_{i = 1}^N L_i(x_i, y_i, W) \\\\ \\mathbf{dw} = \\frac{\\partial L}{ \\partial W} \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial}{\\partial W} L_i(x_i, y_i, W) \\end{align*}\nHowever, this process becomes computationally expensive and slow when dealing with a huge training set ($N$ is very large), as just one step of gradient descent involves looping over the entire training data.\nIn practice, this version of gradient descent is not very feasible. But fear not! Here\u0026rsquo;s a solution: Instead of using the entire training set, we split it into mini-batches, each containing $B$ examples. We can now use this mini-batch to perform a parameter update. This variant called Mini-batch Gradient Descent.\nThe intuition behind this solution lies in the assumption that the examples in the training data are correlated. Therefore, computing the gradient over batches of the training data is a good approximation to the gradient of the full objective. This allows us to achieve much faster convergence by evaluating the mini-batch gradients to perform more frequent parameter updates.\n# Mini-batch Gradient Descent w = initialize_weights() batches = split(data, batch_size) for t in range(num_steps): for mini_batch in batches: dw = compute_gradient(loss_fn, mini_batch, w) w += - learning_rate * dw The batch size is another hyperparameter often chosen from values like 32, 64, 128, or 256, which you might notice are powers of 2. This is because many vectorized operations perform more efficiently when their inputs are sized in powers of 2. We select a batch size as large as possible that still fits comfortably within the memory constraints of the GPU.\nThe extreme case is a setting where the mini-batch contains only one single example $B = 1$. This variant is called Stochastic Gradient Descent (SGD), where we update the parameters by taking one observation at each iteration.\n# Stochastic Gradient Descent (SGD) w = initialize_weights() for t in range(num_steps): for example in data: dw = compute_gradient(loss_fn, example, w) w += - learning_rate * dw Since we are updating the weights at each iteration, the learning curve tends to be erratic in SGD as compared to the mini-batch version. This is why the latter is prefered. Both versions are commonly referred to as SGD in practice, as the distinguishing factor is simply the batch size used during training.\nThere are a couple of challenges with SGD that needs to be addressed before applying it to optimize our models,\nIf the loss landscape contains a local minimum or a saddle point, the SGD algorithm might get trapped in it. If the loss landscape varies rapidly in one direction and slowly in another, then with a constant learning rate, it would progress slowly along the shallow direction and jitter along the steep direction. While choosing a small learning rate may work in this case, it would lead to very slow convergence. Momentum The gradient descent with momentum algorithm, or simply Momentum, draws inspiration from physics. Instead of relying solely on the gradient of the current step to guide the search, momentum also accumulates the gradients from past steps to determine the direction of movement. By doing so, it ensures that we keep moving consistently in the same direction.\nAs a result, if the updates of weights (dw) has been constantly high, we build momentum and quickly descend the surface while jumping out of the local minimums along the way (solving Problem 1).\n# SGD + Momentum v = 0 for t in range(num_steps): v = (m * v) - learning_rate * dw w += v Let’s consider two extreme cases to understand this concept better. If $m = 0$ (no momentum), then the algorithm behaves exactly like gradient descent. On the other hand, if $m = 1$, it rocks back and forth endlessly like a frictionless bowl, never converging. Typically the value of $m$ is set to 0.9 or 0.99.\n\\begin{align*} \\text{SGD: } \u0026amp; \\Delta w = - \\eta * dw \\\\ \\text{SGD + Momentum: } \u0026amp; \\Delta w^{t} = - \\eta * dw + m * \\Delta w^{t-1} \\end{align*}\nWhen the change in weights is large in the past iterations, i.e., the landscape is steep, we take larger steps towards the minimum (with high momentum) and avoid getting trapped in any local minimas. Whereas, as we approach the minimum, the change in weights becomes small, we add a small momentum to our gradient step, maintaining (some) stability for convergence. The illustration below visually depicts this concept.\nMomentum (magenta) with m = 0.99 vs. Gradient Descent (cyan) on a surface with a global minimum (the left well) and local minimum (the right well)\nAdaGrad Instead of keeping track of the sum of gradient-like momentum, the Adaptive Gradient algorithm, or AdaGrad for short, keeps track of the sum of gradient squared and uses that to have an adaptive learning rate.\n# Adagrad grad_squared = 0 for t in range(num_steps): grad_squared += dw * dw w += -learning_rate * dw / (grad_squared.sqrt() + 1e-7) In the direction of a steep descent (dw = high), the learning rate would be damped, and we would take smaller update steps, whereas, in the shallow direction (dw = low), we would take larger steps (solves Problem 2). Furthermore, in the initial iterations, the sum of gradient squares would be small, so the learning rate would be high, and as we keep accumulating gradient squares, the learning rate decays over time (a good feature to have to speed up the learning process!).\nHowever, Adagrad might decay the learning rate even before reaching the minimum as the sum of gradient squared only grows and never shrinks! If the sum of gradient squares becomes too big, the learning rate would be too low to update the weights. To overcome this problem, we use a variant called RMSProp.\nRMSProp RMSProp (short for Root Mean Square Propagation) is a leaky version of AdaGrad that decays the running sum of square gradients and ensures that the learning rate does not become too small.\n# RMSProp grad_squared = 0 for t in range(num_steps): grad_squared = (decay_rate * grad_squared) + (1 - decay_rate) * dw * dw w += -learning_rate * dw / (grad_squared.sqrt() + 1e-7) The decay rate is a hyperparameter usually set to 0.9, and the typical value of the learning rate = 0.001.\nThe behavior of these algorithms can be visualized below. AdaGrad (white) and RMSProp (green) are both faster and more stable than Momentum (as it chooses a better path), but they get stuck in a local minimum (since they solve Problem 2 and not Problem 1).\nMomentum (magenta) with m = 0.99 vs. Gradient Descent (cyan) vs. AdaGrad (white) vs RMSProp (green) with decay rate = 0.9 on a surface with a global minimum (the left well) and local minimum (the right well)\nAdam Adaptive Moment Estimation (Adam) combines RMSProp and Momentum (getting the best of two worlds!). In AdaGrad and RMSProp, with a decay rate closer to one, the sum of squared gradients would be very small in the initial iterations (the moments are biased towards zero), leading to a very high learning rate at the beginning. We overcome this problem using bias correction.\n# Adam m1 = 0 m2 = 0 for t in range(num_steps): m1 = (beta1 * m1) + (1 - beta1) * dw m2 = (beta2 * m1) + (1 - beta2) * dw * dw m1_unbias = m1 / (1 - beta1 ** t) m2_unbias = m2 / (1 - beta2 **t) w += -learning_rate * m1_unbias / (m2_unbias.sqrt() + 1e-7) Beta1 is the decay rate for the first moment, the sum of gradient (aka momentum), commonly set at 0.9. Beta 2 is the decay rate for the second moment, the sum of gradient squared, and it is commonly set at 0.999.\nAdam has become a go-to optimizer for most of the deep learning community today. Learning rates = 1e-3, 5e-4, and 1e-4 can be a great starting point for most models.\nMomentum (magenta) with m = 0.99 vs. Gradient Descent (cyan) vs. AdaGrad (white) vs RMSProp (green) with decay rate = 0.9 vs Adam (blue) with beta1 = 0.9 and beta2 = 0.999 on a surface with a global minimum (the left well) and local minimum (the right well)\nCredits: I have created these visualizations using this visualization tool.\nYou can also look at this cool visualization I came across on Emilien Dupont\u0026rsquo;s blog post, where you can click anywhere on the loss profile to see how different methods converge from that starting point.\n","permalink":"https://yugajmera.github.io/posts/optimization/post/","summary":"The loss function tells us how good our current classifier, with its current weights, is performing on the training data. Because we\u0026rsquo;re a bunch of greedy people, we want to find those golden set of weights that incurs the minimum loss on our training set, squeezing out every ounce of accuracy we can to ace the test set.\nWe generally initialize our model with some weights and then use the training set to search over all the possible values, aiming to arrive at the one that optimally fits our data.","title":"Optimization Methods"},{"content":"While there are a ton of concepts related to Deep learning scrambled all around the internet, I thought why not have just one place where you can find all the fundamental concepts required to write your own neural network. But, if you prefer watching a video over reading a blog and have the time, I highly recommend Justin Johnson\u0026rsquo;s course.\nIn this first part, I will discuss one of the most essential elements of deep learning - the loss function! I call it the \u0026ldquo;Oxygen of Deep Learning\u0026rdquo;, because, without a loss function, a neural network cannot be trained (so it would just be dead!).\nBefore we jump into the nitty-gritty, let\u0026rsquo;s lay down the basics: image classification. It\u0026rsquo;s a core task of computer vision, where our model takes an image as input and spits out a label from a predetermined set of categories. Easy peasy, right?\nNow let\u0026rsquo;s formulate this: Suppose we have an image $x_i$ and its true label $y_i$ from the training dataset. We feed $x_i$ into our model to obtain scores for each class. In the example below, we give an image of a cat, car, and frog as inputs to our model to generate these scores.\nThink of these scores as the model\u0026rsquo;s confidence levels—it\u0026rsquo;s basically saying how sure it is that the image belongs to each specific category. The class with the maximum score becomes our predicted label $\\hat{y}_i$. The scores of the true class are highlighted in bold.\nClearly, the predictions of our classifier is not always correct. A loss function helps us quantify the performance of our model on our data. So, naturally, the lower the loss, the better our model is. It is also called an objective function or a cost function. We compute a loss for each input image and simply take an average across the training set to compute the final loss value.\nSVM Loss The idea behind Support Vector Machine (SVM) loss is pretty straightforward: we want the score of the correct class to be significantly higher than the scores for all the incorrect classes. Makes sense, right? We want our classifier to be confident in its predictions by giving a higher score to the correct category in comparison to the wrong ones by some margin.\nThe SVM loss has the form, \\begin{align} L_i = \\sum_{j \\neq y_i} \\text{max}(0, \\underbrace{s_j}_{\\text{score of jth class}} - \\underbrace{s_{y_i}}_{\\text{score of true class}} + \\underbrace{1}_{\\text{margin}}) \\end{align}\nThe SVM loss is also called Hinge Loss because its shape looks just like a door hinge. When the score of the correct class is higher than all the other scores plus a certain margin, the loss is zero. Otherwise, it increases linearly. It is important to note that the loss for the true class is always zero.\nLet\u0026rsquo;s derive the SVM loss for our example. For the cat image, the true score $s_{y_i} = 3.2$. The loss is computed on the car and frog classes as, \\begin{equation*} L_1 = \\text{max}(0, 5.1 - 3.2 + 1) + \\text{max}(0, -1.7 - 3.2 + 1) = 2.9 + 0 = 2.9 \\end{equation*} Similarly for the other two training examples, \\begin{align*} L_2 \u0026amp;= \\text{max}(0, 1.3 - 4.9 + 1) + \\text{max}(0, 2 - 4.9 + 1) = 0 \\\\ L_3 \u0026amp;= \\text{max}(0, 2.2 + 3.1 + 1) + \\text{max}(0, 2.5 + 3.1 + 1) = 6.3 + 6.6 = 12.9 \\end{align*} The final loss of our classifier is the average of all the losses: $L_{\\text{svm}} = 5.27$.\nCross-Entropy Loss Cross-Entropy loss or Logistic Loss is one of the most commonly used loss functions, where raw classifier scores are interpreted as probabilities. We run the raw scores through an exponential function to ensure the outputs are positive. We then normalize them to obtain a probabilistic distribution over all the categories. This transformation is called the softmax function.\nThe term \u0026ldquo;softmax\u0026rdquo; comes from the fact that it is a differential approximation to the max function. If we were to directly apply the max function to the raw score vector of the cat image in our example, it would yield $[0, 1, 0]$. However, this function is non-differentiable, making it unsuitable for training neural networks. Softmax is therefore a go-to tool in deep learning when you need to compute the max of something while ensuring differentiability. The image above shows the softmax function applied to cat image scores.\nThe softmax function provides our estimated distribution (denoted by $q$). The true or desired distribution is such that all the probability mass is concentrated on the correct class, i.e. $p = [0, .. 1, .. , 0]$ (contains a single 1 at the $y_i$th position).\nThe difference between two probability distributions is measured by cross entropy. The cross-entropy between a \u0026ldquo;true\u0026rdquo; distribution $p$ and an estimated distribution $q$ is defined as: \\begin{equation*} H(p,q) = - \\sum_x p(x) \\text{ log} (q(x)) \\end{equation*}\nHence, the cross-entropy loss would be the negative log of the probability of true class. For the above cat image example, the loss would be $L_1 = -log(0.13) = 2.04$. The cross-entropy loss has the form, \\begin{equation} L_i = -\\text{log} \\underbrace{\\left( \\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}} \\right)}_{\\text{softmax function}} \\end{equation}\nWhile these are the two most commonly used loss functions, a complete list of options can be found here - Loss Functions\n","permalink":"https://yugajmera.github.io/posts/loss-function/post/","summary":"While there are a ton of concepts related to Deep learning scrambled all around the internet, I thought why not have just one place where you can find all the fundamental concepts required to write your own neural network. But, if you prefer watching a video over reading a blog and have the time, I highly recommend Justin Johnson\u0026rsquo;s course.\nIn this first part, I will discuss one of the most essential elements of deep learning - the loss function!","title":"Loss functions"}]