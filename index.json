[{"content":"Building on our previous exploration of GPT-1, let\u0026rsquo;s now modify its architecture to recreate the GPT-2 [1] small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.\nOne major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning. Additionally, GPT-2 is an excellent choice for learning how to implement LLMs, as it can run on a single GPU or even a laptopâ€”unlike GPT-3, which requires GPU clusters for training and inference.\nThe official model code is available here: https://github.com/openai/gpt-2/blob/master/src/model.py\nArchitecture Let\u0026rsquo;s start with our imports:\n# Import functions import torch import torch.nn as nn device = \u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39; print(\u0026#34;Using\u0026#34;, device) Before implementing the model, let\u0026rsquo;s take a look at GPT-2\u0026rsquo;s pre-trained weights from Hugging Faceâ€™s transformers library. This will help us understand the naming conventions used in the architecture:\nfrom transformers import GPT2LMHeadModel # Load pre-trained GPT-2 (small) and retrieve its state dictionary model_hf = GPT2LMHeadModel.from_pretrained(\u0026#34;gpt2\u0026#34;).to(device) # 124M param model model_hf_params = model_hf.state_dict() for name, param in model_hf_params.items(): print(name, param.shape) transformer.wte.weight torch.Size([50257, 768]) transformer.wpe.weight torch.Size([1024, 768]) transformer.h.0.ln_1.weight torch.Size([768]) transformer.h.0.ln_1.bias torch.Size([768]) transformer.h.0.attn.c_attn.weight torch.Size([768, 2304]) transformer.h.0.attn.c_attn.bias torch.Size([2304]) transformer.h.0.attn.c_proj.weight torch.Size([768, 768]) transformer.h.0.attn.c_proj.bias torch.Size([768]) transformer.h.0.ln_2.weight torch.Size([768]) transformer.h.0.ln_2.bias torch.Size([768]) transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072]) transformer.h.0.mlp.c_fc.bias torch.Size([3072]) transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768]) transformer.h.0.mlp.c_proj.bias torch.Size([768]) ... transformer.ln_f.weight torch.Size([768]) transformer.ln_f.bias torch.Size([768]) lm_head.weight torch.Size([50257, 768]) The output lists the layer names along with their corresponding tensor shapes, giving us insights into GPT-2\u0026rsquo;s layer structure:\nThe model is encapsulated under the namespace transformer. wte represents the token embedding weights. wpe represents the position embedding weights. h is a list of hidden decoder blocks. LayerNorms are denoted as ln_1 and ln_2 within each decoder block, with the final LayerNorm represented as ln_f. The scale and shift parameters are renamed to weight and bias, respectively. Our W_qkv weight matrix is renamed to c_attn (short for causal attention), while W_o is renamed to c_proj. The causal attention layer also includes bias tensors, meaning we will set qkv_bias=True. lm_head represents the final linear layer (language modeling head) and does not include a bias term. To ensure compatibility when loading OpenAIâ€™s pre-trained weights, we will follow this naming convention while defining the subcomponents of our model. Letâ€™s implement these next! ðŸš€\nclass MultiHeadAttention(nn.Module): def __init__(self, d_in, d_out, context_length, attn_pdrop, num_heads, qkv_bias=False): super().__init__() self.c_attn = nn.Linear(d_in, 3 * d_out, bias=qkv_bias) self.c_proj = nn.Linear(d_out, d_out) self.attn_dropout = nn.Dropout(attn_pdrop) self.d_h = d_out // num_heads # head dimension self.num_heads = num_heads self.register_buffer(\u0026#34;masked\u0026#34;, torch.tril(torch.ones(context_length, context_length)).view(1, 1, context_length, context_length)) def forward(self, x): # Input vectors x - (B, N, d_in) B, N, d_in = x.shape # Obtain keys, values and queries in one go - (B, N, d_out) qkv = self.c_attn(x) queries, keys, values = qkv.chunk(3, dim=-1) # Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h) k = keys.view(B, N, self.num_heads, self.d_h).transpose(1, 2) v = values.view(B, N, self.num_heads, self.d_h).transpose(1, 2) q = queries.view(B, N, self.num_heads, self.d_h).transpose(1, 2) # Apply scaled dot-product attention with causal mask on each head attn_scores = (q @ k.transpose(2, 3)) * k.shape[-1]**-0.5 attn_scores = attn_scores.masked_fill(self.masked[:, :, :N, :N] == 0, float(\u0026#39;-inf\u0026#39;)) attn_weights = torch.softmax(attn_scores, dim=-1) attn_weights = self.attn_dropout(attn_weights) context_vec = attn_weights @ v # Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out) out = context_vec.transpose(1, 2).contiguous().view(B, N, -1) out = self.c_proj(out) return out class LayerNorm(nn.Module): def __init__(self, emb_dim): super().__init__() self.eps = 1e-5 self.weight = nn.Parameter(torch.ones(emb_dim)) self.bias = nn.Parameter(torch.zeros(emb_dim)) def forward(self, x): mean = x.mean(dim=-1, keepdim=True) var = x.var(dim=-1, keepdim=True, unbiased=False) # Used biased variance estimation (without Bessel\u0026#39;s correction) norm_x = (x - mean) / torch.sqrt(var + self.eps) return self.weight * norm_x + self.bias class GELU(nn.Module): def __init__(self): super().__init__() def forward(self, x): return 0.5 * x * (1 + torch.tanh( torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)) )) class MLP(nn.Module): def __init__(self, emb_dim): super().__init__() self.c_fc = nn.Linear(emb_dim, 4 * emb_dim) self.c_proj = nn.Linear(4 * emb_dim, emb_dim) self.gelu = GELU() def forward(self, x): x = self.gelu(self.c_fc(x)) x = self.c_proj(x) return x Here, Iâ€™ve taken our GPT-1 implementation and adapted it to match GPT-2â€™s naming conventions.\nA key detail is the masked variable, which is defined in register_buffer. This automatically adds it to the modelâ€™s state dictionary, meaning youâ€™ll find entries like transformer.h.0.attn.masked. However, since itâ€™s a non-trainable tensor, it does not exist in Hugging Faceâ€™s state dictionary. So, when loading pre-trained weights, weâ€™ll simply ignore it.\nThe GPT-2 decoder block closely follows the GPT-1 model we implemented earlier, with a few key modifications. Letâ€™s break them down.\nPre-Norm Transformer The original Transformer (as well as our GPT-1 implementation) used Post-Norm residual connections, where LayerNorm is applied after the sublayer and residual addition.\nPre-Norm vs Post-Norm Transformer Layers\nIn this setup, the raw output of each sublayer (such as the self-attention or feedforward network) is directly added to the residual before normalization. This can lead to instability as the network depth increases because the residual sum $[x_l + \\text{Sublayer}(x_l)]$ accumulates unnormalized activations over layers:\nIf $\\text{Sublayer}(x_l)$ outputs are very small values, the residual connection mostly carries $x_l$ forward without significant updates, leading to vanishing gradients.\nIf $\\text{Sublayer}(x_l)$ has large activations, they keep adding up over layers. Although LayerNorm tries to regulate this after the accumulation, the activations have already influenced the residual pathway, potentially leading to exploding gradients.\nEssentially, LayerNorm in Post-Norm setups tries to fix activations at the last momentâ€”after instability has already propagated.\nTo address this, Xiong et al. [2] proposed Pre-Norm residual units, where LayerNorm is applied before the sublayer. This ensures each sublayer receives well-scaled inputs, improving gradient flow and training stability.\nMost modern Large Language Models, including GPT-2, have adopted this Pre-Norm approach. The key changes are:\nLayer Normalization is moved to the input of each sub-block, ensuring that each sublayer receives normalized inputs.\nAn additional Layer Normalization is added after the final self-attention block, further improving stability.\nWith these changes, the GPT-2 architecture now looks like this: GPT-2 architecture\nclass DecoderBlock(nn.Module): def __init__(self, cfg): super().__init__() self.ln_1 = LayerNorm(emb_dim=cfg[\u0026#39;emb_dim\u0026#39;]) self.attn = MultiHeadAttention(d_in=cfg[\u0026#39;emb_dim\u0026#39;], d_out=cfg[\u0026#39;emb_dim\u0026#39;], context_length=cfg[\u0026#39;context_length\u0026#39;], attn_pdrop=cfg[\u0026#39;attn_pdrop\u0026#39;], num_heads=cfg[\u0026#39;n_heads\u0026#39;], qkv_bias=cfg[\u0026#39;qkv_bias\u0026#39;]) self.ln_2 = LayerNorm(emb_dim=cfg[\u0026#39;emb_dim\u0026#39;]) self.mlp = MLP(emb_dim=cfg[\u0026#39;emb_dim\u0026#39;]) self.resid_dropout = nn.Dropout(cfg[\u0026#39;resid_pdrop\u0026#39;]) def forward(self, x): x = x + self.resid_dropout(self.attn(self.ln_1(x))) x = x + self.resid_dropout(self.mlp(self.ln_2(x))) return x And our GPT-2 model will be coded as:\nclass GPT2(nn.Module): def __init__(self, cfg): super().__init__() self.transformer = nn.ModuleDict(dict( wte = nn.Embedding(cfg[\u0026#39;vocab_size\u0026#39;], cfg[\u0026#39;emb_dim\u0026#39;]), # Token Embeddings wpe = nn.Embedding(cfg[\u0026#39;context_length\u0026#39;], cfg[\u0026#39;emb_dim\u0026#39;]), # Position Encoding embd_dropout = nn.Dropout(cfg[\u0026#39;embd_pdrop\u0026#39;]), # Embedding dropout h = nn.Sequential(*[DecoderBlock(cfg) for _ in range(cfg[\u0026#39;n_layers\u0026#39;])]), # Multiple Decoder blocks ln_f = LayerNorm(cfg[\u0026#39;emb_dim\u0026#39;]), # Final layernorm )) self.lm_head = nn.Linear(cfg[\u0026#39;emb_dim\u0026#39;], cfg[\u0026#39;vocab_size\u0026#39;], bias=False) # Language modelling head def forward(self, x): B, N = x.size() token_emb = self.transformer.wte(x) # output: (B, N, D) pos_emb = self.transformer.wpe(torch.arange(N, device=device)) # output: (N, D) x = token_emb + pos_emb # ouput: (B, N, D) x = self.transformer.h(x) x = self.transformer.ln_f(x) logits = self.lm_head(x) # ouput: (B, N, vocab_size) return logits The hyperparameters of our model are defined in a Python dictionary, cfg, which is passed when instantiating the model.\n# Define configuration dictionary GPT_CONFIG_124M = { \u0026#34;vocab_size\u0026#34;: 50257, # 50,000 BPE merges + 256 byte tokens + 1 \u0026lt;|endoftext|\u0026gt; token \u0026#34;context_length\u0026#34;: 1024, # Maximum sequence length \u0026#34;emb_dim\u0026#34;: 768, # Embedding dimension \u0026#34;n_heads\u0026#34;: 12, # Number of attention heads \u0026#34;n_layers\u0026#34;: 12, # Number of transformer blocks \u0026#34;attn_pdrop\u0026#34;: 0.1, # Dropout probability for attention layers \u0026#34;embd_pdrop\u0026#34;: 0.1, # Dropout probability for embeddings \u0026#34;resid_pdrop\u0026#34;: 0.1, # Dropout probability for residual connections \u0026#34;qkv_bias\u0026#34;: True # Whether to use bias in QKV projection } model = GPT2(GPT_CONFIG_124M).to(device) # Print the number of parameters in the model print(sum(p.numel() for p in model.parameters()) / 1e6, \u0026#39;M parameters\u0026#39;) 163.037184 M parameters Oops! Our model has 163M parameters, even though we aimed to replicate the 124M parameter version. What\u0026rsquo;s going on?\nActually, nothing is wrong! The discrepancy arises due to the weight tying scheme used in the GPT-2 model. Letâ€™s dive into it in more detail.\nWeight tying The first layer of our architecture is the token embedding table, which maps each token in our vocabulary to an embedding vector. The last layer is the language modeling head, which reverses this process by mapping the embeddings back to vocabulary space.\nIf we inspect our modelâ€™s state dictionary, we find that both layers have the same shape:\ngpt2_model_params = model.state_dict() print(gpt2_model_params[\u0026#34;transformer.wte.weight\u0026#34;].shape) print(gpt2_model_params[\u0026#34;lm_head.weight\u0026#34;].shape) torch.Size([50257, 768]) torch.Size([50257, 768]) This occurs because of how nn.Embedding and nn.Linear store their weights internally:\nnn.Embedding(dim_in, dim_out) stores its lookup table as torch.Size([dim_in, dim_out]). nn.Linear(features_in, features_out) stores its weight matrix as torch.Size([features_out, features_in]) to perform y = Wx operations. This observation led to the practice of weight tying, where the token embedding and language modeling head share the same set of weights. This approach was also used in the original Transformer paper.\nBy enforcing weight sharing, we inject an inductive bias into the model, indicating that:\nThe way tokens are mapped to embeddings should be similar to how embeddings are mapped back to tokens. The model should maintain consistency between input and output representations. If we visualize the parameter distribution in our model (with just one transformer block), we get: Parameter sizes of our model with one transformer block\nClearly, the majority of parameters reside in the token embeddings and the language modeling layer. By making them share weights, we reduce the overall parameter count, leading to lower memory usage and faster convergence speed since fewer parameters need to be learned.\nWe can enable weight tying by directly assigning the same weight tensor:\n# Weight tying model.transformer.wte.weight = model.lm_head.weight # Print the number of parameters in the model print(sum(p.numel() for p in model.parameters())/1e6, \u0026#39;M parameters\u0026#39;) 124.439808 M parameters Hurray! We now have our own GPT-2 124M model.\nNext, letâ€™s explore the weight initialization scheme used in GPT-2. This is useful if you want to train the model from scratch on your own dataset. However, if you\u0026rsquo;re only interested in loading pre-trained weights, feel free to skip this section.\nWeight initialization The GPT-2 paper doesnâ€™t explicitly mention the weight initialization used, but we can infer it from the model.py code. Hereâ€™s the initialization strategy:\nToken embeddings: Normal distribution with standard deviation of $0.02$. Position embeddings: Normal distribution with standard deviation of $0.01$. Linear layers: Weights: Normal distribution with standard deviation of $0.02$. Bias: Initialized to zero. LayerNorm: The scale and shift parameters are initialized to ones and zeros, respectively, similar to our implementation. Additionally, the GPT-2 paper mentions using a scaling factor of $1/\\sqrt{\\text{N}}$, where $\\text{N}$ is the number of residual layers, on the weights of residual layer at initialization. This is to account for the accumulation of activations on the residual path â€” the standard deviation of activations grows inside the residual stream due to repeated additions. However, the GPT-2 code doesnâ€™t seem to implement this scaling, so weâ€™ll skip it for now.\nHereâ€™s how we can implement the weight initialization:\n# Initialize weights as per GPT-2 def gpt2_init(m): if isinstance(m, nn.Linear): # Applies to linear layers only nn.init.normal_(m.weight, mean=0.0, std=0.02) if m.bias is not None: nn.init.zeros_(m.bias) # Bias initialized to zero elif isinstance(m, nn.Embedding): # Applied to embedding layer only nn.init.normal_(m.weight, mean=0.0, std=0.02) # Apply initialization model.apply(gpt2_init) Note: Iâ€™ve used a standard deviation of 0.02 for the position embeddings as well because the difference doesnâ€™t significantly impact the model, and it keeps the code simpler.\nLoading Weights To load weights from HuggingFace, we simply copy the tensor values over to our model. For this, the tensor shapes need to match exactly between the HuggingFace model and our custom model.\nLetâ€™s compare the state dictionaries of both models side by side to identify any mismatches in tensor shapes:\nfor name, param in model_hf_params.items(): if param.shape != gpt2_model_params[name].shape: print(f\u0026#34;Shape mismatch for parameter {name}: {param.shape} vs {gpt2_model_params[name].shape}\u0026#34;) Shape mismatch for parameter transformer.h.0.attn.c_attn.weight: torch.Size([768, 2304]) vs torch.Size([2304, 768]) Shape mismatch for parameter transformer.h.0.mlp.c_fc.weight: torch.Size([768, 3072]) vs torch.Size([3072, 768]) Shape mismatch for parameter transformer.h.0.mlp.c_proj.weight: torch.Size([3072, 768]) vs torch.Size([768, 3072]) Oops, why don\u0026rsquo;t the shapes match?\nOpenAIâ€™s GPT-2 checkpoints use a Conv1d module on each linear layer in the GPT-2 architecture. This is why the tensors are transposed and do not match directly.\nAs a result, the layers c_attn, c_proj, c_fc and c_proj need to be handled differently. We transpose the weights before copying them to ensure they match the expected shapes and are copied correctly.\nHereâ€™s how to load the weights:\n# Loading weights in your model transposed = [\u0026#39;attn.c_attn.weight\u0026#39;, \u0026#39;attn.c_proj.weight\u0026#39;, \u0026#39;mlp.c_fc.weight\u0026#39;, \u0026#39;mlp.c_proj.weight\u0026#39;] with torch.no_grad(): for name, param in model_hf_params.items(): # check if the parameter name matches if name in gpt2_model_params: # if the parameter has to be transposed if name.endswith(tuple(transposed)): gpt2_model_params[name].copy_(param.t()) # Tranpose the weights and then copy # if the parameter shape matches directly elif param.shape == gpt2_model_params[name].shape: gpt2_model_params[name].copy_(param) # copy the weights over else: print(f\u0026#34;Shape mismatch for parameter {name}: {param.shape} vs {gpt2_model_params[name].shape}\u0026#34;) else: print(f\u0026#34;Parameter {name} not found in your model\u0026#34;) print(\u0026#34;Weights are loaded successfully!\u0026#34;) This code will load the pre-trained HuggingFace weights into our model, handling special treatment for the linear layers to ensure that everything is correctly aligned.\nGenerating text Now that weâ€™ve successfully loaded the weights, it\u0026rsquo;s time to test our model by generating some text. Weâ€™ll use the tiktoken tokenizer to encode the initial prompt, \u0026ldquo;Hello, I\u0026rsquo;m a language model,\u0026rdquo; and pass it into the model to generate a continuation.\nHereâ€™s a basic text generation loop:\n# Generate text from the trained model max_new_tokens = 20 context_length = 1024 import tiktoken enc = tiktoken.get_encoding(\u0026#34;gpt2\u0026#34;) tokens = enc.encode(\u0026#34;Hello, I\u0026#39;m a language model,\u0026#34;) context = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0) # Adds the batch dimension: (B=1, N) stored_context = context for _ in range(max_new_tokens): model.eval() with torch.no_grad(): context = stored_context[:, -context_length:] # Trim the context to fit the model\u0026#39;s context length logits = model(context) # logits: (B=1, N, vocab_size) logits = logits[:, -1, :] # take just the last time step (B, vocab_size) probs = torch.softmax(logits, dim=-1) next_token = torch.argmax(probs, dim=-1. keepdim=True) # Select the most likely next token (B=1, N) stored_context = torch.cat((stored_context, next_token), dim=1) # (B=1, N+1) print(enc.decode(stored_context[0].tolist())) Hello, I\u0026#39;m a language model, not a programming language. I\u0026#39;m a language model. I\u0026#39;m a language model. I\u0026#39;m a The model gets stuck in a loop, generating repeated phrases like \u0026ldquo;I\u0026rsquo;m a language model.\u0026rdquo; This is because we are always selecting the token with the highest probability at each step, which limits the model\u0026rsquo;s creativity and causes repetition. To resolve this, we can explore probabilistic sampling methods.\nProbabilistic Sampling To introduce more variety and creativity in the decoding process, we replace the argmax function with multinomial. This method uses the probability distribution output by the model to sample the next token proportionally to its probability score:\nlogits = model(context) # logits: (B=1, N, vocab_size) logits = logits[:, -1, :] # take just the last time step (B, vocab_size) probs = torch.softmax(logits, dim=-1) next_token = torch.multinomial(probs, num_samples=1) # Sample from the distribution (B=1, N) By using probabilistic sampling, we can explore a range of potential next tokens, leading to more diverse and interesting text.\nWhile this is a good way to sample text, there are other decoding strategies that allow us to control the distribution and selection process to generate more original text.\nTemperature Scaling Letâ€™s understand temperature scaling through an example:\nprint(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)) print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])/0.001, dim=-1)) tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872]) tensor([0., 0., 0., 0., 1.]) When the magnitudes of logits are large, the softmax output saturates and converges to a one-hot encoding. Temperature scaling works similarlyâ€”by dividing the logits by a number greater than zero:\nlogits = model(context) logits = logits[:, -1, :] / temperature probs = torch.softmax(logits, dim=-1) next_token = torch.multinomial(probs, num_samples=1) Temperature scaling allows us to control the randomness of the output:\nTemperature \u0026lt; 1: Produces more confident (sharper) distributions, picking the most likely token almost always. Temperatures \u0026gt; 1: Results in a more uniformly distributed token probabilties, where other tokens are selected more often. This can add more variety but may also produce nonsensical text. Temperature = 1: This is equivalent to not using any temperature scaling. Top-k sampling In top-k sampling, we restrict the sampling process to the top-k most likely tokens and exclude the rest by masking their probabilities. This ensures that we avoid sampling very rare tokens while still providing some diversity in the output.\nWe achieve this by setting the logits of non-selected tokens to negative infinity, so that their softmax probabilities become zero, and the remaining probabilities sum to 1. The implementation is as follows:\ndef top_k_logits(logits, k): if k == 0: return logits # No truncation values, _ = torch.topk(logits, k=k) # Get top-k values min_value = values[:, -1] # Minimum value in top-k return torch.where(logits \u0026lt; min_value, torch.tensor(float(\u0026#39;-inf\u0026#39;)), logits) torch.topk retrieves the values of top-k logits in descending order, and the where function sets the logits of tokens below the lowest logit value to negative infinity. This ensures that only the top-k logits contribute to the probability distribution.\nTop-p (Nucleus Sampling) While top-k gives us the ability to select the top-k tokens to consider in the sampling process, top-p dynamically selects the top tokens whose cumulative probability exceeds a certain threshold, denoted by p. Instead of a fixed number k, it adapts based on the distribution.\nFor example, we first sort the tokens by probability:\nToken A: 0.40 Token B: 0.30 Token C: 0.20 Token D: 0.05 Token E: 0.05 Next, we compute the cumulative probability:\nToken A: 0.40 Token A + B: 0.70 Token A + B + C: 0.90 âœ… (stop here, because we reached p=0.9) Token A + B + C + D: 0.95 Token A + B + C + D + E: 1.00 We keep only tokens A, B, and C since their cumulative probability exceeds p=0.9. The rest are discarded.\nHereâ€™s how it is implemented in code:\ndef top_p_logits(logits, p): # Nucleus Sampling sorted_logits, _ = torch.sort(logits, dim=-1, descending=True) # Sort logits in descending order cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1) # Compute cumulative probabilities # Determine number of indices to include, keeping at least one num_to_keep = torch.clamp((cumulative_probs \u0026lt;= p).sum(dim=-1) - 1, min=0) min_value = sorted_logits[:, num_to_keep] return torch.where(logits \u0026lt; min_value, torch.tensor(float(\u0026#39;-inf\u0026#39;)), logits) Sampling code Now, letâ€™s integrate all the sampling strategies weâ€™ve discussed and implement a sample() function, similar to the one in GPT-2. You can find the original GPT-2 implementation here: https://github.com/openai/gpt-2/blob/master/src/sample.py\ndef sample(max_new_tokens, context_length, start_token=None, context=None, temperature=1, top_k=0, top_p=1): if start_token is None: assert context is not None, \u0026#39;Specify exactly one of start_token and context!\u0026#39; else: assert context is None, \u0026#39;Specify exactly one of start_token and context!\u0026#39; context = torch.full((1, 1), start_token, dtype=torch.long, device=device) stored_context = context for _ in range(max_new_tokens): model.eval() with torch.no_grad(): context = stored_context[:, -context_length:] logits = model(context) # logits: (B=1, N, vocab_size) logits = logits[:, -1, :] / temperature # Scale logits by temperature logits = top_k_logits(logits, k=top_k) # Apply top-k filtering logits = top_p_logits(logits, p=top_p) # Apply top-p (nucleus) sampling probs = torch.softmax(logits, dim=-1) # Convert logits to probabilities next_token = torch.multinomial(probs, num_samples=1) # Sample from the distribution (B=1, N) stored_context = torch.cat((stored_context, next_token), dim=1) # (B=1, N+1) return stored_context Using this function, we can start generating text either using a start token or an initial prompt.\ntokens = enc.encode(\u0026#34;Hello, I\u0026#39;m a language model,\u0026#34;) prompt = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0) # Adds the batch dimension(B=1, N) next_tokens = sample(context=prompt, max_new_tokens=20, context_length=GPT_CONFIG_124M[\u0026#34;context_length\u0026#34;], top_p=0.9, top_k=40, temperature=1.0) print(enc.decode(next_tokens[0].tolist())) Here\u0026rsquo;s the output:\nHello, I\u0026#39;m a language model, and I wanted to do something more powerful than just an English translation of an English sentence.\u0026#34; And with that, we\u0026rsquo;ve explored some of the most exciting text generation techniques that can bring out the true creativity of language models. Now, it\u0026rsquo;s your turn to experiment and create your own stories! Happy coding!\nReferences Andrej Karpathy\u0026rsquo;s video: Let\u0026rsquo;s reproduce GPT-2 (124M) [1] Radford et al., \u0026ldquo;Language Models are Unsupervised Multitask Learners\u0026rdquo;, OpenAI 2019. [2] Xiong et al., \u0026ldquo;On Layer Normalization in the Transformer Architecture\u0026rdquo;, ICML 2020. ","permalink":"https://yugajmera.github.io/posts/10-gpt2.2/post/","summary":"Building on our previous exploration of GPT-1, let\u0026rsquo;s now modify its architecture to recreate the GPT-2 [1] small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.\nOne major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning.","title":"Looking into GPT-2 Part 2: Architecture and Sampling"},{"content":"In this series of posts, we will explore the GPT-2 [1] architecture in detail. Unlike other GPT models, OpenAI has released the code for GPT-2, which allows us to investigate the model\u0026rsquo;s inner workings and see how each part of the architecture is implemented. You can find the official repository here: https://github.com/openai/gpt-2/tree/master\nGPT-2 was introduced in 2019, which, by the fast-paced standards of deep learning and LLM development, is considered a long time ago. However, more recent architectures, such as Meta\u0026rsquo;s LLaMA models, are still built on the same foundational concepts with only minor modifications. Hence, understanding GPT-2 remains as relevant as ever.\nIn this post, we will look at tokenization, one of the most crucial preprocessing steps in LLMs. Tokenization breaks raw text into smaller units called tokens, which can be words, subwords, or special characters. These unique tokens form a vocabulary, each mapped to a unique token ID. The model then uses an embedding table to convert token IDs into dense vector representations, which are fed into the neural network.\nWant to see how different tokenizers work across various LLMs? Try this interactive web app:Tiktoken web app.\nTokenization Different levels of tokenization.\nIn the last post, we used a character-level tokenizer, which splits input text into individual characters. While this approach may seem simple, when dealing with large training data, the number of tokens can grow significantly, making it time-consuming to process.\nA more intuitive approach is word-level tokenization, which splits text into words. This aligns well with human language processing but comes with a major drawbackâ€”it relies on a predefined vocabulary. What happens if a user enters a word that isn\u0026rsquo;t in the vocabulary? Since that word does not exist in our vocabulary, it has no corresponding token ID, the tokenizer throws a KeyError.\nA naive way to handle this is by introducing an \u0026lt;unk\u0026gt; (unknown) token to represent all out-of-vocabulary (OOV) words. However, this approach isn\u0026rsquo;t ideal because it loses information.\nA better solution is subword tokenization, which strikes a balance between character-level and word-level tokenization. It treats frequent words as whole tokens while breaking rare words into smaller subword units.\nOne widely used subword tokenization method is Byte Pair Encoding (BPE) [2], which was employed in GPT models. BPE efficiently decomposes words into subword units and individual characters, eliminating the need for a special unknown word token. Let\u0026rsquo;s dive into the BPE algorithm and see how it works in detail.\nByte pair encoding (BPE) The Byte Pair Encoding (BPE) algorithm builds a vocabulary by iteratively merging the most frequent character pairs into subwords and then merging frequent subwords into larger units (words). This process reduces the number of tokens required to represent an input sequence, thereby decreasing computational costs.\nSince transformer-based models like GPTs have a fixed context length, using BPE helps the model attend to longer sequences as they are now represented with a smaller number of tokens.\nThe Byte pair encoding algorithm builds vocabulary by iteratively merging frequent characters into subwords and frequent subwords into words. Merging repeatedly reduces the number of tokens with which an input sequence is represented with, reducing the computation.\nHow BPE works Initialize with single characters - The vocabulary starts with all individual characters (e.g., a, b, c, \u0026hellip;).\nMerge the most frequent pair - The algorithm identifies the most frequently occurring adjacent character pair in the dataset and merges it into a subword.\nFor example, t and h may frequently appear together in common words like \u0026ldquo;the\u0026rdquo;, \u0026ldquo;then\u0026rdquo;, or \u0026ldquo;therefore\u0026rdquo;, BPE merges them into the subword th. Create a new token - The newly merged subword (e.g., th) is added to the vocabulary and assigned a new token ID.\nRepeat - This process continues iteratively, merging the most common adjacent token pairs at each step. Over time, subwords like \u0026ldquo;ing\u0026rdquo;, \u0026ldquo;ly\u0026rdquo;, \u0026ldquo;tion\u0026rdquo;, etc., may emerge, allowing the vocabulary to represent common word segments efficiently.\nEach iteration compresses the dataset, representing it with fewer tokens while simultaneously expanding the vocabulary size by introducing new subwords.\nTrade-Off: Vocabulary Size vs. Sequence Length Vocabulary size is an important hyperparameter to consider, as it determines the size of the token embedding table and the final language modeling head. Increasing the vocabulary size has the following effects:\nModel Size: A larger vocabulary increases the number of parameters in the model, leading to higher memory and compute requirements.\nRisk of Underfitting: As the vocabulary grows, it may contain many rare or infrequent tokens that appear only a few times in the training data, making it difficult for the model to learn meaningful representations for these tokens.\nThus, a balance must be struck between sequence length and vocabulary size. The number of merges is a key hyperparameter that controls vocabulary size. For example, GPT-1 used 40,000 merges, while GPT-2 used 50,000 merges.\nCoding BPE Tokenizer Before we dive into coding the BPE tokenizer, itâ€™s essential to understand that a tokenizer is typically trained separately from the LLM (Large Language Model). The tokenizer is usually trained on a diverse and extensive dataset to learn an efficient subword vocabulary, enabling it to generalize effectively across various domains and languages.\nUnfortunately, the training code for the GPT-2 tokenizer hasn\u0026rsquo;t been released, and we only have the inference code for encoding and decoding text, which is available here: https://github.com/openai/gpt-2/blob/master/src/encoder.py\nTo run inference, we just need two files: vocab.bpe and encoder.json.\n#!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe #!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json import os, json with open(\u0026#39;encoder.json\u0026#39;, \u0026#39;r\u0026#39;) as f: encoder = json.load(f) with open(\u0026#39;vocab.bpe\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#34;utf-8\u0026#34;) as f: bpe_data = f.read() bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\u0026#39;\\n\u0026#39;)[1:-1]] Now, letâ€™s explore how we can train our own BPE tokenizer and generate similar files that can be stored and later used for inference.\nBits and Bytes In Python, text is represented as a string containing multiple characters. Each character is represented by a Unicode code point, which is an integer value. The ord() function eturns the Unicode code point of a character, and chr() converts a code point back to a character.\ntext = \u0026#34;Hello ðŸ‘‹\u0026#34; for t in text: print(f\u0026#34;{t} -\u0026gt; {ord(t)}\u0026#34;) H -\u0026gt; 72 e -\u0026gt; 101 l -\u0026gt; 108 l -\u0026gt; 108 o -\u0026gt; 111 -\u0026gt; 32 ðŸ‘‹ -\u0026gt; 128075 However, Unicode code points are not stored directly in memory as characters. Computers store text as bytes, and these Unicode characters must be encoded into byte streams before they can be processed. We use UTF-8 encoding, which converts each code point into a sequence of 1 to 4 bytes.\nUTF-8 Encoding Rules:\nCode points from 0â€“127 (Basic ASCII) â†’ Stored in 1 byte. Code points from 128â€“2047 â†’ Stored in 2 bytes. Code points from 2048â€“65535 â†’ Stored in 3 bytes Code points 65536+ â†’ Stored in 4 bytes. # Convert each character in the string to byte stream text = \u0026#34;Hello ðŸ‘‹\u0026#34; byte_arr = text.encode(\u0026#34;utf-8\u0026#34;) print(byte_arr) print(list(byte_arr)) b\u0026#39;Hello \\xf0\\x9f\\x91\\x8b\u0026#39; [72, 101, 108, 108, 111, 32, 240, 159, 145, 139] Hello remains [72, 101, 108, 108, 111, 32] (same as ASCII) ðŸ‘‹ becomes four bytes: [240, 159, 152, 138] (UTF-8 encoding). This would be a valid way to convert text directly into token IDs for the embedding layer of an LLM. However, this approach would represent each character with one to four integers, which would significantly increase the sequence length (10 token IDs for this small text in the example above).\nWhy not directly use Unicode code points (e.g., 128075 for ðŸ‘‹) directly instead of byte streams? This would require us to use the entire space of Unicode symbols to model all Unicode strings, resulting in a base vocabulary of over 130,000, before even adding any multi-symbol tokens.\nTo decode these tokens back to text, we use the decode function.\ntokens = [72, 101, 108, 108, 111, 32, 240, 159, 145, 139] text = bytes(tokens).decode(\u0026#39;utf-8\u0026#39;) print(text) Hello ðŸ‘‹ GPT-2 uses BPE on UTF-8 byte sequences. Since a byte consists of 8 bits, there are $2^8 = 256$ possible values that a single byte can represent, ranging from 0 to 255. UTF-8 can use up to 4 bytes for one character, supporting a wide range of characters from different languages and symbols.\nThus, we can represent any text using this byte vocabulary, which consists of one to four tokens for each character in the text. As a result, the byte-level version of BPE only requires a base vocabulary of 256 tokens.\nThe BPE tokenizer starts with the first 256 byte values (single-byte tokens) in the initial vocabulary.\nvocab = {idx:bytes([idx]) for idx in range(256)} # token ids:utf-8 bytes Our vocabulary is a dictionary that maps token IDs to UTF-8 bytes, which will be helpful for decoding tokens directly back to text.\nHowever, GPT-2\u0026rsquo;s approach is slightly more complex. The encoder.json file contains a dictionary that maps Unicode characters to token IDs. For decoding, it flips the key-value mapping and uses a special function, bytes_to_unicode(), to map Unicode characters to UTF-8 bytes. This process is quite cumbersome, and I prefer to handle it all in one go.\nBigram shrinking The next step is to identify the most frequent pairs of integers (called bigrams) in the entire byte stream, and replace those pairs with a new token ID (e.g., 256), appending it to our vocabulary.\n# Convert text to utf-8 byte tokens text = \u0026#34;the cat in the hat\u0026#34; tokens = list(text.encode(\u0026#34;utf-8\u0026#34;)) print(tokens) # Compute frequency of each bigram occuring in tokens list def get_pairs(tokens): counts = {} for i in range(len(tokens) - 1): counts[(tokens[i], tokens[i+1])] = counts.get((tokens[i], tokens[i+1]), 0) + 1 return counts # Replace the bigram with a new id \u0026amp; return the modified tokens list def merge(tokens, bigram, new_id): new_tokens = [] i = 0 while i \u0026lt; len(tokens): if i \u0026lt; len(tokens)-1 and (tokens[i], tokens[i+1]) == bigram: new_tokens.append(new_id) i += 2 else: new_tokens.append(tokens[i]) i += 1 return new_tokens [116, 104, 101, 32, 99, 97, 116, 32, 105, 110, 32, 116, 104, 101, 32, 104, 97, 116] Performing the merge operation once gives us:\n# Run this cell multiple times with new token IDs pairs = get_pairs(tokens) most_freq_bigram = max(pairs, key=pairs.get) print(\u0026#34;Most frequent bigram:\u0026#34;, most_freq_bigram) tokens = merge(tokens, most_freq_bigram, 256) print(tokens) Iteration 1\nMost frequent bigram: (116, 104) [256, 101, 32, 99, 97, 116, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116] Input text: the cat in the hat Most frequent bigram (116, 104) corresponds to characters th Replace it with a new token ID that is already not is use 256 New byte stream: \u0026lt;256\u0026gt;e cat in \u0026lt;256\u0026gt;e hat Iteration 2\nMost frequent bigram: (256, 101) [257, 32, 99, 97, 116, 32, 105, 110, 32, 257, 32, 104, 97, 116] Input text: \u0026lt;256\u0026gt;e cat in \u0026lt;256\u0026gt;e hat Most frequent bigram (256, 101) corresponds to characters \u0026lt;256\u0026gt;e. Replace it with a new token ID that is already not is use 257 New byte stream: \u0026lt;257\u0026gt; cat in \u0026lt;257\u0026gt; hat Iteration 3\nMost frequent bigram: (257, 32) [258, 99, 97, 116, 32, 105, 110, 32, 258, 104, 97, 116] Input text: \u0026lt;257\u0026gt; cat in \u0026lt;257\u0026gt; hat Most frequent bigram (256, 32) corresponds to characters \u0026lt;256\u0026gt; (and space). Replace it with a new token ID that is already not is use 258 New byte stream: \u0026lt;258\u0026gt;cat in \u0026lt;258\u0026gt;hat The updated vocabulary might look something like this:\n... 256: \u0026#34;th\u0026#34; 257: \u0026#34;\u0026lt;256\u0026gt;e\u0026#34; 258: \u0026#34;\u0026lt;257\u0026gt; \u0026#34; Our vocabulary begins with 256 single-character tokens and grows by one with each merge. We repeat this process for multiple iterations until we achieve a manageable vocabulary size.\nTraining the tokenizer Now that we understand the BPE algorithm, let\u0026rsquo;s train it on the Tiny Shakespeare dataset (from the last post) to obtain a vocabulary set.\n# Convert text to byte stream with open(\u0026#39;input.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: text = f.read() tokens = list(text.encode(\u0026#34;utf-8\u0026#34;)) initial_length = len(tokens) # Hyperparameter vocab_size = 276 # desired final vocabulary size num_merges = vocab_size - 256 vocab = {idx:bytes([idx]) for idx in range(256)} # token ids:utf-8 bytes bpe_merges = [] # keeps a track of all merges for i in range(num_merges): pairs = get_pairs(tokens) most_freq_bigram = max(pairs, key=pairs.get) new_token_id = i + 256 # Merge the most frequent bigram tokens = merge(tokens, most_freq_bigram, new_token_id) bpe_merges.append(most_freq_bigram) # Update the vocabulary with the new merged token vocab[new_token_id] = vocab[most_freq_bigram[0]] + vocab[most_freq_bigram[1]] final_length = len(tokens) print(f\u0026#34;Compression ratio after merging: {initial_length / final_length:.2f}X\u0026#34;) Compression ratio after merging: 1.26X Running this cell trains our tokenizer. We were able to achieve a compression ratio of 1.26 of our text after only 20 merges.\nThe two main variables needed for inference are vocab and bpe_merges. You can store them if you wish for future use.\nvocab contains the final vocabulary set, with token IDs mapped to byte sequences. bpe_merges is a list that stores UTF-8 byte token bigrams. This is equivalent to the vocab.bpe file used in GPT-2 code, which contains Unicode character bigrams. After the merges, we have the updated vocabulary, which can be leveraged to tokenize text during inference efficiently. Letâ€™s take a look at our updated vocabulary:\n... 272: b\u0026#39;ar\u0026#39; 273: b\u0026#39; th\u0026#39; 274: b\u0026#39;on\u0026#39; 275: b\u0026#39;ll\u0026#39; Encoding and Decoding To decode input tokens, we can directly reference them using our vocab dictionary and obtain the corresponding UTF-8 byte sequences. After this, we use the .decode() function to convert the byte sequence back into raw text.\n# Convert token ids back to string (text) def decode(tokens): byte_arr = b\u0026#34;\u0026#34;.join(vocab[idx] for idx in tokens) text = byte_arr.decode(\u0026#39;utf-8\u0026#39;, errors=\u0026#34;replace\u0026#34;) return text The errors=\u0026quot;replace\u0026quot; parameter ensures that if the byte sequence contains invalid or unknown byte values that can\u0026rsquo;t be decoded, Python will replace those bytes with a replacement character (usually \u0026lsquo;ï¿½\u0026rsquo;) instead of raising an error. This avoids interrupting the decoding process.\nAs mentioned earlier, Unicode code points from 0â€“127 (ASCII range) are stored in a single byte. However, code points beyond 127 are represented by multi-byte sequences, where each byte can range from 0â€“255. Thus, bytes between 128 and 255 are never stored aloneâ€”they must be part of a multi-byte sequence.\nIf we attempt to decode a byte like 128 in isolation, it would throw an error because it\u0026rsquo;s an invalid byte when considered alone in UTF-8 encoding. Python will handle this gracefully by replacing such invalid byte sequences with the replacement character \u0026lsquo;ï¿½\u0026rsquo; rather than raising an error.\nLet\u0026rsquo;s test it:\nprint(decode([65, 275, 270, 128])) Allenï¿½ For encoding the input text, we first convert the characters into UTF-8 byte tokens. The bpe_merges list contains all the merge operations we need to perform, and it should be in the same order as when the list was created. Therefore, we create a bpe_ranks dictionary that ranks each merge based on their priority.\nWe then search for pairs in our text, merging the pair with the lowest rank first. We continue merging until we either reduce the token length to less than 2 or exhaust all matching pairs.\n# Convert text to token ids def encode(text): tokens = list(text.encode(\u0026#34;utf-8\u0026#34;)) bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges)))) while len(tokens) \u0026gt; 1: # Ensure we have at least 2 tokens to merge pairs = get_pairs(tokens) # Get all pairs # Choose the bigram that matches in bpe_merges with the lowest rank bigram = min(pairs, key=lambda pair: bpe_ranks.get(pair, float(\u0026#39;inf\u0026#39;))) # When there are no bigrams that exists in bpe_merges, it returns the first pair by default if bigram not in bpe_ranks: break # Merge the selected bigram and create a new merged token ID merged_token_id = bpe_ranks[bigram] + 256 tokens = merge(tokens, bigram, merged_token_id) return tokens Testing our encoding function:\ntokens = encode(\u0026#34;Hello ðŸ‘‹\u0026#34;) print(tokens) print(decode(tokens)) [72, 101, 275, 269, 240, 159, 145, 139] Hello ðŸ‘‹ Regex Patterns for improved Tokenization The authors of GPT-2 observed that directly applying BPE to byte sequences resulted in suboptimal merges due to the greedy frequency-based merging algorithm. For example, they noticed that multiple versions of common words (e.g., dog., dog?, and dog! for the word dog) were merged, which results in inefficient usage of limited vocabulary slots.\nTo overcome this, they employed a forced split of the text according to specific patterns using regular expressions (regex). This ensures that merges occur only within specific text chunks, thus avoiding unwanted merges across character categories (e.g., merging punctuation with words).\nHereâ€™s how we can use regex to split the text:\nimport regex as re gpt2_pat = re.compile(r\u0026#34;\u0026#34;\u0026#34;\u0026#39;s|\u0026#39;t|\u0026#39;re|\u0026#39;ve|\u0026#39;m|\u0026#39;ll|\u0026#39;d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\u0026#34;\u0026#34;\u0026#34;) print(re.findall(gpt2_pat, \u0026#34;Hello world 123, how\u0026#39;re you!?\u0026#34;)) [\u0026#39;Hello\u0026#39;, \u0026#39; world\u0026#39;, \u0026#39; 123\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39; how\u0026#39;, \u0026#34;\u0026#39;re\u0026#34;, \u0026#39; you\u0026#39;, \u0026#39;!?\u0026#39;] Instead of applying the BPE algorithm to the entire string at once, we first split the text into chunks according to the regular expression pattern. Then, we run the BPE algorithm within each individual chunk, ensuring that we achieve better tokenization without suboptimal merges.\nUsing the Tiktoken library In practice, I highly recommend using the tiktoken library from OpenAI for tokenization inference as it is optimized for efficiency. Although it does not include a training loop, has already been trained by OpenAI, we can directly use it for tokenization tasks.\nHereâ€™s how you can use it for encoding a string:\nimport tiktoken enc = tiktoken.get_encoding(\u0026#34;gpt2\u0026#34;) print(enc.encode(\u0026#34;Hello World!\u0026#34;)) [15496, 2159, 0] The output shows the token IDs corresponding to the text \u0026ldquo;Hello World!\u0026rdquo; according to the GPT-2 tokenizer.\nNext, let\u0026rsquo;s encode the entire Shakespeare dataset to examine the compression ratio of the trained GPT-2 tokenizer.\n# Convert text to byte stream with open(\u0026#39;input.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: text = f.read() tokens = list(text.encode(\u0026#34;utf-8\u0026#34;)) initial_length = len(tokens) tokens = enc.encode(text) final_length = len(tokens) print(f\u0026#34;Compression ratio of GPT-2: {initial_length / final_length:.2f}X\u0026#34;) Compression ratio of GPT-2: 3.30X Special Context Tokens In GPT-like large language models, it\u0026rsquo;s common to use special tokens to help manage the flow and structure of the data.\nEnd-of-Text token in GPT-2 vocabulary One such token is the \u0026lt;|endoftext|\u0026gt; token, which is used to separate two unrelated texts, especially when multiple independent documents or books are concatenated for training. This helps the model understand that even though the texts are joined together, they are separate and distinct.\nGPT-2â€™s vocabulary includes:\n256 raw byte tokens, 50,000 merged tokens, and 1 \u0026lt;|endoftext|\u0026gt; token. This brings the total vocabulary size to 50,257 tokens.\nHereâ€™s an example from the tiktoken web app:\nScreenshot from tiktoken webapp. Note that the \u0026lt;|endoftext|\u0026gt; token is the last token in the vocabulary corresponding to token id 50256.\nFine-Tuned Conversational Models Additionally, fine-tuned models, especially those used for conversation-based tasks, often introduce new special tokens to track the flow of conversation between a user and an assistant. These tokens help to distinguish different parts of the dialogue and maintain coherence. Some of these tokens include:\n\u0026lt;|im_start|\u0026gt;: Marks the start of an imaginary monologue. user/assistant: Used to denote the speaker (either the user or the assistant). \u0026lt;|im_sep|\u0026gt;: A separator token used to separate different sections or turns in the conversation. \u0026lt;|im_end|\u0026gt;: Marks the end of the monologue or conversation. These tokens are introduced during the fine-tuning phase when the model is trained on a conversational dataset. They can appear in sequences like this:\n\u0026lt;|im_start|\u0026gt; # marks the start of an assistant\u0026#39;s monologue \u0026lt;|user|\u0026gt; How are you? \u0026lt;|im_sep|\u0026gt; \u0026lt;|assistant|\u0026gt; I\u0026#39;m doing great! How can I assist you today? \u0026lt;|im_end|\u0026gt; # marks the end of the assistant\u0026#39;s monologue Hereâ€™s another example from the tiktoken web app showing these tokens:\nScreenshot from tiktoken webapp showing special tokens used in gpt3.5-turbo conversational model.\nIn practice, these tokens are essential for managing structured dialogues, especially in use cases like chatbots or virtual assistants, where knowing the flow of conversation and who is \u0026ldquo;speaking\u0026rdquo; is crucial.\nReferences Sebastian Rashcka\u0026rsquo;s blog: Implementing A Byte Pair Encoding (BPE) Tokenizer From Scratch Andrej Karpathy\u0026rsquo;s video: Let\u0026rsquo;s build the GPT Tokenizer [1] Radford et al., \u0026ldquo;Language Models are Unsupervised Multitask Learners\u0026rdquo;, OpenAI 2019. [2] Sennrich et al., \u0026ldquo;Neural Machine Translation of Rare Words with Subword Units\u0026rdquo;, ACL 2016. ","permalink":"https://yugajmera.github.io/posts/09-gpt2.1/post/","summary":"In this series of posts, we will explore the GPT-2 [1] architecture in detail. Unlike other GPT models, OpenAI has released the code for GPT-2, which allows us to investigate the model\u0026rsquo;s inner workings and see how each part of the architecture is implemented. You can find the official repository here: https://github.com/openai/gpt-2/tree/master\nGPT-2 was introduced in 2019, which, by the fast-paced standards of deep learning and LLM development, is considered a long time ago.","title":"Looking into GPT-2 Part 1: BPE Tokenizer"},{"content":"By now, you\u0026rsquo;ve probably used OpenAI\u0026rsquo;s ChatGPTâ€”a chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.\nChatGPT interface\nThe unprecedented success of ChatGPT fueled further research into the technology behind itâ€”Large Language Models (LLMs). Over the past two posts, weâ€™ve built the theoretical foundation, and now it\u0026rsquo;s time to get hands-on: coding a GPT-like LLM from the ground up.\nLarge Language Models (LLMs) An LLM is a neural network designed to understand, generate, and interpret human language. As the name suggests, LLMs are simply language models with an extremely large number of parameters (billions or even trillions) trained on vast datasetsâ€”potentially encompassing all publicly available internet text.\nWhile language modeling has been an area of research for decades (with RNNs, LSTMs, and Attention mechanisms), the introduction of Transformers in 2017 revolutionized the field.\nEvolution of language models\nWith Transformers proving their effectiveness, the next logical step was scaling. This movement gained traction with Google\u0026rsquo;s BERT and OpenAIâ€™s GPT models in 2018. These two architectures define the major types of LLMs we see today.\nTypes of LLMs Recalling the architecture of Transformers from the last post, it consists of two components: the encoder and the decoder. It was proposed as a replacement for the encoder-decoder structure based on RNN + Attention for Seq2Seq learning, where the encoder reads the input text and the decoder produces predictions for the task.\nThe Transformer Architecture.\nAlthough the encoder-decoder architecture may seem natural for machine translation tasks (for which it was originally developed), it is not always necessary.\nRepresentation Models: Encoder-Only Models First school of thought: \u0026ldquo;Is a Decoder Necessary if I Only Want to Perform Data-to-Numbers Conversion?\u0026rdquo;\nFor example, in text classification tasks like sentiment analysis and spam detection or regression tasks like stock price prediction, where the goal is simply to convert data into categories or numerical values, the decoder can be omitted.\nA good example of this approach is Bidirectional Encoder Representations from Transformers (BERT)[1], an encoder-only Transformer model that uses bidirectional processing to understand the context and relationships of the input data.\nThe input is concatenated with a special token [CLS] (stands for classification) at the beginning of a sentence. Using the self-attention mechanism, this token aggregates information from all words, capturing the overall meaning of a sentence. The hidden state representation corresponding to it is then fed into an output layer for classification.\nBERT for classification and regression tasks.\nGenerative Models: Decoder-Only Models Second school of thought: \u0026ldquo;Is an Encoder Necessary for Language Generation?\u0026rdquo;\nTasks like translation, summarization, and Q\u0026amp;A involve transforming an input sequence into an output sequence. Traditionally, this process has been handled using a model composed of an encoder for understanding the input and a decoder for generating the output.\nHowever, this process can be reframedâ€”what if the input and output are treated as one continuous sequence? For example, a machine translation taskâ€”\u0026ldquo;we are eating bread\u0026rdquo; in English to \u0026ldquo;estamos comiendo pan\u0026rdquo; in Spanishâ€”can be reframed as a language generation task: \u0026ldquo;Translate English to Spanish: we are eating bread.\u0026rdquo;\nA language generation or text completion task is an autoregressive problem, which can be handled using a decoder-only model. One such example is the Generative Pre-trained Transformer (GPT)[2].\nGPT-1 Architecture.\nSince the decoder contains a \u0026ldquo;masked\u0026rdquo; self-attention layer, it attends only to previously generated tokens or the available context, allowing it to generate coherent sequences that follow the context.\nChronologically, GPT was first introduced in early 2018, followed by BERT later that year. Both architectures were implemented with distinct purposes but achieved great success due to their similar training approach.\nTraining LLMs LLMs are trained in two stages using a semi-supervised learning procedure that combines unsupervised pre-training and supervised fine-tuning.\nTraining stages of an LLM.\n1. Unsupervised Pre-training In this first stage, an LLM is trained on vast amounts of raw, unlabeled data available on the internet. This allows the model to acquire broad world knowledge and develop an understanding of language semantics, including grammar and sentence structures.\nThis task-agnostic model is often referred to as a base model or a foundation model.\nBERT pre-training uses the masked language model (MLM) objective, where random tokens in the input are masked, and the model is trained to predict them based on the surrounding context. This unique training strategy (masked word prediction) makes such models well-suited for text classification tasks.\nFor generative models like GPT, the language modeling objective is usedâ€”given a sequence of tokens, the model predicts the next token in the sequence, a simple next-word prediction task. This approach helps the model learn how words and phrases fit together naturally, making it capable of generating coherent and contextually relevant text based on the given context.\nSince these objectives do not require labeled data, they enable training on massive, unlabeled text datasets. To capture a wide range of knowledge, the training data must be as diverse as possible.\nKey points:\nRequires large datasetsâ€”pre-training involves downloading and processing vast internet text data. High computational cost and time-intensive (very expensive). Compresses internet knowledge and models language semantics. A trained GPT model functions as a text completer. 2. Supervised Fine-Tuning (SFT) The second stage involves adapting the pre-trained model to specific tasks using labeled data. Since task-specific datasets require manual annotation, they are significantly smaller compared to the massive datasets used in pre-training. This semi-supervised approach enables LLMs to adapt effectively to new tasks with relatively small amounts of labeled data.\nFine-tuning can be categorized into two types:\nClassification Fine-Tuning: Labeled data consists of text samples and associated class labels (e.g., emails labeled as \u0026ldquo;spam\u0026rdquo; or \u0026ldquo;not spam\u0026rdquo;).\nInstruction Fine-Tuning: Labeled data consists of instruction-response pairs (e.g., \u0026ldquo;Translate this sentence into Spanish: we are eating bread\u0026rdquo; â†’ \u0026ldquo;estamos comiendo pan\u0026rdquo;).\nKey points:\nComputationally cheaper than pre-training Requires less data (fine-tuned on a narrower, manually labeled dataset). Tailors the LLM to a specific task or domain. Open LLMs Organizations developing open LLMs often share model weights and architectures with the public. Examples include Cohereâ€™s Command R, Mistral models, Microsoftâ€™s Phi, and Metaâ€™s Llama models.\nThese organizations typically release two types of models:\nBase Models: Unrefined models primarily used for fine-tuning, saving the cost of pre-training. These models function as text generators, producing content based on internet-derived knowledge.\nAssistant Models: Pre-fine-tuned models optimized for specific tasks like Q\u0026amp;A, conversations, and assistance. They are often labeled as \u0026ldquo;Instruct\u0026rdquo; or \u0026ldquo;SFT\u0026rdquo; (Supervised Fine-Tuned) models.\nCoding GPT-1 from scratch This section demonstrates how to pre-train a small-scale GPT-1 model for educational purposes. Large-scale pre-training requires significant computational resources (GPT-3 pretraining cost is estimated at $4.6 million), so the community typically uses pre-trained base models.\nWe\u0026rsquo;ll train a character-level GPT-1 model using the Tiny Shakespeare dataset, containing 40,000 lines from Shakespeare\u0026rsquo;s plays. The model will generate text character by character in an unsupervised setting, learning through next-character prediction.\nIn order to use text as input to our LLM, we first split it into individual characters, convert each character into integer tokens, and then transform these tokens into embedding vectors. These embeddings serve as numerical representations of the text, making it suitable for neural network processing.\nData pre-processing First, we import necessary libraries and select the appropriate device.\n# Import functions import torch import torch.nn as nn device = \u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39; print(\u0026#34;Using\u0026#34;, device) Next, we download and inspect the dataset. The dataset contains 1.1 million characters.\n# Download the tiny shakespeare dataset !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt with open(\u0026#39;input.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: text = f.read() print(\u0026#34;Length of the dataset:\u0026#34;, len(text)) print(text[:100]) Length of the dataset: 1115394 First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: Yo Tokenization Tokenization is the process of breaking text into smaller units called tokens. These tokens can be individual words, characters, or special symbols. For example:\nInput: \u0026quot;Hello, world. Is this a test?\u0026quot; After tokenization: ['Hello', ',', 'world', '.', 'Is', 'this', 'a', 'test', '?'] For our character-level model, each character itself will be treated as a token, meaning no further splitting is required. We first create a vocabulary, which is the set of all unique characters in our dataset. This defines all possible tokens our model can process as input and generate as output.\n# Create vocabulary all_chars = sorted(list(set(text))) print(\u0026#34;All characters that occur in the dataset:\u0026#34;, \u0026#39;\u0026#39;.join(all_chars)) vocab_size = len(all_chars) print(\u0026#34;Vocab size (number of unique characters in the dataset):\u0026#34;, vocab_size) All characters that occur in the dataset: !$\u0026amp;\u0026#39;,-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz Vocab size (number of unique characters in the dataset): 65 We do not convert all text to lowercase because capitalization helps the model distinguish between proper and common nouns, understand sentence structure, and generate correctly capitalized text.\nNext, we map each character in our vocabulary to an integer token ID (ranging from $0$ to $64$). This mapping allows us to later convert token IDs into embedding vectors.\n# Tokenize each character - convert the chars to ints char_to_int = {all_chars[i]:i for i in range(len(all_chars))} int_to_char = {i:all_chars[i] for i in range(len(all_chars))} encode = lambda s: [char_to_int[i] for i in s] decode = lambda s: [int_to_char[i] for i in s] # Tokenize entire dataset data = torch.tensor(encode(text)) print(\u0026#34;Tokenized first 10 characters:\u0026#34;) print(data[:10].tolist()) Tokenized first 10 characters: [18, 47, 56, 57, 58, 1, 15, 47, 58, 47] The encode function maps characters to token IDs, while decode reverses the process to reconstruct text.\nIn summary,\nTokenization breaks down the input text (training data) into individual tokens. We build a vocabulary out of all unique tokens. Each token is mapped to a unique interger ID. Create batches The next step is to generate input-target pairs for training our language model.\nSince training on the entire dataset at once is computationally expensive, we sample small chunks of the dataset (called block data) and train on them instead. The maximum size of these chunks is fixed for each task and is referred to as the block size or context length.\nThe context length is a vital parameter in LLMs, as it determines the maximum number of tokens the model can process in a single pass. A larger context window allows the model to capture longer dependencies and even process entire documents, but it also comes with increased computational cost.\nLet\u0026rsquo;s take an example to understand this better. It is intuitive to assume that $\\text{x}$ represents the input tokens, while $\\text{y}$ contains the target tokens, which are simply the inputs shifted by one position. This setup aligns with the next-token prediction task that our model will be trained on.\nblock_size = 8 # what is the maximum context length for predictions? block_data = data[:block_size+1] x = data[:block_size] y = data[1:block_size+1] print(\u0026#34;Training dataset chunck:\u0026#34;, block_data.tolist()) print(\u0026#34;x:\u0026#34;, x.tolist()) print(\u0026#34;y:\u0026#34;, y.tolist()) # How training occurs context = [] for i in range(block_size): context.append(x[i].item()) target = y[i] print(f\u0026#34;Context: {context}, Target: {target}\u0026#34;) Training dataset chunck: [18, 47, 56, 57, 58, 1, 15, 47, 58] x: [18, 47, 56, 57, 58, 1, 15, 47] y: [47, 56, 57, 58, 1, 15, 47, 58] Context: [18], Target: 47 Context: [18, 47], Target: 56 Context: [18, 47, 56], Target: 57 Context: [18, 47, 56, 57], Target: 58 Context: [18, 47, 56, 57, 58], Target: 1 Context: [18, 47, 56, 57, 58, 1], Target: 15 Context: [18, 47, 56, 57, 58, 1, 15], Target: 47 Context: [18, 47, 56, 57, 58, 1, 15, 47], Target: 58 Here, the model iteratively builds context from previous tokens. At each step, it predicts the next token based on what it has seen so far.\nThis sliding context window ensures that:\nThe model is trained on varied-length inputs. It generalizes well to different sequence lengths. Now that we have understood the concept of block size, let\u0026rsquo;s create batches.\n# Build the data loader (train/val split) n = int(0.9*len(data)) train_data = data[:n] val_data = data[n:] def make_batch(split, block_size, batch_size): data = train_data if split == \u0026#34;train\u0026#34; else val_data idx = torch.randint(len(data) - block_size, (batch_size,)) x, y = [], [] for i in idx: x.append(data[i:block_size+i]) y.append(data[i+1:block_size+i+1]) return torch.stack(x), torch.stack(y) In practice, input text can be longer than the modelâ€™s supported context length. In such cases, we truncate the text, keeping only the most recent tokens up to the maximum length.\nModel Architecture Letâ€™s break down the key components of the GPT-1 architecture and implement a bare-bones version of the model:\nToken Embeddings: Converts integer token IDs from the vocabulary into a $\\text{D}$-dimensional embedding vector using a simple lookup table. In our case, each character token in the input sequence is represented as a vector.\nPositional Encoding: Since transformers do not inherently understand the order of tokens, we add a $\\text{D}$-dimensional positional encoding to each token. GPT models use a learnable lookup table for this purpose.\nInput vectors: The final input vector is obtained by summing the token embeddings and positional encodings. This results in an input tensor of shape $\\text{N} \\times \\text{D}$, where $\\text{N}$ is the number of tokens (up to the context length) and $\\text{D}$ is the embedding dimension.\nModel Architecture (Decoder-Only Transformer):\nThe model consists of $\\text{L}$ transformer blocks, each with: Masked Multi-Head Self-Attention â†’ Layer Norm â†’ MLP â†’ Layer Norm A fully connected layer (language modeling head) that projects the $\\text{D}$-dimensional output back into the vocabulary space. A softmax layer that converts these logits into probabilities for the next-token prediction. GPT-1 architecture\nMasked Multi-head Self-attention layer This follows the same mechanism as in the previous post, so I wonâ€™t go into detail here.\nclass MultiHeadAttention(nn.Module): def __init__(self, d_in, d_out, context_length, attn_pdrop, num_heads, qkv_bias=False): super().__init__() self.W_qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias) self.W_o = nn.Linear(d_out, d_out) self.attn_dropout = nn.Dropout(attn_pdrop) self.d_h = d_out // num_heads # head dimension self.num_heads = num_heads self.register_buffer(\u0026#34;masked\u0026#34;, torch.tril(torch.ones(context_length, context_length)).view(1, 1, context_length, context_length)) def forward(self, x): # Input vectors x - (B, N, d_in) B, N, d_in = x.shape # Obtain keys, values and queries in one go - (B, N, d_out) qkv = self.W_qkv(x) queries, keys, values = qkv.chunk(3, dim=-1) # Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h) k = keys.view(B, N, self.num_heads, self.d_h).transpose(1, 2) v = values.view(B, N, self.num_heads, self.d_h).transpose(1, 2) q = queries.view(B, N, self.num_heads, self.d_h).transpose(1, 2) # Apply scaled dot-product attention with causal mask on each head attn_scores = (q @ k.transpose(2, 3)) * k.shape[-1]**-0.5 attn_scores = attn_scores.masked_fill(self.masked[:, :, :N, :N] == 0, float(\u0026#39;-inf\u0026#39;)) attn_weights = torch.softmax(attn_scores, dim=-1) attn_weights = self.attn_dropout(attn_weights) context_vec = attn_weights @ v # Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out) out = context_vec.transpose(1, 2).contiguous().view(B, N, -1) out = self.W_o(out) return out The only change I have made here is that I use a single linear layer to obtain the key, value, and query matrices in one go. Instead of applying three separate linear layers, we can use this trick to project the input into a single tensor with three times the output dimension and then split it along the last dimension. This reduces the number of matrix multiplications, making the computation more efficient.\nLayer Normalization As you might recall, layer normalization improves the stability and efficiency of training. The input to this layer, $\\text{x}$, has a shape of $\\text{B} \\times \\text{N} \\times \\text{D}$ = [batch size, number of tokens, embedding dimension], and normalization is performed across the embedding dimension.\nclass LayerNorm(nn.Module): def __init__(self, emb_dim): super().__init__() self.eps = 1e-5 self.scale = nn.Parameter(torch.ones(emb_dim)) self.shift = nn.Parameter(torch.zeros(emb_dim)) def forward(self, x): mean = x.mean(dim=-1, keepdim=True) var = x.var(dim=-1, keepdim=True, unbiased=False) # Used biased variance estimation (without Bessel\u0026#39;s correction) norm_x = (x - mean) / torch.sqrt(var + self.eps) return self.scale * norm_x + self.shift The variable eps is a small constant added to the variance to prevent division by zero. scale and shift are two trainable parameters that allow the model to learn optimal transformations. We set unbiased=False, which means we divide by $n$ instead of $n-1$, resulting in a slightly biased variance estimate (i.e., not using Bessel\u0026rsquo;s correction), as done in GPT models. However, this has negligible impact when the embedding dimension is large. Feed forward layer Next, we will implement a small neural network used as a part of the transformer block in LLMs.\nHistorically, the ReLU activation function has been widely used due to its simplicity and effectiveness. However, in modern LLMs, the Gaussian Error Linear Unit (GELU) is preferred for its improved performance.\nGELU vs ReLU activation functions\nGELU can be thought of as a smoother version of ReLU. Its smooth transitions allow for better optimization properties during training, leading to more nuanced parameter adjustments.\nclass GELU(nn.Module): def __init__(self): super().__init__() def forward(self, x): return 0.5 * x * (1 + torch.tanh( torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)) )) class MLP(nn.Module): def __init__(self, emb_dim): super().__init__() self.layer1 = nn.Linear(emb_dim, 4 * emb_dim) self.layer2 = nn.Linear(4 * emb_dim, emb_dim) self.gelu = GELU() def forward(self, x): x = self.gelu(self.layer1(x)) x = self.layer2(x) return x Decoder block Now, we assemble all the components weâ€™ve built so far into a Transformer decoder block. This block forms the foundation of GPT models and is repeated multiple times throughout the architecture.\nclass DecoderBlock(nn.Module): def __init__(self, d_model, context_length, num_heads, attn_pdrop, resid_pdrop): super().__init__() self.masked_multi_head_sa = MultiHeadAttention(d_in=d_model, d_out=d_model, context_length=context_length, attn_pdrop=attn_pdrop, num_heads=num_heads) self.ln1 = LayerNorm(d_model) self.mlp = MLP(emb_dim=d_model) self.ln2 = LayerNorm(d_model) self.resid_dropout = nn.Dropout(resid_pdrop) def forward(self, x): attn_output = self.resid_dropout(self.masked_multi_head_sa(x)) x = self.ln1(x + attn_output) mlp_output = self.resid_dropout(self.mlp(x)) x = self.ln2(x + mlp_output) return x GPT-1 model With the decoder block implemented, we now have all the necessary components to build the GPT-1 architecture.\nclass GPT1(nn.Module): def __init__(self, D_embd, vocab_size, context_length, num_blocks, num_heads, dropout): super().__init__() # Token Embeddings - Convert integer word tokens (vocab) to a D-dimensional embedding vector self.token_embedding_table = nn.Embedding(vocab_size, D_embd) # Position Encoding - Encode each position to a D-dimensional embedding vector self.position_embedding_table = nn.Embedding(context_length, D_embd) # Embedding dropout self.embd_dropout = nn.Dropout(dropout) # Define multiple Decoder blocks self.blocks = nn.Sequential(*[DecoderBlock(d_model=D_embd, context_length=context_length, dropout=dropout, num_heads=num_heads) for _ in range(num_blocks)]) # Final FC layer to project the D-dimensional vector back to vocab space self.lm_head = nn.Linear(D_embd, vocab_size, bias=False) def forward(self, x): B, N = x.size() token_emb = self.token_embedding_table(x) # output: (B, N, D) pos_emb = self.position_embedding_table(torch.arange(N, device=device)) # output: (N, D) x = token_emb + pos_emb # ouput: (B, N, D) x = self.embd_dropout(x) x = self.blocks(x) logits = self.lm_head(x) # ouput: (B, N, vocab_size) return logits Tokenized text is first converted into token embeddings, which are then augmented with positional embeddings. This combined representation forms a tensor that passes through a series of transformer blocks, outputting a tensor of the same dimensionality. The final language modeling head (a linear layer without bias) projects this output into the vocabulary space, generating logits for each token in the vocabulary.\nWeight initialization Since LayerNorm is used extensively throughout the model, weights were initialized from a zero-mean Gaussian distribution with a standard deviation of $0.02$, which the authors noted was sufficient.\n# Initialize weights as per GPT-1: Normal distribution with std=0.02 def gpt1_init(m): if isinstance(m, nn.Linear): # Applies to linear layers only nn.init.normal_(m.weight, mean=0.0, std=0.02) if m.bias is not None: nn.init.zeros_(m.bias) # Bias initialized to zero Pre-Training Now, let\u0026rsquo;s initialize a small GPT model and perform a forward pass as a sanity check.\n# Define hyperparameters block_size = 32 batch_size = 16 model = GPT1(D_embd=64, vocab_size=vocab_size, context_length=block_size, num_heads=4, num_blocks=4, dropout=0.1).to(device) # Apply initialization model = model.apply(gpt1_init) # print the number of parameters in the model print(sum(p.numel() for p in model.parameters())/1e6, \u0026#39;M parameters\u0026#39;) # Forward pass with one example xb, yb = make_batch(\u0026#34;train\u0026#34;, batch_size=batch_size, block_size=block_size) print(\u0026#34;Input shape:\u0026#34;, xb.size()) print(\u0026#34;Target shape:\u0026#34;, yb.size()) context, target = xb.to(device), yb.to(device) logits = model(context) print(\u0026#34;Model output shape:\u0026#34;, logits.size()) loss = F.cross_entropy(logits.view(-1, vocab_size), target.view(-1)) print(\u0026#34;Initial Loss:\u0026#34;, loss.item()) 0.209536 M parameters Input shape: torch.Size([16, 32]) Target shape: torch.Size([16, 32]) Model output shape: torch.Size([16, 32, 65]) Initial Loss: 4.262292861938477 As you can see, the model outputs a tensor of shape [16, 32, 65], since we passed 16 input texts with 32 tokens each. The last dimension corresponds to the vocabulary size of the tokenizer. To compute the loss, we collapse the first two dimensions, concatenating all tokens together and averaging the loss over the batch.\nAt initialization, all tokens in our vocabulary are equally likely, meaning each has a probability of $1/65$. If we manually compute the cross-entropy loss, we get: $$ \\text{loss} = -\\ln(1/65) = 4.174 $$ This is approximately the same as our computed value, indicating that we are on the right track.\nWeight decay The GPT-1 model used the Adam optimizer with a modified version of L2 regularizationâ€”known as the AdamW optimizer, applying a weight decay $0.01$ only to non-bias and non-gain (LayerNorm) weights, meaning only Linear layer weights were regularized.\nSince Linear weights are 2D tensors, while biases and LayerNorm weights are 1D, we can use this criterion to separate them into decay and non-decay parameter groups.\nHereâ€™s how to implement it in PyTorch:\n# Separate parameters decay_params = [] no_decay_params = [] for name, param in model.named_parameters(): if param.requires_grad: if param.dim() \u0026gt;= 2: decay_params.append(param) else: no_decay_params.append(param) # Define optimizer with different parameter groups optimizer = torch.optim.AdamW( [ {\u0026#34;params\u0026#34;: decay_params, \u0026#34;weight_decay\u0026#34;: 0.01}, # Linear weights {\u0026#34;params\u0026#34;: no_decay_params, \u0026#34;weight_decay\u0026#34;: 0.0} # Biases \u0026amp; LayerNorm weights ], lr=1e-3 ) With that, weâ€™re ready to pre-train the model on our dataset. ðŸš€\nmax_iter = 5000 eval_iter = 200 # Define the loss function \u0026amp; optimizer criterion = torch.nn.CrossEntropyLoss() for iter in range(max_iter): model.train() # Sample a batch of data xb, yb = make_batch(\u0026#39;train\u0026#39;, batch_size=batch_size, block_size=block_size) context, target = xb.to(device), yb.to(device) # Forward pass and optimization optimizer.zero_grad() logits = model(context) loss = criterion(logits.view(-1, vocab_size), target.view(-1)) loss.backward() optimizer.step() # Evaluate the loss on train and val if iter%100 == 0 or iter==0 or iter==(max_iter-1): with torch.no_grad(): model.eval() out = {} for split in [\u0026#39;train\u0026#39;, \u0026#39;val\u0026#39;]: running_loss = 0 for k in range(eval_iter): xb, yb = make_batch(split, batch_size=batch_size, block_size=block_size) context, target = xb.to(device), yb.to(device) logits = model(context) loss = criterion(logits.view(-1, vocab_size), target.view(-1)) running_loss += loss.item() out[split] = running_loss / eval_iter print(\u0026#39;\\n Step:{}/{}, Train Loss:{:.4f}, Val Loss:{:.4f}\u0026#39;.format(iter, max_iter, out[\u0026#39;train\u0026#39;], out[\u0026#39;val\u0026#39;])) ... Step:4999/5000, Train Loss:1.7101, Val Loss:1.8699 Generating text The inference process involves generating new text based on patterns the model has learned from the training data. We start with an initial context, typically the integer token 0, which represents the newline character in our vocabulary. This serves as the seed input to the model.\nThe model then outputs a probability distribution over all tokens in the vocabulary. We sample from this distribution to generate the next token. This newly generated token is appended to the existing context and fed back into the model to produce the next prediction. By iterating this process, the model generates coherent text that follows the structure and style of the training dataâ€”such as Shakespearean plays in the case of our Tiny Shakespeare dataset.\n# Generate text from the trained model max_new_tokens = 300 context = torch.zeros((1, 1), dtype=torch.long, device=device) # Shape (B=1, N=1) stored_context = context for _ in range(max_new_tokens): model.eval() with torch.no_grad(): context = stored_context[:, -block_size:] # Trim the context to fit the model\u0026#39;s context length logits = model(context) # logits: (B=1, N, vocab_size) logits = logits[:, -1, :] # take just the last time step (B, vocab_size) probs = torch.softmax(logits, dim=-1) next_token = torch.multinomial(probs, num_samples=1) # Sample from the distribution (B=1, N) stored_context = torch.cat((stored_context, next_token), dim=1) # (B=1, N+1) print(\u0026#39;\u0026#39;.join(decode(stored_context[0].tolist()))) O this must says it toill dake to feare, What in, I reigreaton all him draster. he spit me with hermattan? COMANIUSIUS: As you my lastless; tell was with cantal you was? QUEEN ELIZABETH: Ay, forwas now can, and you fear\u0026#39;d, more my mune chamnot Moounce Let not the kise him? hy bu do to him, Unce I Since we sample at every step, text generation is stochastic, meaning that even with the same initial context, we may obtain different outputs. This ensures that the generated text is diverse rather than simply memorizing the training data.\nI know the generation is not very good since we\u0026rsquo;ve trained a character-level model, and it doesn\u0026rsquo;t grasp the meaning of words well. But still, it does a good job at generating Shakespeare-like text.\nSummary of GPT-1 paper Here are some key implementation details of the GPT-1 model.\nTokenizer: Byte Pair Encoding (BPE) with a vocabulary size of 40,000 merges. We\u0026rsquo;ll go into more detail on this in the next post.\nContext length: $N = 512$ tokens.\nPosition Embeddings: Learned instead of the sinusoidal version proposed in Transformers.\nArchitecture hyperparameters:\nNumber of layers (decoder blocks): $L = 12$ Number of attention heads: $H = 12$ Embedding dimension: $d_{model} = 768$ MLP size: $d_{ff} = 4 \\times d_{model} = 3072$ Residual, embedding and attention dropout: $p = 0.1$ Pre-Training hyperparameters:\nInitialization: Initialized from a zero-mean Gaussian distribution with a standard deviation of $0.02$. Optimizer: AdamW Batch size: 64 (randomly sampled) Weight decay: 0.01 Learning rate: Increased linearly from zero to a maximum value of $2.5 \\times 10^{-4}$ over the first 2000 updates, then annealed to zero using a cosine schedule. Number of epochs: 100 Fine-tuning hyperparameters:\nDropout in the classifier: $p = 0.1$ Learning rate: $6.25e^{-5}$ with a linear decay schedule and warmup over 0.2% of training steps. Batch size: 32 Number of epochs: 3 (sufficient for most cases). Weight decay: 0.5 References Medium Post on Most Successful Transformer Variants: Introducing BERT and GPT\nAndrej Karpathy\u0026rsquo;s video: Let\u0026rsquo;s build GPT: from scratch, in code, spelled out.\n[1] Devlin et al., \u0026ldquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u0026rdquo;, NAACL 2019.\n[2] Radford et al., \u0026ldquo;Improving Language Understanding by Generative Pre-Training\u0026rdquo;, OpenAI, 2018.\n","permalink":"https://yugajmera.github.io/posts/08-gpt/post/","summary":"By now, you\u0026rsquo;ve probably used OpenAI\u0026rsquo;s ChatGPTâ€”a chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.\nChatGPT interface\nThe unprecedented success of ChatGPT fueled further research into the technology behind itâ€”Large Language Models (LLMs).","title":"Introduction to LLMs: Coding GPT from scratch"},{"content":"In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to \u0026ldquo;attend\u0026rdquo; to different parts of the input at each time step while generating the output sequence.\nAttention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption.\nImage captioning task with an RNN decoder with Attention.\nGiven its usefulness, let\u0026rsquo;s abstract the attention mechanism from sequence modeling and generalize it into a layer that can be inserted into any network.\nGeneral Attention Layer (left) Attention mechanism in the image captioning task. (right) Generalized attention mechanism represented with vectors.\nRecall that in the attention mechanism:\nInput:\nFeatures: $\\mathbf{z}$ (Shape: $\\text{H} \\times \\text{W} \\times \\text{D}$) Hidden state: $\\mathbf{h}$ (Shape: $\\text{D}$) Similarity function: $f_{\\text{att}}$ Operations:\nAlignment: $e_{i, j} = f_{\\text{att}} (\\mathbf{h}, \\mathbf{z}_{i,j})$ Attention: $a = \\text{softmax} (e) $ Output:\nContext vector: $\\mathbf{c} = \\sum_{i, j} \\text{ } a_{i,j} \\text{ } \\mathbf{z}_{i,j}$ (Shape: $\\text{D}$) This mechanism can be generalized to operate on any set of vectors, making it more broadly applicable in deep learning.\nTo formalize this generalization of the attention mechanism, letâ€™s redefine its components:\nThe input features are now represented as a set of vectors, $\\mathbf{x}$, with shape $(\\text{N} \\times \\text{D})$, where $\\text{N} = \\text{H} \\times \\text{W}$. These vectors are the elements we want to attend over.\n$\\text{N}$ represents the number of vectors $\\text{D}$ represents the dimension of each vector. The hidden state of the decoder is renamed as a query vector, $\\mathbf{q}$ (Shape: $\\text{D}$).\nThe similarity function $f_{\\text{att}}$, typically implemented as a Multi-Layer Perceptron (MLP), compares the query vector to each input vector.\nThe output context vector is denoted as $\\mathbf{y}$.\nOur general attention mechanism now looks like this:\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}$) Query vector: $\\mathbf{q}$ (Shape: $\\text{D}$) Similarity function: $f_{\\text{att}}$ Operations:\nAlignment: $e_{j} = f_{\\text{att}} (\\mathbf{q}, \\mathbf{x}_{j})$ Attention: $a = \\text{softmax} (e) $ Output:\nContext vector: $\\mathbf{y} = \\sum_j a_{j} \\text{ } \\mathbf{x}_{j}$ (Shape: $\\text{D}$) Modifications Scaled dot product for similarity function The similarity function we used earlier is called additive attention, where a feed-forward network with a single hidden layer computes the compatibility function between query vectors and input vectors.\nA more commonly used alternative is dot-product (multiplicative) attention, where the query vector and input vectors are combined using a dot product. It can also looked as a measure of similarity as a dot product quantifies how closely two vectors are aligned.\nWhile both methods have similar theoretical complexity, dot-product attention is computationally more efficient as it can be implemented using optimized matrix multiplication code.\nHowever, when the vector dimension $\\text{D}$ is large, which is typically greater than 1000 for large language models (LLMs), the resulting alignment scores can have high magnitudes. Since the softmax function normalizes these scores to compute attention weights, large magnitudes can cause softmax to saturate, leading to vanishing gradients during backpropagation.\nLet\u0026rsquo;s look at the example below to understand this better,\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1) tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872]) This looks as expected. But when the magnitudes are large, it saturates and converges to a one-hot encoding.\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*1000, dim=-1) tensor([0., 0., 0., 0., 1.]) To mitigate this, we scale the dot product by $\\frac{1}{\\sqrt{\\text{D}}}$. This adjustment reduces the impact of large vector magnitudes, similar to initialization techniques like Xavier or Kaiming. This approach, known as scaled dot-product attention, is widely used in modern models.\nMultiple Query vectors At each time step of the decoder, we use one query vector (i.e., one hidden state) to compute a probability distribution over the inputs, producing one context vector.\nWe can generalize this concept to handle multiple query vectors simultaneously, each generating a corresponding output vector. This allows us to compute multiple attention context vectors in parallel.\n(left) A general attention layer. (right) The same layer with multiple query vectors.\nWith this modification, the attention layer includes:\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}$) Query vectors: $\\mathbf{q}$ (Shape: $\\text{M} \\times \\text{D}$) Similarity function: scaled dot product Operations:\nAlignment: $e_{i, j} = (\\mathbf{q}_i \\cdot \\mathbf{x}_{j} )$ / $\\sqrt{\\text{D}}$ (Shape: $\\text{M} \\times \\text{N}$) Attention: $a = \\text{softmax} (e) $ Context vectors: $\\mathbf{y}_i = \\sum_j a_{i,j} \\text{ } \\mathbf{x}_{j}$ Output:\nOutput vectors: $\\mathbf{y}$ (Shape: $\\text{M} \\times \\text{D}$) Key and Value vectors In the attention mechanism, we use the input vectors in two different ways:\nTo generate the alignment scores that compare the input vectors with each query vector via the similarity function.\nTo compute the output context vectors by taking a weighted sum of input vectors and the attention weights.\nTo handle these roles effectively, the input vectors are separated into key vectors ($\\mathbf{k}$) and value vectors ($\\mathbf{v}$). Both are derived using learnable projection matrices applied to the input vectors:\nKeys are used to compute alignment scores with the query. Values are used to construct the context vector. The separation of keys and values enables the model to use input vectors differently for comparison and retrieval. For example:\nQuery - What am I looking for? Google search: \u0026ldquo;How tall is the Empire State Building?\u0026rdquo; Keys - What do I contain? Google compares the query with a set of webpages that may contain the answer. Values - Information in the token that will be communicated Returns the webpage saying, â€œAt its top floor, the Empire State Building stands 1,250 feet (380 meters) tall.â€ Since the information used for matching (keys) is different from the information returned (values), we separate them into two distinct vectors. The key determines relevance or alignment, whereas the value is used to retrieve the actual information. This gives the model flexibility in handling different types of information.\n(left) General attention layer with multiple query vectors. (right) The same layer with separate key and value vectors.\nWith these modifications, the attention layer is described as follows:\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}$) Query vectors: $\\mathbf{q}$ (Shape: $\\text{M} \\times \\color{blue}{\\text{D}_k}$) Similarity function: scaled dot product Operations:\nKey vectors: ${\\color{blue}{\\mathbf{k}}} = \\mathbf{x} {\\color{blue}{W_\\mathbf{k}}}$ (Shape: $\\text{N} \\times \\color{blue}{\\text{D}_k}$) Value vectors: ${\\color{orange}{\\mathbf{v}}} = \\mathbf{x} {\\color{orange}{W_\\mathbf{v}}}$ (Shape: $\\text{N} \\times {\\color{orange}{\\text{D}_v}}$) Alignment: $e_{i, j} = (\\mathbf{q}_i \\cdot {\\color{blue}{\\mathbf{k}_j}} )$ / $\\sqrt{\\color{blue}{\\text{D}_k}}$ Attention: $a = \\text{softmax} (e) $ Context vectors: $\\mathbf{y}_i = \\sum_j a_{i,j} \\text{ } {\\color{orange}{\\mathbf{v}_j}}$ Output:\nOutput vectors: $\\mathbf{y}$ (Shape: $\\text{M} \\times {\\color{orange}{\\text{D}_v}}$) Matrix Representation The entire attention mechanism can be expressed compactly in matrix form:\n\\begin{align} \\text{Attention} (\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax} (\\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{\\text{D}_k}}) \\mathbf{V} \\end{align}\nWhere:\n$\\mathbf{Q}$: Matrix of query vectors (Shape: $\\text{M} \\times \\text{D}_k$) $\\mathbf{K}$: Matrix of key vectors (Shape: $\\text{N} \\times \\text{D}_k$) $\\mathbf{V}$: Matrix of value vectors (Shape: $\\text{N} \\times \\text{D}_v$) This is the most common representation of attention that weâ€™ve derived so far.\nNotably, the attention mechanism itself has no learnable parameters, and the attention weights are not learned; instead, they are computed using a simple scaled dot product function followed by a softmax operation.\nSelf-Attention layer One special case of the attention layer is the self-attention layer, where we only have input vectors and no explicit query vectors. In this case, we use a query matrix to derive the query vectors from our input vectors.\nSince each input vector serves as its own query, we end up comparing each vector in the input set with every other vector. This design leverages the power of attention while eliminating the need for external query vectors, enabling the model to capture relationships between different parts of the input.\nSelf-attention enhances input representations by allowing each position in a sequence to interact with and weigh the importance of all other positions within the same sequence.\nA self-attention layer\nIn order to seperate the input and output dimensions, we denote $\\text{D}_k = \\text{D}_v = \\text{D}_{out}$.\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}_{in}$) Similarity function: scaled dot product Operations:\nKey vectors: ${\\color{blue}{\\mathbf{k}}} = \\mathbf{x} {\\color{blue}{W_\\mathbf{k}}}$ (Shape: $\\text{N} \\times \\text{D}_{out}$)\nValue vectors: ${\\color{orange}{\\mathbf{v}}} = \\mathbf{x} {\\color{orange}{W_\\mathbf{v}}}$ (Shape: $\\text{N} \\times \\text{D}_{out}$)\nQuery vectors: ${\\color{green}{\\mathbf{q}}} = \\mathbf{x} {\\color{green}{W_\\mathbf{q}}}$ (Shape: $\\text{N} \\times \\text{D}_{out}$)\nAlignment: $e_{i, j} = ({\\color{green}{\\mathbf{q}_i}} \\cdot {\\color{blue}{\\mathbf{k}_j}} ) / \\sqrt{\\text{D}}$\nAttention: $a = \\text{softmax} (e) $\nContext vectors: $\\mathbf{y}_i = \\sum_j a_{i,j} \\text{ } {\\color{orange}{\\mathbf{v}_j}}$\nOutput:\nOutput vectors: $\\mathbf{y}$ (Shape: $\\text{N} \\times \\text{D}_{out}$) This forms a new type of neural network layer, where we input a set of vectors and output another set of vectors, effectively allowing the model to attend to different parts of its own input.\nLetâ€™s take a look at how this would be implemented in code.\nclass SelfAttention(nn.Module): def __init__(self, d_in, d_out, qkv_bias=False): super().__init__() self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias) def forward(self, x): # Input vectors x - (B, N, d_in) # Obtain keys, values and queries - (B, N, d_out) keys = self.W_k(x) values = self.W_v(x) queries = self.W_q(x) # Apply scaled dot-product attention attn_scores = (queries @ keys.T) * keys.shape[-1]**-0.5 attn_weights = torch.softmax(attn_scores, dim=-1) context_vec = attn_weights @ values return context_vec The bias term is omitted in the weight matrices to ensure pure matrix multiplication.\nSetting dim=-1 in the softmax function instructs it to normalize along the last dimension, which corresponds to the columns (since attn_scores has the shape [batch, row N, column N]), ensuring that the values in each row sum to 1.\nPositional encoding A key consideration with the self-attention layer is that it is permutation equivariant. This means that if we change the order of the input vectors, we would still compute the same key, value, and query vectors, but they would be permuted in the same way the input vectors were permuted. As a result, the set of output vectors would remain the same, but their order would change.\nSelf-attention layer is permutation equivariant $f(s(x)) = s(f(x))$.\nThe self-attention layer does not inherently account for the order of the input vectors; it processes them as a set, irrespective of their sequence. While this property works well for certain tasks, it poses a challenge for tasks like machine translation or text generation, where the order of tokens is crucial.\nFor example, the sentences \u0026ldquo;The dog chased the cat\u0026rdquo; and \u0026ldquo;The cat chased the dog\u0026rdquo; would appear identical to the transformer but convey different meanings.\nTo make the layer position-aware, positional encodings are added to the input vectors. These encodings capture the position of each element in the sequence. A function ${\\color{purple}{pos}}: \\text{N} \\rightarrow \\mathbb{R}^D$ transforms each position $j$ into a unique, $D$-dimensional positional vector.\nConcatenate special positional encoding $\\color{purple}{p_j}$ to each input vector $\\mathbf{x}_j$\nThere are two common ways to obtain this positional encoding function:\nLearnable Lookup Table:\nA lookup table is learned during training that assigns a unique encoding to each position. This approach learns parameters for each position $t \\in [0, N)$, where $T$ is the maximum sequence length, leading to a lookup table of size $N \\times D$. Fixed Function:\nA fixed function is designed that outputs a unique, deterministic encoding for each position. This approach doesnâ€™t require any learnable parameters. Masked Self-Attention layer A variant of the self-attention layer, called masked self-attention or causal attention, is used for tasks like language modeling, where the goal is to predict the next word given the previous words. This is similar to a decoder RNN, where new words in the output sequence are generated one by one, with previously generated words providing context.\nWith the standard self-attention layer, the model can attend to all input tokens at once, which isn\u0026rsquo;t ideal for such tasks. To make the model \u0026ldquo;look\u0026rdquo; only at previous words while generating the next one, we mask future tokens. This prevents the model from attending to words that come after the current position in the sequence.\nTo achieve this, we set the attention weights for all future tokens to zero, ensuring that the model can only attend to current and past tokens when computing the context vector. This is particularly useful in decoders, where sequences are generated step-by-step.\nA Masked self-attention layer (Causal Attention).\nRather than zeroing out the attention weights of future tokens and renormalizing them, we can assign negative infinity to those positions and apply softmax directly. This ensures they receive zero probability while maintaining a row sum of oneâ€”all in a single step.\nThe mask will have a shape of $\\text{N} \\times \\text{N}$, where $\\text{N}$ represents the number of tokens in the sequence. We use PyTorch\u0026rsquo;s tril function to create a mask where values above the diagonal are zero. These positions are then replaced with negative infinity in the attention scores matrix.\nLetâ€™s modify our code for causal attention.\nclass CausalAttention(nn.Module): def __init__(self, d_in, d_out, context_length, attn_pdrop, qkv_bias=False): super().__init__() self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias) self.attn_dropout = nn.Dropout(attn_pdrop) # Create a causal mask self.register_buffer(\u0026#34;mask\u0026#34;, torch.tril(torch.ones(context_length, context_length))) def forward(self, x): # Input vectors x - (B, N, d_in) B, N, d_in = x.shape # Obtain keys, values and queries - (B, N, d_out) keys = self.W_k(x) values = self.W_v(x) queries = self.W_q(x) # Apply scaled dot-product attention with causal mask attn_scores = (queries @ keys.T) * keys.shape[-1]**-0.5 attn_scores = attn_scores.masked_fill(self.mask[:N, :N] == 0, float(\u0026#39;-inf\u0026#39;)) attn_weights = torch.softmax(attn_scores, dim=-1) attn_weights = self.attn_dropout(attn_weights) context_vec = attn_weights @ values return context_vec Context length refers to the maximum number of tokens allowed in any sequence (we will explore this in detail in the next post). It helps define the largest mask, which can then be applied to sequences of any length in our training data.\nWe have also used a register_buffer here, which ensures that the buffer is automatically moved to the appropriate device (CPU or GPU) along with the model, preventing device mismatch errors.\nIt is also common practice to zero out additional elements in the attention weight matrix by applying a dropout layer, also called attention dropout. As you may recall, dropout is a regularization technique that helps prevent overfitting. Since some attention weights are zeroed out based on the dropout probability, the remaining weights are re-scaled to compensate for the reduction. Note that this dropout layer is disabled during inference.\nMulti-head Self-Attention layer Instead of performing a single attention function over the entire input vector space, the multi-head self-attention mechanism splits the input into multiple representation subspaces and performs attention in parallel across these subspaces. This allows the model to attend to different aspects of the input simultaneously.\nIn this case, we divide each input vector into $H$ chunks of equal size and feed them into several parallel attention layers. The input to each head has dimension $\\text{d}_{h} = \\text{D}/H$.\nThe outputs from all attention heads are concatenated (dimension $H \\cdot \\text{d}_h$) and passed through a projection linear layer to produce the final output of dimension $\\text{D}_{out}$. While this projection layer is not strictly necessary, it is commonly used in many LLM architectures.\nA Multi-head self-attention layer\nThe above image may seem more intuitive, but it is not optimal for computation. If we first split the input vectors $\\mathbf{x}$, we would need to compute the keys, values, and queries independently for each head, increasing computation.\nInstead, a more efficient approach is to compute the keys, values, and queries for the entire input vector dimension first, and then split them into $H$ equal chunks. After computing attention independently for each head, we combine the results.\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}_{in}$) Similarity function: scaled dot product Operations:\nKey vectors: ${\\color{blue}{\\mathbf{k}}} = \\mathbf{x} {\\color{blue}{W_\\mathbf{k}}}$ (Shape: $\\text{N} \\times \\text{D}_{out}$)\nValue vectors: ${\\color{orange}{\\mathbf{v}}} = \\mathbf{x} {\\color{orange}{W_\\mathbf{v}}}$ (Shape: $\\text{N} \\times \\text{D}_{out}$)\nQuery vectors: ${\\color{green}{\\mathbf{q}}} = \\mathbf{x} {\\color{green}{W_\\mathbf{q}}}$ (Shape: $\\text{N} \\times \\text{D}_{out}$)\nSplit key, value and query vectors. For each head:\nAlignment: $e_{i, j} = (\\mathbf{q}_i \\cdot \\mathbf{k}_j ) / \\sqrt{\\text{d}_{h}}$ Attention: $a = \\text{softmax} (e) $ Output vectors: ${\\mathbf{y}_i} = \\sum_j a_{i,j} \\text{ } \\mathbf{v}_j$ (Shape: $\\text{N} \\times \\text{d}_{h}$) Output:\nOutput vectors: $\\mathbf{y} = \\text{Concat} (\\text{y}^0, \\cdots, \\text{y}^{\\text{H} - 1}) W_o$ (Shape: $\\text{N} \\times \\text{D}_{out}$) Now, let\u0026rsquo;s implement a multi-head attention module that runs several causal attention heads in parallel.\nclass MultiHeadAttention(nn.Module): def __init__(self, d_in, d_out, context_length, attn_pdrop, num_heads, qkv_bias=False): super().__init__() self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_o = nn.Linear(d_out, d_out) self.attn_dropout = nn.Dropout(attn_pdrop) self.d_h = d_out // num_heads # head dimension self.num_heads = num_heads self.register_buffer(\u0026#34;masked\u0026#34;, torch.tril(torch.ones(context_length, context_length)).view(1, 1, context_length, context_length)) def forward(self, x): # Input vectors x - (B, N, d_in) B, N, d_in = x.shape # Obtain keys, values and queries - (B, N, d_out) keys = self.W_k(x) values = self.W_v(x) queries = self.W_q(x) # Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h) k = keys.view(B, N, self.num_heads, self.d_h).transpose(1, 2) v = values.view(B, N, self.num_heads, self.d_h).transpose(1, 2) q = queries.view(B, N, self.num_heads, self.d_h).transpose(1, 2) # Apply scaled dot-product attention with causal mask on each head attn_scores = (q @ k.transpose(2, 3)) * k.shape[-1]**-0.5 attn_scores = attn_scores.masked_fill(self.masked[:, :, :N, :N] == 0, float(\u0026#39;-inf\u0026#39;)) attn_weights = torch.softmax(attn_scores, dim=-1) attn_weights = self.attn_dropout(attn_weights) context_vec = attn_weights @ v # Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out) out = context_vec.transpose(1, 2).contiguous().view(B, N, -1) out = self.W_o(out) return out The splitting of key, value, and query tensors is achieved through tensor reshaping and transposition. First, we reshape them to introduce the num_heads dimension, then transpose to bring num_heads before the num_tokens dimension. This allows us to compute attention in parallel across all heads using batched matrix multiplication.\nAfter computing attention, the context vectors from all heads are transposed back to their original shape and flattened, effectively combining the outputs from all heads.\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, the attention mechanism would average over these subspaces, potentially losing valuable information.\nAdditionally, since each head operates on a reduced dimension, the total computational cost remains comparable to that of single-head attention with full dimensionality.\nTransformer The Transformer [1] was the first sequence model that relies solely on self-attention to compute representations of its input and output without using any recurrent units. This innovation marked a turning point for natural language processing (NLP), outperforming all state-of-the-art models of its time. It is often referred to as the \u0026ldquo;ImageNet moment\u0026rdquo; for NLP.\nLet\u0026rsquo;s take a closer look at the complete architecture of the Transformer.\nEmbeddings and Positional Encoding The input and output word tokens are converted into vectors of dimension $\\text{D} = d_{\\text{model}} = 512$ by learned embeddings.\nNext, positional encodings are added to input embedding vectors to inject information about the position of the tokens in the sequence. A fixed sinusoidal function is used to compute the positional encodings, which are of the same dimension $d_\\text{model}$ as the embeddings, so that the two can be summed together.\nEncoder block The encoder is composed of a stack of $L = 6$ identical layers, each consisting of two sub-layers:\nMulti-head Self-Attention: Each output from this layer depends on every input, allowing for interactions between all vectors in the input sequence. Since the inputs come from the previous encoder layer, each position in the encoder can attend to all positions in the previous layer.\nHyperparamters: $H = 8$, $\\text{D}_{in} = \\text{D}_{out} = d_{model} = 512$ Feed-forward network: Each input vector passes through an MLP independently, consisting of two linear layers and a ReLU activation in between. This layer internally expands the embedding dimension into a higer-dimensional space (factor of 4), which allows for exploration of a richer representation space.\nLinear 1 $(512, 2048)$ -\u0026gt; ReLU -\u0026gt; Linear 2 $(2048, 512)$ Additionally:\nResidual Dropout: Each of the two sublayers is followed by a dropout layer. Additionally, dropout is applied to the sum of the embeddings and position encodings. $p = 0.1$.\nResidual connection: A residual connection is used around each of the two sub-layers to improve the gradient flow through the model, and overcome the vanishing gradient problem.\nLayer Normalization: Each sub-layer is followed by layer normalization to aid optimization (similar to BatchNorm in convolutional layers).\nThe Encoder of the Transformer.\nInput: A set of vectors $\\mathbf{x}$ (Shape: $\\text{N} \\times 512$) Output: A set of vectors $\\mathbf{y}$ (Shape: $\\text{N} \\times 512$) The interaction between vectors occurs only in the self-attention layer. LayerNorm and MLP operate on each input vector independently. The uniformity in input and output dimensions enables the stacking of multiple layers, thus making the model more scalable.\nDecoder block The decoder consists of $L = 6$ identical layers, each with three sub-layers:\nMasked Multi-head Self-Attention: To prevent the decoder from attending to future tokens in the output sequence, we use masked self-attention, which ensures that each position in the decoder can only attend to past and current tokens.\nHyperparamters: $H = 8$, $\\text{D}_{in} = \\text{D}_{out} = 512$. Multi-head Cross-Attention over Encoder outputs: This layer allows each position in the decoder to attend to all positions in the input sequence, passing relevant context from the encoder. This mimics the traditional encoder-decoder attention mechanism used in seq2seq models.\nHyperparameters: $H = 8$, $\\text{D}_{in} = \\text{D}_{out} = 512$. Feed-forward network: Same structure as in the encoder:\nLinear 1 (512, 2048) -\u0026gt; ReLU -\u0026gt; Linear 2 (2048, 512) Similar to the encoder, each sublayer is followed by a dropout layer, with residual connections added around each one. Finally, layer normalization is applied to each vector independently.\nThe Decoder of the Transformer.\nInput: Decoder sequence: A set of vectors $\\mathbf{x}$ (Shape: $\\text{M} \\times 512$) Encoder context: A set of context vectors $\\mathbf{c}$ (Shape: $\\text{N} \\times 512$) Output: A set of vectors $\\mathbf{y}$ (Shape: $\\text{M} \\times 512$) The masked self-attention sub-layer ensures autoregressive behavior by restricting attention to past inputs. The multi-head attention over encoder outputs bridges the encoder and decoder, allowing the decoder to focus on relevant parts of the input sequence.\nThe decoder block is followed by a linear layer and softmax function to convert the decoder output into predicted next-token probabilities.\nDuring inference, the decoder sequence begins with a \u0026lt; START \u0026gt; token embedding (Shape: $1 \\times 512$). As the model predicts each subsequent token, we append it to this sequence.\nKey characteristics Parallel computation: Unlike RNNs, the Transformer processes entire sequences simultaneously, allowing alignment and attention scores for all inputs to be computed in parallel, significantly improving efficiency on large datasets.\nFlexibility of inputs: It can effectively handle both unordered sets and ordered sequences (with positional encodings).\nGlobal context: Self-attention enables the model to capture long-range dependencies across the entire sequence.\nScalability: The Transformer\u0026rsquo;s architecture is highly scalable, with a few key hyperparameters that can be adjusted to meet various requirements:\nNumber of Layers $L$: Applies equally to the encoder and decoder. Hidden size $d_{\\text{model}}$: Defines the dimensionality of the model. MLP size $d_{ff}$: Specifies the output size of the first layer in the feed-forward MLP. Heads $H$: Determines the number of attention heads in multi-head self-attention (encoder), masked multi-head self-attention (decoder), and multi-head cross-attention (decoder). The Transformer Architecture.\nThe Transformer\u0026rsquo;s innovative architecture has become the foundation for many advancements in NLP, inspiring models like BERT and GPT, and driving us in the new era of LLMs.\nReferences [1] Vaswani et al, \u0026ldquo;Attention is all you need\u0026rdquo;, NeurIPS 2017.\n","permalink":"https://yugajmera.github.io/posts/07-transformer/post/","summary":"In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to \u0026ldquo;attend\u0026rdquo; to different parts of the input at each time step while generating the output sequence.\nAttention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption.","title":"Generalizing Attention with Transformers"},{"content":"In previous discussions, we focused on feedforward neural networks, which take a single image as input, process it through multiple layers of convolution, normalization, or fully connected layers, and output a single label for image classification tasks. This is a one-to-one relationship: a single image maps to a single output label.\nHowever, there are other types of problems we want to solve using deep learning that involve variable-length sequences as both inputs and outputs. Here are some examples:\nOne-to-many: Consider the task of image captioning, where the input is a single image, and the output is a sentenceâ€”a sequence of words describing the content of the image in natural language.\nMany-to-one: An example of this is sentiment analysis, where the input is a sentence, and the task is to classify whether it expresses a positive or negative sentiment.\nMany-to-many: In this case, we want to produce an output for each input in the sequence. For example, in video classification, where we may wish to label each frame of the video.\nSequence-to-sequence (Seq2Seq): In tasks like machine translation, the input is a sentence in one language (e.g., English), and the output is a translation of that sentence in another language (e.g., Spanish). The lengths of the input and output sequences are not necessarily equal.\nInputs are red, outputs are blue, and green boxes represent the RNN\u0026rsquo;s internal state (more on this soon). [Modified from Andrej Karpathy\u0026rsquo;s blog]\nConventional neural networks or convolutional networks are designed to handle fixed-size input vectors (such as images) and produce fixed-size output vectors (such as class probabilities). To handle sequences of arbitrary lengths, we use Recurrent Neural Networks (RNNs). RNNs can process sequences of vectors, enabling us to handle sequences in the input, the output, or both.\nRecurrent Neural Networks An RNN processes inputs sequentially, maintaining an internal state called a hidden state $\\mathbf{h}$ (a vector) that encodes information about the inputs it has seen so far.\nA vanilla RNN for a many-to-many task is shown below:\nAt each time step $t$, the RNN takes an input $\\mathbf{x}_t$ (shown in red) and updates its hidden state $\\mathbf{h}_t$ using a recurrence formula:\n\\begin{align} \\mathbf{h}_t \u0026amp;= \\text{tanh} (W_{hh} \\text{ } \\mathbf{h}_{t-1} + W_{xh} \\text{ } \\mathbf{x}_t + b_h) \\\\ \\mathbf{y}_t \u0026amp;= W_{hy} \\text{ } \\mathbf{h}_t \\end{align}\nThis updated hidden state is used to produce an output $\\mathbf{y}_t$ (shown in blue) at each time step. A tanh nonlinearity is used in this model because it was developed in the earlier days of neural network research.\nWe usually initialize the first hidden state $h_0$ to a zero vector. Note that the same weight matrices are used at every timestep in the sequence. This shared weight setup allows RNNs to handle sequences of arbitrary length by unrolling the computation graph over any number of timesteps, all while using the same set of weights.\nThis iterative generation process enables the model to progressively generate a coherent sequence of words, building each step upon the context of previous ones.\nTraining Each word must be encoded as a vector to make the network to process it as they cannot process raw text. We use a fixed vocabulary and convert each unique word in the vocabulary into a one-hot encoded (or some learnable function) vector before training. This process of converting data into a vector format is called embedding.\nThen, we use a softmax layer at the end of our network to predict a probability distribution over this vector of words, selecting the word with the highest probability as the output.\nTo train our RNN, we apply a loss function (e.g., cross-entropy) at each time step in the sequence. The loss is calculated between the output vector $\\mathbf{y}_t$ and the ground truth label (or one-hot encoded word vector) to obtain a loss value per time step. By summing these individual losses across all time steps, we obtain the total loss, which we then use for backpropagation through the network.\nComputational graph for a many-to-many RNN that produces one output per timestep in our input sequence.\nLimitations During backpropagation, the chain rule applies across time steps. Because the same weight matrix is used for all time steps, it is multiplied repeatedly, which can cause two issues:\nExploding gradient problem If the values in the weight matrix are greater than 1, repeated multiplication can cause the gradients to grow exponentially, leading to unstable training.\nTo address this, we use a technique called gradient clipping, where we scale down the gradients if their norm exceeds a certain threshold. This helps control and limit the magnitude of the gradients during training.\nVanishing gradient problem Conversely, if the values in the weight matrix are less than 1, repeated multiplication can shrink the gradients exponentially, leading to the vanishing gradient problem.\nTo mitigate this issue, we use a variant of the RNN called the Long Short-Term Memory (LSTM) network. LSTMs have specialized gating mechanisms and a dedicated cell state that help manage the flow of information and gradients over longer sequences.\nWithout going into detail, the concepts weâ€™ve discussed for RNNs remain the same, except that the mathematical formulation for updating the hidden state is more complex in LSTMs. You can read more about LSTMs here: Colah\u0026rsquo;s blog.\nExtending to other tasks Let\u0026rsquo;s explore how RNNs can be adapted for different sequence-based tasks.\nOne-to-Many: Image Captioning In the image captioning task, the goal is to generate a descriptive sentence based on a single image. This is done in two main steps:\nFeature extraction: First, we feed the input image into a pre-trained convolutional neural network (CNN) to extract a feature vector, capturing important information about the image content.\nGenerating the sequence: Next, we pass this image feature vector to an RNN, which uses a recurrence formula to generate a sequence of words that describes the image.\nAn RNN for an image captioning task at test time.\nTo incorporate the image features in the recurrence formula, we modify it as follows:\n\\begin{align} \\mathbf{h}_t \u0026amp;= \\text{tanh} (W_{hh} \\text{ } \\mathbf{h}_{t-1} + W_{xh} \\text{ } \\mathbf{x}_t + {\\color{purple}{W_{ih}} \\text{ } \\mathbf{v}} + b_h) \\\\ \\mathbf{y}_t \u0026amp;= W_{hy} \\text{ } \\mathbf{h}_t \\end{align}\nwhere $\\mathbf{v}$ is the feature vector of the input image.\nTraining is similar to what we discussed earlier, but it differs at test time. During testing, we feed the image feature vector along with an initial seed token, \u0026ldquo;\u0026lt; START \u0026gt;\u0026rdquo;, into the RNN. This produces a probability distribution for the first word in the caption.\nFor instance, if \u0026ldquo;man\u0026rdquo; has the highest probability in this distribution, we select it as the first word in the caption. We then feed the it\u0026rsquo;s embedding vector back into the RNN as the next input to generate the following word.\nWe repeat this process, generating words sequentially and unrolling the graph to obtain $\\mathbf{y}_{t=1:T}$. Sampling stops when we encounter the end token \u0026ldquo;\u0026lt; END \u0026gt;\u0026rdquo;, which marks the end of the sentence.\nMany-to-One: Sentiment Analysis In a sentiment analysis task, the RNN receives word embeddings for each word in an input sentence. After processing the entire sequence, the RNN produces a single output prediction $\\mathbf{y}$ based on the final hidden state. This output might be a binary classification, such as \u0026ldquo;1\u0026rdquo; for positive sentiment or \u0026ldquo;0\u0026rdquo; for negative sentiment.\nAn RNN for a sentiment analysis task.\nThe final hidden state effectively summarizes the information from the entire input sequence, capturing the context the network needs to make a sentiment prediction.\nSequence to sequence (Many-to-One + One-to-Many): Machine Translation The seq2seq model was first introduced for machine translation [1], where the goal is to transform an input sequence (source) into a corresponding output sequence (target), with both sequences being of arbitrary lengths. These models are also commonly used in applications like chatbots and personal assistants, where they generate meaningful responses to input queries.\nIt typically uses a combination of two RNNs in an encoder-decoder style architecture:\nEncoder RNN (Many-to-One): The encoder takes the input sequence and outputs a fixed-length vector that summarizes the content of the input. This output vector, which is the last hidden state of the encoder, is often called the context vector or thought vector.\nDecoder RNN (One-to-Many): We then feed this context vector as the initial hidden state into the decoder RNN, which generates the target sequence as its output.\nA seq2seq model for translating a sentence from English to Spanish.\nIt\u0026rsquo;s important to note that the encoder and decoder RNNs have different weight matrices since they handle different sequences, which may vary in length.\nA common practice is to separate the context vector and the initial hidden state of the decoder, as both serve different purposes:\nThe context vector captures the input information from the encoder, which is passed on to the decoder to help generate the output sequence. This is often set to the last hidden state of the encoder, $\\mathbf{c} = \\mathbf{h}_T$, and is used in the recurrence formula at each time step of the decoder.\nThe initial decoder state is used to start the decoding process and is typically derived from a projection or a feed-forward layer applied to the encoder\u0026rsquo;s final hidden state. This approach allows the initial state to be optimized specifically for the decoder, rather than directly copying the encoderâ€™s final state.\nA seq2seq model with seperate context vector and initial decoder state.\nThe context vector passes information from the encoder to the decoder. However, since all the input information is bottlenecked into this single fixed-size vector, it becomes difficult for the model to retain information from longer input sequences. Often, the earlier parts of the input may be \u0026ldquo;forgotten\u0026rdquo; by the time the encoder finishes processing. To address this limitation, we use a mechanism called Attention [2].\nAttention Instead of relying solely on the encoder\u0026rsquo;s last hidden state to build a single context vector, attention enables the model to dynamically create context vectors at each decoding step. We let the decoder weigh all the encoderâ€™s hidden states according to their relevance to the current step in the decoding process.\nTo focus on the parts of the input sequence that are most relevant to the current output, the decoder follows these steps:\nAlignment scores: An alignment function (often an MLP) takes the current hidden state of the decoder and each hidden state of the encoder, producing a score (scalar value) for each encoder state. These scores indicate the relevance of each encoder hidden state to the current decoding step.\n$$ e_{t, i} = f_{\\text{att}} (\\mathbf{s}_{t-1}, \\mathbf{h}_i) $$\nAlignment weights: The alignment scores are passed through a softmax function to produce a probability distribution. These values tell us how much weight to assign to each encoder hidden state when constructing the context vector.\n$$ a_{t, i} = \\text{softmax} (e_{t, i}) $$\nNew context vector: The context vector for the current time step is constructed by taking a weighted sum of the attention weights and the encoder hidden states. $$ \\mathbf{c}_t = \\sum_i a_{t, i} \\mathbf{h}_i $$ Update step: The decoder uses this new context vector in the recurrence formula to obtain the next hidden state: \\begin{align} \\mathbf{s}_t \u0026amp;= \\text{tanh} (W_{ss} \\text{ } \\mathbf{s}_{t-1} + W_{yh} \\text{ } \\mathbf{y}_{t-1} + {\\color{purple}{W_{cs}} \\text{ } \\mathbf{c}_t} + b_s) \\\\ \\mathbf{y}_t \u0026amp;= W_{hy} \\text{ } \\mathbf{s}_t \\end{align}\nWe repeat these four steps for every time step in the decoderâ€™s sequence, with the initial decoder state coming from the encoderâ€™s final hidden state.\nThe intuition here is that as each word in the output sentence is generated, the context vector â€œattendsâ€ to the most relevant part of the input sentence. For example, in a English-to-Spanish translation, when the model is generating the word \u0026ldquo;estamos\u0026rdquo; for \u0026ldquo;we are\u0026rdquo; it will focus more on the corresponding English encoder states. Sample attention weights might look like:\n\\begin{align} a_{11} [\\text{we}] = a_{12} [\\text{are}] = 0.45, \\\\ a_{13} [\\text{eating}] = a_{14} [\\text{bread}] = 0.05 \\end{align}\nSimilarly, for the word \u0026ldquo;comiendo\u0026rdquo; (from \u0026ldquo;eating\u0026rdquo;), the weights could be:\n\\begin{align} a_{21} [\\text{we}] = a_{24} [\\text{bread}] = 0.05, \\\\ a_{22} [\\text{are}] = 0.1, \\\\ a_{23} [\\text{eating}] = 0.8 \\end{align}\nSince the seq2seq model with attention is trainable end-to-end, the network learns on its own which parts of the input sequence to focus on for each output word. This flexibility allows it to dynamically adjust focus as needed, creating a more accurate and contextually aware output sequence.\nWe can visualize this intuition through the attention weight matrix from a trained seq2seq model, where each pixel reflects the attention value between corresponding words in the source (e.g., English) and target (e.g., French) sentences. Higher attention weights (white boxes) indicate stronger relevance or \u0026ldquo;focus\u0026rdquo; on particular input words when generating specific output words, showing how the model aligns corresponding words across the two languages.\nThe x-axis and y-axis correspond to the words in the source sentence (English) and the generated translation (French), respectively. Each pixel shows the attention weight value.\nImage Captioning with Visual Attention The attention mechanism enables models to generate sequences by focusing on different parts of the input at each generation step. Importantly, this mechanism doesn\u0026rsquo;t rely on the input being a sequence; it simply lets the model attend to the most relevant parts of the input, which can be structured in any way. This flexibility makes attention mechanisms applicable not only to sequential data but also to other types of inputs, such as images.\nLet\u0026rsquo;s explore how attention can be applied to the image captioning task [3].\nEncoder (feature extraction): We begin by using a CNN to extract a grid of feature vectors, where each vector corresponds to a specific spatial location in the input image. These feature vectors capture the image\u0026rsquo;s content in a spatially structured manner.\nDecoder (RNN):\nInitial hidden state: This grid of feature vectors is then fed into an MLP to predict the initial hidden state of the decoder RNN.\nAttention mechanism: At each step of the generation process, the attention mechanism combines the decoderâ€™s current hidden state with the feature grid to construct a new context vector. This context vector reflects the parts of the image that are most relevant for generating the next word in the caption.\nAn RNN with Attention for an image captioning task at test time.\nBy using different context vectors at each timestep, the model can \u0026ldquo;attend\u0026rdquo; to different parts of the input image as it generates each word. Similar to sequence-to-sequence models, we can visualize attention weights overlayed on the image to gain insight into which parts of the image the model is focusing on at each step.\nAttention over time. As the model generates each word, its attention changes to reflect the relevant parts of the image.\nIn the image above, the model attends to the bird when generating \u0026ldquo;bird flying over,\u0026rdquo; and shifts focus to the water region for the word \u0026ldquo;water.\u0026rdquo; This is similar to how humans visually explore a scene, focusing on different parts depending on what weâ€™re describing.\nThis interpretabilityâ€”visualizing attention weightsâ€”sets attention mechanisms apart, as they provide insight into how a model makes its decisions, making them more transparent and understandable than other neural network approaches.\nA more detailed version on Attention can be found in Lil\u0026rsquo;s Log.\nReferences [1] Sutskever et al, â€œSequence to sequence learning with neural networksâ€, NeurIPS 2014.\n[2] Bahdanau et al, â€œNeural machine translation by jointly learning to align and translateâ€, ICLR 2015.\n[3] Xu et al, â€œShow, Attend and Tell: Neural Image Caption Generation with Visual Attentionâ€, ICML 2015.\n","permalink":"https://yugajmera.github.io/posts/06-attention/post/","summary":"In previous discussions, we focused on feedforward neural networks, which take a single image as input, process it through multiple layers of convolution, normalization, or fully connected layers, and output a single label for image classification tasks. This is a one-to-one relationship: a single image maps to a single output label.\nHowever, there are other types of problems we want to solve using deep learning that involve variable-length sequences as both inputs and outputs.","title":"Sequence Modeling with Recurrent Neural Networks and Attention"},{"content":"The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was an annual competition that took place from 2010 to 2017, attracting teams from around the world to showcase their best-performing image classification models. This challenge became a crucial benchmark in the field, with its winners significantly influencing the landscape of image recognition and deep learning research.\nThe competition used a subset of the ImageNet dataset, containing 1.3M training examples across 1000 different classes, with 50k validation and 100k test examples. The images were resized to $256 \\times 256$ pixels to standardize input, as they were originally downloaded from the web in various sizes.\nEvaluation was based on two key performance indicators: top-1 and top-5 error rates. The top-5 error rate is the fraction of test images for which the correct label is not among the model\u0026rsquo;s five most likely predictions. Teams with the lowest top-5 error rate emerged as the winners of this challenge.\nWinners of the challenge each year vs. Top-5 error rate\nEarly years For the first two years, 2010 and 2011, the winning models were not based on neural networks at all. Instead, they relied on multiple layers of hand-designed feature extractors with linear neural networks on top. At the time, training convolutional neural networks (CNNs) on large-scale, high-resolution images was prohibitively expensive due to the computational limitations of available hardware.\nThe breakthrough came in 2012 with the introduction of GPUs, which enabled a highly optimized implementations of 2D convolutions. This, combined with the large-scale ImageNet dataset that provided sufficient labeled examples, made it feasible to train deep CNN models without severe overfitting.\nSeveral factors contributed to the huge leap in 2012:\nData: The availability of the large-scale, well-labeled ImageNet dataset. Computation: Advances in GPUs enabled efficient training of deep networks. Algorithm: The introduction of AlexNet, a deep convolutional network architecture that took full advantage of the above advances, leading to a dramatic improvement in performance. AlexNet (2012) AlexNet [1] was the largest convolutional neural network trained at that time and achieved the best results reported on the ImageNet dataset, outperforming all other competitors by a large margin. This breakthrough made CNNs a mainstream topic in the field of computer vision, and AlexNet became one of the most influential works in the field.\nArchitecture The model has 8 layers: 5 convolutional layers and 3 fully-connected layers.\nActivation function: While Sigmoid and Tanh were common at the time, AlexNet was the first CNN architecture to use ReLU activation. The authors showed empirically that CNNs with ReLU train several times faster than their equivalents using Tanh units.\nNormalization: The authors used \u0026ldquo;Local Response Normalization\u0026rdquo; (LRN) after 1st and 2nd convolutional layers, which aided generalization, reducing the top-5 error rate by 1.2%. Although this technique has largely fallen out of use, but it was an early precursor to batch normalization.\nPooling layer: The network includes max-pooling layers with a $3 \\times 3$ kernel and a stride of 2, referred to as \u0026ldquo;overlapping pooling\u0026rdquo; due to the overlap of the kernels. The authors observed during training that this approach makes it slightly more difficult for the model to overfit, resulting in 0.3% lower error rate compared to a non-overlapping scheme (kernel size of 2 and stride of 2).\nDropout: The network\u0026rsquo;s size (60 million parameters) made overfitting a significant problem, even with such a large dataset. To address this, dropout with a probability of $0.5$ was added to the first two fully-connected layers, where the majority of parameters are located. Without dropout, the network required double the number of iterations for convergence.\nModel summary of AlexNet\nData Manipulation Preprocessing: The mean image of the training set (per-pixel mean) is subtracted from each pixel.\nAugmentation: Two data augmentation techniques are used to further reduce overfitting. These augmentations are performed on-the-fly on the CPU, while the GPUs trained the previous batch, making them computationally \u0026ldquo;free\u0026rdquo;.\nImage translations and horizontal reflection:\nTraining: Random $224 \\times 224$ patches (and their horizontal reflections) are extracted from $256 \\times 256$ images for training.\nNumber of transformations: $(256 - 224) * (256 - 224) = 1024$ Horizontal reflections: $1024 * 2 = 2048$ Testing: The network makes a prediction by extracting five $224 \\times 224$ patches (four corner patches and the center patch) as well as their horizontal reflections, averaging predictions across all ten patches.\nPCA color augmentation (also called Fancy PCA): This technique alters the intensities of the RGB channels in the training images. It captures an important property of imagesâ€”that object identity remains invariant to changes in intensity and illumination color.\nTraining Key hyperparameters used for training AlexNet:\nInitialization:\nWeights: Initialized from a zero-mean Gaussian distribution with a standard deviation of $0.01$ for each layer. Bias: Initialized to $1$ for the 2nd, 4th, and 5th convolutional layers, as well as in the fully connected layers. This choice accelerates early learning by providing ReLUs with positive inputs. All other layers have their biases initialized to $0$. Loss Function: Cross-entropy loss.\nOptimizer: Stochastic gradient descent (SGD)\nMomentum $m = 0.9$ L2 weight decay $\\lambda = 5e^{-4}$ Batch size: 128 Learning rate: Initially set to $0.01$ and reduced three times prior to termination by dividing by 10 when the validation error rate stopped improving with the current learning rate.\nNumber of epochs: 90\nTraining time: 5 to 6 days on two GTX 580 3GB GPUs.\nThe image above illustrates a typical AlexNet architecture. The model was spread across two GTX 580 3GB GPUs, as it was too large to fit into the memory of a single GPU. The GPUs communicated only in certain layers to optimize computation. However, with modern hardware, this complexity is unnecessary; today, the entire model can be trained on a single GPU (Google Colab, for instance, offers 12GB/16GB GPUs).\nSubmission: The CNN architecture described above achieved a top-5 error rate of 18.2%. The authors submitted an ensemble of 5 similar CNNs that yielded an error rate of 16.4%, winning the 2012 challenge.\nZFNet (2013) With AlexNet stealing the show in 2012, there was a significant increase in the number of CNN models submitted to ILSVRC 2013. The winner was ZFNet [2], an improved version of AlexNet that tweaked some layer configurations to achieve better performance.\nFirst layer adjustment: Alexnet used a large filter size of $11 \\times 11$ with a stride of 4 in the first layer. While this aggressive downsampling reduced computational cost, it also resulted in the loss of relevant pixel information. To address this, ZFNet used a $7 \\times 7$ sized filter with a stride of 2 in the first layer. To justify these changes, the authors proposed \u0026ldquo;deconvnet\u0026rdquo;, a technique to project output feature maps from each convolutional layer back to the input pixel space. This allows us to visualize different types of features learned by each layer, providing insights into the inner workings of CNNs:\nVisualization: Initial layers learn to detect general patterns such as corners, edges, and textures, while deeper layers capture class-specific details like dog faces, bird legs, and other object parts.\nEvolution during training: Lower layers of the model converged within a few epochs, while the upper layers only developed after a considerable number of epochs, demonstrating the need to let the models train until fully converged.\nInvariance: Small transformations strongly affect the first layer, while higher layers show greater stability, with minimal impact from translations and scalings. The network output remains stable under these transformations.\nOcclusion: When an object is occluded, the probability of the correct class drops significantly, indicating that the model relies heavily on local structure within the image rather than broad scene context.\nModel size adjustment: The authors also conducted an ablation study that revealed performance gains from increasing the size of the middle convolutional layers. Consequently, they modified layers 3, 4, and 5 to have 512, 1024, and 512 output channels, respectively. An ensemble of 6 CNNsâ€”five with the modified first layer and one incorporating both modificationsâ€”achieved the lowest error rate.\nImageNet 2012 classification error rates\nAlexNet and ZFNet were designed in a somewhat ad-hoc manner, with an arbitrary number of convolution and pooling layers, and the configurations of each layer set by trial and error. This makes scaling them quite challenging.\nVGGNet (2014) In 2014, the second-place winner of the ImageNet challenge was VGGNet [3], developed by the Visual Geometry Group at Oxford. This architecture was one of the first to have a principled design that guided the overall configuration of the network, enabling the creation of deeper networks and achieving significant improvements over previous configurations.\nArchitecture Let\u0026rsquo;s take a look at a side-by-side comparison of AlexNet, ZFNet, VGG-16, and VGG-19 architectures.\nVGGNet features clean and simple design principles. The configuration for each stage is fixed as follows:\nConvolutional layers: Kernel size of $3 \\times 3$, with a stride of 1 and padding of 1 (same padding).\nWhy? This is the smallest kernel capable of capturing directionality (left/right, up/down, center). This design choice ensures that the network remains compact while allowing for greater depth. Max-pooling layers: Kernel size of $2 \\times 2$, with a stride of 2.\nChannels: Starting from 64, the number of channels doubles after each pooling layer, reaching a maximum of 512.\nWhen the pooling layer downsamples the feature map by half, we double the number of channels to preserve the overall volume, thereby maintaining consistent time complexity (FLOPs) across each layer. Activation: ReLU non-linearities are used thoughout the network.\nNormalization: No normalization is applied, as it does not improve performance and instead increases memory consumption and computation time.\nDropout: Added to the first two fully connected layers, with a dropout ratio of $0.5$.\nWhile AlexNet has 5 convolutional layers, VGGNet comprises 5 stages:\nStage 1: conv-conv-pool\nStage 2: conv-conv-pool\nStage 3: conv-conv-conv-[conv]-pool\nStage 4: conv-conv-conv-[conv]-pool\nStage 5: conv-conv-conv-[conv]-pool\nTwo VGGNet variants were presented: VGG-16 and VGG-19, with 16 and 19 layers, respectively. VGG-19 includes an additional convolutional layer in stages 3, 4, and 5.\nThe convolutional layers are stacked to increase the receptive field. For instance, a stack of two $3 \\times 3$ layers achieves an effective receptive field of $5 \\times 5$, and three layers result in $7 \\times 7$. What do we gain by using three $3 \\times 3$ layers instead of a single $7 \\times 7$ layer?\nMore activations:Three non-linearities instead of one, making the decision function more discriminative.\nFewer parameters: Assuming the input and output have $C$ channels:\nThree $3 \\times 3$ layers: $3(3 * 3 * C^2) = 27C^2$ params. A single $7 \\times 7$ layer: $7 * 7 * C^2 = 49C^2$ params. Data Manipulation Preprocessing: The mean RGB value from the training set (per-channel mean) is subtracted from each pixel.\nAugmentation: Training images are first rescaled to a training scale, $S$. These rescaled images are then randomly cropped to $224 \\times 224$ and undergo random horizontal flipping and random RGB color shifting (similar to AlexNet).\nSingle training scale:\nTraining: Models are trained at two fixed scales, $S = 256$ or $S = 384$. Testing: The output is averaged over three test image versions rescaled at $ \\{ S - 32, S, S + 32 \\} $. Multi-training scale:\nTraining: Each image is rescaled individually by randomly sampling $S$ from the range $[S_{min} =256, S_{max} = 512]$. This approach, called scale jittering, enables the model to recognize objects at different scales. Testing: The output is averaged over three test image versions rescaled at $ \\{ S_{min}, S_{avg}, S_{max} \\} = \\{256, 384, 512 \\} $. This provides three trained versions of the same network: two single-scale models trained at fixed scales and one model trained using multiple scales.\nTraining Training hyperparameters and choices are similar to AlexNet.\nInitialization:\nPre-training: A shallow network is first trained with random initialization (weights from a zero-mean Gaussian distribution with $0.01$ variance and biases set to $0$), and then re-trained with additional convolutional layers. Xavier initialization: After submission, the authors discovered that Xavier initialization enables training without the need for pre-training. Loss Function: Cross-entropy loss.\nOptimizer: Stochastic gradient descent (SGD)\nMomentum $m = 0.9$ L2 weight decay $\\lambda = 5e^{-4}$ Batch size: 256 Learning rate: Initialized at $0.01$, and reduced three times by dividing by 10 when the validation accuracy stopped improving.\nNumber of epochs: 74\nTraining time: 2-3 weeks on four Titan Black 6GB GPUs.\nDespite having more parameters and greater depth than AlexNet, VGGNet required fewer epochs to converge due to (a) implicit regularization imposed by greater depth and and smaller convolutional filter sizes, and (b) pre-initialization of certain layers.\nSubmission: The authors submitted an ensemble of 7 networksâ€”six single-scale models and one multi-scale modelâ€”resulting in a top-5 error rate of 7.3%.\nResNet (2015) By 2015, it had become clear that increasing a networkâ€™s depth significantly improved its performance. However, deep networks typically face two major challenges:\nOverfitting: Regularization techniques like Batch Normalization (BatchNorm) help mitigate overfitting and enable higher learning rates.\nVanishing/Exploding gradients: Non-saturating activations like ReLU, combined with Kaiming Initialization to preserve signal variance, help address this. BatchNorm further ensures that forward-propagated signals maintain stable, non-zero variances.\nWThe development of BatchNorm and Kaiming Initialization in 2015 set the stage for experimenting with deeper models. However, a â€œdegradation problemâ€ came into light: as network depth increased, accuracy would initially saturate (which might be expected) and then degrade rapidly.\nTraining error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer networks. The deeper network has higher training and test error.\nThis degradation is not caused by overfitting, as deeper networks exhibit higher training error than their shallower counterparts, as shown in the figure above. The hypothesis suggests that this may be an optimization problem, indicating that deeper models are challenging to optimize, leading to underfiting.\nArchitecture Intuitively, we expect a deeper model to perform at least as well as a shallower model since it could theoretically emulate the shallower network by copying its layers and and setting the extra layers to identity. The fact that deeper models performed worse suggests that the solvers struggle to approximate identity mappings with multiple non-linear layers.\nResidual Block To address this, ResNet [4] introduced a new network design that simplifies learning identity mappings. If we let the stacked non-linear layers fit a mapping $\\text{F(x)}$ for an input $\\text{x}$, we can add a skip connection to recasted the original mapping to $\\text{F(x) + x}$.\nTo the extreme, if an indentity mapping was optimal, the solver would push the residual to zero, i.e. drive the weights of non-linear layers toward zero to approximate the identity mapping.\nWhile it is unlikely that identity mappings are optimal in practice, this reformulation helps precondition the problem, making it easier for the solver to find perturbations relative to an identity mapping rather than learning a new function from scratch.\nThese shortcut connections neither add extra parameters nor increase computational complexity, and they can be trained end-to-end using SGD with backpropagation without modification.\nA residual network, or ResNet, is formed by stacking multiple residual blocks. The image below shows a comparison of VGG-19, 34-layer plain, and residual networks.\nSimilar to VGGNet, ResNet is divided into four stages, each with a different number of residual blocks. The architecture includes:\nAggresive Stem: The input is aggressively downsampled with a $7 \\times 7$ filter and stride 2 (similar to ZFNet) before applying residual blocks.\nResidual block: Each residual block contains two $3 \\times 3$ convolutional layers.\nElement-wise addition is performed on two feature maps, channel by channel.\nSkip connection across two stages (shown by dotted lines): A $1 \\times 1$ convolution is added to the input to match the channel dimension, using a stride of 2 to match the spatial dimension. The output is given by, $$ \\text{H(x) = F(x) + W x} $$ where $\\text{W}$ is the projection shortcut, used solely for changing dimensions; other shortcuts are identity.\nChannels: Starting with 64 channels, the number is doubled after each stage, up to 512.\nStrided convolution: Instead of using pooling layers to halve the feature map after each stage, a stride of 2 is used in the first convolution of the next stage.\nNormalization: Each convolution is followed by a BatchNorm layer and a ReLU activation function.\nGlobal average pooling: Instead of using fully connected layers, the network ends with a global average pooling layer followed by a single linear layer with softmax to generate class scores.\nWhy? Fully connected layers have large number of parameters, increasing memory usage. Average pooling is applied to the last convolutional layer ($512 \\times 7 \\times 7$) using a $7 \\times 7$ kernel to cover the entire spatial structure. The authors presented two variants, ResNet-18 and ResNet-34, which have lower complexity than VGG-19, which has 19.6 GFLOPs.\nAs the 34-layer network performed better than the 18-layer network, it was clear that adding more layers could yield better performance. However, this also increases computational costs.\nBottleneck Residual Block To reduce computation, a bottleneck block is introduced with three convolutions: $1 \\times 1$, $3 \\times 3$ and $1 \\times 1$. The pointwise convolutions are used to reduce and then restore dimensions.\nComparing FLOPs = $(C_{out} \\times H\u0026rsquo; \\times W\u0026rsquo;)* (C_{in} \\times K_w \\times K_h)$,\nBasic residual block: $2 * [(64 \\times 56 \\times 56) * (64 \\times 3 \\times 3)]$ = $0.24$ GFLOPs.\nBottleneck residual block: $2 * [(64 \\times 56 \\times 56) * (256 \\times 1 \\times 1)]$ + $[(64 \\times 56 \\times 56) * (64 \\times 3 \\times 3)]$ = $0.22$ GFLOPs.\nThese additions introduce extra layers and non-linearities with a slight reduction in computational cost, enabling ResNet to add more layers without significantly increasing overall complexity.\nReplacing all basic blocks in ResNet-34 with bottleneck blocks results in the ResNet-50 architecture, a widely-used baseline. The authors further expanded ResNet with 101 and 152-layer variants using different numbers of bottleneck blocks.\nArchitecture of ResNet variants. Downsampling is performed by the first convolution of stages 2, 3, and 4 with a stride of 2. Error rates shown are for single-crop testing, as reported by torchvision.\nData Manipulation Preprocessing: A per-pixel mean is subtracted, as in AlexNet.\nAugmentation: Similar to VGGNet, multi-scale training is applied.\nTraining: Images are randomly rescaled in range $[256, 480]$ for scale augmentation, then randomly cropped to $224 \\times 224$, and subjected to random horizontal flipping and random RGB color shifting.\nTesting: Scores are averaged across multiple scales: $\\{ 224, 256, 384, 480, 640 \\}$.\nTraining Ttraining hyperparameters and choices are as follows.\nInitialization: Kaiming Initialization\nLoss Function: Cross-entropy loss.\nOptimizer: Stochastic gradient descent (SGD)\nMomentum $m = 0.9$ L2 weight decay $\\lambda = 1e^{-4}$ Batch size: 256 Learning rate: Initialized at $0.1$ and divided by 10 when the error plateus.\nNumber of epochs: 120 ($60 \\times 10^{4}$ iterations)\nSubmission: The authors submitted an ensemble of six networks with varying depths, achieving a 3.57% error rateâ€”surpassing human performance and securing the 2015 ImageNet challenge win.\nTransfer Learning Beyond using these CNN models solely for inspiration, transfer learning enables us to apply them directly to many tasksâ€”even with limited training data!\nThe core idea is to take a model pre-trained on ImageNet, remove its last fully-connected layer, and freeze the weights of the remaining layers. With this setup, the pre-trained model becomes an excellent feature extractor, effectively capturing complex patterns from input images.\nSmaller dataset: For smaller datasets, we typically train a new linear layer on top of this feature extractor, tailored to our specific task. This approach has proven effective across various downstream tasks, delivering impressive performance even with minimal data.\nLarger dataset: For larger datasets, you can go further with fine-tuning. Here, we combine the new linear layer with the pre-trained network and jointly train them on the new data. This gradual adjustment allows the network to adapt to the nuances of the new dataset while retaining valuable prior knowledge.\nReferences [1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, \u0026ldquo;ImageNet Classification with Deep Convolutional Neural Networks\u0026rdquo;, NeurIPS 2012.\n[2] Matthew D. Zeiler and Rob Fergus. â€œVisualizing and Understanding Convolutional Networksâ€, ECCV 2014.\n[3] Simonyan and Zissermann, â€œVery Deep Convolutional Networks for Large-Scale Image Recognitionâ€, ICLR 2015.\n[4] He et al, â€œDeep Residual Learning for Image Recognitionâ€, CVPR 2016.\n","permalink":"https://yugajmera.github.io/posts/05-imagenet/post/","summary":"The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was an annual competition that took place from 2010 to 2017, attracting teams from around the world to showcase their best-performing image classification models. This challenge became a crucial benchmark in the field, with its winners significantly influencing the landscape of image recognition and deep learning research.\nThe competition used a subset of the ImageNet dataset, containing 1.3M training examples across 1000 different classes, with 50k validation and 100k test examples.","title":"ImageNet Challenge: The Olympics of Deep Learning"},{"content":"Linear classifiers or MLPs that we have discussed so far don\u0026rsquo;t respect the 2D spatial structure of input images. These images are flattened into a 1D vector before passing them through the network which destroys the spatial structure of the image.\nThis creates a need for a new computational model that can operate on images while preserving spatial relationships â€” Convolutional Neural Networks (CNNs). Let\u0026rsquo;s understand the components of this CNN model.\nConvolutional Layer To respect the 2D structure of the input image, we use a 2D learnable weight matrix of shape $(K_w, K_h)$, called a kernel, which convolves over the input image. That is, the kernel slides spatially across the image channel, computing dot products at each position.\nRecall that the input image is a 3D tensor of shape $(C, H, W)$, where $C = 3$ for an RGB image. For the example below, consider a single-channel input of shape $(5,5)$ convolving with a $3 \\times 3$ kernel.\nThe values in this kernel matrix are weights learned during training. The same kernel is applied across all positions of the input channel, and each position computes a dot product, outputting a single number.\nThe intuition behind convolving kernels with images is rooted in classical computer vision. Kernels help extract feature information from an image, such as edges, textures, and patterns. These are useful for tasks like blurring, sharpening, edge detection, and more.\nInstead of manually designing these kernels for feature extraction, we let the model learn them from the training data. This allows the network to automatically discover \u0026ldquo;good\u0026rdquo; kernels that extract the most relevant features for correctly classifying the input image.\nDifferent channels of the image encode distinct features, so it\u0026rsquo;s essential to learn a different kernel matrix for each input channel and combine the results to form a unique feature representation of the image.\nFilter Since we have a unique kernel matrix for each input channel, the learnable matrix is a 3D tensor of weights with shape $(C, K_w, K_h)$, known as a filter. The depth of the filter (i.e., the number of kernels) always matches the number of input channels.\nThe figure below shows an example of a convolution operation on an RGB image with $3 \\times 3$ kernels.\nThese outputs of convolutions are summed across all channels, along with a bias term, to produce a single value. This value forms one entry in the 2D output called an activation map, or feature map.\nSimilar to a linear classifier where we have: $$ f(\\mathbf{x}, \\mathbf{w}) = \\mathbf{w}_1 \\mathbf{x}_1 + \\mathbf{w}_2 \\mathbf{x}_2 + \\mathbf{w}_3 \\mathbf{x}_3 + \\mathbf{b} $$ In a convolution, we have: $$ A(\\mathbf{x}, \\mathbf{k}_{1:3}) = \\mathbf{k}_1 * \\mathbf{x}_{\\text{channel 1}} + \\mathbf{k}_2 * \\mathbf{x}_{\\text{channel 2}} + \\mathbf{k}_3 * \\mathbf{x}_{\\text{channel 3}} + \\mathbf{b} $$\nEach 3D filter produces one 2D feature map, with each map corresponding to a specific feature or pattern detected in the input. Since we want multiple features to be detected, we use multiple 3D filters. The output is formed by stacking the activation maps computed from each filter, resulting in a 3D tensor, where the depth corresponds to the number of filters used.\nIn the example below, a $(3, 32, 32)$ RGB image is convolved with six filters of $5 \\times 5$ kernels.\nTo formulate this, let $N$ be the number of images in a mini-batch, $C_{in}$ the number of input channels, and $C_{out}$ the number of output channels (or filters) in the convolution layer:\n\\begin{align} \\underbrace{N \\times C_{in} \\times H \\times W}_{\\text{Input size}} + \\underbrace{C_{out} \\times C_{in} \\times K_w \\times K_h}_{\\text{Filters size}} \\Rightarrow \\underbrace{N \\times C_{out} \\times H\u0026rsquo; \\times W\u0026rsquo;}_{\\text{Output size}} \\end{align}\nAssuming $H = W$ (square image) and $K_w = K_h$ (square kernel), the output size is given by: $$ W\u0026rsquo; = \\frac{(W - K_w + 2P)}{S} + 1 $$ where $P$ is the padding size and $S$ is the stride. Let\u0026rsquo;s understand the significance of these terms.\nPadding When we convolve a filter with an image, the spatial size of the output reduces, i.e., $W\u0026rsquo; \u0026lt; W$. The feature map shrinks, as shown below.\nTo preserve the spatial size, we pad the input image with zeros around the borders. For instance, $P = 1$ refers to adding a single border of zeros around the input.\nPadding also helps retain edge information, as without it, edge pixels contribute less to the output because the filter doesn\u0026rsquo;t fully cover them.\nThe two most common padding terms are:\nValid padding: No padding is applied, i.e., $P = 0$. Same padding: The output size equals the input size, i.e., $W\u0026rsquo; = W$. This is achieved with $P = (K_w - 1)/2 $. Stride Earlier, I mentioned that we convolve our filter with every pixel of the input channel. However, we can choose to convolve with every alternate pixel by setting the stride $S = 2$.\nStride refers to the number of pixels by which the filter moves across the input image during the convolution operation. When using a stride greater than 1, the filter skips certain pixels, effectively reducing the spatial size of the input.\nThe image below shows a convolution operation with padding = 1 and stride = 2.\nUsing larger strides can effectively downsample the input, reducing computational costs and speeding up training. However, it may result in skipping over fine-grained details in the input image.\nNext, let\u0026rsquo;s examine how the spatial size of feature maps affects computation costs in a convolutional layer.\nComputational Cost Consider a convolutional layer with 10 filters of size $5 \\times 5$, stride 1, and pad 2 (same padding), applied to an RGB image of size $(3, 32, 32)$.\nInput: $[C_{in} \\times H \\times W] = 3 \\times 32 \\times 32$ Convolution: $[C_{out} \\times C_{in} \\times K_w \\times K_h]$ = 10 filters of size $3 \\times 5 \\times 5$ Output: $[C_{out} \\times H\u0026rsquo; \\times W\u0026rsquo;] = 10 \\times 32 \\times 32$ For a batch size of $B=1$, let\u0026rsquo;s calculate:\n1. Memory: To store the output as floats: $(C_{out} \\times H\u0026rsquo; \\times W\u0026rsquo;) * 4/1024$ $(10 \\times 32 \\times 32) * 4/1024$ = 40 KB.\n2. Number of learnable parameters: $\\underbrace{(C_{out} * C_{in} \\times K_w \\times K_h)}_{\\text{filters}} + \\underbrace{C_{out}}_{\\text{bias}}$ $(10 * 3 \\times 5 \\times 5) + 10$ = 760.\n3. Number of Floating-point operations (FLOPs): Total multiply-add operations $\\underbrace{(C_{out} \\times H\u0026rsquo; \\times W\u0026rsquo;)}_{\\text{output size}} * \\underbrace{(C_{in} \\times K_w \\times K_h)}_{\\text{filter size}}$\n$10 \\times 32 \\times 32 = 10240$ outputs, each of which is the inner product of input with a $3 \\times 5 \\times 5 = 75$ tensor. Total = $75 * 10240$ = 768k FLOPs.\nMemory calculations help estimate the maximum batch size that can be used without exceeding GPU memory limits, learnable parameters define the modelâ€™s capacity to learn from data, and FLOPs influence the computational time during training and inference.\nReceptive Fields Receptive fields refer to the specific regions of the input data that a feature map responds to. In the convolution shown below, each element of the output layer depends on a $3 \\times 3$ receptive field in the input.\nTo cover the entire input of size $(7, 7)$ with $3 \\times 3$ kernels, at least 3 convolution layers are needed to ensure that a single output can \u0026ldquo;see\u0026rdquo; the whole input image.\nStacking multiple convolution layers increases the size of the receptive fields, allowing the network to capture more global information about the input. However, this stacking would result in one large convolution, so we typically apply activation functions (non-linearities) after each convolution. ReLU is a commonly used activation function in this context.\nFor large images, many layers may be required to capture global information, which can become computationally expensive. A solution is to downsample the feature map within the network, effectively increasing the receptive fields of subsequent layers while reducing computation. This is where pooling layers come into play.\nPooling Layer Unlike convolutional layers, which apply a filter to extract features, pooling layers summarize information within a localized region of the input. One common pooling method is max pooling, where we take the maximum value from the elements within a defined kernel.\nPooling layers have two hyperparameters: kernel size and stride. For example, consider a max pooling layer with a kernel size of 2 and a stride of 2:\nThis operation effectively reduces the spatial dimensions of the input by half, while retaining the most prominent features (the strongest responses) and discarding less relevant details.\nThe max operation introduces translational invariance: whether a key feature, like a cat\u0026rsquo;s ear, is located in the top-left corner of the filter or the bottom-right, the output remains the same since we are taking the maximum over the kernel. This property allows the model to be more robust to small spatial shifts in the input.\nHere are some key features of pooling layers:\nPooling layers do not contain learnable parameters, which simplifies the backpropagation process. During backpropagation, they simply pass gradients back to the locations of the maximum values from the previous layer. As pooling is a non-linear operation, it adds non-linearity to the model without requiring an additional activation function. Another pooling method, average pooling, calculates the average of the elements within the kernel. This approach tends to smooth out the feature map by considering all values within the window, making it useful for reducing noise. However, max pooling is generally preferred for its ability to highlight the most significant features in the input.\nNetwork Architecture To maintain the spatial dimension of the input, same padding with stride 1 is frequently used. Below are some common choices for hyperparameters in convolutional layers:\n$C_{in}$, $C_{out}$ = 32, 64, 128, 256 (in powers of 2) Kernel = $5 \\times 5$, Padding = 2, Stride = 1 Kernel = $3 \\times 3$, Padding = 1, Stride = 1 Kernel = $3 \\times 3$, Padding = 1, Stride = 2 [downsample by 2] Kernel = $1 \\times 1$, Padding = 0, Stride = 1 [Pointwise convolution] Using channel dimensions that are powers of 2 allows for more efficient memory allocation on GPUs.\nThe $1 \\times 1$ convolution layer is also called pointwise convolution because the filter operates on each pixel individually across the depth (channels) of the input. This allows changing the number of channels while keeping the spatial dimensions intact, as the depth of the resulting feature map is determined by the number of filters used.\nA classical architecture for a convolutional neural network follows this structure: $$ [\\text{Conv, ReLU, Pool}]_{\\times N} \\rightarrow Flatten \\rightarrow [\\text{FC, ReLU}]_{\\times M} \\rightarrow FC $$\nThe input image is first processed through multiple layers of convolution, followed by ReLU activations, and then pooling to downsample the feature maps.\nThe initial layers of a CNN learn to detect basic patterns such as edges, corners, and simple textures. These low-level features are often universal across different images, making them not specific to any particular object class.\nAs the network deepens, subsequent layers begin to detect more complex patterns by combining low-level features to recognize higher-level representations, such as shapes, textures, or parts of objects (e.g., eyes, wheels, or fur textures).\nAfter the convolutional and pooling layers, the output is a multi-dimensional tensor representing the learned features. This tensor is then flattened into a 1D vector and fed into fully connected (FC) layers with ReLU activations. These layers combine the learned features to make predictions.\nThe final fully-connected layer produces the output, typically class scores for each category in a classification task.\nVisualization of features in a fully trained model. The left image shows the kernels learned by the first convolutional layer of AlexNet [1], while the right image displays the features learned in Layers 3-5 of ZFNet [2].\nCoding Let\u0026rsquo;s modify our model from here to a convolutional neural network for classifying handwritten digits in the MNIST dataset.\n# Define the model class CNN(torch.nn.Module): def __init__(self): super(CNN, self).__init__() self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1) # Output: 32 x 28 x 28 self.max_pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2) # Output: 32 x 14 x 14 self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1) # Output: 64 x 14 x 14 self.max_pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) # Output: 64 x 7 x 7 self.fc1 = torch.nn.Linear(7 * 7 * 64, 128) self.fc2 = torch.nn.Linear(128, 10) self.relu = torch.nn.ReLU() def forward(self, x): x = self.relu(self.conv1(x)) x = self.max_pool1(x) x = self.relu(self.conv2(x)) x = self.max_pool2(x) x = x.view(x.shape[0], -1) # Flatten x = self.relu(self.fc1(x)) x = self.fc2(x) return x # Initialize the model and move it to the selected device model = CNN().to(device) I used the Adam optimizer with a learning rate of 0.001, achieving a test accuracy of 99%.\nTo view the computational statistics of our model, we can use torchsummary:\nfrom torchsummary import summary summary(model, (1, 28, 28)) We can draw the following conclusions from this:\nThe memory used to store the output as floats is primarily consumed by the convolutional layers. Almost all learnable parameters are found in the fully connected layers. While it\u0026rsquo;s possible to create deeper convolutional neural networks, training them can be quite challenging. Like linear networks, they face overfitting issues, and convergence becomes increasingly difficult as depth increases. A common solution to this problem is Batch Normalization [3].\nBatch Normalization The idea behind batch normalization is to normalize the activations of a layer so that they have zero mean and unit variance. But why is this necessary?\nAs a neural network trains, the distribution of inputs to its layers can shift due to weight updates. This phenomenon, known as internal covariate shift, can cause the learning algorithm to struggle, effectively chasing a moving target. Batch normalization helps mitigate this shift, stabilizing the training process and improving optimization.\nIn addition, normalizing the outputs of a layer ensures that the distribution is well-behaved before passing through the non-linear activation function, which helps prevent issues like saturation.\nIn practice, batch normalization is implemented as a layer that processes inputs before they are passed to the next layer. The normalization can be mathematically described as:\n\\begin{align} \u0026amp; \\hat{x}_{ij} = \\frac{x_{ij} - \\mu_j}{\\sqrt{\\sigma_j^2 + \\epsilon}} \\\\ \u0026amp; y_{ij} = \\gamma_j \\hat{x}_{ij} + \\beta_j \\end{align}\nHere, $\\mu_j$ and $\\sigma_j$ are the mean and standard deviation calculated across the mini-batch of inputs for each channel.\nSince maintaining zero mean and unit variance can be too restrictive, we introduce scale ($\\gamma$) and shift ($\\beta$) parameters that can be learned during training. This ensures that the activations of the layer remain Gaussian throughout the training process.\nA beneficial side effect of this normalization process is that it introduces some randomness, which can enhance regularization and improve the modelâ€™s generalization ability.\nUnused Bias The bias added to $x_{ij}$ in the previous layer is effectively canceled out when the mean of the outputs is subtracted during batch normalization. As a result, the bias in the layer preceding the batch normalization layer becomes redundant and can be omitted.\nTraining vs Testing Since the $\\mu_j$ and $\\sigma_j$ are computed from the mini-batch, their values depend on the specific batch. For example, if one test batch contains [cat, dog, frog] and another contains [cat, car, horse], the output for the common image of the cat will differ due to varying means and standard deviations.\nTo address this, batch normalization behaves differently during training and testing. During testing, we do not compute the mean and standard deviation from the batch; instead, we fix these values and use the running averages collected during training.\nThe running averages are updated during each training step as:\n\\begin{align} \\mu_{\\text{running}} = (1 - \\beta) * \\mu_{\\text{running}} + \\beta * \\mu_j \\end{align}\nwhere $\\beta$, called the momentum, is typically set to $0.1$.\nAs a result, during inference, batch normalization becomes a linear operation, allowing it to be easily fused with preceding linear or convolutional layers. Typically, the batch norm layer is inserted after fully connected or convolutional layers and before the activation function.\nBenefits of Batch Normalization In summary, batch normalization offers several advantages:\nMakes deep networks much easier to train by mitigating internal covariate shift. Allows for higher learning rates, which can lead to faster convergence during training. Normalization process makes networks more robust to weight initialization. Acts as a form of regularization during training. Zero computational overhead at test time (uses fixed parameters so can be fused with the previous layer) Variants While batch normalization is powerful, it behaves differently during training and testing. To address this, several variants have been developed.\nLayer Normalization Instead of averaging over batch dimensions, it averages over the feature dimension, resulting in per-channel mean and standard deviation. This makes it independent of the batch size, and it behaves the same during both training and testing. Layer normalization is commonly used in RNNs and Transformers, where batch sizes can vary significantly.\nInstance Normalization Here, the normalization is done over the spatial dimensions of each image, resulting in per-image mean and standard deviation. It also behaves consistently during training and testing, and is often used in style transfer and image generation tasks.\nGroup Normalization Instead of normalizing across the entire channel dimension like layer normalization, group normalization splits the channels into groups and normalizes within each group. This method is particularly effective for tasks like object detection and is commonly used in group convolutions.\nThe figure below provides an intuitive understanding of the four types of normalizations.\nCoding Let\u0026rsquo;s add Batch Normalization to our model. Since Batch Normalization behaves differently during training and testing, we must ensure to use model.train() and model.eval() to switch between these modes. This guarantees that we compute the mean and variance in training mode and use the running averages in evaluation mode.\nAdditionally, since most of the learnable parameters are in the fully connected layers, thereâ€™s a higher risk of overfitting. To address this, Iâ€™ll include a dropout layer for regularization.\n# Creating the architecture class DigitClassification(torch.nn.Module): def __init__(self): super(DigitClassification, self).__init__() # First convolutional block: Conv -\u0026gt; BatchNorm -\u0026gt; ReLU -\u0026gt; MaxPool self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1, bias=False) self.bn1 = torch.nn.BatchNorm2d(32) self.max_pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2) # Second convolutional block: Conv -\u0026gt; BatchNorm -\u0026gt; ReLU -\u0026gt; MaxPool self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False) self.bn2 = torch.nn.BatchNorm2d(64) self.max_pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) # Fully connected layers: FC -\u0026gt; ReLU -\u0026gt; Dropout -\u0026gt; FC self.fc1 = torch.nn.Linear(7 * 7 * 64, 128) self.dropout = torch.nn.Dropout(p=0.5) self.fc2 = torch.nn.Linear(128, 10) self.relu = torch.nn.ReLU() pass def forward(self, x): x = self.relu(self.bn1(self.conv1(x))) x = self.max_pool1(x) x = self.relu(self.bn2(self.conv2(x))) x = self.max_pool2(x) x = x.view(x.shape[0], -1) # Flatten x = self.relu(self.fc1(x)) x = self.dropout(x) x = self.fc2(x) return x With this implementation, I achieved a test accuracy of 99.3%, giving us the best model weâ€™ve had until now.\nIt can be challenging to make numerous decisions regarding the architecture of a CNN model and its various hyperparameters in pursuit of the best possible accuracy. Therefore, itâ€™s always beneficial to review state-of-the-art CNN models that have been successful in the past for inspiration. Letâ€™s explore that in our next post.\nReferences [1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, \u0026ldquo;ImageNet Classification with Deep Convolutional Neural Networks\u0026rdquo;, NeurIPS 2012.\n[2] Matthew D. Zeiler and Rob Fergus. â€œVisualizing and Understanding Convolutional Networksâ€, ECCV 2014.\n[3] Ioffe and Szegedy, â€œBatch normalization: Accelerating deep network training by reducing internal covariate shiftâ€, ICML 2015.\n","permalink":"https://yugajmera.github.io/posts/04-cnn/post/","summary":"Linear classifiers or MLPs that we have discussed so far don\u0026rsquo;t respect the 2D spatial structure of input images. These images are flattened into a 1D vector before passing them through the network which destroys the spatial structure of the image.\nThis creates a need for a new computational model that can operate on images while preserving spatial relationships â€” Convolutional Neural Networks (CNNs). Let\u0026rsquo;s understand the components of this CNN model.","title":"Convolutional Neural Networks: Deep Learning for Image Recognition"},{"content":"Weâ€™ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost readyâ€”but we still need the toppings to make it more appealing. In this part, weâ€™re going to discuss the available toppingsâ€”concepts that enhance optimization and help us reach a better final solution for the modelâ€™s weights. The most important topping among them is Regularization.\nRegularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set. But in practice, this doesnâ€™t always work as expected! When we focus solely on minimizing loss on the training dataâ€”essentially trying to fit it perfectlyâ€”we run the risk of not generalizing well to the test data, a phenomenon known as overfitting.\nTo illustrate this, letâ€™s consider an example. The blue dots in the figure below represent the training data. We fit two models: $m_1$ (a polynomial model) and $m_2$ (a linear model). Model $m_2$ has more layers, leading to a more complex decision boundary that fits the training data almost perfectly.\nIt\u0026rsquo;s important to remember that both the training and test sets are assumed to be sampled from a common dataset, represented as $p_{\\text{data}}$. Our goal should be to fit this probability distribution and not just the training data alone, as this ensures good performance on unseen test data. Now, letâ€™s examine the test set, represented by the yellow points in the next figure.\nWhile $m_2$ fits the training data perfectly, it struggles with the test data because it overfits the training set. Model $m_1$, despite being simpler, performs better on the test set as it generalizes better to the underlying distribution $p_{\\text{data}}$.\nIf a simpler model like $m_1$ performs better on the test set, you might wonder why we donâ€™t always use smaller models. The reason is that a smaller model runs the risk of underfitting, where it fails to capture the complexities of the data.\nSince the true distribution $p_{\\text{data}}$ is unknown to us, our best approach is to use a larger, more flexible model and apply techniques to help it generalize wellâ€”this is where regularization comes in. Regularization helps ensure that our model doesnâ€™t overfit by encouraging it to find simpler decision boundaries.\nThere are several ways to regularize a model, and weâ€™ll dive into each of these methods in detail next.\nWeight Decay The simplest way to make a deeper model behave more like a shallow one is to effectively reduce the influence of some weights by shrinking them. To discourage the model from becoming overly complex, we add a penalty to the loss function based on the magnitude of the modelâ€™s weights.\n$$ L(\\mathbf{W}) = \\frac{1}{N} \\sum_{i = 1}^N L_i(x_i, y_i, \\mathbf{W}) + \\lambda R(\\mathbf{W}) $$\nwhere $\\lambda$ is the hyperparameter that controls the regularization strength, i.e., how much to shrink the weights. Since we minimize this loss function during optimization, the added term helps decay some weights in the model. $R(\\mathbf{W})$, the regularization term, varies depending on the type of weight decay:\n\\begin{align} \\text{L2: } \u0026amp; R(\\mathbf{W}) = \\sum_{k} \\sum_{l} \\mathbf{W}_{k,l}^2 \\\\ \\text{L1: } \u0026amp; R(\\mathbf{W}) = \\sum_k \\sum_l |\\mathbf{W}_{k,l}| \\\\ \\text{Elastic Net (L1 + L2): } \u0026amp; R(\\mathbf{W}) = \\sum_k \\sum_l \\beta \\mathbf{W}_{k,l}^2 + |\\mathbf{W}_{k,l}| \\end{align}\nL2 regularization shrinks weights in proportion to their size, meaning larger weights get shrunk more (since we add their square to the loss function). This type of regularization prefers to keep all available features but with smaller weight values.\nL1 regularization shrinks all the weights by the same amount, with the smaller weights getting zeroed out. The intuition here is that small weights indicate that the feature associated with that weight is less important, so zeroing it out wonâ€™t significantly affect the output.\nLet\u0026rsquo;s take a simple example to understand this better: \\begin{align} \\mathbf{x} \u0026amp;= [1, 1, 1, 1] \\\\ \\mathbf{w}_1 \u0026amp;= [1, 0, 0, 0] \\\\ \\mathbf{w}_2 \u0026amp;= [0.25, 0.25, 0.25, .0.25] \\end{align} Both weight vectors produce the same output, $\\mathbf{w}_1^T \\mathbf{x} = \\mathbf{w}_2^T \\mathbf{x}$. However, L1 regularization would prefer $\\mathbf{w}_1$ because it sparsifies the weights (i.e., zeros out some weights), while L2 regularization would prefer $\\mathbf{w}_2$, since it retains all the features but with smaller values. Since we usually want to keep all features active, L2 regularization is the more commonly used approach. A typical value for the L2 regularization parameter $\\lambda$ is 1e-4.\nLetâ€™s now look at the mechanics of weight decay. When we apply L2 regularization, it modifies the gradient update step as follows: \\begin{align} L(\\mathbf{w}) \u0026amp;= L_{\\text{data}}(\\mathbf{w}) + \\color{blue} \\lambda |\\mathbf{w}|^2 \\\\ g_t \u0026amp;= \\nabla L_{\\text{data}}(\\mathbf{w}_t) + \\color{blue} 2 \\lambda \\mathbf{w}_t \\\\ s_t \u0026amp;= \\text{optimizer} (g_t) = \\text{optimizer} (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ \\mathbf{w}_{t+1} \u0026amp;= \\mathbf{w}_t - \\eta s_t \\end{align}\nThe blue term represents the weight decay and it can directly be incorporated into our optimizer. This can be done in PyTorch using the weight_decay argument.\n# Define the optimizer with L2 regularization optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-4) While L2 regularization and weight decay are often used interchangeably for SGD and SGD with momentum, they diverge in behavior when applied to adaptive methods like AdaGrad, RMSProp, and Adam. Letâ€™s explore why.\nDecoupled Weight decay Consider adding L2 regularization to Adam:\n\\begin{align} m_1 \u0026amp;= \\beta_1 m_1 + (1 - \\beta_1) (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ m_2 \u0026amp;= \\beta_2 m_2 + (1 - \\beta_2) (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ {m_1}_{\\text{unbias}} \u0026amp;= m_1 / (1 - \\beta_1^t) \\\\ {m_2}_{\\text{unbias}} \u0026amp;= m_2 / (1 - \\beta_2^t) \\\\ \\mathbf{w}_{t+1} \u0026amp;= \\mathbf{w}_{t} - \\eta * \\frac{{m_1}_{\\text{unbias}}}{\\sqrt{{m_2}_{\\text{unbias}} + \\epsilon}} \\end{align}\nBy substituting the unbiased value of $m_1$ into the update step, we get:\n\\begin{align} \\mathbf{w}_{t+1} \u0026amp;= \\mathbf{w}_{t} - \\eta * \\frac{\\beta_1 m_1 + (1 - \\beta_1) (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} )}{(\\sqrt{{m_2}_{\\text{unbias}} + \\epsilon}) ((1 - \\beta_1^t))} \\end{align}\nNotice how the blue term is normalized by the square root of the unbiased second moment, which tracks the sum of squared gradients. As a result, if the gradients for a particular weight are large, that weight will be regularized less than weights with smaller gradients. This means that the effect of regularization becomes dependent not only on the size of the weights but also on the size of the gradients.\nThis leads to unintended behavior and making L2 regularization less effective than intended. In contrast, with SGD and SGD with Momentum, L2 regularization works as expected: \\begin{align} \\text{SGD: } \u0026amp; \\Delta \\mathbf{w}^{t} = - \\eta * (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ \\text{SGD + Momentum: } \u0026amp; \\Delta \\mathbf{w}^{t} = - \\eta * (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) + m * \\Delta \\mathbf{w}^{t-1} \\end{align}\nTo fix this issue in Adam, a variant called AdamW [1] was introduced. In this variant, the weight decay term is decoupled from the optimizer and added directly to the update step.\n\\begin{align} L(\\mathbf{w}) \u0026amp;= L_{\\text{data}}(\\mathbf{w}) \\\\ \\mathbf{dw} = g_t \u0026amp;= \\nabla L_{\\text{data}}(\\mathbf{w}_t) \\\\ s_t \u0026amp;= \\text{optimizer} (g_t) \\\\ \\mathbf{w}_{t+1} \u0026amp;= \\mathbf{w}_t - \\eta (s_t + {\\color{orange} 2 \\lambda \\mathbf{w}_t} ) \\end{align}\nThis adjustment ensures that the weight decay term remains unaffected by the moving averages, keeping it directly proportional to the weights themselves.\nAdamW has been shown to significantly improve the generalization performance of the standard Adam optimizer. It is always the preferred choice when we want effective regularization with adaptive optimizers.\nDropout Another powerful regularization technique is Dropout [2], where we randomly set a fraction of neurons in a layer to zero during each forward pass. The dropout rate (probability of dropping a neuron) is a hyperparameter, typically set to 0.5.\nBy zeroing out neurons, the network is forced to avoid relying too heavily on any individual neuron or set of neurons, encouraging the model to distribute learning across different parts of the network. This approach effectively simulates training a large number of different sub-networks, each with a unique structure but sharing weights.\nIn terms of interpretation, dropout creates a more robust network by introducing redundancy in the learned features. Since neurons are randomly excluded during each forward pass, the model must learn multiple independent representations of the data. This redundancy helps prevent overfitting, as the model doesn\u0026rsquo;t rely on specific patterns that might only exist in the training data, generalizing better on unseen data.\nInverted dropout Dropout introduces randomness into the model during training, which can make prediction outputs non-deterministic. However, during inference, we want consistent predictions. To handle this, we use inverted dropout, that rescales the output of active neurons during training, to adjust for the dropped ones.\nWhen neurons are randomly dropped, the outputs of the remaining neurons are scaled up by the inverse of the dropout rate to maintain the overall magnitude of activations. For instance, if half of the neurons are dropped (dropout rate = 0.5), we double the outputs of the remaining neurons during training to compensate.\nDropout is most commonly applied to fully connected layers, as these layers tend to contain the majority of learnable parameters, making them more prone to overfitting. Here\u0026rsquo;s how you can add it to your network:\nclass TwoLayerNet(torch.nn.Module): def __init__(self): super(TwoLayerNet, self).__init__() self.fc1 = torch.nn.Linear(28 * 28 * 1, 512) self.fc2 = torch.nn.Linear(512, 10) self.dropout = torch.nn.Dropout(p=0.5) # Add dropout self.relu = torch.nn.ReLU() def forward(self, x): x = self.relu(self.fc1(x)) x = self.dropout(x) x = self.fc2(x) return x Early Stopping Another way to prevent overfiting is to stop training when the validation accuracy starts to decrease. While training, we typically plot three curves:\nTraining loss vs Number of iterations Training accuracy vs Number of iterations Validation accuracy vs Number of iterations These curves provide insight into the \u0026ldquo;health\u0026rdquo; of the model. Ideally, training loss should decay exponentially with each iteration, while both training and validation accuracy should increase. A large gap between the training and validation accuracy suggests overfitting, whereas little to no gap indicates underfitting.\nAt each iteration, we save the model\u0026rsquo;s weights as checkpoints and later select the checkpoint where the validation accuracy is at its maximum. Training beyond this point might increase the training accuracy (as the model starts overfitting), but it could perform poorly on the test set because validation accuracy reflects the model\u0026rsquo;s ability to generalize.\nThis technique is called early stopping because it prevents the model from training all the way to the full number of iterations (or num_epochs hyperparameter), stopping earlier to capture the best version of the model without overfitting.\nData Manipulation The next topping is data manipulation, where we explore how to process and modify the training dataset to make it more suitable for efficient learning.\nPreprocessing At every step of the optimization algorithm, the loss function is computed on a mini-batch of the training data. Therefore, the shape of the loss landscape, which the optimization algorithm navigates, depends directly on how the training data is structured. To make this landscape more favorable for optimization, we often preprocess the data before feeding it into the neural network.\nA common preprocessing step is normalizing the data, which zero-centers the data and ensures all features have the same variance. This step helps smooth out the optimization process by making the model less sensitive to small weight changes.\nThe pixel values range from [0, 255], and are divided by 255 on conversion to tensors, resulting in in values in range [0, 1]. We then normalize these values with a mean of 0.5 and a standard deviation of 0.5 to obtain pixel values in the range [-1, 1]. This can be included in the list of transforms in PyTorch:\n# Apply transforms to dataset transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), # Convert image to Tensor torchvision.transforms.Normalize((0.5,), (0.5,)) # Normalize with mean=0.5, std=0.5 ]) The same transform is also applied to the test set to maintain consistency.\nOther standard preprocessing techniques for image data include:\nSubtracting the mean image Subtracting per-channel mean (mean along each color channel) Subtracting per-channel mean and divide by the per-channel standard deviation Augmentation Data augmentation is a widely-used technique that increases both the size and diversity of the training data by applying label-preserving transformations to the input images. Not only does this artificially expand the dataset, but it also adds regularization by introducing noise in the training data, which helps prevent overfitting.\nIn computer vision, augmentations like flipping, rotating, scaling, blurring, jittering, and cropping are widely used as an implicit form of regularization. Augmentations also encode invariances into the modelâ€”this means we teach the model to learn that certain transformations shouldn\u0026rsquo;t change the output.\nThink about your specific task: what transformations should not change the desired network output? The answer will vary depending on the problem. For instance, a vertical flip might make sense for images of white blood cells but not for car images (as cars are never upside down in real-world scenarios).\nAugmentations can also be included in the transforms list,\n# Apply transforms to dataset transform = torchvision.transforms.Compose([ torchvision.transforms.RandomRotation(degrees=15), # Randomly rotate by +/- 15 degrees torchvision.transforms.ToTensor(), # Convert image to Tensor torchvision.transforms.Normalize((0.5,), (0.5,)) # Normalize with mean=0.5, std=0.5 ]) Note that augmentations are typically excluded from the test set to ensure the model is evaluated on the original data distribution.\nLearning Rate Schedules All the optimizers weâ€™ve discussed so farâ€”SGD, SGD with Momentum, Adagrad, RMSProp, and Adamâ€”use a fixed learning rate as a hyperparameter to guide the search for the global minimum. As you may have learned by now, this learning rate is a crucial variable in our learning process.\nDifferent learning rates produce different learning behaviors, as shown in the figure below. Therefore, it\u0026rsquo;s essential to choose an appropriate learning rate, ideally the red one. However, finding that one \u0026ldquo;perfect\u0026rdquo; learning rate through trial and error is not always feasible.\nWhat if we donâ€™t keep the learning rate fixed and instead change it during the training process? We can start with a high learning rate to allow our optimization to make quick progress in the initial iterations and then gradually decay it over time, ensuring that the model converges to a lower loss at the end. This would lead to faster convergence and better performance characteristics.\nThis mechanism of changing the learning rate over the course of training is called learning rate scheduling. Letâ€™s look at some commonly used learning rate schedules.\nStep Schedule In a step schedule, we start with a high learning rate (similar to the green curve in the figure), and when the loss curve starts to plateau, we decrease the learning rate. This process continues until convergence.\nFor example, we might reduce the learning rate by a factor of 0.1 after epochs 30, 60, and 90. The learning curve would then look something like this.\nSince weâ€™re decaying the learning rate at arbitrary points during training, this schedule introduces additional hyperparameters, like the number of steps and when to decay. A common approach is to monitor the loss and decay the learning rate whenever the loss plateaus.\nDecay Schedule Instead of selecting fixed points to adjust the learning rate, we can define a function that dictates how the learning rate should decay over time. This eliminates the need for extra hyperparameters. Starting with an initial rate, these functions gradually reduce the it over time.\nHere are some commonly used decay functions:\nCosine schedule: This is a popular choice for computer vision problems. The learning rate follows a cosine function that smoothly decays over time. If $\\eta_0$ is the initial learning rate and $T$ is the total number of epochs, the learning rate at epoch $t$ is given by: $$ \\eta_t = \\frac{\\eta_0}{2} ( 1 + cos \\frac{t \\pi}{T}) $$\nLinear schedule: In this approach, the learning rate decays linearly over time, which is shown to work well for language models. $$ \\eta_t = \\eta_0 (1 - \\frac{t}{T}) $$\nInverse Square root: Commonly used in training Transformer models, this schedule follows an inverse square root decay. One drawback is that the model spends very little time at the higher learning rate. $$ \\eta_t = \\frac{\\eta_0}{ \\sqrt{t} } $$\nCyclic Schedule In addition to decaying the learning rate monotonically, we can adopt a cyclic learning rate schedule, which alternate between high and low learning rates during training. These schedules help prevent the optimization process from getting stuck in local minima.\nIn this approach, the learning rate decreases smoothly within each cycle, following a cosine decay curve. Once the cycle completes, the learning rate \u0026ldquo;warms up\u0026rdquo; by resetting to a higher value. This method, also known as Warm Restarts [3], allows the optimizer to periodically explore different regions of the loss landscape.\nCyclic schedules are particularly useful for training models on large datasets, as the periodic warm restarts enhance exploration, reducing the chances of premature convergence to suboptimal solutions.\nUpdating the code Let\u0026rsquo;s modify the training loop to include a learning rate scheduler: cosine decay.\n# Define the loss function criterion = torch.nn.CrossEntropyLoss() # Define the optimizer optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) # Define the cosine annealing learning rate scheduler num_epochs = 10 scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs) for epoch in range(num_epochs): # Training loop for (image, label) in train_loader: # Send the batch of images and labels to the GPU image, label = image.to(device), label.to(device) # Flatten the image image = image.view(image.shape[0], -1) # Forward pass and optimization output = model(image) loss = criterion(output, label) loss.backward() optimizer.step() optimizer.zero_grad() # Update the learning rate at the end of each epoch scheduler.step() The .step() function updates the learning rate based on the cosine decay schedule at the end of each epoch.\nUsing learning rate schedules with optimizers like SGD or SGD with Momentum is highly recommended for improving training efficiency and model performance. However, for adaptive methods like Adam, a constant learning rate often works well, as these methods automatically adjust learning rates on a per-parameter basis.\nOur deep learning cake is now complete, it\u0026rsquo;s time to dig in and savor the delicious knowledge we\u0026rsquo;ve baked togetherâ€”enjoy every slice as you continue your journey in this exciting field!\nReferences [1] Loshchilov and Hutter, â€œDecoupled Weight Decay Regularizationâ€, ICLR 2019.\n[2] Srivastava et al, â€œDropout: A simple way to prevent neural networks from overfittingâ€, JMLR 2014.\n[3] Loshchilov and Hutter, â€œSGDR: Stochastic Gradient Descent with Warm Restartsâ€, ICLR 2017.\n","permalink":"https://yugajmera.github.io/posts/03-dl3/post/","summary":"Weâ€™ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost readyâ€”but we still need the toppings to make it more appealing. In this part, weâ€™re going to discuss the available toppingsâ€”concepts that enhance optimization and help us reach a better final solution for the modelâ€™s weights. The most important topping among them is Regularization.\nRegularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set.","title":"Deep Learning Basics Part 3: The Cherry on Top"},{"content":"In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.\nHowever, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable. We could then apply a linear classifier on top of these features to classify each image. This would result in a non-linear decision boundary when projected back to the input space.\nWhile possible, this approach is cumbersome since it requires manually defining feature extraction methods for each specific task. Instead, we want our model to jointly learn both the feature representation and the linear classifier. This is where neural networks come in. Neural networks are built using chunks of linear classifiers, which is why they are also called Multi-Layer Perceptrons (MLPs).\nNeural Networks A neural network is composed of interconnected layers, where every neuron in one layer connects to every neuron in the next layer. A typical deep learning model has multiple layers, where the \u0026ldquo;depth\u0026rdquo; refers to the number of layers or the number of learnable weight matrices.\n\\begin{align} \\text{Linear function: } \u0026amp; f = W x \\\\ \\text{2-Layer Neural Network: } \u0026amp; f = W_2 \\hspace{2mm} z(W_1 x) \\\\ \\text{3-Layer Neural Network: } \u0026amp; f = W_3 \\hspace{2mm} z_2(W_2 \\hspace{2mm} z_1(W_1 x)) \\\\ \\end{align}\nEach layer is followed by a non-linear function called an activation function $z_i$. Activation functions are critical because, without them, stacking multiple linear layers would simply result in a linear classifier ($ y = W_2 W_1 x = W x $).\nThe non-linearity introduced by activation functions allows neural networks to create complex decision boundaries. As the number of layers (and activation functions) increases, the decision boundary becomes more intricate in the input space, enabling the model to fit the data more effectively.\nActivation functions There are several popular activation functions to choose from, each with its pros and cons. Let\u0026rsquo;s briefly explore them:\nSigmoid This function transforms input data into the range $[0, 1]$, which can be interpreted as a probability (i.e., the likelihood of a particular feature being present). But, it has significant drawbacks:\nFor large positive or negative inputs, the output saturates (the curve becomes parallel to the x-axis), causing the gradient to approach zero ($\\mathbf{dw} \\approx 0$). This is known as the vanishing gradient problem, which effectively kills the learning process. Computing the exponential function is expensive, slowing down training. Tanh The tanh function is a scaled and shifted version of sigmoid that outputs in the range $[-1, 1]$, making it zero-centered. This allows for both positive and negative outputs, improving gradient flow compared to sigmoid. However:\nIt still suffers from the vanishing gradient problem. Like sigmoid, it relies on exponentials, making it computationally expensive. ReLU ReLU is the most commonly used activation function due to its computational efficiency, which leads to faster training. But,\nIt has a dying ReLU problemâ€”when the inputs are negative, the gradient becomes zero, and those neurons stop learning. These become inactive (as they always ouput zero), reducing the capacity of the model. Leaky ReLU Leaky ReLU is a variation that addresses the dying ReLU problem by adding a small, non-zero gradient for negative input. This prevents neurons from dying in the negative region.\nThe slope in the negative region (usually 0.1) is a hyperparameter, introducing another variable to tune ðŸ˜”. There are advanced variations like Exponential Linear Unit (ELU), Scaled ELU (SELU), and Gaussian Error Linear Unit (GELU), each offering specific benefits. A good rule of thumb is to start with ReLU as your default choice. For finer improvements, you can experiment with these advanced variants to push your modelâ€™s accuracy by a small margin.\nWeight Initialization Once the architecture of the network is fixed, the next step is to initialize the weights of each layer. Weight initialization sets the starting point on the loss landscape for gradient descent. Each new set of initial weights kicks off the optimization process from a different spot, potentially leading to different final solutions. Therefore, the choice of weight initialization is crucial, as it significantly influences the optimization path and the model\u0026rsquo;s convergence.\nInitalizing with zeros - If the weights are initialized to a constant value, such as zero, the activations (outputs of neurons: linear layer + activation function) will also be zero, and the gradients will be zero. This effectively kills learning, preventing the network from even starting to train!\nW_l = 0 # very bad idea Initializing with small random numbers - Using a Gaussian distribution with zero mean and a standard deviation of 0.01 can work for shallow networks. However, for deeper networks, repeatedly multiplying small weights (less than 1) through many layers will shrink the activations as they propagate, causing them to approach zero. This leads to the vanishing gradient problem with tanh and ReLU non-linearities.\nW_l = 0.01 * np.random.randn(Din, Dout) Initializing with larger standard deviation - If weights are initialized with a larger standard deviation, the activations can grow exponentially as they propagate, leading to very large outputs and gradients. This is known as the exploding gradient problem.\nThus, weight initialization must strike a balance to avoid both extremesâ€”activations that neither vanish nor explode. This concept is applied in Xavier Initialization [1].\nXavier Initialization If we can ensure that the variance of the output of a layer is the same as the variance of the input, the scale of activations won\u0026rsquo;t change as they propagate allowing us to avoid vanishing or exploding gradients. To derive this, let\u0026rsquo;s consider a linear layer:\n\\begin{align} y \u0026amp;= \\sum_{i=1}^{D_{in}} x_i w_i \\\\ Var(y) \u0026amp;= D_{in} * Var(x_i w_i) \\\\ \u0026amp;= D_{in} * (E[x_i^2]E[w_i^2] - E[x_i]^2E[w_i]^2) \u0026amp;\\text{[Assume x,w independent]} \\\\ \u0026amp;= D_{in} * (E[x_i^2]E[w_i^2]) \u0026amp;\\text{[Assume x,w are zero-mean]} \\\\ \u0026amp;= D_{in} * Var(x_i) * Var(w_i) \\end{align} To maintain the variance between input and output, $Var(y) = Var(x_i)$, $$ \\Rightarrow Var(w_i) = 1/D_{in} $$\nTherefore, we initialize the weights of each layer with zero mean and a standard deviation equal to the inverse square root of the input dimension of that particular layer.\nW_l = np.random.randn(Din, Dout) * np.sqrt(1 / Din) Note the input $x_i$ refers to the activation (output) from the previous layer. Our derivation assumes that this activation should be zero-centered (or have zero-mean), which holds true for the activation functions like tanh that are centered around zero. However, this initialization does not work well for networks with ReLU non-linearities and needs to be adjusted. This modification is incorporated in Kaiming Initialization [2].\nKaiming/ He Initialization Assuming that the inputs are zero-centered, and ReLU kills all non-negative inputs, we effectively obtaining only half of our expected outputs, resulting in the the variance being halved, $Var(y) = Var(x_i) / 2$. To maintain the scale of activations across layers, $$ \\Rightarrow Var(w_i) = 2/D_{in} $$\nW_l = np.random.randn(Din, Dout) * np.sqrt(2 / Din) Backpropagation Most of our choices so far have revolved around gradients, the essential flour of our deep learning cake. To compute these derivatives of the loss function with respect to the weights of our neural network, we rely on a computation graphâ€”our recipe for success!\nA computation graph is a directed graph that represents the computations inside our model. Let\u0026rsquo;s take a simple example of a computation graph that represents the following equation: \\begin{align} q = x + y \\ \\ \\text{and} \\ \\ f = z * q \\end{align}\nFirst, we perform a forward pass through the graph (denoted in green), where we compute the outputs of each node sequentially, given the inputs: $x = -2, y = 5, z = -4$. \\begin{align} q \u0026amp;= x + y = -2 + 5 = 3\\\\ f \u0026amp; = z * q = -4 * 3 = -12 \\end{align}\nBackpropagation is the algorithm we use to compute gradients in this computation graph. In our backward pass (denoted in red), we apply the chain rule to compute the derivatives of each node in reverse order with respect to the output $f$. \\begin{align} \\frac{df}{df} \u0026amp;= 1 \\\\ \\frac{df}{dq} \u0026amp;= z = -4 \\\\ \\frac{df}{dz} \u0026amp;= q = 3 \\\\ \\frac{df}{dx} \u0026amp;= \\frac{df}{dq} \\frac{dq}{dx} = -4 * 1 = -4 \\\\ \\frac{df}{dy} \u0026amp;= \\frac{df}{dq} \\frac{dq}{dy} = -4 * 1 = -4 \\end{align}\nIn this way, we compute the derivative of the output with respect to the inputs. In deep learning, the output $f$ is typically the loss function, and we use this method to compute the gradients of the loss with respect to the weights. With more complex neural networks, we follow the same procedureâ€”but don\u0026rsquo;t worry, you don\u0026rsquo;t have to compute these gradients manually! Libraries like PyTorch handle it for us.\nCoding from scratch Okay, enough theoryâ€”letâ€™s dive into coding a neural network from scratch using the PyTorch library. Here are a couple of key PyTorch terminologies to understand:\nTensor: Similar to a NumPy array but with the ability to run on GPUs. Autograd: A package that builds computational graphs from Tensors and automatically computes gradients, allowing for easy backpropagation. For our example, we\u0026rsquo;ll build a simple neural network for image classification. The input is a grayscale image of size (28, 28), flattened to a 784-dimensional vector, and our model will have two layers with ReLU activation, outputting scores for 10 classes:\n[Input -\u0026gt; Linear -\u0026gt; ReLU -\u0026gt; Linear -\u0026gt; Output]\nTraining Pipeline We\u0026rsquo;ll follow this pipeline for training:\n1. Initialize the model and weights # Initialize the weights w1 = torch.randn(784, 512, requires_grad=True) * torch.sqrt(2 / 784) w2 = torch.randn(512, 10, requires_grad=True) * torch.sqrt(2 / 512) Since we want to compute the gradients with respect to weights, we initialize them as tensors with the requires_grad = True flag. This tells Pytorch to enable autograd on these tensors and build a computational graph that tracks how these tensors are used in subsequent operations.\nWe initialize the weights using Kaiming initialization because we have the ReLU activation function in our network. Additionally, we prefer powers of 2 for hidden dimensions, such as 512, 256, 128, and 64, due to their computational efficiency on GPUs.\n2. Perform a forward pass of the model # Forward Pass y_pred = X_batch.mm(w1).clamp(min=0).mm(w2) Here, X_batch represents the mini-batch of inputs, while y_batch contains the corresponding labels. The output of the model, y_pred, is the score vector, which contains scores for each category. The clamp(min=0) operation implements the ReLU activation.\n3. Compute the loss # Compute the loss loss = loss_fn(y_pred, y_batch) The loss function calculates the average loss between predicted scores (y_pred) and the true labels(y_batch) across the mini-batch. This loss value is then used to compute gradients for updating the model parameters during training.\n4. Backpropagate the gradients # Compute gradients of loss wrt weights loss.backward() After computing the loss, the gradient of the loss with respect to the weights is automatically computed by calling loss.backward(). This function performs backpropagation on all the inputs that have the requires_grad flag set to true, accumulating the gradients in their .grad() attributes.\n5. Update the weights # Gradient descent step with torch.no_grad(): # Tells Pytorch not to build a graph w1 -= learning_rate * w1.grad() w2 -= learning_rate * w2.grad() # Set gradients to zero w1.zero_grad() w2.zero_grad() The torch.no_grad() context manager ensures that no computation graph is built during the weight update step.\nIt\u0026rsquo;s crucial to run .zero_grad() function to clear the current gradients after each update step and obtain fresh gradients in the next passâ€”otherwise, gradients would keep accumulating.\nThe entire code put together looks something like the following. We train the model for 10 steps, also known as epochs in machine learning terminology.\n# Initialize the weights w1 = torch.randn(784, 512, requires_grad=True) * torch.sqrt(2 / 784) w2 = torch.randn(512, 10, requires_grad=True) * torch.sqrt(2 / 512) # Split data into mini-batches mini_batches = split(data, batch_size) num_epochs = 10 # Train for epoch in range(num_epochs): for (X_batch, y_batch) in mini_batches: # Forward Pass y_pred = X_batch.mm(w1).clamp(min=0).mm(w2) # Compute the loss loss = loss_fn(y_pred, y_batch) # Backpropagate loss.backward() # Gradient descent step with torch.no_grad(): # Tells PyTorch not to build a graph w1 -= learning_rate * w1.grad w2 -= learning_rate * w2.grad # Set gradients to zero w1.zero_grad() w2.zero_grad() Pretty simple, right? Kudos! You can now code a neural network!\nWhile this was a naive way to code a deep learning model, letâ€™s leverage PyTorch\u0026rsquo;s high-level APIs to rewrite the same model in a more scalable manner.\nModule: A layer or model class where weights are automatically set to requires_grad=True and initialized using Kaiming initialization. Loss functions: PyTorch provides a variety of loss functions to choose from, depending on the task at hand. We use Cross-Entropy loss for our example below. optim: PyTorch\u0026rsquo;s package for optimization algorithms including SGD, SGD with Momentum, and Adam. DataLoader: Helps in creating mini-batches efficiently. class TwoLayerNet(torch.nn.Module): def __init__(self): super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(784, 512) self.linear2 = torch.nn.Linear(512, 10) self.relu = torch.nn.ReLU() def forward(self, x): x = self.relu(self.linear1(x)) x = self.linear2(x) return x # Initialize the model model = TwoLayerNet() # Define the loss function criterion = torch.nn.CrossEntropyLoss() # Define the optimizer optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Create mini-batches mini_batches = torch.utils.data.DataLoader(data, batch_size=batch_size) num_epochs = 10 # Train for epoch in range(num_epochs): for (X_batch, y_batch) in mini_batches: # Forward Pass y_pred = model(X_batch) # Compute the loss loss = criterion(y_pred, y_batch) # Backpropagate loss.backward() # Gradient descent step optimizer.step() # Set gradients to zero optimizer.zero_grad() With these powerful APIs, PyTorch takes care of most of the grunt work, allowing us to focus on the higher-level logic.\nThis approach to writing neural networks has become the standard due to its clarity and efficiency, and youâ€™ll notice that nearly all deep learning code follows a similar structure. Next, let\u0026rsquo;s apply this to create a model that can classify handwritten digits (MNIST Dataset).\nMNIST The MNIST dataset consists of 70,000 grayscale images of size (28, 28) pixels, featuring handwritten digits from 0 to 9, making up 10 classes.\nThe first step is to import the relevant libraries that we will be using throughout our code.\nimport torch import torchvision import matplotlib.pyplot as plt Downloading \u0026amp; Pre-processing the dataset Let\u0026rsquo;s download the dataset in two subsets: the training set containing 60,000 images and the test set containing 10,000 images. PyTorch provides transforms to convert these datasets into tensors. Each pixel value is originally in the range [0, 255], gets divided by 255 during the conversion to tensors, resulting in a range [0, 1].\n# Convert images to tensors transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()]) # Download the dataset mnist_trainset = torchvision.datasets.MNIST(root=\u0026#39;./data\u0026#39;, train=True, download=True, transform=transform) mnist_testset = torchvision.datasets.MNIST(root=\u0026#39;./data\u0026#39;, train=False, download=True, transform=transform) Loading the dataset We randomly split the training data into a training set containing 80% of the images and a validation set containing the rest 20%. We then create data loaders for each set, choosing a batch size of 64 for this example.\nRemember, the test set is reserved for evaluation at the very end of our pipeline, and we use a batch size of 1 here since no processing is performed on it.\n# Randomly split the dataset into train (80%) and validation (20%) len_train = int(0.8 * len(mnist_trainset)) len_val = len(mnist_trainset) - len_train train_dataset, val_dataset = torch.utils.data.random_split(mnist_trainset, [len_train, len_val]) # Data loaders for training, validation, and testing train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True) val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True) test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=1) Define the Architecture Letâ€™s use the same two-layer neural network that weâ€™ve been discussing.\n# select the device on which the model will train use_cuda = torch.cuda.is_available() device = torch.device(\u0026#34;cuda:0\u0026#34; if use_cuda else \u0026#34;cpu\u0026#34;) print(\u0026#34;Device: \u0026#34;, device) # Define the model class TwoLayerNet(torch.nn.Module): def __init__(self): super(TwoLayerNet, self).__init__() self.fc1 = torch.nn.Linear(28 * 28 * 1, 512) self.fc2 = torch.nn.Linear(512, 10) self.relu = torch.nn.ReLU() def forward(self, x): x = self.relu(self.fc1(x)) x = self.fc2(x) return x # Initialize the model and move it to the selected device model = TwoLayerNet().to(device) If you are using Google Colab, donâ€™t forget to change the runtime to GPU to take advantage of hardware acceleration.\nTrain the model The training loop looks exactly like before.\n# Define the loss function criterion = torch.nn.CrossEntropyLoss() # Define the optimizer optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) num_epochs = 10 train_loss, train_acc, val_acc = [], [], [] for epoch in range(num_epochs): # Training loop running_loss = 0 correct, total = 0, 0 for (image, label) in train_loader: # Send the batch of images and labels to the GPU image, label = image.to(device), label.to(device) # Flatten the image image = image.view(image.shape[0], -1) # Forward pass and optimization output = model(image) loss = criterion(output, label) loss.backward() optimizer.step() optimizer.zero_grad() # Track training loss running_loss += loss.item() # Get model predictions (category with max score) _, predicted = torch.max(output, dim=1) # Track total labels and correct predictions total += label.shape[0] correct += (predicted == label).sum().item() train_loss.append(running_loss / len(train_loader)) train_acc.append(correct / total) # Validation loop correct, total = 0, 0 for (image, label) in val_loader: # Send the batch of images and labels to the GPU image, label = image.to(device), label.to(device) # Flatten the image image = image.view(image.shape[0], -1) # Get model predictions output = model(image) _, predicted = torch.max(output, dim=1) # Track total labels and correct predictions total += label.shape[0] correct += (predicted == label).sum().item() val_acc.append(correct/total) print(\u0026#39;\\nEpoch: {}/{}, Train Loss: {:.4f}, Train Accuracy: {:.4f}, Val Accuracy: {:.4f}\u0026#39;.format(epoch + 1, num_epochs, train_loss[-1], train_acc[-1], val_acc[-1])) # Plot training statistics plt.figure(1) plt.plot(range(1, num_epochs + 1), train_loss, label=\u0026#39;Training Loss\u0026#39;) plt.xlabel(\u0026#34;Epochs\u0026#34;) plt.ylabel(\u0026#34;Loss\u0026#34;) plt.title(\u0026#39;Loss vs Number of Epochs\u0026#39;) plt.legend() plt.figure(2) plt.plot(range(1, num_epochs + 1), train_acc, label=\u0026#39;Training Accuracy\u0026#39;) plt.plot(range(1, num_epochs + 1), val_acc, label=\u0026#39;Validation Accuracy\u0026#39;) plt.xlabel(\u0026#34;Epochs\u0026#34;) plt.ylabel(\u0026#34;Accuracy\u0026#34;) plt.title(\u0026#39;Accuracy vs Number of Epochs\u0026#39;) plt.legend() plt.show() Additionally, we plot training statistics to assess the model\u0026rsquo;s training performance. Typically, we visualize three curves:\nTraining loss vs Number of Epochs Training accuracy vs Number of Epochs Validation accuracy vs Number of Epochs We calculate the training loss for the entire epoch by summing the average losses across batches and dividing by the number of batches in the training loader. To compute accuracy, we compare the model\u0026rsquo;s predictions with the actual labels. A similar approach is taken for validation, except we do not backpropagate the loss for weight updates.\nThe training loss should ideally decrease with each epoch, while both training and validation accuracy should increase. Below are the plots obtained after running the code above.\nTest the model We evaluate the model\u0026rsquo;s accuracy on unseen data using the same approach as in the validation loop.\n# Get test accuracy correct, total = 0, 0 for (image, label) in test_loader: # Send image and label to the GPU image, label = image.to(device), label.to(device) # Flatten the image image = image.view(image.shape[0], -1) # Get model predictions output = model(image) _, predicted = torch.max(output, dim=1) # Track total labels and correct predictions total += label.shape[0] correct += (predicted == label).sum().item() print(\u0026#34;Test Accuracy: \u0026#34;, correct/total) With this model, I achieved 97.5% accuracy on the test set after training for just 10 epochs!\nThis simple yet effective two-layer neural network demonstrates how accessible deep learning can be, enabling us to achieve impressive results with this tiny dataset.\nOur deep learning cake is almost readyâ€”now, it\u0026rsquo;s time to add the toppings!\nReferences [1] Glorot and Bengio, â€œUnderstanding the difficulty of training deep feedforward neural networksâ€, JMLR 2010.\n[2] He et al, â€œDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificationâ€, ICCV 2015.\n","permalink":"https://yugajmera.github.io/posts/02-dl2/post/","summary":"In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.\nHowever, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable.","title":"Deep Learning Basics Part 2: The Icing"},{"content":"While there is a wealth of deep learning content scattered across the internet, I wanted to create a one-stop solution where you can find all the fundamental concepts needed to write your own neural network from scratch.\nThis series is inspired by Justin Johnson\u0026rsquo;s course, and Stanford\u0026rsquo;s CS231n. I would also highly recommend watching Andrej Karpathy\u0026rsquo;s videos.\nImage Classification Before diving into the details, letâ€™s start with the basics: image classification. This is a core task in computer vision where a model takes an image as input and assigns it a category label from a fixed set of predefined categories.\nFor us humans, this is a trivial task, but for computers, itâ€™s much more complex. An image, represented as $(C, H, W)$, is essentially a grid of numbers ranging from 0 to 255, where each value defines the brightness of a pixel. Here, $H$ is the height, $W$ is the width, and $C$ is the number of channels, which is 3 for an RGB image. This makes it difficult for a computer to \u0026ldquo;see\u0026rdquo; the image in the same way we do.\nOne option is to encode our human knowledge about objects and patterns into the model to help recognize images. However, a more robust and scalable approach is data-driven: we train a model that learns how to recognize different types of objects in images from data. The pipeline looks something like this:\nCollect a dataset of images and their corresponding labels. Use deep learning (DL) to train a classifier. Evaluate the classifier on new, unseen images. Dataset The dataset is usually divided into 3 parts: training set, validation set and test set. We use the training set to train our model.\nHyperparameters are variables that must be set before training begins, and their values cannot be derived from the training data. Since these parameters are highly problem-dependent, they are usually selected through trial and error. After training, we evaluate our trained model on the validation set and use it to fine-tune the hyperparameters, selecting the values that yield the best performance on this set.\nOnce everything is finalized, we evaluate the model on the test set only once, at the very end. This gives us an unbiased estimate of how the model will perform on truly unseen data, helping us predict its real-world performance.\nLinear Classifier Now, letâ€™s define our first and most basic model: a linear classifier (also known as a perceptron). Suppose we have a dataset, $\\{x_i, y_i\\}_{i=1}^N$, where $x_i$ is an RGB image of size $(3, 32, 32)$ and $y_i$ is its corresponding label (integer) across three categories: cat (label=0), car (label=1), and frog (label=2).\nWe feed the image to our model as a flattened vector of shape $(3072, 1)$. The linear classifier consists of a weight matrix $W$ and a bias $b$ (offset). In practice, we concatenate the input with 1s at the bottom row to combine the weights and bias into a single matrix, $\\mathbf{W}$, using a technique called the bias trick.\n$$ f(\\mathbf{x}, \\mathbf{W}) = W \\mathbf{x} + b = \\begin{bmatrix} \\mathbf{W} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ 1 \\end{bmatrix} $$\nThe model outputs a score vector of size $(3, 1)$, which defines the class scores for each of the 3 predefined categories.\nThink of these scores as the model\u0026rsquo;s confidence levelsâ€”it\u0026rsquo;s basically saying how sure it is that the image belongs to each specific category. The class with the highest score becomes our predicted label $\\hat{y}_i$. The scores of the true class are highlighted in bold.\nClearly, the predictions of our classifier are not always correct. This brings us to our first essential concept in deep learning: the loss function.\nLoss function A loss function helps us quantify the performance of our model on the data. Naturally, lower the loss, the better our classifier is. It is also called an objective function or a cost function.\nWe compute the loss for each input image and simply take an average across the training set to compute the final loss value.\n$$ L(\\mathbf{W}) = \\frac{1}{N} \\sum_{i=1}^N L_i \\left( f(x_i, \\mathbf{W}), y_i \\right) $$\nSVM Loss The idea behind Support Vector Machine (SVM) loss is pretty straightforward: we want the score of the correct class to be significantly higher than the scores given to all the incorrect classes. Makes sense, right? We want our classifier to be confident in its predictions, meaning it should assign a higher score to the correct category, compared to the wrong ones, by a certain margin.\nThe SVM loss has the form, $$ L_i = \\sum_{j \\neq y_i} \\text{max}(0, \\underbrace{s_j}_{\\text{score of jth class}} - \\underbrace{s_{y_i}}_{\\text{score of true class}} + \\underbrace{1}_{\\text{margin}}) $$\nItâ€™s also called Hinge Loss because its shape looks just like a door hinge. When the score for the correct class is higher than all the other scores by a certain margin, the loss becomes zero. Otherwise, it increases linearly. A key point to note is that the loss is always computed for the incorrect categories, and for the correct class, it\u0026rsquo;s always zero.\nLet\u0026rsquo;s derive the SVM loss for our toy example. For the cat image, the score of the true class $s_{y_i} = 3.2$. The loss for the car and frog classes are calculated as, $$ L_1 = \\text{max}(0, 5.1 - 3.2 + 1) + \\text{max}(0, -1.7 - 3.2 + 1) = 2.9 + 0 = 2.9 $$\nSimilarly, for the other two training examples, \\begin{align} L_2 \u0026amp;= \\text{max}(0, 1.3 - 4.9 + 1) + \\text{max}(0, 2 - 4.9 + 1) = 0 \\\\ L_3 \u0026amp;= \\text{max}(0, 2.2 + 3.1 + 1) + \\text{max}(0, 2.5 + 3.1 + 1) = 6.3 + 6.6 = 12.9 \\end{align}\nThe final loss of our classifier is the average of all the losses: $L_{\\text{svm}} = 5.27$.\nCross-Entropy Loss Cross-Entropy Loss, also known as Logistic Loss, is one of the most commonly used loss functions in deep learning. It helps interpret raw classifier scores as probabilities, which makes sense because we want our modelâ€™s output to reflect how likely it thinks a certain class is correct.\nTo do this, we run the raw scores through an exponential function to ensure all the outputs are positive (since probabilities are always non-negative). After that, we normalize them so they sum to 1, giving us a valid probability distribution over all the categories. This entire transformation is known as the softmax function. The image below shows how the softmax function works on our cat image scores.\nThe term \u0026ldquo;softmax\u0026rdquo; comes from the fact that itâ€™s a smooth, differentiable approximation to the max function. If we were to apply the max function directly to the raw scoresâ€”like in our cat image exampleâ€”it would output something like $[0, 1, 0]$, which is a valid probability distribution. But the problem is, the max function isnâ€™t differentiable, which means we canâ€™t use it to train neural networks (youâ€™ll see why shortly). This makes softmax a go-to tool in deep learning when you need to highlight the maximum value while ensuring the function remains differentiable.\nThe softmax function provides our estimated probability distribution, denoted by $q$. The true or desired distribution is one where all the probability mass is concentrated on the correct class, i.e., $p = [0, \\dots, 1, \\dots, 0]$, with a single 1 at the $y_i$th position (also known as a one-hot encoded vector). The difference between two probability distributions is measured using cross entropy. The cross-entropy between a true distribution $p$ and an estimated distribution $q$ is defined as: \\begin{equation} H(p,q) = - \\sum_x p(x) \\text{ log} (q(x)) \\end{equation}\nThus, the cross-entropy loss is simply the negative log of the predicted probability for the true class. For our cat image example, the loss would be $L_1 = -\\log(0.13) = 2.04$.\nThe cross-entropy loss has the form, \\begin{equation} L_i = -\\text{log} \\underbrace{\\left( \\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}} \\right)}_{\\text{softmax function}} \\end{equation}\nSome important things to note:\nThe loss can only reach zero if the output of softmax is a perfect one-hot vector. However, this situation is practically impossible because the exponential function only approaches zero when the score is negative infinity. Therefore, achieving $L_{\\text{ce}} = 0$ is not feasible. On initialization, when the weights are initialized with small random values (we will get to this in the next part), the loss would be $L_{\\text{ce}} = - \\log(\\frac{1}{c}) = \\log(c)$ where $c$ is the number of categories. This serves as a quick check: if you observe this loss value at the start of training, itâ€™s a sign that everything is working as expected. The loss function tells us how well our current classifier is doing (with its existing weights) on the training data. Since weâ€™re all a bit greedy, we aim to find that golden set of weights that minimizes this loss. This brings us to the second key concept in deep learning: optimization.\nOptimization Optimization is the process of utilizing the training data to search through all possible values of $\\mathbf{W}$ to find the best one that results in the minimum loss.\nIntuitively, it is like traversing a large high-dimensional landscape, where every point (x,y) on the ground represents the value of the weight matrix $\\mathbf{W}$ and the height of that point is the value of loss function $L(\\mathbf{W})$. Our goal is to navigate to the bottom-most point of this terrain, where we\u0026rsquo;ll find the weights that yield the minimal loss.\nSince we don\u0026rsquo;t have the exact equation of the landscape, computing the minimum of the curve is not straightforward (it\u0026rsquo;s not a convex optimization problem). Instead, we take iterative, small steps towards the direction of the local minimum, hoping to eventually reach the lowest point. The derivative of a function gives us the slope, so this direction of steepest descent will be the negative gradient of the loss function with respect to the weights. This is the famous gradient descent algorithm.\nGradient Descent In the algorithm below, commonly referred to as the vanilla version of gradient descent, we start by initializing the weights with some arbitrary values. Then, we loop for a fixed number of iterations, where for each iteration, we compute the gradient for our current weights and take a step in the direction of the negative gradient to update the weight values.\n# Vanilla Gradient Descent w = initialize_weights() for t in range(num_steps): dw = compute_gradient(loss_fn, data, w) w += - learning_rate * dw The size of each step is a hyperparameter that controls how much we move in the negative gradient direction during each iteration of the optimization process. In other words, it controls how fast our network learns, which is why it\u0026rsquo;s referred to as the learning rate.\nA higher learning rate means taking larger steps toward the negative gradient, which might lead to faster convergence but can also result in unstable behavior. On the other hand, lower learning rate is more stable but may take longer to converge. Therefore it is crucial to select an optimal learning rate.\nThe number of iterations determines how many times we run this algorithm for the model to converge. Generally, more iterations lead to better results, as we continuously approach the minimum, and so, this hyperparameter is constrained by computational resources and time.\nJust as we averaged the losses over all images in the training data to compute the overall loss, we also take the average of individual gradients when calculating the overall gradient.\n\\begin{align} L(\\mathbf{W}) \u0026amp;= \\frac{1}{N} \\sum_{i = 1}^N L_i(x_i, y_i, \\mathbf{W}) \\\\ \\mathbf{dw} = \\frac{\\partial L}{ \\partial \\mathbf{W}} \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial}{\\partial \\mathbf{W}} L_i(x_i, y_i, \\mathbf{W}) \\end{align}\nThis process becomes computationally expensive and slow when dealing with a huge training set ($N$ is very large), as just one step of gradient descent involves looping over the entire training dataset.\nIn practice, this version of gradient descent is not very feasible as we almost always have large datasets. Instead of computing the gradient over all examples, we approximate it\u0026rsquo;s value by computing it over small sub-samples of the training set. The intuition here is that we assume that the examples in the training data are correlated, so calculating the gradient over batches of the training data is a good approximation to the gradient of the full objective.\nWe split the training dataset into mini-batches, each containing $B$ examples, and use these mini-batches to perform weight update steps. This approach allows for much faster convergence because we can perform more frequent parameter updates using the mini-batch gradients. This variant is known as Mini-batch Stochastic Gradient Descent (SGD).\n# Mini-batch Gradient Descent w = initialize_weights() batches = split(data, batch_size) for t in range(num_steps): for mini_batch in batches: dw = compute_gradient(loss_fn, mini_batch, w) w += - learning_rate * dw The samples are drawn randomly, and the batch size is another hyperparameter often selected from values like 32, 64, 128, or 256â€”commonly powers of 2. This is because many vectorized operations perform more efficiently when their inputs are sized in powers of 2. A larger batch size provides a better estimate of the true gradient and is often chosen as large as possible until you run out of GPU memory.\nThe extreme case where the mini-batch contains only a single example ($B = 1$), represents a pure Stochastic Gradient Descent. In this approach, we update the parameters more frequently by taking one observation at each iteration, resulting in a much more erratic learning curve compared to the mini-batch version. This is why mini-batch SGD is generally preferred and is almost always used in practice.\nMini-batch SGD, while effective, does have some challenges and limitations:\nLoss Landscape Variation: If the loss landscape changes rapidly in one direction and slowly in another, a constant learning rate would cause the algorithm to progress slowly along the shallow direction while jittering along the steep direction. Although a smaller learning rate might stabilize the optimization in steeper regions, it would significantly reduce progress in the shallow regions, leading to slower overall convergence. Local Minima and Saddle Points: When the loss function has local minima or saddle points, the gradients at these locations are zero. As a result, gradient descent can get stuck and be unable to escape these points. Noisy Gradients: Since our gradients are computed from mini-batches, they can be noisy, which may affect the stability and convergence of the optimization process. A solution to these problems is a technique called Momentum [1], which draws inspiration from physics.\nMomentum Instead of relying solely on the gradient of the current step to guide the search, momentum also accumulates the gradients from past steps to determine the direction of movement. This helps the optimization process build speed in directions with consistent gradients while smoothing out updates. As a result, when the gradient updates ($\\mathbf{dw}$) consistently point in the same direction, we build momentum and quickly descend the surface, potentially escaping local minima or saddle points along the way.\n\\begin{align} \\text{SGD: } \u0026amp; \\Delta \\mathbf{w} = - \\eta * \\mathbf{dw} \\\\ \\text{SGD + Momentum: } \u0026amp; \\Delta \\mathbf{w}^{t} = - \\eta * \\mathbf{dw} + m * \\Delta \\mathbf{w}^{t-1} \\end{align}\nMomentum helps address the three challenges of mini-batch SGD:\nLoss Landscape Variation: With a smaller learning rate, we can avoid jittering in steep areas, while also maintaining a faster pace in shallow regions. As we progress through flatter areas, we build velocity, which speeds up convergence without increasing the learning rate.\nLocal Minima and Saddle Points: By accumulating previous gradients, we ensure there is always an update to the weights, even when gradients temporarily become small or zero. This enables the optimizer to push through saddle points or escape shallow local minima.\nNoisy Gradients: Momentum smooths out noisy gradient updates by averaging them over time, resulting in more stable updates. This reduces the erratic behavior caused by mini-batch noise and leads to smoother convergence overall.\n# SGD + Momentum v = 0 for t in range(num_steps): v = - (learning_rate * dw) + (m * v) w += v In the algorithm above, the variable $v$ accumulates velocity as a running mean of gradients, while $m$ serves as a friction or decay factor. To understand this concept better, consider two extreme cases:\nNo momentum ($m = 0$): The algorithm behaves exactly like standard gradient descent. Maximum momentum ($m = 1)$: The updates oscillate indefinitely, similar to a frictionless bowl, failing to converge. In practice, values for $m$ are typically set around 0.9 or 0.99, striking a balance between acceleration and stability.\nSince we build velocity over time, we risk overshooting even after reaching the minima, which can lead to jittering. Therefore, we need a way to decay the weight updates over time, and this is where Adagrad [2] comes in.\nAdaGrad Instead of keeping track of the sum of gradients like momentum, the Adaptive Gradient algorithm, or AdaGrad for short, maintains a running sum of squared gradients to have an adaptive learning rate.\n# Adagrad grad_squared = 0 for t in range(num_steps): grad_squared += dw * dw w += -learning_rate * dw / (grad_squared.sqrt() + 1e-7) In the direction of a steep descent (where $\\mathbf{dw}$ is high), AdaGrad dampens the progress, resulting in smaller update steps. Conversely, in shallow regions (where $\\mathbf{dw}$ is low), it allows for larger updates. This adaptive approach effectively addresses the problem of landscape variation.\nDuring the initial iterations, the sum of the squared gradients is small, leading to a higher learning rate. As we continue accumulating squared gradients, the learning rate gradually decays over timeâ€”a beneficial feature to have!\nSince the sum of squared gradients only increases over time, it can lead to situations where the learning rate becomes too low to make meaningful updates, potentially even before reaching the minimum. To overcome this limitation, we use a variant called RMSProp, which decays this running sum of squared gradients, ensuring that the learning rate does not become too small.\nRMSProp Root Mean Square Propagation, or RMSProp for short, is a leaky version of AdaGrad that introduces a decay rate to the running sum of squared gradients.\n# RMSProp grad_squared = 0 for t in range(num_steps): grad_squared = (decay_rate * grad_squared) + (1 - decay_rate) * dw * dw w += -learning_rate * dw / (grad_squared.sqrt() + 1e-7) The decay rate is a hyperparameter typically set to 0.9, while a common choice for the learning rate is 0.001. This mechanism allows RMSProp to effectively balance the adaptation of the learning rate, preventing the rapid decay issues encountered with AdaGrad.\nAdam Adaptive Moment Estimation, or Adam [3] for short, combines the strengths of both RMSProp and Momentum, utilizing a running sum of gradients as well as the squared gradients to optimize the learning process.\n# Adam m1 = 0 m2 = 0 for t in range(1, num_steps): m1 = (beta1 * m1) + (1 - beta1) * dw m2 = (beta2 * m1) + (1 - beta2) * dw * dw m1_unbias = m1 / (1 - beta1 **t) m2_unbias = m2 / (1 - beta2 **t) w += -learning_rate * m1_unbias / (m2_unbias.sqrt() + 1e-7) The sum of gradients and squared gradients are very small at the beginning of training (biased towards zero), which can lead to excessively large updates. To address this issue, Adam employs a technique called bias correction as shown in algorithm above.\nThis adjustment ensures that the updates remain stable and effective, especially in the early stages of training when the optimizer may be prone to making erratic updates due to the low initial values of the moments.\nBeta1 is the decay rate for the first moment (the sum of gradient, aka momentum), and it is commonly set to 0.9. Beta 2 is the decay rate for the second moment (the sum of gradient squared), typically set at 0.999. Adam has become a go-to optimizer for much of the deep learning community today. Learning rates of 1e-3, 5e-4, and 1e-4 serve as great starting points for most models.\nThe visualization below demonstrates how different optimizers behave on a loss landscape featuring both local and global minima, all starting from a common point.\nGradient Descent (cyan) vs. Momentum (magenta) with m = 0.99 vs. AdaGrad (white) vs RMSProp (green) with decay rate = 0.9 vs Adam (blue) with beta1 = 0.9 and beta2 = 0.999, on a surface with a global minimum (the left well) and local minimum (the right well). Source: visualization tool\nNow that we have prepared the base of our deep learning cake, itâ€™s time to add the icing in the next part!\nReferences [1] Sutskever et al, â€œOn the importance of initialization and momentum in deep learningâ€, ICML 2013.\n[2] Duchi et al, â€œAdaptive subgradient methods for online learning and stochastic optimizationâ€, JMLR 2011.\n[3] Kingma and Ba, â€œAdam: A method for stochastic optimizationâ€, ICLR 2015.\n","permalink":"https://yugajmera.github.io/posts/01-dl1/post/","summary":"While there is a wealth of deep learning content scattered across the internet, I wanted to create a one-stop solution where you can find all the fundamental concepts needed to write your own neural network from scratch.\nThis series is inspired by Justin Johnson\u0026rsquo;s course, and Stanford\u0026rsquo;s CS231n. I would also highly recommend watching Andrej Karpathy\u0026rsquo;s videos.\nImage Classification Before diving into the details, letâ€™s start with the basics: image classification.","title":"Deep Learning Basics Part 1: The Base  of the Cake"}]