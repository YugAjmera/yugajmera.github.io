[{"content":"Imagine being able to generate photorealistic 3D models of objects and scenes that can be viewed from any angle, with details so realistic that they are indistinguishable from reality. That\u0026rsquo;s what the Neural Radiance Fields (NeRF) is capable of doing and much more. With more than 50 papers related to NeRFs in the CVPR 2022, it is one of the most influential papers of all time.\nNeural fields A neural field is a neural network that parametrizes a signal. In our case, this signal is either a single 3D scene or an object. It is important to note that a single network needs to be trained to encode (capture) a single scene. It is worth mentioning that, unlike standard machine learning, the objective is to overfit the neural network to a particular scene. Essentially, neural fields embed the scene into the weights of the network.\n3D scenes are usually stored in computer graphics using voxel grids or polygon meshes. However, voxels are costly to store and polygon meshes are limited to hard surfaces. Neural fields are gaining popularity as they are efficient and compact representations of objects or scenes that are differentiable, continuous, and can have arbitrary dimensions and resolutions. Neural radiance fields are a special case of Neural fields that solve the view synthesis problem.\nNeural Radiance Fields (NeRFs) NeRFs as proposed by Mildenhall et al accept a single continuous 5D coordinate as input, which consists of a spatial location $(x, y, z)$ and viewing direction $(\\theta, \\phi)$. This particular point of the object/scene is fed into an MLP, which outputs the corresponding color intensities $c = (r, g, b)$ and volume density $\\sigma$.\nThe network’s weights are optimized to encode the representation of the scene so that the model can easily render novel views seen from any point in space.\nRay Marching To gain a better grasp of the different stages in NeRF training, let\u0026rsquo;s use this 3D scene as an instance.\nThe training dataset includes images ($H \\times W$) captured from $n$ different viewpoints of the same scene. Each image\u0026rsquo;s camera viewpoint is stored as $x_c, y_c, z_c$ in the world space. Since the camera is always \u0026ldquo;aimed\u0026rdquo; at the object, we only need two more rotation parameters to fully describe the pose: the inclination and azimuth angles ($\\theta$, $\\phi$).\nNow, for each camera pose, we \u0026ldquo;shoot\u0026rdquo; a ray from the camera (or viewer\u0026rsquo;s eye), through every pixel of the image, resulting in $H \\times W$ rays per pose. Each ray is described by two vectors,\n$\\mathbf{o}$ : a vector denoting the origin of the ray. $\\mathbf{d}$ : a normalized vector denoting the direction of the ray. An arbitrary point on the ray can then be defined as $r(t) = \\mathbf{o} + t * \\mathbf{d}$.\nThis process of ray tracing is known as backward tracing, as it involves tracing the path of light rays from the camera to the object, as opposed to tracing from the light source to the object.\nInput: A set of camera poses $(x_c, , y_c, , z_c,, , \\theta, , \\phi)_n$\nOutput: A bundle of rays for every pose $(r_{\\mathbf{o}, \\mathbf{d}})_{H \\times W \\times n}$\nSampling Query points You may wonder, what is done with the rays? We trace them from the camera through the scene by adjusting the parameter $t$ until they intersect with some interesting location (object) in the scene. To find these locations, we incrementally step along the ray and sample points at regular intervals.\nBy querying a trained neural network at these 3D points along the viewing ray, we can determine if they belong to the object volume and obtain their visual properties to render an image. However, sampling points along a ray is challenging, as too many non-object points won\u0026rsquo;t provide useful information, and focusing only on high-density regions may miss interesting areas.\nIn our toy example, we uniformly sample along the ray by taking $m$ samples. However, to improve performance, the authors use \u0026ldquo;hierarchical volume sampling\u0026rdquo; to allocate samples proportionally to their expected impact on the final rendering.\nInput: A bundle of rays for every pose $(r_{\\mathbf{o}, \\mathbf{d}})_{H \\times W \\times n}$\nOutput: A set of 3D query points for every ray $(x_p, , y_p, , z_p)_{m \\times H \\times W \\times n}$\nPositional Encoding Once we collect the query points for every ray, we are potentially ready to feed them into our neural network. However, the authors found that resulting renderings perform poorly at representing high-frequency variations in color and geometry that make images perceptually sharp and vivid for the human eye.\nThis observation is consistent with Rahaman et al. who show that deep networks have a tendency to learn lower-frequency functions. They claim that mapping the inputs to a higher dimensional space using high-frequency functions before passing them to the network enables better fitting of data that contains high-frequency variation.\nThe authors use the positional encoding containing sine and cosine functions of varying frequencies.\n$$ \\begin{equation} \\begin{split} a \u0026amp;=b+c\\ \u0026amp;=e+f \\end{split} \\end{equation} $$\nwhere $L$ is the number of dimensions in the positional encoding. The function $\\gamma(.)$ is applied separately to each of the three coordinate values in $\\mathbf{x}$ (which are normalized to lie in [−1, 1]). As viewing direction is also an input to our network, we embed it as well. The authors use $L = 10$ for embeding the 3D query points and $L = 4$ for embeding the viewing direction.\nInput: A set of 3D query points for every ray $(\\mathbf{x} = x_p, , y_p, , z_p){m \\times H \\times W \\times n}$, and 3D viewing direction $ (\\mathbf{d} = \\theta, \\phi, \\psi){n}$\nOutput: Embedings of query points $\\gamma(\\mathbf{x})_{m \\times H \\times W \\times n}$ and viewing direction $\\gamma(\\mathbf{d})_n$\nNeural Network inference To achieve multiview consistency in a neural network, we restrict the network to predict the volume density $\\sigma$ as a function of only the location in 3D space, while allowing the RGB color $c$ to be predicted as a function of both location and viewing direction.\nThe MLP architecture consists of 8 fully-connected layers, each with 256 channels and ReLU activations. A skip connection is included in the network that concatenates the input to the fifth layer\u0026rsquo;s activation. It takes the encoded query points $\\gamma(\\mathbf{x})$ as input and produces two outputs: the volume density $\\sigma$ and a 256-dimensional feature vector.\nThis feature vector is then concatenated with the embedded viewing direction $\\gamma(\\mathbf{d})$ and passed through another fully-connected layer with 128 channels and a ReLU activation to produce the view-dependent RGB color $c$ (tuple in the range from 0 to 1). The authors claim that a model trained without view dependence has difficulty representing specularities.\nBoth of these pieces of information combined allow us to compute the volume density profile for every ray as shown in the figure below.\n\\begin{equation} C(r) = \\int_{t_n}^{t_f} T(t) ; \\sigma(r(t)) ; c(r(t), \\mathbf{d}) ; dt \\end{equation}\nwhere function $T(t)$ denotes the accumulated transmittance along the ray from $t_n$ to $t$, i.e., the probability that the ray travels from $t_n$ to $t$ without hitting any other particle.\n\\begin{equation*} T(t) = \\text{exp} \\left( - \\int_{t_n}^{t} \\sigma(r(s)) ; ds \\right) \\end{equation*}\n\\begin{equation*} C(r) \\approx \\sum_{i = 1}^N T(t_i) ; \\alpha_i ; c_i \\end{equation*} where $\\alpha_i = (1 - \\text{exp}(-\\sigma_i , \\delta_i))$, which can be viewed as a measure of opacity and $\\delta_i = t_{i+1} - t_i$ is the distance between two quadrature points. The accumulated transmittance is then a cumulative product of all the transmittance (which is 1 - opacity) behind it,\n\\begin{equation*} T_i = \\prod_{j=1}^{i-1} (1 - \\alpha_j) \\end{equation*}\n","permalink":"http://localhost:1313/posts/nerf_intro/","summary":"Imagine being able to generate photorealistic 3D models of objects and scenes that can be viewed from any angle, with details so realistic that they are indistinguishable from reality. That\u0026rsquo;s what the Neural Radiance Fields (NeRF) is capable of doing and much more. With more than 50 papers related to NeRFs in the CVPR 2022, it is one of the most influential papers of all time.\nNeural fields A neural field is a neural network that parametrizes a signal.","title":"Understanding Neural Radiance Fields (NeRFs)"}]