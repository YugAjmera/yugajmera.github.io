[{"content":"We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.\nRegularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set. But in practice, this doesn’t always work as expected! When we focus solely on minimizing loss on the training data—essentially trying to fit it perfectly—we run the risk of not generalizing well to the test data, a phenomenon known as overfitting.\nTo illustrate this, let’s consider an example. The blue dots in the figure below represent the training data. We fit two models: $m_1$ (a polynomial model) and $m_2$ (a linear model). Model $m_2$ has more layers, leading to a more complex decision boundary that fits the training data almost perfectly.\nIt\u0026rsquo;s important to remember that both the training and test sets are assumed to be sampled from a common dataset, represented as $p_{\\text{data}}$. Our goal should be to fit this probability distribution and not just the training data alone, as this ensures good performance on unseen test data. Now, let’s examine the test set, represented by the yellow points in the next figure.\nWhile $m_2$ fits the training data perfectly, it struggles with the test data because it overfits the training set. Model $m_1$, despite being simpler, performs better on the test set as it generalizes better to the underlying distribution $p_{\\text{data}}$.\nIf a simpler model like $m_1$ performs better on the test set, you might wonder why we don’t always use smaller models. The reason is that a smaller model runs the risk of underfitting, where it fails to capture the complexities of the data.\nSince the true distribution $p_{\\text{data}}$ is unknown to us, our best approach is to use a larger, more flexible model and apply techniques to help it generalize well—this is where regularization comes in. Regularization helps ensure that our model doesn’t overfit by encouraging it to find simpler decision boundaries.\nThere are several ways to regularize a model, and we’ll dive into each of these methods in detail next.\nWeight Decay The simplest way to make a deeper model behave more like a shallow one is to effectively reduce the influence of some weights by shrinking them. To discourage the model from becoming overly complex, we add a penalty to the loss function based on the magnitude of the model’s weights.\n$$ L(\\mathbf{W}) = \\frac{1}{N} \\sum_{i = 1}^N L_i(x_i, y_i, \\mathbf{W}) + \\lambda R(\\mathbf{W}) $$\nwhere $\\lambda$ is the hyperparameter that controls the regularization strength, i.e., how much to shrink the weights. Since we minimize this loss function during optimization, the added term helps decay some weights in the model. $R(\\mathbf{W})$, the regularization term, varies depending on the type of weight decay:\n\\begin{align} \\text{L2: } \u0026amp; R(\\mathbf{W}) = \\sum_{k} \\sum_{l} \\mathbf{W}_{k,l}^2 \\\\ \\text{L1: } \u0026amp; R(\\mathbf{W}) = \\sum_k \\sum_l |\\mathbf{W}_{k,l}| \\\\ \\text{Elastic Net (L1 + L2): } \u0026amp; R(\\mathbf{W}) = \\sum_k \\sum_l \\beta \\mathbf{W}_{k,l}^2 + |\\mathbf{W}_{k,l}| \\end{align}\nL2 regularization shrinks weights in proportion to their size, meaning larger weights get shrunk more (since we add their square to the loss function). This type of regularization prefers to keep all available features but with smaller weight values.\nL1 regularization shrinks all the weights by the same amount, with the smaller weights getting zeroed out. The intuition here is that small weights indicate that the feature associated with that weight is less important, so zeroing it out won’t significantly affect the output.\nLet\u0026rsquo;s take a simple example to understand this better: \\begin{align} \\mathbf{x} \u0026amp;= [1, 1, 1, 1] \\\\ \\mathbf{w}_1 \u0026amp;= [1, 0, 0, 0] \\\\ \\mathbf{w}_2 \u0026amp;= [0.25, 0.25, 0.25, .0.25] \\end{align} Both weight vectors produce the same output, $\\mathbf{w}_1^T \\mathbf{x} = \\mathbf{w}_2^T \\mathbf{x}$. However, L1 regularization would prefer $\\mathbf{w}_1$ because it sparsifies the weights (i.e., zeros out some weights), while L2 regularization would prefer $\\mathbf{w}_2$, since it retains all the features but with smaller values. Since we usually want to keep all features active, L2 regularization is the more commonly used approach. A typical value for the L2 regularization parameter $\\lambda$ is 1e-4.\nLet’s now look at the mechanics of weight decay. When we apply L2 regularization, it modifies the gradient update step as follows: \\begin{align} L(\\mathbf{w}) \u0026amp;= L_{\\text{data}}(\\mathbf{w}) + \\color{blue} \\lambda |\\mathbf{w}|^2 \\\\ g_t \u0026amp;= \\nabla L_{\\text{data}}(\\mathbf{w}_t) + \\color{blue} 2 \\lambda \\mathbf{w}_t \\\\ s_t \u0026amp;= \\text{optimizer} (g_t) = \\text{optimizer} (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ \\mathbf{w}_{t+1} \u0026amp;= \\mathbf{w}_t - \\eta s_t \\end{align}\nThe blue term represents the weight decay and it can directly be incorporated into our optimizer. This can be done in PyTorch using the weight_decay argument.\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-4) While L2 regularization and weight decay are often used interchangeably for SGD and SGD with momentum, they diverge in behavior when applied to adaptive methods like AdaGrad, RMSProp, and Adam. Let’s explore why.\nDecoupled Weight decay Consider adding L2 regularization to Adam:\n\\begin{align} m_1 \u0026amp;= \\beta_1 m_1 + (1 - \\beta_1) (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ m_2 \u0026amp;= \\beta_2 m_2 + (1 - \\beta_2) (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ {m_1}_{\\text{unbias}} \u0026amp;= m_1 / (1 - \\beta_1^t) \\\\ {m_2}_{\\text{unbias}} \u0026amp;= m_2 / (1 - \\beta_2^t) \\\\ \\mathbf{w}_{t+1} \u0026amp;= \\mathbf{w}_{t} - \\eta * \\frac{{m_1}_{\\text{unbias}}}{\\sqrt{{m_2}_{\\text{unbias}} + \\epsilon}} \\end{align}\nBy substituting the unbiased value of $m_1$ into the update step, we get:\n\\begin{align} \\mathbf{w}_{t+1} \u0026amp;= \\mathbf{w}_{t} - \\eta * \\frac{\\beta_1 m_1 + (1 - \\beta_1) (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} )}{(\\sqrt{{m_2}_{\\text{unbias}} + \\epsilon}) ((1 - \\beta_1^t))} \\end{align}\nNotice how the blue term is normalized by the square root of the unbiased second moment, which tracks the sum of squared gradients. As a result, if the gradients for a particular weight are large, that weight will be regularized less than weights with smaller gradients. This means that the effect of regularization becomes dependent not only on the size of the weights but also on the size of the gradients.\nThis leads to unintended behavior and making L2 regularization less effective than intended. In contrast, with SGD and SGD with Momentum, L2 regularization works as expected: \\begin{align} \\text{SGD: } \u0026amp; \\Delta \\mathbf{w}^{t} = - \\eta * (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ \\text{SGD + Momentum: } \u0026amp; \\Delta \\mathbf{w}^{t} = - \\eta * (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) + m * \\Delta \\mathbf{w}^{t-1} \\end{align}\nTo fix this issue in Adam, a variant called AdamW [1] was introduced. In this variant, the weight decay term is decoupled from the optimizer and added directly to the update step.\n\\begin{align} L(\\mathbf{w}) \u0026amp;= L_{\\text{data}}(\\mathbf{w}) \\\\ \\mathbf{dw} = g_t \u0026amp;= \\nabla L_{\\text{data}}(\\mathbf{w}_t) \\\\ s_t \u0026amp;= \\text{optimizer} (g_t) \\\\ \\mathbf{w}_{t+1} \u0026amp;= \\mathbf{w}_t - \\eta (s_t + {\\color{orange} 2 \\lambda \\mathbf{w}_t} ) \\end{align}\nThis adjustment ensures that the weight decay term remains unaffected by the moving averages, keeping it directly proportional to the weights themselves.\nAdamW has been shown to significantly improve the generalization performance of the standard Adam optimizer. It is always the preferred choice when we want effective regularization with adaptive optimizers.\nDropout Another powerful regularization technique is Dropout [2], where we randomly set a fraction of neurons in a layer to zero during each forward pass. The dropout rate (probability of dropping a neuron) is a hyperparameter, typically set to 0.5.\nBy zeroing out neurons, the network is forced to avoid relying too heavily on any individual neuron or set of neurons, encouraging the model to distribute learning across different parts of the network. This approach effectively simulates training a large number of different sub-networks, each with a unique structure but sharing weights.\nIn terms of interpretation, dropout creates a more robust network by introducing redundancy in the learned features. Since neurons are randomly excluded during each forward pass, the model must learn multiple independent representations of the data. This redundancy helps prevent overfitting, as the model doesn\u0026rsquo;t rely on specific patterns that might only exist in the training data, generalizing better on unseen data.\nInverted dropout Dropout introduces randomness into the model during training, which can make prediction outputs non-deterministic. However, during inference, we want consistent predictions. To handle this, we use inverted dropout, that rescales the output of active neurons during training, to adjust for the dropped ones.\nWhen neurons are randomly dropped, the outputs of the remaining neurons are scaled up by the inverse of the dropout rate to maintain the overall magnitude of activations. For instance, if half of the neurons are dropped (dropout rate = 0.5), we double the outputs of the remaining neurons during training to compensate.\nDropout is most commonly applied to fully connected layers, as these layers tend to contain the majority of learnable parameters, making them more prone to overfitting.\nEarly Stopping Another way to prevent overfiting is to stop training when the validation accuracy starts to decrease. While training, we typically plot three curves:\nTraining loss vs Number of iterations Training accuracy vs Number of iterations Validation accuracy vs Number of iterations These curves provide insight into the \u0026ldquo;health\u0026rdquo; of the model. Ideally, training loss should decay exponentially with each iteration, while both training and validation accuracy should increase. A large gap between the training and validation accuracy suggests overfitting, whereas little to no gap indicates underfitting.\nAt each iteration, we save the model\u0026rsquo;s weights as checkpoints and later select the checkpoint where the validation accuracy is at its maximum. Training beyond this point might increase the training accuracy (as the model starts overfitting), but it could perform poorly on the test set because validation accuracy reflects the model\u0026rsquo;s ability to generalize.\nThis technique is called early stopping because it prevents the model from training all the way to the full number of iterations (or num_steps hyperparameter), stopping earlier to capture the best version of the model without overfitting.\nData Manipulation The next topping is data manipulation, where we explore how to process and modify the training dataset to make it more suitable for efficient learning.\nPreprocessing At every step of the optimization algorithm, the loss function is computed on a mini-batch of the training data. Therefore, the shape of the loss landscape, which the optimization algorithm navigates, depends directly on how the training data is structured. To make this landscape more favorable for optimization, we often preprocess the data before feeding it into the neural network.\nA common preprocessing step is normalizing the data, which zero-centers the data and ensures all features have the same variance. This step helps smooth out the optimization process by making the model less sensitive to small weight changes. The same mean and standard deviation used to normalize the training data is also applied to the test set to maintain consistency.\nOther standard preprocessing techniques for image data include:\nSubtracting the mean image Subtracting per-channel mean (mean along each color channel) Subtracting per-channel mean and divide by the per-channel standard deviation Augmentation Data augmentation is a widely-used technique that increases both the size and diversity of the training data by applying transformations to the input images. Not only does this artificially expand the dataset, but it also adds regularization by introducing noise in the training data, which helps prevent overfitting.\nIn computer vision, augmentations like flipping, rotating, scaling, blurring, jittering, and cropping are widely used as an implicit form of regularization. Augmentations also encode invariances into the model—this means we teach the model to learn that certain transformations shouldn\u0026rsquo;t change the output.\nThink about your specific task: what transformations should not change the desired network output? The answer will vary depending on the problem. For instance, a vertical flip might make sense for images of white blood cells but not for car images (as cars are never upside down in real-world scenarios).\nLearning Rate Schedules All the optimizers we’ve discussed so far—SGD, SGD with Momentum, Adagrad, RMSProp, and Adam—use a fixed learning rate as a hyperparameter to guide the search for the global minimum. As you may have learned by now, this learning rate is a crucial variable in our learning process.\nDifferent learning rates produce different learning behaviors, as shown in the figure below. Therefore, it\u0026rsquo;s essential to choose an appropriate learning rate, ideally the red one. However, finding that one \u0026ldquo;perfect\u0026rdquo; learning rate through trial and error is not always feasible.\nWhat if we don’t keep the learning rate fixed and instead change it during the training process? We can start with a high learning rate to allow our optimization to make quick progress in the initial iterations and then gradually decay it over time, ensuring that the model converges to a lower loss at the end. This would lead to faster convergence and better performance characteristics.\nThis mechanism of changing the learning rate over the course of training is called learning rate scheduling. Let’s look at some commonly used learning rate schedules.\nStep Schedule In a step schedule, we start with a high learning rate (similar to the green curve in the figure), and when the loss curve starts to plateau, we decrease the learning rate. This process continues until convergence.\nFor example, we might reduce the learning rate by a factor of 0.1 after epochs 30, 60, and 90. The learning curve would then look something like this.\nSince we’re decaying the learning rate at arbitrary points during training, this schedule introduces additional hyperparameters, like the number of steps and when to decay. A common approach is to monitor the loss and decay the learning rate whenever the loss plateaus.\nDecay Schedule Instead of selecting fixed points to adjust the learning rate, we can define a function that dictates how the learning rate should decay over time. This eliminates the need for extra hyperparameters. Starting with an initial rate, these functions gradually reduce the it over time.\nHere are some commonly used decay functions:\nCosine schedule: This is a popular choice for computer vision problems. The learning rate follows a cosine function that smoothly decays over time. If $\\eta_0$ is the initial learning rate and $T$ is the total number of epochs, the learning rate at epoch $t$ is given by: $$ \\eta_t = \\frac{\\eta_0}{2} ( 1 + cos \\frac{t \\pi}{T}) $$\nLinear schedule: In this approach, the learning rate decays linearly over time, which is shown to work well for language models. $$ \\eta_t = \\eta_0 (1 - \\frac{t}{T}) $$\nInverse Square root: Commonly used in training Transformer models, this schedule follows an inverse square root decay. One drawback is that the model spends very little time at the higher learning rate. $$ \\eta_t = \\frac{\\eta_0}{ \\sqrt{t} } $$\nCyclic Schedule In addition to decaying the learning rate monotonically, we can adopt a cyclic learning rate schedule, which alternate between high and low learning rates during training. These schedules help prevent the optimization process from getting stuck in local minima.\nIn this approach, the learning rate decreases smoothly within each cycle, following a cosine decay curve. Once the cycle completes, the learning rate \u0026ldquo;warms up\u0026rdquo; by resetting to a higher value. This method, also known as Warm Restarts [3], allows the optimizer to periodically explore different regions of the loss landscape.\nCyclic schedules are particularly useful for training models on large datasets, as the periodic warm restarts enhance exploration, reducing the chances of premature convergence to suboptimal solutions.\nUsing learning rate schedules with optimizers like SGD or SGD with Momentum is highly recommended for improving training efficiency and model performance. However, for adaptive methods like Adam, a constant learning rate often works well, as these methods automatically adjust learning rates on a per-parameter basis.\nCoding the toppings Let’s modify the code from the previous part and add the toppings we discussed: L2 Regularization, Dropout, and Learning Rate Scheduling.\nclass TwoLayerNet(nn.Module): def __init__(self, dropout_rate=0.5): super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(3072, 100) self.dropout = torch.nn.Dropout(p=0.5) self.linear2 = torch.nn.Linear(100, 10) def forward(self, X): output1 = torch.nn.functional.relu(self.linear1(X)) output1 = self.dropout(output1) y_pred = self.linear2(output1) return y_pred # Define the model model = TwoLayerNet() # Define the loss function criterion = nn.CrossEntropyLoss() # Define the optimizer with L2 regularization optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4) # Define the cosine annealing learning rate scheduler scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps) # Create mini-batches mini_batches = torch.utils.data.DataLoader(data, batch_size=batch_size) # Train for t in range(num_steps): for X_batch, y_batch in mini_batches: y_pred = model(X_batch) loss = criterion(y_pred, y_batch) loss.backward() optimizer.step() optimizer.zero_grad() # Step the scheduler at the end of each epoch scheduler.step() In this code, we have:\nAdded Dropout after the activation function. Applied L2 Regularization using the weight_decay argument in the optimizer. Implemented a cosine decay learning rate schedule that dynamically adjusts the learning rate during training. The .step() function updates the learning rate based on the schedule at the end of each epoch. Our deep learning cake is now complete, it\u0026rsquo;s time to dig in and savor the delicious knowledge we\u0026rsquo;ve baked together—enjoy every slice as you continue your journey in this exciting field!\nReferences [1] Loshchilov and Hutter, “Decoupled Weight Decay Regularization”, ICLR 2019.\n[2] Srivastava et al, “Dropout: A simple way to prevent neural networks from overfitting”, JMLR 2014.\n[3] Loshchilov and Hutter, “SGDR: Stochastic Gradient Descent with Warm Restarts”, ICLR 2017.\n","permalink":"https://yugajmera.github.io/posts/03-dl3/post/","summary":"We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.\nRegularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set.","title":"Deep Learning Basics Part 3: The Cherry on Top"},{"content":"In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.\nHowever, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable. We could then apply a linear classifier on top of these features to classify each image. This would result in a non-linear decision boundary when projected back to the input space.\nWhile possible, this approach is cumbersome since it requires manually defining feature extraction methods for each specific task. Instead, we want our model to jointly learn both the feature representation and the linear classifier. This is where neural networks come in. Neural networks are built using chunks of linear classifiers, which is why they are also called Multi-Layer Perceptrons (MLPs).\nNeural Networks A neural network is composed of interconnected layers, where every neuron in one layer connects to every neuron in the next layer. A typical deep learning model has multiple layers, where the \u0026ldquo;depth\u0026rdquo; refers to the number of layers or the number of learnable weight matrices.\n\\begin{align} \\text{Linear function: } \u0026amp; f = W x \\\\ \\text{2-Layer Neural Network: } \u0026amp; f = W_2 \\hspace{2mm} z(W_1 x) \\\\ \\text{3-Layer Neural Network: } \u0026amp; f = W_3 \\hspace{2mm} z_2(W_2 \\hspace{2mm} z_1(W_1 x)) \\\\ \\end{align}\nEach layer is followed by a non-linear function called an activation function $z_i$. Activation functions are critical because, without them, stacking multiple linear layers would simply result in a linear classifier ($ y = W_2 W_1 x = W x $).\nThe non-linearity introduced by activation functions allows neural networks to create complex decision boundaries. As the number of layers (and activation functions) increases, the decision boundary becomes more intricate in the input space, enabling the model to fit the data more effectively.\nActivation functions There are several popular activation functions to choose from, each with its pros and cons. Let\u0026rsquo;s briefly explore them:\nSigmoid This function transforms input data into the range $[0, 1]$, which can be interpreted as a probability (i.e., the likelihood of a particular feature being present). But, it has significant drawbacks:\nFor large positive or negative inputs, the output saturates (the curve becomes parallel to the x-axis), causing the gradient to approach zero ($\\mathbf{dw} \\approx 0$). This is known as the vanishing gradient problem, which effectively kills the learning process. Computing the exponential function is expensive, slowing down training. Tanh The tanh function is a scaled and shifted version of sigmoid that outputs in the range $[-1, 1]$, making it zero-centered. This allows for both positive and negative outputs, improving gradient flow compared to sigmoid. However:\nIt still suffers from the vanishing gradient problem. Like sigmoid, it relies on exponentials, making it computationally expensive. ReLU ReLU is the most commonly used activation function due to its computational efficiency, which leads to faster training. But,\nIt has a dying ReLU problem—when the inputs are negative, the gradient becomes zero, and those neurons stop learning. These become inactive (as they always ouput zero), reducing the capacity of the model. Leaky ReLU Leaky ReLU is a variation that addresses the dying ReLU problem by adding a small, non-zero gradient for negative input. This prevents neurons from dying in the negative region.\nThe slope in the negative region (usually 0.1) is a hyperparameter, introducing another variable to tune 😔. There are advanced variations like Exponential Linear Unit (ELU), Scaled ELU (SELU), and Gaussian Error Linear Unit (GELU), each offering specific benefits. A good rule of thumb is to start with ReLU as your default choice. For finer improvements, you can experiment with these advanced variants to push your model’s accuracy by a small margin.\nWeight Initialization Once the architecture of the network is fixed, the next step is to initialize the weights of each layer. Weight initialization sets the starting point on the loss landscape for gradient descent. Each new set of initial weights kicks off the optimization process from a different spot, potentially leading to different final solutions. Therefore, the choice of weight initialization is crucial, as it significantly influences the optimization path and the model\u0026rsquo;s convergence.\nInitalizing with zeros - If the weights are initialized to a constant value, such as zero, the activations (outputs of neurons: linear layer + activation function) will also be zero, and the gradients will be zero. This effectively kills learning, preventing the network from even starting to train!\nW_l = 0 # very bad idea Initializing with small random numbers - Using a Gaussian distribution with zero mean and a standard deviation of 0.01 can work for shallow networks. However, for deeper networks, repeatedly multiplying small weights (less than 1) through many layers will shrink the activations as they propagate, causing them to approach zero. This leads to the vanishing gradient problem with tanh and ReLU non-linearities.\nW_l = 0.01 * np.random.randn(Din, Dout) Initializing with larger standard deviation - If weights are initialized with a larger standard deviation, the activations can grow exponentially as they propagate, leading to very large outputs and gradients. This is known as the exploding gradient problem.\nThus, weight initialization must strike a balance to avoid both extremes—activations that neither vanish nor explode. This concept is applied in Xavier Initialization [1].\nXavier Initialization If we can ensure that the variance of the output of a layer is the same as the variance of the input, the scale of activations won\u0026rsquo;t change as they propagate allowing us to avoid vanishing or exploding gradients. To derive this, let\u0026rsquo;s consider a linear layer:\n\\begin{align} y \u0026amp;= \\sum_{i=1}^{D_{in}} x_i w_i \\\\ Var(y) \u0026amp;= D_{in} * Var(x_i w_i) \\\\ \u0026amp;= D_{in} * (E[x_i^2]E[w_i^2] - E[x_i]^2E[w_i]^2) \u0026amp;\\text{[Assume x,w independent]} \\\\ \u0026amp;= D_{in} * (E[x_i^2]E[w_i^2]) \u0026amp;\\text{[Assume x,w are zero-mean]} \\\\ \u0026amp;= D_{in} * Var(x_i) * Var(w_i) \\end{align} To maintain the variance between input and output, $Var(y) = Var(x_i)$, $$ \\Rightarrow Var(w_i) = 1/D_{in} $$\nTherefore, we initialize the weights of each layer with zero mean and a standard deviation equal to the inverse square root of the input dimension of that particular layer.\nW_l = np.random.randn(Din, Dout) * np.sqrt(1/Din) Note the input $x_i$ refers to the activation (output) from the previous layer. Our derivation assumes that this activation should be zero-centered (or have zero-mean), which holds true for the activation functions like tanh that are centered around zero. However, this initialization does not work well for networks with ReLU non-linearities and needs to be adjusted. This modification is incorporated in Kaiming Initialization [2].\nKaiming/ He Initialization Assuming that the inputs are zero-centered, and ReLU kills all non-negative inputs, we effectively obtaining only half of our expected outputs, resulting in the the variance being halved, $Var(y) = Var(x_i) / 2$. To maintain the scale of activations across layers, $$ \\Rightarrow Var(w_i) = 2/D_{in} $$\nW_l = np.random.randn(Din, Dout) * np.sqrt(2/Din) Backpropagation Most of our choices so far have revolved around gradients, the essential flour of our deep learning cake. To compute these derivatives of the loss function with respect to the weights of our neural network, we rely on a computation graph—our recipe for success!\nA computation graph is a directed graph that represents the computations inside our model. Let\u0026rsquo;s take a simple example of a computation graph that represents the following equation: \\begin{align} q = x + y \\ \\ \\text{and} \\ \\ f = z * q \\end{align}\nFirst, we perform a forward pass through the graph (denoted in green), where we compute the outputs of each node sequentially, given the inputs: $x = -2, y = 5, z = -4$. \\begin{align} q \u0026amp;= x + y = -2 + 5 = 3\\\\ f \u0026amp; = z * q = -4 * 3 = -12 \\end{align}\nBackpropagation is the algorithm we use to compute gradients in this computation graph. In our backward pass (denoted in red), we apply the chain rule to compute the derivatives of each node in reverse order with respect to the output $f$. \\begin{align} \\frac{df}{df} \u0026amp;= 1 \\\\ \\frac{df}{dq} \u0026amp;= z = -4 \\\\ \\frac{df}{dz} \u0026amp;= q = 3 \\\\ \\frac{df}{dx} \u0026amp;= \\frac{df}{dq} \\frac{dq}{dx} = -4 * 1 = -4 \\\\ \\frac{df}{dy} \u0026amp;= \\frac{df}{dq} \\frac{dq}{dy} = -4 * 1 = -4 \\end{align}\nIn this way, we compute the derivative of the output with respect to the inputs. In deep learning, the output $f$ is typically the loss function, and we use this method to compute the gradients of the loss with respect to the weights. With more complex neural networks, we follow the same procedure—but don\u0026rsquo;t worry, you don\u0026rsquo;t have to compute these gradients manually! Libraries like PyTorch handle it for us.\nCoding from scratch Okay, enough theory—let’s dive into coding a neural network from scratch using the PyTorch library. Here are a couple of key PyTorch terminologies to understand:\nTensor: Similar to a NumPy array but with the ability to run on GPUs. Autograd: A package that builds computational graphs from Tensors and automatically computes gradients. For our example, we will build a simple neural network for image classification. The input is an image of size (32 x 32 x 3 = 3072), and our model will have two layers with ReLU activation, outputting scores for 10 classes: [Input -\u0026gt; Linear -\u0026gt; ReLU -\u0026gt; Linear -\u0026gt; Output]\nWe\u0026rsquo;ll follow this pipeline for training:\nInitialize the model and weights. # Initialize the weights w1 = torch.randn(3072, 100, requires_grad=True) * torch.sqrt(2/3072) w2 = torch.randn(100, 10, requires_grad=True) * torch.sqrt(2/100) Since we want to compute the gradients with respect to weights, we initialize the weights as tensors with the requires_grad = True flag. This tells Pytorch to enable autograd on these tensors and build a computational graph as the tensors are used in operations later in our code. We initialize the weights using Kaiming initialization because we use the ReLU activation function.\nPerform a forward pass of the model to get predictions. # Forward Pass y_pred = X_batch.mm(w1).clamp(min=0).mm(w2) X_batch is the mini-batch of inputs, y_batch is the corresponding labels, and y_pred is the model\u0026rsquo;s output.\nCompute the loss value (between true labels and predictions) via the loss function. # Compute the loss loss = loss_fn(y_pred, y_batch) The loss function returns the average loss over a mini-batch, which we use to compute gradients.\nUse backpropagation to compute the gradients. # Compute gradients of loss wrt weights loss.backward() Once we compute the loss, the gradient of the loss with respect to the weights can automatically be computed by running loss.backward(). This function implements backpropagation on all the inputs that have the requires_grad flag set to true and accumulates the gradients their .grad() attribute.\nUpdate the weights using gradient descent. # Gradient descent step with torch.no_grad(): # Tells Pytorch not to build a graph w1 -= learning_rate * w1.grad() w2 -= learning_rate * w2.grad() # Set gradients to zero w1.zero_grad() w2.zero_grad() The torch.no_grad() function ensures that no computation graph is built during the weight update step. It\u0026rsquo;s crucial to run .zero_grad() function to clear the current gradients after each update step and accumulate fresh gradients-—otherwise, gradients would keep accumulating.\nThe entire code put together looks something like this:\n# Initialize the weights w1 = torch.randn(3072, 100, requires_grad=True) * torch.sqrt(2/3072) w2 = torch.randn(100, 10, requires_grad=True) * torch.sqrt(2/100) # Split data into mini-batches mini_batches = split(data, batch_size) for t in range(num_steps): for X_batch, y_batch in mini_batches: # Forward Pass y_pred = X_batch.mm(w1).clamp(min=0).mm(w2) # Compute the loss loss = loss_fn(y_pred, y_batch) # Compute gradients of loss wrt weights loss.backward() # Gradient descent step with torch.no_grad(): # Tells Pytorch not to build a graph w1 -= learning_rate * w1.grad() w2 -= learning_rate * w2.grad() # Set gradients to zero w1.zero_grad() w2.zero_grad() Pretty simple, right? Kudos! You can now code a neural network!\nNext, let’s leverage PyTorch\u0026rsquo;s high-level APIs to implement the same model in a more scalable manner:\nDataLoader: Helps in creating mini-batches efficiently. Module: A layer or model class where weights are automatically set to requires_grad=True and initialized using Kaiming initialization. optim: PyTorch\u0026rsquo;s package for optimization algorithms like Stochastic Gradient Descent (SGD) and Adam. Loss functions: PyTorch provides a variety of loss functions to choose from, depending on the task at hand. class TwoLayerNet(torch.nn.Module): def __init__(self): super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(3072, 100) self.linear2 = torch.nn.Linear(100, 10) def forward(self, X): output1 = torch.nn.functional.relu(self.linear1(X)) y_pred = self.linear2(output1) return y_pred # Define the model model = TwoLayerNet() # Define the loss function criterion = nn.CrossEntropyLoss() # Define the optimizer optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Create mini-batches mini_batches = torch.utils.data.DataLoader(data, batch_size=batch_size) # Train for t in range(num_steps): for X_batch, y_batch in mini_batches: y_pred = model(X_batch) loss = criterion(y_pred, y_batch) loss.backward() optimizer.step() optimizer.zero_grad() With these powerful APIs, PyTorch takes care of most of the grunt work, allowing us to focus on the higher-level logic. This approach to writing neural networks has become the standard due to its clarity and efficiency, and you’ll notice that nearly all deep learning code follows a similar structure.\nOur deep learning cake is almost ready—now, it\u0026rsquo;s time to add the toppings!\nReferences [1] Glorot and Bengio, “Understanding the difficulty of training deep feedforward neural networks”, JMLR 2010.\n[2] He et al, “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification”, ICCV 2015.\n","permalink":"https://yugajmera.github.io/posts/02-dl2/post/","summary":"In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.\nHowever, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable.","title":"Deep Learning Basics Part 2: The Icing"},{"content":"While there is a wealth of deep learning content scattered across the internet, I wanted to create a one-stop solution where you can find all the fundamental concepts needed to write your own neural network from scratch.\nThis series is inspired by Justin Johnson\u0026rsquo;s course, and Stanford\u0026rsquo;s CS231n.\nImage Classification Before diving into the details, let’s start with the basics: image classification. This is a core task in computer vision where a model takes an image as input and assigns it a category label from a fixed set of predefined categories.\nFor us humans, this is a trivial task, but for computers, it’s much more complex. An image, represented as $(H, W, C)$, is essentially a grid of numbers ranging from 0 to 255, where each value defines the brightness of a pixel. Here, $H$ is the height, $W$ is the width, and $C$ is the number of channels, which is 3 for an RGB image. This makes it difficult for a computer to \u0026ldquo;see\u0026rdquo; the image in the same way we do.\nOne option is to encode our human knowledge about objects and patterns into the model to help recognize images. However, a more robust and scalable approach is data-driven: we train a model that learns how to recognize different types of objects in images from data. The pipeline looks something like this:\nCollect a dataset of images and their corresponding labels. Use deep learning (DL) to train a classifier. Evaluate the classifier on new, unseen images. Dataset The dataset is usually divided into 3 parts: training set, validation set and test set. We use the training set to train our model.\nHyperparameters are variables that must be set before training begins, and their values cannot be derived from the training data. Since these parameters are highly problem-dependent, they are usually selected through trial and error. After training, we evaluate our trained model on the validation set and use it to fine-tune the hyperparameters, selecting the values that yield the best performance on this set.\nOnce everything is finalized, we evaluate the model on the test set only once, at the very end. This gives us an unbiased estimate of how the model will perform on truly unseen data, helping us predict its real-world performance.\nLinear Classifier Now, let’s define our first and most basic model: a linear classifier (also known as a perceptron). Suppose we have a dataset, $\\{x_i, y_i\\}_{i=1}^N$, where $x_i$ is an RGB image of size $(32, 32, 3)$ and $y_i$ is its corresponding label (integer) across three categories: cat (label=0), car (label=1), and frog (label=2).\nWe feed the image to our model as a flattened vector of shape $(3072, 1)$. The linear classifier consists of a weight matrix $W$ and a bias $b$ (offset). In practice, we concatenate the input with 1s at the bottom row to combine the weights and bias into a single matrix, $\\mathbf{W}$, using a technique called the bias trick.\n$$ f(\\mathbf{x}, \\mathbf{W}) = W \\mathbf{x} + b = \\begin{bmatrix} \\mathbf{W} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ 1 \\end{bmatrix} $$\nThe model outputs a score vector of size $(3, 1)$, which defines the class scores for each of the 3 predefined categories.\nThink of these scores as the model\u0026rsquo;s confidence levels—it\u0026rsquo;s basically saying how sure it is that the image belongs to each specific category. The class with the highest score becomes our predicted label $\\hat{y}_i$. The scores of the true class are highlighted in bold.\nClearly, the predictions of our classifier are not always correct. This brings us to our first essential concept in deep learning: the loss function.\nLoss function A loss function helps us quantify the performance of our model on the data. Naturally, lower the loss, the better our classifier is. It is also called an objective function or a cost function.\nWe compute the loss for each input image and simply take an average across the training set to compute the final loss value.\n$$ L(\\mathbf{W}) = \\frac{1}{N} \\sum_{i=1}^N L_i \\left( f(x_i, \\mathbf{W}), y_i \\right) $$\nSVM Loss The idea behind Support Vector Machine (SVM) loss is pretty straightforward: we want the score of the correct class to be significantly higher than the scores given to all the incorrect classes. Makes sense, right? We want our classifier to be confident in its predictions, meaning it should assign a higher score to the correct category, compared to the wrong ones, by a certain margin.\nThe SVM loss has the form, $$ L_i = \\sum_{j \\neq y_i} \\text{max}(0, \\underbrace{s_j}_{\\text{score of jth class}} - \\underbrace{s_{y_i}}_{\\text{score of true class}} + \\underbrace{1}_{\\text{margin}}) $$\nIt’s also called Hinge Loss because its shape looks just like a door hinge. When the score for the correct class is higher than all the other scores by a certain margin, the loss becomes zero. Otherwise, it increases linearly. A key point to note is that the loss is always computed for the incorrect categories, and for the correct class, it\u0026rsquo;s always zero.\nLet\u0026rsquo;s derive the SVM loss for our toy example. For the cat image, the score of the true class $s_{y_i} = 3.2$. The loss for the car and frog classes are calculated as, $$ L_1 = \\text{max}(0, 5.1 - 3.2 + 1) + \\text{max}(0, -1.7 - 3.2 + 1) = 2.9 + 0 = 2.9 $$\nSimilarly, for the other two training examples, \\begin{align} L_2 \u0026amp;= \\text{max}(0, 1.3 - 4.9 + 1) + \\text{max}(0, 2 - 4.9 + 1) = 0 \\\\ L_3 \u0026amp;= \\text{max}(0, 2.2 + 3.1 + 1) + \\text{max}(0, 2.5 + 3.1 + 1) = 6.3 + 6.6 = 12.9 \\end{align}\nThe final loss of our classifier is the average of all the losses: $L_{\\text{svm}} = 5.27$.\nCross-Entropy Loss Cross-Entropy Loss, also known as Logistic Loss, is one of the most commonly used loss functions in deep learning. It helps interpret raw classifier scores as probabilities, which makes sense because we want our model’s output to reflect how likely it thinks a certain class is correct.\nTo do this, we run the raw scores through an exponential function to ensure all the outputs are positive (since probabilities are always non-negative). After that, we normalize them so they sum to 1, giving us a valid probability distribution over all the categories. This entire transformation is known as the softmax function. The image below shows how the softmax function works on our cat image scores.\nThe term \u0026ldquo;softmax\u0026rdquo; comes from the fact that it’s a smooth, differentiable approximation to the max function. If we were to apply the max function directly to the raw scores—like in our cat image example—it would output something like $[0, 1, 0]$, which is a valid probability distribution. But the problem is, the max function isn’t differentiable, which means we can’t use it to train neural networks (you’ll see why shortly). This makes softmax a go-to tool in deep learning when you need to highlight the maximum value while ensuring the function remains differentiable.\nThe softmax function provides our estimated probability distribution, denoted by $q$. The true or desired distribution is one where all the probability mass is concentrated on the correct class, i.e., $p = [0, \\dots, 1, \\dots, 0]$, with a single 1 at the $y_i$th position (also known as a one-hot encoded vector). The difference between two probability distributions is measured using cross entropy. The cross-entropy between a true distribution $p$ and an estimated distribution $q$ is defined as: \\begin{equation} H(p,q) = - \\sum_x p(x) \\text{ log} (q(x)) \\end{equation}\nThus, the cross-entropy loss is simply the negative log of the predicted probability for the true class. For our cat image example, the loss would be $L_1 = -\\log(0.13) = 2.04$.\nThe cross-entropy loss has the form, \\begin{equation} L_i = -\\text{log} \\underbrace{\\left( \\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}} \\right)}_{\\text{softmax function}} \\end{equation}\nSome important things to note:\nThe loss can only reach zero if the output of softmax is a perfect one-hot vector. However, this situation is practically impossible because the exponential function only approaches zero when the score is negative infinity. Therefore, achieving $L_{\\text{ce}} = 0$ is not feasible. On initialization, when the weights are initialized with small random values (we will get to this in the next part), the loss would be $L_{\\text{ce}} = - \\log(\\frac{1}{c}) = \\log(c)$ where $c$ is the number of categories. This serves as a quick check: if you observe this loss value at the start of training, it’s a sign that everything is working as expected. The loss function tells us how well our current classifier is doing (with its existing weights) on the training data. Since we’re all a bit greedy, we aim to find that golden set of weights that minimizes this loss. This brings us to the second key concept in deep learning: optimization.\nOptimization Optimization is the process of utilizing the training data to search through all possible values of $\\mathbf{W}$ to find the best one that results in the minimum loss.\nIntuitively, it is like traversing a large high-dimensional landscape, where every point (x,y) on the ground represents the value of the weight matrix $\\mathbf{W}$ and the height of that point is the value of loss function $L(\\mathbf{W})$. Our goal is to navigate to the bottom-most point of this terrain, where we\u0026rsquo;ll find the weights that yield the minimal loss.\nSince we don\u0026rsquo;t have the exact equation of the landscape, computing the minimum of the curve is not straightforward (it\u0026rsquo;s not a convex optimization problem). Instead, we take iterative, small steps towards the direction of the local minimum, hoping to eventually reach the lowest point. The derivative of a function gives us the slope, so this direction of steepest descent will be the negative gradient of the loss function with respect to the weights. This is the famous gradient descent algorithm.\nGradient Descent In the algorithm below, commonly referred to as the vanilla version of gradient descent, we start by initializing the weights with some arbitrary values. Then, we loop for a fixed number of iterations, where for each iteration, we compute the gradient for our current weights and take a step in the direction of the negative gradient to update the weight values.\n# Vanilla Gradient Descent w = initialize_weights() for t in range(num_steps): dw = compute_gradient(loss_fn, data, w) w += - learning_rate * dw The size of each step is a hyperparameter that controls how much we move in the negative gradient direction during each iteration of the optimization process. In other words, it controls how fast our network learns, which is why it\u0026rsquo;s referred to as the learning rate.\nA higher learning rate means taking larger steps toward the negative gradient, which might lead to faster convergence but can also result in unstable behavior. On the other hand, lower learning rate is more stable but may take longer to converge. Therefore it is crucial to select an optimal learning rate.\nThe number of iterations determines how many times we run this algorithm for the model to converge. Generally, more iterations lead to better results, as we continuously approach the minimum, and so, this hyperparameter is constrained by computational resources and time.\nJust as we averaged the losses over all images in the training data to compute the overall loss, we also take the average of individual gradients when calculating the overall gradient.\n\\begin{align} L(\\mathbf{W}) \u0026amp;= \\frac{1}{N} \\sum_{i = 1}^N L_i(x_i, y_i, \\mathbf{W}) \\\\ \\mathbf{dw} = \\frac{\\partial L}{ \\partial \\mathbf{W}} \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial}{\\partial \\mathbf{W}} L_i(x_i, y_i, \\mathbf{W}) \\end{align}\nThis process becomes computationally expensive and slow when dealing with a huge training set ($N$ is very large), as just one step of gradient descent involves looping over the entire training dataset.\nIn practice, this version of gradient descent is not very feasible as we almost always have large datasets. Instead of computing the gradient over all examples, we approximate it\u0026rsquo;s value by computing it over small sub-samples of the training set. The intuition here is that we assume that the examples in the training data are correlated, so calculating the gradient over batches of the training data is a good approximation to the gradient of the full objective.\nWe split the training dataset into mini-batches, each containing $B$ examples, and use these mini-batches to perform weight update steps. This approach allows for much faster convergence because we can perform more frequent parameter updates using the mini-batch gradients. This variant is known as Mini-batch Stochastic Gradient Descent (SGD).\n# Mini-batch Gradient Descent w = initialize_weights() batches = split(data, batch_size) for t in range(num_steps): for mini_batch in batches: dw = compute_gradient(loss_fn, mini_batch, w) w += - learning_rate * dw The samples are drawn randomly, and the batch size is another hyperparameter often selected from values like 32, 64, 128, or 256—commonly powers of 2. This is because many vectorized operations perform more efficiently when their inputs are sized in powers of 2. A larger batch size provides a better estimate of the true gradient and is often chosen as large as possible until you run out of GPU memory.\nThe extreme case where the mini-batch contains only a single example ($B = 1$), represents a pure Stochastic Gradient Descent. In this approach, we update the parameters more frequently by taking one observation at each iteration, resulting in a much more erratic learning curve compared to the mini-batch version. This is why mini-batch SGD is generally preferred and is almost always used in practice.\nMini-batch SGD, while effective, does have some challenges and limitations:\nLoss Landscape Variation: If the loss landscape changes rapidly in one direction and slowly in another, a constant learning rate would cause the algorithm to progress slowly along the shallow direction while jittering along the steep direction. Although a smaller learning rate might stabilize the optimization in steeper regions, it would significantly reduce progress in the shallow regions, leading to slower overall convergence. Local Minima and Saddle Points: When the loss function has local minima or saddle points, the gradients at these locations are zero. As a result, gradient descent can get stuck and be unable to escape these points. Noisy Gradients: Since our gradients are computed from mini-batches, they can be noisy, which may affect the stability and convergence of the optimization process. A solution to these problems is a technique called Momentum [1], which draws inspiration from physics.\nMomentum Instead of relying solely on the gradient of the current step to guide the search, momentum also accumulates the gradients from past steps to determine the direction of movement. This helps the optimization process build speed in directions with consistent gradients while smoothing out updates. As a result, when the gradient updates ($\\mathbf{dw}$) consistently point in the same direction, we build momentum and quickly descend the surface, potentially escaping local minima or saddle points along the way.\n\\begin{align} \\text{SGD: } \u0026amp; \\Delta \\mathbf{w} = - \\eta * \\mathbf{dw} \\\\ \\text{SGD + Momentum: } \u0026amp; \\Delta \\mathbf{w}^{t} = - \\eta * \\mathbf{dw} + m * \\Delta \\mathbf{w}^{t-1} \\end{align}\nMomentum helps address the three challenges of mini-batch SGD:\nLoss Landscape Variation: With a smaller learning rate, we can avoid jittering in steep areas, while also maintaining a faster pace in shallow regions. As we progress through flatter areas, we build velocity, which speeds up convergence without increasing the learning rate.\nLocal Minima and Saddle Points: By accumulating previous gradients, we ensure there is always an update to the weights, even when gradients temporarily become small or zero. This enables the optimizer to push through saddle points or escape shallow local minima.\nNoisy Gradients: Momentum smooths out noisy gradient updates by averaging them over time, resulting in more stable updates. This reduces the erratic behavior caused by mini-batch noise and leads to smoother convergence overall.\n# SGD + Momentum v = 0 for t in range(num_steps): v = - (learning_rate * dw) + (m * v) w += v In the algorithm above, the variable $v$ accumulates velocity as a running mean of gradients, while $m$ serves as a friction or decay factor. To understand this concept better, consider two extreme cases:\nNo momentum ($m = 0$): The algorithm behaves exactly like standard gradient descent. Maximum momentum ($m = 1)$: The updates oscillate indefinitely, similar to a frictionless bowl, failing to converge. In practice, values for $m$ are typically set around 0.9 or 0.99, striking a balance between acceleration and stability.\nSince we build velocity over time, we risk overshooting even after reaching the minima, which can lead to jittering. Therefore, we need a way to decay the weight updates over time, and this is where Adagrad [2] comes in.\nAdaGrad Instead of keeping track of the sum of gradients like momentum, the Adaptive Gradient algorithm, or AdaGrad for short, maintains a running sum of squared gradients to have an adaptive learning rate.\n# Adagrad grad_squared = 0 for t in range(num_steps): grad_squared += dw * dw w += -learning_rate * dw / (grad_squared.sqrt() + 1e-7) In the direction of a steep descent (where $\\mathbf{dw}$ is high), AdaGrad dampens the progress, resulting in smaller update steps. Conversely, in shallow regions (where $\\mathbf{dw}$ is low), it allows for larger updates. This adaptive approach effectively addresses the problem of landscape variation.\nDuring the initial iterations, the sum of the squared gradients is small, leading to a higher learning rate. As we continue accumulating squared gradients, the learning rate gradually decays over time—a beneficial feature to have!\nSince the sum of squared gradients only increases over time, it can lead to situations where the learning rate becomes too low to make meaningful updates, potentially even before reaching the minimum. To overcome this limitation, we use a variant called RMSProp, which decays this running sum of squared gradients, ensuring that the learning rate does not become too small.\nRMSProp Root Mean Square Propagation, or RMSProp for short, is a leaky version of AdaGrad that introduces a decay rate to the running sum of squared gradients.\n# RMSProp grad_squared = 0 for t in range(num_steps): grad_squared = (decay_rate * grad_squared) + (1 - decay_rate) * dw * dw w += -learning_rate * dw / (grad_squared.sqrt() + 1e-7) The decay rate is a hyperparameter typically set to 0.9, while a common choice for the learning rate is 0.001. This mechanism allows RMSProp to effectively balance the adaptation of the learning rate, preventing the rapid decay issues encountered with AdaGrad.\nAdam Adaptive Moment Estimation, or Adam [3] for short, combines the strengths of both RMSProp and Momentum, utilizing a running sum of gradients as well as the squared gradients to optimize the learning process.\n# Adam m1 = 0 m2 = 0 for t in range(1, num_steps): m1 = (beta1 * m1) + (1 - beta1) * dw m2 = (beta2 * m1) + (1 - beta2) * dw * dw m1_unbias = m1 / (1 - beta1 **t) m2_unbias = m2 / (1 - beta2 **t) w += -learning_rate * m1_unbias / (m2_unbias.sqrt() + 1e-7) The sum of gradients and squared gradients are very small at the beginning of training (biased towards zero), which can lead to excessively large updates. To address this issue, Adam employs a technique called bias correction as shown in algorithm above.\nThis adjustment ensures that the updates remain stable and effective, especially in the early stages of training when the optimizer may be prone to making erratic updates due to the low initial values of the moments.\nBeta1 is the decay rate for the first moment (the sum of gradient, aka momentum), and it is commonly set to 0.9. Beta 2 is the decay rate for the second moment (the sum of gradient squared), typically set at 0.999. Adam has become a go-to optimizer for much of the deep learning community today. Learning rates of 1e-3, 5e-4, and 1e-4 serve as great starting points for most models.\nThe visualization below demonstrates how different optimizers behave on a loss landscape featuring both local and global minima, all starting from a common point.\nGradient Descent (cyan) vs. Momentum (magenta) with m = 0.99 vs. AdaGrad (white) vs RMSProp (green) with decay rate = 0.9 vs Adam (blue) with beta1 = 0.9 and beta2 = 0.999, on a surface with a global minimum (the left well) and local minimum (the right well). Source: visualization tool\nNow that we have prepared the base of our deep learning cake, it’s time to add the icing in the next part!\nReferences [1] Sutskever et al, “On the importance of initialization and momentum in deep learning”, ICML 2013.\n[2] Duchi et al, “Adaptive subgradient methods for online learning and stochastic optimization”, JMLR 2011.\n[3] Kingma and Ba, “Adam: A method for stochastic optimization”, ICLR 2015.\n","permalink":"https://yugajmera.github.io/posts/01-dl1/post/","summary":"While there is a wealth of deep learning content scattered across the internet, I wanted to create a one-stop solution where you can find all the fundamental concepts needed to write your own neural network from scratch.\nThis series is inspired by Justin Johnson\u0026rsquo;s course, and Stanford\u0026rsquo;s CS231n.\nImage Classification Before diving into the details, let’s start with the basics: image classification. This is a core task in computer vision where a model takes an image as input and assigns it a category label from a fixed set of predefined categories.","title":"Deep Learning Part 1: The Base  of the Cake"}]