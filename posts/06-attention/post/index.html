<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Sequence Modeling with Recurrent Neural Networks and Attention | YA&#39;s Almanac</title>
<meta name="keywords" content="">
<meta name="description" content="In previous discussions, we focused on feedforward neural networks, which take a single image as input, process it through multiple layers of convolution, normalization, or fully connected layers, and output a single label for image classification tasks. This is a one-to-one relationship: a single image maps to a single output label.
However, there are other types of problems we want to solve using deep learning that involve variable-length sequences as both inputs and outputs.">
<meta name="author" content="">
<link rel="canonical" href="https://yugajmera.github.io/posts/06-attention/post/">
<link crossorigin="anonymous" href="https://yugajmera.github.io/assets/css/stylesheet.334cb677465a1d1e9767dd05b097c98de3a5b4085e0d43c708104df17bc49585.css" integrity="sha256-M0y2d0ZaHR6XZ90FsJfJjeOltAheDUPHCBBN8XvElYU=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://yugajmera.github.io/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yugajmera.github.io/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yugajmera.github.io/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yugajmera.github.io/assets/apple-touch-icon.png">
<link rel="mask-icon" href="https://yugajmera.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://yugajmera.github.io/posts/06-attention/post/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
<style>
   mjx-container[display="true"] {
       margin: 1.5em 0 ! important
   }
</style>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-S759YBMKJE', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Sequence Modeling with Recurrent Neural Networks and Attention" />
<meta property="og:description" content="In previous discussions, we focused on feedforward neural networks, which take a single image as input, process it through multiple layers of convolution, normalization, or fully connected layers, and output a single label for image classification tasks. This is a one-to-one relationship: a single image maps to a single output label.
However, there are other types of problems we want to solve using deep learning that involve variable-length sequences as both inputs and outputs." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yugajmera.github.io/posts/06-attention/post/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-11-08T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-11-08T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Sequence Modeling with Recurrent Neural Networks and Attention"/>
<meta name="twitter:description" content="In previous discussions, we focused on feedforward neural networks, which take a single image as input, process it through multiple layers of convolution, normalization, or fully connected layers, and output a single label for image classification tasks. This is a one-to-one relationship: a single image maps to a single output label.
However, there are other types of problems we want to solve using deep learning that involve variable-length sequences as both inputs and outputs."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yugajmera.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Sequence Modeling with Recurrent Neural Networks and Attention",
      "item": "https://yugajmera.github.io/posts/06-attention/post/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Sequence Modeling with Recurrent Neural Networks and Attention",
  "name": "Sequence Modeling with Recurrent Neural Networks and Attention",
  "description": "In previous discussions, we focused on feedforward neural networks, which take a single image as input, process it through multiple layers of convolution, normalization, or fully connected layers, and output a single label for image classification tasks. This is a one-to-one relationship: a single image maps to a single output label.\nHowever, there are other types of problems we want to solve using deep learning that involve variable-length sequences as both inputs and outputs.",
  "keywords": [
    
  ],
  "articleBody": "In previous discussions, we focused on feedforward neural networks, which take a single image as input, process it through multiple layers of convolution, normalization, or fully connected layers, and output a single label for image classification tasks. This is a one-to-one relationship: a single image maps to a single output label.\nHowever, there are other types of problems we want to solve using deep learning that involve variable-length sequences as both inputs and outputs. Here are some examples:\nOne-to-many: Consider the task of image captioning, where the input is a single image, and the output is a sentence—a sequence of words describing the content of the image in natural language.\nMany-to-one: An example of this is sentiment analysis, where the input is a sentence, and the task is to classify whether it expresses a positive or negative sentiment.\nMany-to-many: In this case, we want to produce an output for each input in the sequence. For example, in video classification, where we may wish to label each frame of the video.\nSequence-to-sequence (Seq2Seq): In tasks like machine translation, the input is a sentence in one language (e.g., English), and the output is a translation of that sentence in another language (e.g., Spanish). The lengths of the input and output sequences are not necessarily equal.\nInputs are red, outputs are blue, and green boxes represent the RNN’s internal state (more on this soon). [Modified from Andrej Karpathy’s blog]\nConventional neural networks or convolutional networks are designed to handle fixed-size input vectors (such as images) and produce fixed-size output vectors (such as class probabilities). To handle sequences of arbitrary lengths, we use Recurrent Neural Networks (RNNs). RNNs can process sequences of vectors, enabling us to handle sequences in the input, the output, or both.\nRecurrent Neural Networks An RNN processes inputs sequentially, maintaining an internal state called a hidden state $\\mathbf{h}$ (a vector) that encodes information about the inputs it has seen so far.\nA vanilla RNN for a many-to-many task is shown below:\nAt each time step $t$, the RNN takes an input $\\mathbf{x}_t$ (shown in red) and updates its hidden state $\\mathbf{h}_t$ using a recurrence formula:\n\\begin{align} \\mathbf{h}_t \u0026= \\text{tanh} (W_{hh} \\text{ } \\mathbf{h}_{t-1} + W_{xh} \\text{ } \\mathbf{x}_t + b_h) \\\\ \\mathbf{y}_t \u0026= W_{hy} \\text{ } \\mathbf{h}_t \\end{align}\nThis updated hidden state is used to produce an output $\\mathbf{y}_t$ (shown in blue) at each time step. A tanh nonlinearity is used in this model because it was developed in the earlier days of neural network research.\nWe usually initialize the first hidden state $h_0$ to a zero vector. Note that the same weight matrices are used at every timestep in the sequence. This shared weight setup allows RNNs to handle sequences of arbitrary length by unrolling the computation graph over any number of timesteps, all while using the same set of weights.\nThis iterative generation process enables the model to progressively generate a coherent sequence of words, building each step upon the context of previous ones.\nTraining Each word must be encoded as a vector to make the network to process it. We use a fixed vocabulary and convert each unique word in the vocabulary into a one-hot encoded vector before training. Then, we use a softmax layer at the end of our network to predict a probability distribution over this vector of words, selecting the word with the highest probability as the output.\nTo train our RNN, we apply a loss function (e.g., cross-entropy) at each time step in the sequence. The loss is calculated between the output vector $\\mathbf{y}_t$ and the ground truth label (or one-hot encoded word vector) to obtain a loss value per time step. By summing these individual losses across all time steps, we obtain the total loss, which we then use for backpropagation through the network.\nComputational graph for a many-to-many RNN that produces one output per timestep in our input sequence.\nLimitations During backpropagation, the chain rule applies across time steps. Because the same weight matrix is used for all time steps, it is multiplied repeatedly, which can cause two issues:\nExploding gradient problem If the values in the weight matrix are greater than 1, repeated multiplication can cause the gradients to grow exponentially, leading to unstable training.\nTo address this, we use a technique called gradient clipping, where we scale down the gradients if their norm exceeds a certain threshold. This helps control and limit the magnitude of the gradients during training.\nVanishing gradient problem Conversely, if the values in the weight matrix are less than 1, repeated multiplication can shrink the gradients exponentially, leading to the vanishing gradient problem.\nTo mitigate this issue, we use a variant of the RNN called the Long Short-Term Memory (LSTM) network. LSTMs have specialized gating mechanisms and a dedicated cell state that help manage the flow of information and gradients over longer sequences.\nWithout going into detail, the concepts we’ve discussed for RNNs remain the same, except that the mathematical formulation for updating the hidden state is more complex in LSTMs. You can read more about LSTMs here: Colah’s blog.\nExtending to other tasks Let’s explore how RNNs can be adapted for different sequence-based tasks.\nOne-to-Many: Image Captioning In the image captioning task, the goal is to generate a descriptive sentence based on a single image. This is done in two main steps:\nFeature extraction: First, we feed the input image into a pre-trained convolutional neural network (CNN) to extract a feature vector, capturing important information about the image content.\nGenerating the sequence: Next, we pass this image feature vector to an RNN, which uses a recurrence formula to generate a sequence of words that describes the image.\nAn RNN for an image captioning task at test time.\nTo incorporate the image features in the recurrence formula, we modify it as follows:\n\\begin{align} \\mathbf{h}_t \u0026= \\text{tanh} (W_{hh} \\text{ } \\mathbf{h}_{t-1} + W_{xh} \\text{ } \\mathbf{x}_t + {\\color{purple}{W_{ih}} \\text{ } \\mathbf{v}} + b_h) \\\\ \\mathbf{y}_t \u0026= W_{hy} \\text{ } \\mathbf{h}_t \\end{align}\nwhere $\\mathbf{v}$ is the feature vector of the input image.\nTraining is similar to what we discussed earlier, but it differs at test time. During testing, we feed the image feature vector along with an initial seed token, “\u003c START \u003e”, into the RNN. This produces a probability distribution for the first word in the caption.\nFor instance, if “man” has the highest probability in this distribution, we select it as the first word in the caption. We then feed the it’s embedding vector back into the RNN as the next input to generate the following word.\nWe repeat this process, generating words sequentially and unrolling the graph to obtain $\\mathbf{y}_{t=1:T}$. Sampling stops when we encounter the end token “\u003c END \u003e”, which marks the end of the sentence.\nMany-to-One: Sentiment Analysis In a sentiment analysis task, the RNN receives word embeddings for each word in an input sentence. After processing the entire sequence, the RNN produces a single output prediction $\\mathbf{y}$ based on the final hidden state. This output might be a binary classification, such as “1” for positive sentiment or “0” for negative sentiment.\nAn RNN for a sentiment analysis task.\nThe final hidden state effectively summarizes the information from the entire input sequence, capturing the context the network needs to make a sentiment prediction.\nSequence to sequence (Many-to-One + One-to-Many): Machine Translation The seq2seq model was first introduced for machine translation [1], where the goal is to transform an input sequence (source) into a corresponding output sequence (target), with both sequences being of arbitrary lengths. These models are also commonly used in applications like chatbots and personal assistants, where they generate meaningful responses to input queries.\nIt typically uses a combination of two RNNs in an encoder-decoder style architecture:\nEncoder RNN (Many-to-One): The encoder takes the input sequence and outputs a fixed-length vector that summarizes the content of the input. This output vector, which is the last hidden state of the encoder, is often called the context vector or thought vector.\nDecoder RNN (One-to-Many): We then feed this context vector as the initial hidden state into the decoder RNN, which generates the target sequence as its output.\nA seq2seq model for translating a sentence from English to Spanish.\nIt’s important to note that the encoder and decoder RNNs have different weight matrices since they handle different sequences, which may vary in length.\nA common practice is to separate the context vector and the initial hidden state of the decoder, as both serve different purposes:\nThe context vector captures the input information from the encoder, which is passed on to the decoder to help generate the output sequence. This is often set to the last hidden state of the encoder, $\\mathbf{c} = \\mathbf{h}_T$, and is used in the recurrence formula at each time step of the decoder.\nThe initial decoder state is used to start the decoding process and is typically derived from a projection or a feed-forward layer applied to the encoder’s final hidden state. This approach allows the initial state to be optimized specifically for the decoder, rather than directly copying the encoder’s final state.\nA seq2seq model with seperate context vector and initial decoder state.\nThe context vector passes information from the encoder to the decoder. However, since all the input information is bottlenecked into this single fixed-size vector, it becomes difficult for the model to retain information from longer input sequences. Often, the earlier parts of the input may be “forgotten” by the time the encoder finishes processing. To address this limitation, we use a mechanism called Attention [2].\nAttention Instead of relying solely on the encoder’s last hidden state to build a single context vector, attention enables the model to dynamically create context vectors at each decoding step. We let the decoder weigh all the encoder’s hidden states according to their relevance to the current step in the decoding process.\nTo focus on the parts of the input sequence that are most relevant to the current output, the decoder follows these steps:\nAlignment scores: An alignment function (often an MLP) takes the current hidden state of the decoder and each hidden state of the encoder, producing a score (scalar value) for each encoder state. These scores indicate the relevance of each encoder hidden state to the current decoding step.\n$$ e_{t, i} = f_{\\text{att}} (\\mathbf{s}_{t-1}, \\mathbf{h}_i) $$\nAlignment weights: The alignment scores are passed through a softmax function to produce a probability distribution. These values tell us how much weight to assign to each encoder hidden state when constructing the context vector.\n$$ a_{t, i} = \\text{softmax} (e_{t, i}) $$\nNew context vector: The context vector for the current time step is constructed by taking a weighted sum of the attention weights and the encoder hidden states. $$ c_t = \\sum_i a_{t, i} \\mathbf{h}_i $$ Update step: The decoder uses this new context vector in the recurrence formula to obtain the next hidden state: \\begin{align} \\mathbf{s}_t \u0026= \\text{tanh} (W_{ss} \\text{ } \\mathbf{s}_{t-1} + W_{yh} \\text{ } \\mathbf{y}_{t-1} + {\\color{purple}{W_{cs}} \\text{ } \\mathbf{c}_t} + b_s) \\\\ \\mathbf{y}_t \u0026= W_{hy} \\text{ } \\mathbf{s}_t \\end{align}\nWe repeat these four steps for every time step in the decoder’s sequence, with the initial decoder state coming from the encoder’s final hidden state.\nThe intuition here is that as each word in the output sentence is generated, the context vector “attends” to the most relevant part of the input sentence. For example, in a English-to-Spanish translation, when the model is generating the word “estamos” for “we are” it will focus more on the corresponding English encoder states. Sample attention weights might look like:\n\\begin{align} a_{11} [\\text{we}] = a_{12} [\\text{are}] = 0.45, \\\\ a_{13} [\\text{eating}] = a_{14} [\\text{bread}] = 0.05 \\end{align}\nSimilarly, for the word “comiendo” (from “eating”), the weights could be:\n\\begin{align} a_{21} [\\text{we}] = a_{24} [\\text{bread}] = 0.05, \\\\ a_{22} [\\text{are}] = 0.1, \\\\ a_{23} [\\text{eating}] = 0.8 \\end{align}\nSince the seq2seq model with attention is trainable end-to-end, the network learns on its own which parts of the input sequence to focus on for each output word. This flexibility allows it to dynamically adjust focus as needed, creating a more accurate and contextually aware output sequence.\nWe can visualize this intuition through the attention weight matrix from a trained seq2seq model, where each pixel reflects the attention value between corresponding words in the source (e.g., English) and target (e.g., French) sentences. Higher attention weights (white boxes) indicate stronger relevance or “focus” on particular input words when generating specific output words, showing how the model aligns corresponding words across the two languages.\nThe x-axis and y-axis correspond to the words in the source sentence (English) and the generated translation (French), respectively. Each pixel shows the attention weight value.\nImage Captioning with Attention The attention mechanism enables models to generate sequences by focusing on different parts of the input at each generation step. Importantly, this mechanism doesn’t rely on the input being a sequence; it simply lets the model attend to the most relevant parts of the input, which can be structured in any way. This flexibility makes attention mechanisms applicable not only to sequential data but also to other types of inputs, such as images.\nLet’s explore how attention can be applied to the image captioning task [3].\nFeature extraction: We begin by using a CNN to extract a grid of feature vectors, where each vector corresponds to a specific spatial location in the input image. These feature vectors capture the image’s content in a spatially structured manner.\nInitial hidden state: This grid of feature vectors is then used to predict the initial hidden state of the decoder RNN.\nAttention mechanism: At each step of the generation process, the attention mechanism combines the decoder’s current hidden state with the feature grid to construct a new context vector. This context vector reflects the parts of the image that are most relevant for generating the next word in the caption.\nAn RNN with Attention for an image captioning task at test time.\nBy using different context vectors at each timestep, the model can “attend” to different parts of the input image as it generates each word. Similar to sequence-to-sequence models, we can visualize attention weights overlayed on the image to gain insight into which parts of the image the model is focusing on at each step.\nAttention over time. As the model generates each word, its attention changes to reflect the relevant parts of the image.\nIn the image above, the model attends to the bird when generating “bird flying over,” and shifts focus to the water region for the word “water.” This is similar to how humans visually explore a scene, focusing on different parts depending on what we’re describing.\nThis interpretability—visualizing attention weights—sets attention mechanisms apart, as they provide insight into how a model makes its decisions, making them more transparent and understandable than other neural network approaches.\nReferences [1] Sutskever et al, “Sequence to sequence learning with neural networks”, NeurIPS 2014.\n[2] Bahdanau et al, “Neural machine translation by jointly learning to align and translate”, ICLR 2015.\n[3] Xu et al, “Show, AKend, and Tell: Neural Image CapAon GeneraAon with Visual AKenAon”, ICML 2015.\n",
  "wordCount" : "2521",
  "inLanguage": "en",
  "datePublished": "2024-11-08T00:00:00Z",
  "dateModified": "2024-11-08T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yugajmera.github.io/posts/06-attention/post/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "YA's Almanac",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yugajmera.github.io/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yugajmera.github.io/" accesskey="h" title="YA&#39;s Almanac (Alt + H)">YA&#39;s Almanac</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yugajmera.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Sequence Modeling with Recurrent Neural Networks and Attention
    </h1>
    <div class="post-meta"><span title='2024-11-08 00:00:00 +0000 UTC'>November 8, 2024</span>&nbsp;·&nbsp;12 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#recurrent-neural-networks" aria-label="Recurrent Neural Networks">Recurrent Neural Networks</a><ul>
                        
                <li>
                    <a href="#training" aria-label="Training">Training</a></li>
                <li>
                    <a href="#limitations" aria-label="Limitations">Limitations</a><ul>
                        
                <li>
                    <a href="#exploding-gradient-problem" aria-label="Exploding gradient problem">Exploding gradient problem</a></li>
                <li>
                    <a href="#vanishing-gradient-problem" aria-label="Vanishing gradient problem">Vanishing gradient problem</a></li></ul>
                </li>
                <li>
                    <a href="#extending-to-other-tasks" aria-label="Extending to other tasks">Extending to other tasks</a><ul>
                        
                <li>
                    <a href="#one-to-many-image-captioning" aria-label="One-to-Many: Image Captioning">One-to-Many: Image Captioning</a></li>
                <li>
                    <a href="#many-to-one-sentiment-analysis" aria-label="Many-to-One: Sentiment Analysis">Many-to-One: Sentiment Analysis</a></li>
                <li>
                    <a href="#sequence-to-sequence-many-to-one--one-to-many-machine-translation" aria-label="Sequence to sequence (Many-to-One &#43; One-to-Many): Machine Translation">Sequence to sequence (Many-to-One + One-to-Many): Machine Translation</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#attention" aria-label="Attention">Attention</a><ul>
                        
                <li>
                    <a href="#image-captioning-with-attention" aria-label="Image Captioning with Attention">Image Captioning with Attention</a></li></ul>
                </li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>In previous discussions, we focused on feedforward neural networks, which take a single image as input, process it through multiple layers of convolution, normalization, or fully connected layers, and output a single label for image classification tasks. This is a <strong>one-to-one</strong> relationship: a single image maps to a single output label.</p>
<p>However, there are other types of problems we want to solve using deep learning that involve variable-length sequences as both inputs and outputs. Here are some examples:</p>
<p><strong>One-to-many</strong>: Consider the task of image captioning, where the input is a single image, and the output is a sentence—a sequence of words describing the content of the image in natural language.</p>
<p><strong>Many-to-one</strong>: An example of this is sentiment analysis, where the input is a sentence, and the task is to classify whether it expresses a positive or negative sentiment.</p>
<p><strong>Many-to-many</strong>: In this case, we want to produce an output for each input in the sequence. For example, in video classification, where we may wish to label each frame of the video.</p>
<p><strong>Sequence-to-sequence (Seq2Seq)</strong>: In tasks like machine translation, the input is a sentence in one language (e.g., English), and the output is a translation of that sentence in another language (e.g., Spanish). The lengths of the input and output sequences are not necessarily equal.</p>
<figure class="align-center ">
    <img loading="lazy" src="../sequence-tasks.png#center"
         alt="Inputs are red, outputs are blue, and green boxes represent the RNN&amp;rsquo;s internal state (more on this soon). [Modified from Andrej Karpathy&amp;rsquo;s blog]"/> <figcaption>
            <p>Inputs are red, outputs are blue, and green boxes represent the RNN&rsquo;s internal state (more on this soon). [Modified from <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">Andrej Karpathy&rsquo;s blog</a>]</p>
        </figcaption>
</figure>

<p>Conventional neural networks or convolutional networks are designed to handle fixed-size input vectors (such as images) and produce fixed-size output vectors (such as class probabilities). To handle sequences of arbitrary lengths, we use Recurrent Neural Networks (RNNs). RNNs can process sequences of vectors, enabling us to handle sequences in the input, the output, or both.</p>
<h2 id="recurrent-neural-networks">Recurrent Neural Networks<a hidden class="anchor" aria-hidden="true" href="#recurrent-neural-networks">#</a></h2>
<p>An RNN processes inputs sequentially, maintaining an internal state called a hidden state $\mathbf{h}$ (a vector) that encodes information about the inputs it has seen so far.</p>
<p>A vanilla RNN for a many-to-many task is shown below:</p>
<figure class="align-center ">
    <img loading="lazy" src="../rnn.png#center"/> 
</figure>

<p>At each time step $t$, the RNN takes an input $\mathbf{x}_t$ (shown in red) and updates its hidden state $\mathbf{h}_t$ using a recurrence formula:</p>
<p>\begin{align}
\mathbf{h}_t &amp;= \text{tanh} (W_{hh} \text{ } \mathbf{h}_{t-1} + W_{xh} \text{ } \mathbf{x}_t + b_h) \\
\mathbf{y}_t &amp;= W_{hy} \text{ } \mathbf{h}_t
\end{align}</p>
<p>This updated hidden state is used to produce an output $\mathbf{y}_t$ (shown in blue) at each time step. A tanh nonlinearity is used in this model because it was developed in the earlier days of neural network research.</p>
<p>We usually initialize the first hidden state $h_0$ to a zero vector. Note that the same weight matrices are used at every timestep in the sequence. This shared weight setup allows RNNs to handle sequences of arbitrary length by unrolling the computation graph over any number of timesteps, all while using the same set of weights.</p>
<p>This iterative generation process enables the model to progressively generate a coherent sequence of words, building each step upon the context of previous ones.</p>
<h3 id="training">Training<a hidden class="anchor" aria-hidden="true" href="#training">#</a></h3>
<p>Each word must be encoded as a vector to make the network to process it. We use a fixed vocabulary and convert each unique word in the vocabulary into a one-hot encoded vector before training. Then, we use a softmax layer at the end of our network to predict a probability distribution over this vector of words, selecting the word with the highest probability as the output.</p>
<p>To train our RNN, we apply a loss function (e.g., cross-entropy) at each time step in the sequence. The loss is calculated between the output vector $\mathbf{y}_t$ and the ground truth label (or one-hot encoded word vector) to obtain a loss value per time step. By summing these individual losses across all time steps, we obtain the total loss, which we then use for backpropagation through the network.</p>
<figure class="align-center ">
    <img loading="lazy" src="../rnn-train.png#center"
         alt="Computational graph for a many-to-many RNN that produces one output per timestep in our input sequence."/> <figcaption>
            <p>Computational graph for a many-to-many RNN that produces one output per timestep in our input sequence.</p>
        </figcaption>
</figure>

<h3 id="limitations">Limitations<a hidden class="anchor" aria-hidden="true" href="#limitations">#</a></h3>
<p>During backpropagation, the chain rule applies across time steps. Because the same weight matrix is used for all time steps, it is multiplied repeatedly, which can cause two issues:</p>
<h4 id="exploding-gradient-problem">Exploding gradient problem<a hidden class="anchor" aria-hidden="true" href="#exploding-gradient-problem">#</a></h4>
<p>If the values in the weight matrix are greater than 1, repeated multiplication can cause the gradients to grow exponentially, leading to unstable training.</p>
<p>To address this, we use a technique called gradient clipping, where we scale down the gradients if their norm exceeds a certain threshold. This helps control and limit the magnitude of the gradients during training.</p>
<h4 id="vanishing-gradient-problem">Vanishing gradient problem<a hidden class="anchor" aria-hidden="true" href="#vanishing-gradient-problem">#</a></h4>
<p>Conversely, if the values in the weight matrix are less than 1, repeated multiplication can shrink the gradients exponentially, leading to the vanishing gradient problem.</p>
<p>To mitigate this issue, we use a variant of the RNN called the Long Short-Term Memory (LSTM) network. LSTMs have specialized gating mechanisms and a dedicated cell state that help manage the flow of information and gradients over longer sequences.</p>
<p>Without going into detail, the concepts we’ve discussed for RNNs remain the same, except that the mathematical formulation for updating the hidden state is more complex in LSTMs. You can read more about LSTMs here: <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah&rsquo;s blog</a>.</p>
<h3 id="extending-to-other-tasks">Extending to other tasks<a hidden class="anchor" aria-hidden="true" href="#extending-to-other-tasks">#</a></h3>
<p>Let&rsquo;s explore how RNNs can be adapted for different sequence-based tasks.</p>
<h4 id="one-to-many-image-captioning">One-to-Many: Image Captioning<a hidden class="anchor" aria-hidden="true" href="#one-to-many-image-captioning">#</a></h4>
<p>In the image captioning task, the goal is to generate a descriptive sentence based on a single image. This is done in two main steps:</p>
<ol>
<li>
<p><strong>Feature extraction</strong>: First, we feed the input image into a pre-trained convolutional neural network (CNN) to extract a feature vector, capturing important information about the image content.</p>
</li>
<li>
<p><strong>Generating the sequence</strong>: Next, we pass this image feature vector to an RNN, which uses a recurrence formula to generate a sequence of words that describes the image.</p>
</li>
</ol>
<figure class="align-center ">
    <img loading="lazy" src="../image-captioning-sample.png#center"
         alt="An RNN for an image captioning task at test time."/> <figcaption>
            <p>An RNN for an image captioning task at test time.</p>
        </figcaption>
</figure>

<p>To incorporate the image features in the recurrence formula, we modify it as follows:</p>
<p>\begin{align}
\mathbf{h}_t &amp;= \text{tanh} (W_{hh} \text{ } \mathbf{h}_{t-1} + W_{xh} \text{ } \mathbf{x}_t + {\color{purple}{W_{ih}} \text{ } \mathbf{v}} + b_h) \\
\mathbf{y}_t &amp;= W_{hy} \text{ } \mathbf{h}_t
\end{align}</p>
<p>where $\mathbf{v}$ is the feature vector of the input image.</p>
<p>Training is similar to what we discussed earlier, but it differs at test time. During testing, we feed the image feature vector along with an initial seed token, &ldquo;&lt; START &gt;&rdquo;, into the RNN. This produces a probability distribution for the first word in the caption.</p>
<p>For instance, if &ldquo;man&rdquo; has the highest probability in this distribution, we select it as the first word in the caption. We then feed the it&rsquo;s embedding vector back into the RNN as the next input to generate the following word.</p>
<p>We repeat this process, generating words sequentially and unrolling the graph to obtain $\mathbf{y}_{t=1:T}$. Sampling stops when we encounter the end token &ldquo;&lt; END &gt;&rdquo;, which marks the end of the sentence.</p>
<h4 id="many-to-one-sentiment-analysis">Many-to-One: Sentiment Analysis<a hidden class="anchor" aria-hidden="true" href="#many-to-one-sentiment-analysis">#</a></h4>
<p>In a sentiment analysis task, the RNN receives word embeddings for each word in an input sentence. After processing the entire sequence, the RNN produces a single output prediction $\mathbf{y}$ based on the final hidden state. This output might be a binary classification, such as &ldquo;1&rdquo; for positive sentiment or &ldquo;0&rdquo; for negative sentiment.</p>
<figure class="align-center ">
    <img loading="lazy" src="../sentiment-analysis.png#center"
         alt="An RNN for a sentiment analysis task." width="600"/> <figcaption>
            <p>An RNN for a sentiment analysis task.</p>
        </figcaption>
</figure>

<p>The final hidden state effectively summarizes the information from the entire input sequence, capturing the context the network needs to make a sentiment prediction.</p>
<h4 id="sequence-to-sequence-many-to-one--one-to-many-machine-translation">Sequence to sequence (Many-to-One + One-to-Many): Machine Translation<a hidden class="anchor" aria-hidden="true" href="#sequence-to-sequence-many-to-one--one-to-many-machine-translation">#</a></h4>
<p>The seq2seq model was first introduced for machine translation <a href="#references">[1]</a>, where the goal is to transform an input sequence (source) into a corresponding output sequence (target), with both sequences being of arbitrary lengths. These models are also commonly used in applications like chatbots and personal assistants, where they generate meaningful responses to input queries.</p>
<p>It typically uses a combination of two RNNs in an encoder-decoder style architecture:</p>
<ul>
<li>
<p><strong>Encoder RNN (Many-to-One)</strong>: The encoder takes the input sequence and outputs a fixed-length vector that summarizes the content of the input. This output vector, which is the last hidden state of the encoder, is often called the context vector or thought vector.</p>
</li>
<li>
<p><strong>Decoder RNN (One-to-Many)</strong>: We then feed this context vector as the initial hidden state into the decoder RNN, which generates the target sequence as its output.</p>
</li>
</ul>
<figure class="align-center ">
    <img loading="lazy" src="../seq2seq-translation.png#center"
         alt="A seq2seq model for translating a sentence from English to Spanish."/> <figcaption>
            <p>A seq2seq model for translating a sentence from English to Spanish.</p>
        </figcaption>
</figure>

<p>It&rsquo;s important to note that the encoder and decoder RNNs have different weight matrices since they handle different sequences, which may vary in length.</p>
<p>A common practice is to separate the context vector and the initial hidden state of the decoder, as both serve different purposes:</p>
<ul>
<li>
<p>The context vector captures the input information from the encoder, which is passed on to the decoder to help generate the output sequence. This is often set to the last hidden state of the encoder, $\mathbf{c} = \mathbf{h}_T$, and is used in the recurrence formula at each time step of the decoder.</p>
</li>
<li>
<p>The initial decoder state is used to start the decoding process and is typically derived from a projection or a feed-forward layer applied to the encoder&rsquo;s final hidden state. This approach allows the initial state to be optimized specifically for the decoder, rather than directly copying the encoder’s final state.</p>
</li>
</ul>
<figure class="align-center ">
    <img loading="lazy" src="../seq2seq-translation2.png#center"
         alt="A seq2seq model with seperate context vector and initial decoder state."/> <figcaption>
            <p>A seq2seq model with seperate context vector and initial decoder state.</p>
        </figcaption>
</figure>

<p>The context vector passes information from the encoder to the decoder. However, since all the input information is bottlenecked into this single fixed-size vector, it becomes difficult for the model to retain information from longer input sequences. Often, the earlier parts of the input may be &ldquo;forgotten&rdquo; by the time the encoder finishes processing. To address this limitation, we use a mechanism called Attention <a href="#references">[2]</a>.</p>
<h2 id="attention">Attention<a hidden class="anchor" aria-hidden="true" href="#attention">#</a></h2>
<p>Instead of relying solely on the encoder&rsquo;s last hidden state to build a single context vector, attention enables the model to dynamically create context vectors at each decoding step. We let the decoder weigh all the encoder’s hidden states according to their relevance to the current step in the decoding process.</p>
<p>To focus on the parts of the input sequence that are most relevant to the current output, the decoder follows these steps:</p>
<ol>
<li>
<p><strong>Alignment scores</strong>: An alignment function (often an MLP) takes the current hidden state of the decoder and each hidden state of the encoder, producing a score (scalar value) for each encoder state. These scores indicate the relevance of each encoder hidden state to the current decoding step.</p>
<p>$$
e_{t, i} = f_{\text{att}} (\mathbf{s}_{t-1}, \mathbf{h}_i)
$$</p>
</li>
</ol>
<figure class="align-center ">
    <img loading="lazy" src="../attention-1.png#center"/> 
</figure>

<ol start="2">
<li>
<p><strong>Alignment weights</strong>: The alignment scores are passed through a softmax function to produce a probability distribution. These values tell us how much weight to assign to each encoder hidden state when constructing the context vector.</p>
<p>$$
a_{t, i} = \text{softmax} (e_{t, i})
$$</p>
</li>
</ol>
<figure class="align-center ">
    <img loading="lazy" src="../attention-2.png#center"/> 
</figure>

<ol start="3">
<li><strong>New context vector</strong>: The context vector for the current time step is constructed by taking a weighted sum of the attention weights and the encoder hidden states.
$$
c_t = \sum_i a_{t, i} \mathbf{h}_i
$$</li>
</ol>
<figure class="align-center ">
    <img loading="lazy" src="../attention-3.png#center"/> 
</figure>

<ol start="4">
<li><strong>Update step</strong>: The decoder uses this new context vector in the recurrence formula to obtain the next hidden state:</li>
</ol>
<p>\begin{align}
\mathbf{s}_t &amp;= \text{tanh} (W_{ss} \text{ } \mathbf{s}_{t-1} + W_{yh} \text{ } \mathbf{y}_{t-1} + {\color{purple}{W_{cs}} \text{ } \mathbf{c}_t} + b_s) \\
\mathbf{y}_t &amp;= W_{hy} \text{ } \mathbf{s}_t
\end{align}</p>
<p>We repeat these four steps for every time step in the decoder’s sequence, with the initial decoder state coming from the encoder’s final hidden state.</p>
<p>The intuition here is that as each word in the output sentence is generated, the context vector “attends” to the most relevant part of the input sentence. For example, in a English-to-Spanish translation, when the model is generating the word &ldquo;estamos&rdquo; for &ldquo;we are&rdquo; it will focus more on the corresponding English encoder states. Sample attention weights might look like:</p>
<p>\begin{align}
a_{11} [\text{we}] = a_{12} [\text{are}] = 0.45, \\
a_{13} [\text{eating}] = a_{14} [\text{bread}] = 0.05
\end{align}</p>
<p>Similarly, for the word &ldquo;comiendo&rdquo; (from &ldquo;eating&rdquo;), the weights could be:</p>
<p>\begin{align}
a_{21} [\text{we}] = a_{24} [\text{bread}] = 0.05, \\
a_{22} [\text{are}] = 0.1, \\
a_{23} [\text{eating}] = 0.8
\end{align}</p>
<p>Since the seq2seq model with attention is trainable end-to-end, the network learns on its own which parts of the input sequence to focus on for each output word. This flexibility allows it to dynamically adjust focus as needed, creating a more accurate and contextually aware output sequence.</p>
<p>We can visualize this intuition through the attention weight matrix from a trained seq2seq model, where each pixel reflects the attention value between corresponding words in the source (e.g., English) and target (e.g., French) sentences. Higher attention weights (white boxes) indicate stronger relevance or &ldquo;focus&rdquo; on particular input words when generating specific output words, showing how the model aligns corresponding words across the two languages.</p>
<figure class="align-center ">
    <img loading="lazy" src="../attention-weights.png#center"
         alt="The x-axis and y-axis correspond to the words in the source sentence (English) and the generated translation (French), respectively. Each pixel shows the attention weight value." width="450"/> <figcaption>
            <p>The x-axis and y-axis correspond to the words in the source sentence (English) and the generated translation (French), respectively. Each pixel shows the attention weight value.</p>
        </figcaption>
</figure>

<h3 id="image-captioning-with-attention">Image Captioning with Attention<a hidden class="anchor" aria-hidden="true" href="#image-captioning-with-attention">#</a></h3>
<p>The attention mechanism enables models to generate sequences by focusing on different parts of the input at each generation step. Importantly, this mechanism doesn&rsquo;t rely on the input being a sequence; it simply lets the model attend to the most relevant parts of the input, which can be structured in any way. This flexibility makes attention mechanisms applicable not only to sequential data but also to other types of inputs, such as images.</p>
<p>Let&rsquo;s explore how attention can be applied to the image captioning task <a href="#references">[3]</a>.</p>
<ol>
<li>
<p><strong>Feature extraction</strong>: We begin by using a CNN to extract a grid of feature vectors, where each vector corresponds to a specific spatial location in the input image. These feature vectors capture the image&rsquo;s content in a spatially structured manner.</p>
</li>
<li>
<p><strong>Initial hidden state</strong>: This grid of feature vectors is then used to predict the initial hidden state of the decoder RNN.</p>
</li>
<li>
<p><strong>Attention mechanism</strong>: At each step of the generation process, the attention mechanism combines the decoder’s current hidden state with the feature grid to construct a new context vector. This context vector reflects the parts of the image that are most relevant for generating the next word in the caption.</p>
</li>
</ol>
<figure class="align-center ">
    <img loading="lazy" src="../image-captioning-attention-sample.png#center"
         alt="An RNN with Attention for an image captioning task at test time."/> <figcaption>
            <p>An RNN with Attention for an image captioning task at test time.</p>
        </figcaption>
</figure>

<p>By using different context vectors at each timestep, the model can &ldquo;attend&rdquo; to different parts of the input image as it generates each word. Similar to sequence-to-sequence models, we can visualize attention weights overlayed on the image to gain insight into which parts of the image the model is focusing on at each step.</p>
<figure class="align-center ">
    <img loading="lazy" src="../attention-weights-caption.png#center"
         alt="Attention over time. As the model generates each word, its attention changes to reflect the relevant parts of the image."/> <figcaption>
            <p>Attention over time. As the model generates each word, its attention changes to reflect the relevant parts of the image.</p>
        </figcaption>
</figure>

<p>In the image above, the model attends to the bird when generating &ldquo;bird flying over,&rdquo; and shifts focus to the water region for the word &ldquo;water.&rdquo; This is similar to how humans visually explore a scene, focusing on different parts depending on what we’re describing.</p>
<p>This interpretability—visualizing attention weights—sets attention mechanisms apart, as they provide insight into how a model makes its decisions, making them more transparent and understandable than other neural network approaches.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1] Sutskever et al, “Sequence to sequence learning with neural networks”, NeurIPS 2014.</p>
<p>[2] Bahdanau et al, “Neural machine translation by jointly learning to align and translate”, ICLR 2015.</p>
<p>[3] Xu et al, “Show, AKend, and Tell: Neural Image CapAon GeneraAon with Visual AKenAon”, ICML 2015.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://yugajmera.github.io/posts/05-imagenet/post/">
    <span class="title">Next »</span>
    <br>
    <span>ImageNet Challenge: The Olympics of Deep Learning</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://yugajmera.github.io/">YA&#39;s Almanac</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
