<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Implementing Backpropagation | YA Logs</title>
<meta name="keywords" content="">
<meta name="description" content="While we have talked about how optimization algorithms use the negative gradient of the loss function with respect to the weights to update the parameters of our model, we will now focus on how to actually compute these gradients on an arbitrary loss function. We use a backpropagation technique that creates a computation graph to perform a forward and backward pass on the model. In the forward pass, we compute outputs of each layer of our neural network sequentially.">
<meta name="author" content="">
<link rel="canonical" href="https://yugajmera.github.io/posts/backprop/post/">
<link crossorigin="anonymous" href="https://yugajmera.github.io/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://yugajmera.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yugajmera.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yugajmera.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yugajmera.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://yugajmera.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://yugajmera.github.io/posts/backprop/post/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<meta property="og:title" content="Implementing Backpropagation" />
<meta property="og:description" content="While we have talked about how optimization algorithms use the negative gradient of the loss function with respect to the weights to update the parameters of our model, we will now focus on how to actually compute these gradients on an arbitrary loss function. We use a backpropagation technique that creates a computation graph to perform a forward and backward pass on the model. In the forward pass, we compute outputs of each layer of our neural network sequentially." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yugajmera.github.io/posts/backprop/post/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-08-05T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Implementing Backpropagation"/>
<meta name="twitter:description" content="While we have talked about how optimization algorithms use the negative gradient of the loss function with respect to the weights to update the parameters of our model, we will now focus on how to actually compute these gradients on an arbitrary loss function. We use a backpropagation technique that creates a computation graph to perform a forward and backward pass on the model. In the forward pass, we compute outputs of each layer of our neural network sequentially."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yugajmera.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Implementing Backpropagation",
      "item": "https://yugajmera.github.io/posts/backprop/post/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Implementing Backpropagation",
  "name": "Implementing Backpropagation",
  "description": "While we have talked about how optimization algorithms use the negative gradient of the loss function with respect to the weights to update the parameters of our model, we will now focus on how to actually compute these gradients on an arbitrary loss function. We use a backpropagation technique that creates a computation graph to perform a forward and backward pass on the model. In the forward pass, we compute outputs of each layer of our neural network sequentially.",
  "keywords": [
    
  ],
  "articleBody": " While we have talked about how optimization algorithms use the negative gradient of the loss function with respect to the weights to update the parameters of our model, we will now focus on how to actually compute these gradients on an arbitrary loss function. We use a backpropagation technique that creates a computation graph to perform a forward and backward pass on the model. In the forward pass, we compute outputs of each layer of our neural network sequentially. In the backward pass, we apply the chain rule for computing derivatives of each individual layer backward.\nConsider the example of a computation graph shown below.\nFirst we perform a forward pass where we have the value of inputs $x = -2, y = 5, z = -4$, \\begin{align*} q \u0026= x + y = -2 + 5 = -3\\\\ f \u0026 = z * q = -4 * -3 = -12 \\end{align*} Now lets compute the backward pass, \\begin{align*} \u0026 \\frac{df}{df} = 1, \\hspace{5mm} \\frac{df}{dq} = z = -4, \\hspace{5mm} \\frac{df}{dz} = q = -3 \\\\ \u0026 \\frac{df}{dx} = \\frac{df}{dq} \\frac{dq}{dx} = -4 * 1 = -4, \\hspace{5mm} \\frac{df}{dy} = \\frac{df}{dq} \\frac{dq}{dy} = -4 * 1 = -4 \\\\ \\end{align*} This way, we have computed the derivative of the output with respect to the input. These gradients involved in chain rule have names, \\begin{align*} \\underbrace{\\frac{df}{dy}}_{\\text{downstream gradient}} = \\underbrace{\\frac{df}{dq}}_{\\text{upstream gradient}} \\hspace{2mm} \\underbrace{\\frac{dq}{dy}}_{\\text{local gradient}} \\end{align*}\nWith a much more complicated neural network model, we follow the same procedure but don’t worry, you don’t have to compute the chain rule manually; Pytorch comes to the rescue!\nConsider this simple neural network model on which we will apply backpropagation. It an image classification task with each image of shape (32 X 32 X 3 = 3072) input dimensions. The model with two layers outputs scores for 10 classes as shown below. [Linear -\u003e ReLU -\u003e Linear]\nPytorch uses the autograd package to build computational graphs out of Tensors and automatically compute gradients. Initializing weights as tensors with requires_grad = True flag enables autograd. Once we compute the loss, the gradient of this loss with respect to the weights can automatically be computed by running loss.backward(). This function implements backpropagation on all the inputs that have require grad set and accumulates the gradients in .grad() variable.\nThe torch.no_grad() function makes sure a computation graph is not created on the operations within it. It is crucial to run .zero_grad() function that zeroes out the current gradients to accumulate fresh gradients in the next step (as it does not overwrite automatically). A simple mini-batch gradient descent algorithm looks as follows,\nw1 = torch.randn(3072, 100, requires_grad=True) w2 = torch.randn(100, 10, requires_grad=True) mini_batches = split(data, batch_size) # Split data into mini-batches for t in range(num_steps): for X_batch, y_batch in mini_batches: y_pred = X_batch.mm(w1).clamp(min=0).mm(w2) # Forward Pass loss = loss_fn(y_pred, y_batch) loss.backward() # Compute gradients wrt loss with torch.no_grad(): # Tells Pytorch not to build a graph w1 -= learning_rate * w1.grad() w2 -= learning_rate* w2.grad() w1.zero_grad() # Set gradients to zero w2.zero_grad() X_batch is the input vector (mini-batch) to our model and y_batch is the corresponding label vector. y_pred is the scores vector that our model outputs. The loss function returns the average loss over a mini-batch which is then used to compute the gradients. Pytorch provides the Dataloader primitive to create mini-batches.\nIt was easy to define the model, initialize weights, and write the forward pass. But what if we had a neural network with 50 layers? Explicitly writing code for all this would be very cumbersome. Pytorch provides the nn.Sequential API to define models easily. The weights have required grad set True implicitly and are initialized using the Kaiming initialization we discussed in the last post.\nmodel = torch.nn.Sequential(torch.nn.Linear(3072, 100), torch.nn.ReLU(), torch.nn.Linear(100, 10)) mini_batches = torch.utils.data.DataLoader(data, batch_size=batch_size) for t in range(num_steps): for X_batch, y_batch in mini_batches: y_pred = model(X_batch) # Forward Pass loss = loss_fn(y_pred, y_batch) loss.backward() # Compute gradients wrt loss with torch.no_grad(): # Tells Pytorch not to build a graph for param in model.parameters(): param -= learning_rate * param.grad() model.zero_grad() # Set gradients to zero It’s annoying to write the weight update code all the time. Pytorch provides the optim package (makes life easier once again!) for implementing the standard gradient descent rules that we have discussed. In our training loop, after computing gradients, we use this optimizer to automatically make the gradient step for us (one step of update) and zero out the gradients. This is what a typical training block looks like in practice,\nmodel = torch.nn.Sequential(torch.nn.Linear(3072, 100), torch.nn.ReLU(), torch.nn.Linear(100, 10)) optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) mini_batches = torch.utils.data.DataLoader(data, batch_size=batch_size) for t in range(num_steps): for X_batch, y_batch in mini_batches: y_pred = model(X_batch) loss = loss_fn(y_pred, y_batch) loss.backward() optimizer.step() optimizer.zero_grad() ",
  "wordCount" : "785",
  "inLanguage": "en",
  "datePublished": "2022-08-05T00:00:00Z",
  "dateModified": "2022-08-05T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yugajmera.github.io/posts/backprop/post/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "YA Logs",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yugajmera.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yugajmera.github.io/" accesskey="h" title="YA Logs (Alt + H)">YA Logs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yugajmera.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Implementing Backpropagation
    </h1>
    <div class="post-meta"><span title='2022-08-05 00:00:00 +0000 UTC'>August 5, 2022</span>&nbsp;·&nbsp;4 min

</div>
  </header> 

  <div class="post-content"><p><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg_vLHE_ZzSybRTFgB_xIlOs7gBIba6WBm5oUDK61slCO_qtS_FaNsBVDpckABfzPLmDHStb5xNcLVkjlmS7H3Pn6HJDZyhDISZUuspkfsjlRc9SZr_gKiOMQU5qnKrNqye6khwDwgLx1X8TuEqUYdF-3vjgjZpExIyeSn6dIMHrpOTpEbA_AchXaTLtg/s320/backprop.jpg" alt=""  />
</p>
<p>While we have talked about how optimization algorithms use the negative gradient of the loss function with respect to the weights to update the parameters of our model, we will now focus on how to actually compute these gradients on an arbitrary loss function. We use a backpropagation technique that creates a computation graph to perform a forward and backward pass on the model. In the forward pass, we compute outputs of each layer of our neural network sequentially. In the backward pass, we apply the chain rule for computing derivatives of each individual layer backward.</p>
<p>Consider the example of a computation graph shown below.</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9xPVRRIScu6CZ0_7UvebWoohp9Izh4MKNywErVz5Ynbm_OpZQih5BoxUyj4c_D9t3BX6jU5OOp_Q2m4vQc59ffE2YOLEAvVhBCwlwWg-7WqILuizDnRCU3kVNz-4FNhTGRImLAgXnnE8SdQE7MNCdNIzTZFOn9TP2bhV4UGpLzfZrvFQ-rD1LDAaYDw/s535/eg.PNG"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEh9xPVRRIScu6CZ0_7UvebWoohp9Izh4MKNywErVz5Ynbm_OpZQih5BoxUyj4c_D9t3BX6jU5OOp_Q2m4vQc59ffE2YOLEAvVhBCwlwWg-7WqILuizDnRCU3kVNz-4FNhTGRImLAgXnnE8SdQE7MNCdNIzTZFOn9TP2bhV4UGpLzfZrvFQ-rD1LDAaYDw/s320/eg.PNG" alt=""  />
</a></p>
<p>First we perform a forward pass where we have the value of inputs $x = -2, y = 5, z = -4$, \begin{align*} q &amp;= x + y = -2 + 5 = -3\\ f &amp; = z * q = -4 * -3 = -12 \end{align*} Now lets compute the backward pass, \begin{align*} &amp; \frac{df}{df} = 1, \hspace{5mm} \frac{df}{dq} = z = -4, \hspace{5mm} \frac{df}{dz} = q = -3 \\ &amp; \frac{df}{dx} = \frac{df}{dq} \frac{dq}{dx} = -4 * 1 = -4, \hspace{5mm} \frac{df}{dy} = \frac{df}{dq} \frac{dq}{dy} = -4 * 1 = -4 \\ \end{align*} This way, we have computed the derivative of the output with respect to the input. These gradients involved in chain rule have names, \begin{align*} \underbrace{\frac{df}{dy}}_{\text{downstream gradient}} = \underbrace{\frac{df}{dq}}_{\text{upstream gradient}} \hspace{2mm} \underbrace{\frac{dq}{dy}}_{\text{local gradient}} \end{align*}</p>
<p>With a much more complicated neural network model, we follow the same procedure but don&rsquo;t worry, you don&rsquo;t have to compute the chain rule manually; Pytorch comes to the rescue!</p>
<p>Consider this simple neural network model on which we will apply backpropagation. It an image classification task with each image of shape (32 X 32 X 3 = 3072) input dimensions. The model with two layers outputs scores for 10 classes as shown below. [Linear -&gt; ReLU -&gt; Linear]</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaHU7Euahuuq5iE5gBD6X5OPtyQIK8KdW1uY3PTj3NrV09Hs9a27vK5QlCRtX0WLc_hZYBN_sjX1DiAg6n5EBfMkOpUO5DpM2kHSBlzJEsCE9GToVM5O5lMSuE0L6O0-l82BdsV4vDemYeZYKV5q2V79-t5wyAllOkSgsaj9lWyThX7_jocBDSO9SJMQ/s741/NN.PNG"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiaHU7Euahuuq5iE5gBD6X5OPtyQIK8KdW1uY3PTj3NrV09Hs9a27vK5QlCRtX0WLc_hZYBN_sjX1DiAg6n5EBfMkOpUO5DpM2kHSBlzJEsCE9GToVM5O5lMSuE0L6O0-l82BdsV4vDemYeZYKV5q2V79-t5wyAllOkSgsaj9lWyThX7_jocBDSO9SJMQ/w400-h185/NN.PNG" alt=""  />
</a></p>
<p>Pytorch uses the autograd package to build computational graphs out of Tensors and automatically compute gradients. Initializing weights as tensors with <code>requires_grad = True</code> flag enables autograd. Once we compute the loss, the gradient of this loss with respect to the weights can automatically be computed by running <code>loss.backward()</code>. This function implements backpropagation on all the inputs that have require grad set and accumulates the gradients in <code>.grad()</code> variable.</p>
<p>The <code>torch.no_grad()</code> function makes sure a computation graph is not created on the operations within it. It is crucial to run <code>.zero_grad()</code> function that zeroes out the current gradients to accumulate fresh gradients in the next step (as it does not overwrite automatically). A simple mini-batch gradient descent algorithm looks as follows,</p>
<pre><code>w1 = torch.randn(3072, 100, requires_grad=True)
w2 = torch.randn(100, 10, requires_grad=True)

mini_batches = split(data, batch_size)        # Split data into mini-batches

for t in range(num_steps):
  for X_batch, y_batch in mini_batches:
      y_pred = X_batch.mm(w1).clamp(min=0).mm(w2)  # Forward Pass
      loss = loss_fn(y_pred, y_batch)
      loss.backward()          # Compute gradients wrt loss
      
      with torch.no_grad():    # Tells Pytorch not to build a graph
           w1 -= learning_rate * w1.grad()
           w2 -= learning_rate* w2.grad()
      w1.zero_grad()      # Set gradients to zero
      w2.zero_grad()
</code></pre>
<p><code>X_batch</code> is the input vector (mini-batch) to our model and <code>y_batch</code> is the corresponding label vector. <code>y_pred</code> is the scores vector that our model outputs. The loss function returns the average loss over a mini-batch which is then used to compute the gradients. Pytorch provides the <code>Dataloader</code> primitive to create mini-batches.</p>
<p>It was easy to define the model, initialize weights, and write the forward pass. But what if we had a neural network with 50 layers? Explicitly writing code for all this would be very cumbersome. Pytorch provides the <code>nn.Sequential</code> API to define models easily. The weights have required grad set True implicitly and are initialized using the Kaiming initialization we discussed in the last post.</p>
<pre><code>model = torch.nn.Sequential(torch.nn.Linear(3072, 100),
 torch.nn.ReLU(),
        torch.nn.Linear(100, 10))
        
mini_batches = torch.utils.data.DataLoader(data, batch_size=batch_size)

for t in range(num_steps):
  for X_batch, y_batch in mini_batches:
      y_pred = model(X_batch)  # Forward Pass
      loss = loss_fn(y_pred, y_batch)
      loss.backward()          # Compute gradients wrt loss
      
    with torch.no_grad(): # Tells Pytorch not to build a graph
      for param in model.parameters():
           param -= learning_rate * param.grad()
    model.zero_grad()          # Set gradients to zero
</code></pre>
<p>It&rsquo;s annoying to write the weight update code all the time. Pytorch provides the <code>optim</code> package (makes life easier once again!) for implementing the standard gradient descent rules that we have discussed. In our training loop, after computing gradients, we use this optimizer to automatically make the gradient step for us (one step of update) and zero out the gradients. This is what a typical training block looks like in practice,</p>
<pre><code>model = torch.nn.Sequential(torch.nn.Linear(3072, 100),
 torch.nn.ReLU(),
        torch.nn.Linear(100, 10))
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
mini_batches = torch.utils.data.DataLoader(data, batch_size=batch_size)
for t in range(num_steps):
  for X_batch, y_batch in mini_batches:
      y_pred = model(X_batch)            
      loss = loss_fn(y_pred, y_batch)
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()</code></pre>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://yugajmera.github.io/posts/learning-rates/post/">
    <span class="title">« Prev</span>
    <br>
    <span>Using Learning Rate Schedules for Training</span>
  </a>
  <a class="next" href="https://yugajmera.github.io/posts/neural-networks/post/">
    <span class="title">Next »</span>
    <br>
    <span>Neural Networks: Activation functions and Weight Initialization</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://yugajmera.github.io/">YA Logs</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
