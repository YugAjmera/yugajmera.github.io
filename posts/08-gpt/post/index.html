<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Introduction to LLMs: Coding GPT from scratch | YA&#39;s Almanac</title>
<meta name="keywords" content="">
<meta name="description" content="By now, you&rsquo;ve probably used OpenAI&rsquo;s ChatGPT—a chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.
ChatGPT interface
The unprecedented success of ChatGPT fueled further research into the technology behind it—Large Language Models (LLMs).">
<meta name="author" content="">
<link rel="canonical" href="https://yugajmera.github.io/posts/08-gpt/post/">
<link crossorigin="anonymous" href="https://yugajmera.github.io/assets/css/stylesheet.74991b51f7611c2303d0b5649703d675530589621885eb78236e099c22aa93a5.css" integrity="sha256-dJkbUfdhHCMD0LVklwPWdVMFiWIYhet4I24JnCKqk6U=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://yugajmera.github.io/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yugajmera.github.io/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yugajmera.github.io/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yugajmera.github.io/assets/apple-touch-icon.png">
<link rel="mask-icon" href="https://yugajmera.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://yugajmera.github.io/posts/08-gpt/post/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
<style>
   mjx-container[display="true"] {
       margin: 1.5em 0 ! important
   }
</style>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-S759YBMKJE', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Introduction to LLMs: Coding GPT from scratch" />
<meta property="og:description" content="By now, you&rsquo;ve probably used OpenAI&rsquo;s ChatGPT—a chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.
ChatGPT interface
The unprecedented success of ChatGPT fueled further research into the technology behind it—Large Language Models (LLMs)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yugajmera.github.io/posts/08-gpt/post/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-01-27T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-01-27T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Introduction to LLMs: Coding GPT from scratch"/>
<meta name="twitter:description" content="By now, you&rsquo;ve probably used OpenAI&rsquo;s ChatGPT—a chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.
ChatGPT interface
The unprecedented success of ChatGPT fueled further research into the technology behind it—Large Language Models (LLMs)."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yugajmera.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Introduction to LLMs: Coding GPT from scratch",
      "item": "https://yugajmera.github.io/posts/08-gpt/post/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Introduction to LLMs: Coding GPT from scratch",
  "name": "Introduction to LLMs: Coding GPT from scratch",
  "description": "By now, you\u0026rsquo;ve probably used OpenAI\u0026rsquo;s ChatGPT—a chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.\nChatGPT interface\nThe unprecedented success of ChatGPT fueled further research into the technology behind it—Large Language Models (LLMs).",
  "keywords": [
    
  ],
  "articleBody": "By now, you’ve probably used OpenAI’s ChatGPT—a chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.\nChatGPT interface\nThe unprecedented success of ChatGPT fueled further research into the technology behind it—Large Language Models (LLMs). Over the past two posts, we’ve built the theoretical foundation, and now it’s time to get hands-on: coding a GPT-like LLM from the ground up.\nLarge Language Models (LLMs) An LLM is a neural network designed to understand, generate, and interpret human language. As the name suggests, LLMs are simply language models with an extremely large number of parameters (billions or even trillions) trained on vast datasets—potentially encompassing all publicly available internet text.\nWhile language modeling has been an area of research for decades (with RNNs, LSTMs, and Attention mechanisms), the introduction of Transformers in 2017 revolutionized the field.\nEvolution of language models\nWith Transformers proving their effectiveness, the next logical step was scaling. This movement gained traction with Google’s BERT and OpenAI’s GPT models in 2018. These two architectures define the major types of LLMs we see today.\nTypes of LLMs Recalling the architecture of Transformers from the last post, it consists of two components: the encoder and the decoder. It was proposed as a replacement for the encoder-decoder structure based on RNN + Attention for Seq2Seq learning, where the encoder reads the input text and the decoder produces predictions for the task.\nThe Transformer Architecture.\nAlthough the encoder-decoder architecture may seem natural for machine translation tasks (for which it was originally developed), it is not always necessary.\nRepresentation Models: Encoder-Only Models First school of thought: “Is a Decoder Necessary if I Only Want to Perform Data-to-Numbers Conversion?”\nFor example, in text classification tasks like sentiment analysis and spam detection or regression tasks like stock price prediction, where the goal is simply to convert data into categories or numerical values, the decoder can be omitted.\nA good example of this approach is Bidirectional Encoder Representations from Transformers (BERT)[1], an encoder-only Transformer model that uses bidirectional processing to understand the context and relationships of the input data.\nThe input is concatenated with a special token [CLS] (stands for classification) at the beginning of a sentence. Using the self-attention mechanism, this token aggregates information from all words, capturing the overall meaning of a sentence. The hidden state representation corresponding to it is then fed into an output layer for classification.\nBERT for classification and regression tasks.\nGenerative Models: Decoder-Only Models Second school of thought: “Is an Encoder Necessary for Language Generation?”\nTasks like translation, summarization, and Q\u0026A involve transforming an input sequence into an output sequence. Traditionally, this process has been handled using a model composed of an encoder for understanding the input and a decoder for generating the output.\nHowever, this process can be reframed—what if the input and output are treated as one continuous sequence? For example, a machine translation task—“we are eating bread” in English to “estamos comiendo pan” in Spanish—can be reframed as a language generation task: “Translate English to Spanish: we are eating bread.”\nA language generation or text completion task is an autoregressive problem, which can be handled using a decoder-only model. One such example is the Generative Pre-trained Transformer (GPT)[2].\nGPT-1 Architecture.\nSince the decoder contains a “masked” self-attention layer, it attends only to previously generated tokens or the available context, allowing it to generate coherent sequences that follow the context.\nChronologically, GPT was first introduced in early 2018, followed by BERT later that year. Both architectures were implemented with distinct purposes but achieved great success due to their similar training approach.\nTraining LLMs LLMs are trained in two stages using a semi-supervised learning procedure that combines unsupervised pre-training and supervised fine-tuning.\nTraining stages of an LLM.\n1. Unsupervised Pre-training In this first stage, an LLM is trained on vast amounts of raw, unlabeled data available on the internet. This allows the model to acquire broad world knowledge and develop an understanding of language semantics, including grammar and sentence structures.\nThis task-agnostic model is often referred to as a base model or a foundation model.\nBERT pre-training uses the masked language model (MLM) objective, where random tokens in the input are masked, and the model is trained to predict them based on the surrounding context. This unique training strategy (masked word prediction) makes such models well-suited for text classification tasks.\nFor generative models like GPT, the language modeling objective is used—given a sequence of tokens, the model predicts the next token in the sequence, a simple next-word prediction task. This approach helps the model learn how words and phrases fit together naturally, making it capable of generating coherent and contextually relevant text based on the given context.\nSince these objectives do not require labeled data, they enable training on massive, unlabeled text datasets. To capture a wide range of knowledge, the training data must be as diverse as possible.\nKey points:\nRequires large datasets—pre-training involves downloading and processing vast internet text data. High computational cost and time-intensive (very expensive). Compresses internet knowledge and models language semantics. A trained GPT model functions as a text completer. 2. Supervised Fine-Tuning (SFT) The second stage involves adapting the pre-trained model to specific tasks using labeled data. Since task-specific datasets require manual annotation, they are significantly smaller compared to the massive datasets used in pre-training. This semi-supervised approach enables LLMs to adapt effectively to new tasks with relatively small amounts of labeled data.\nFine-tuning can be categorized into two types:\nClassification Fine-Tuning: Labeled data consists of text samples and associated class labels (e.g., emails labeled as “spam” or “not spam”).\nInstruction Fine-Tuning: Labeled data consists of instruction-response pairs (e.g., “Translate this sentence into Spanish: we are eating bread” → “estamos comiendo pan”).\nKey points:\nComputationally cheaper than pre-training Requires less data (fine-tuned on a narrower, manually labeled dataset). Tailors the LLM to a specific task or domain. Open LLMs Organizations developing open LLMs often share model weights and architectures with the public. Examples include Cohere’s Command R, Mistral models, Microsoft’s Phi, and Meta’s Llama models.\nThese organizations typically release two types of models:\nBase Models: Unrefined models primarily used for fine-tuning, saving the cost of pre-training. These models function as text generators, producing content based on internet-derived knowledge.\nAssistant Models: Pre-fine-tuned models optimized for specific tasks like Q\u0026A, conversations, and assistance. They are often labeled as “Instruct” or “SFT” (Supervised Fine-Tuned) models.\nCoding GPT-1 from scratch This section demonstrates how to pre-train a small-scale GPT-1 model for educational purposes. Large-scale pre-training requires significant computational resources (GPT-3 pretraining cost is estimated at $4.6 million), so the community typically uses pre-trained base models.\nWe’ll train a character-level GPT-1 model using the Tiny Shakespeare dataset, containing 40,000 lines from Shakespeare’s plays. The model will generate text character by character in an unsupervised setting, learning through next-character prediction.\nIn order to use text as input to our LLM, we first split it into individual characters, convert each character into integer tokens, and then transform these tokens into embedding vectors. These embeddings serve as numerical representations of the text, making it suitable for neural network processing.\nData pre-processing First, we import necessary libraries and select the appropriate device.\n# Import functions import torch import torch.nn as nn import torch.nn.functional as F import matplotlib.pyplot as plt # for making figures %matplotlib inline device = 'cuda' if torch.cuda.is_available() else 'cpu' print(\"Using\", device) Next, we download and inspect the dataset. The dataset contains 1.1 million characters.\n# Download the tiny shakespeare dataset !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt with open('input.txt', 'r', encoding='utf-8') as f: text = f.read() print(\"Length of the dataset:\", len(text)) print(text[:100]) Length of the dataset: 1115394 First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: Yo Tokenization Tokenization is the process of breaking text into smaller units called tokens. These tokens can be individual words, characters, or special symbols. For example:\nInput: \"Hello, world. Is this a test?\" After tokenization: ['Hello', ',', 'world', '.', 'Is', 'this', 'a', 'test', '?'] For our character-level model, each character itself will be treated as a token, meaning no further splitting is required. We first create a vocabulary, which is the set of all unique characters in our dataset. This defines all possible tokens our model can process as input and generate as output.\n# Create vocabulary all_chars = sorted(list(set(text))) print(\"All characters that occur in the dataset:\", ''.join(all_chars)) vocab_size = len(all_chars) print(\"Vocab size (number of unique characters in the dataset):\", vocab_size) All characters that occur in the dataset: !$\u0026',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz Vocab size (number of unique characters in the dataset): 65 We do not convert all text to lowercase because capitalization helps the model distinguish between proper and common nouns, understand sentence structure, and generate correctly capitalized text.\nNext, we map each character in our vocabulary to an integer token ID (ranging from $0$ to $64$). This mapping allows us to later convert token IDs into embedding vectors.\n# Tokenize each character - convert the chars to ints char_to_int = {all_chars[i]:i for i in range(len(all_chars))} int_to_char = {i:all_chars[i] for i in range(len(all_chars))} encode = lambda s: [char_to_int[i] for i in s] decode = lambda s: [int_to_char[i] for i in s] # Tokenize entire dataset data = torch.tensor(encode(text)) print(\"Tokenized first 10 characters:\") print(data[:10].tolist()) Tokenized first 10 characters: [18, 47, 56, 57, 58, 1, 15, 47, 58, 47] The encode function maps characters to token IDs, while decode reverses the process to reconstruct text.\nIn summary,\nTokenization breaks down the input text (training data) into individual tokens. We build a vocabulary out of all unique tokens. Each token is mapped to a unique interger ID. Create batches The next step is to generate input-target pairs for training our language model.\nSince training on the entire dataset at once is computationally expensive, we sample small chunks of the dataset (called block data) and train on them instead. The maximum size of these chunks is fixed for each task and is referred to as the block size or context length.\nThe context length is a vital parameter in LLMs, as it determines the maximum number of tokens the model can process in a single pass. A larger context window allows the model to capture longer dependencies and even process entire documents, but it also comes with increased computational cost.\nLet’s take an example to understand this better. It is intuitive to assume that $\\text{x}$ represents the input tokens, while $\\text{y}$ contains the target tokens, which are simply the inputs shifted by one position. This setup aligns with the next-token prediction task that our model will be trained on.\nblock_size = 8 # what is the maximum context length for predictions? block_data = data[:block_size+1] x = data[:block_size] y = data[1:block_size+1] print(\"Training dataset chunck:\", block_data.tolist()) print(\"x:\", x.tolist()) print(\"y:\", y.tolist()) # How training occurs context = [] for i in range(block_size): context.append(x[i].item()) target = y[i] print(f\"Context: {context}, Target: {target}\") Training dataset chunck: [18, 47, 56, 57, 58, 1, 15, 47, 58] x: [18, 47, 56, 57, 58, 1, 15, 47] y: [47, 56, 57, 58, 1, 15, 47, 58] Context: [18], Target: 47 Context: [18, 47], Target: 56 Context: [18, 47, 56], Target: 57 Context: [18, 47, 56, 57], Target: 58 Context: [18, 47, 56, 57, 58], Target: 1 Context: [18, 47, 56, 57, 58, 1], Target: 15 Context: [18, 47, 56, 57, 58, 1, 15], Target: 47 Context: [18, 47, 56, 57, 58, 1, 15, 47], Target: 58 Here, the model iteratively builds context from previous tokens. At each step, it predicts the next token based on what it has seen so far.\nThis sliding context window ensures that:\nThe model is trained on varied-length inputs. It generalizes well to different sequence lengths. Now that we have understood the concept of block size, let’s create batches.\n# Build the data loader (train/val split) n = int(0.9*len(data)) train_data = data[:n] val_data = data[n:] def make_batch(split, block_size, batch_size): data = train_data if split == \"train\" else val_data idx = torch.randint(len(data) - block_size, (batch_size,)) x, y = [], [] for i in idx: x.append(data[i:block_size+i]) y.append(data[i+1:block_size+i+1]) return torch.stack(x), torch.stack(y) In practice, input text can be longer than the model’s supported context length. In such cases, we truncate the text, keeping only the most recent tokens up to the maximum length.\nModel Architecture Let’s break down the key components of the GPT-1 architecture and implement a bare-bones version of the model:\nToken Embeddings: Converts integer token IDs from the vocabulary into a $\\text{D}$-dimensional embedding vector using a simple lookup table. In our case, each character token in the input sequence is represented as a vector.\nPositional Encoding: Since transformers do not inherently understand the order of tokens, we add a $\\text{D}$-dimensional positional encoding to each token. GPT models use a learnable lookup table for this purpose.\nInput vectors: The final input vector is obtained by summing the token embeddings and positional encodings. This results in an input tensor of shape $\\text{N} \\times \\text{D}$, where $\\text{N}$ is the number of tokens (up to the context length) and $\\text{D}$ is the embedding dimension.\nModel Architecture (Decoder-Only Transformer):\nThe model consists of $\\text{L}$ transformer blocks, each with: Masked Multi-Head Self-Attention → Layer Norm → MLP → Layer Norm A fully connected layer (language modeling head) that projects the $\\text{D}$-dimensional output back into the vocabulary space. A softmax layer that converts these logits into probabilities for the next-token prediction. GPT-1 model\nMasked Multi-head Self-attention layer This follows the same mechanism as in the previous post, so I won’t go into detail here.\nclass MultiHeadAttention(nn.Module): def __init__(self, d_in, d_out, context_length, attn_dropout, num_heads, qkv_bias=False): super().__init__() self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_o = nn.Linear(d_out, d_out) self.dropout = nn.Dropout(attn_dropout) self.d_h = d_out // num_heads # head dimension self.num_heads = num_heads self.register_buffer(\"masked\", torch.tril(torch.ones(context_length, context_length)).view(1, 1, context_length, context_length)) def forward(self, x): # Input vectors x - (B, N, d_in) B, N, d_in = x.shape # Obtain keys, values and queries - (B, N, d_out) keys = self.W_k(x) values = self.W_v(x) queries = self.W_q(x) # Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h) k = keys.view(B, N, self.num_heads, self.d_h).transpose(1, 2) v = values.view(B, N, self.num_heads, self.d_h).transpose(1, 2) q = queries.view(B, N, self.num_heads, self.d_h).transpose(1, 2) # Apply scaled dot-product attention with causal mask on each head attn_scores = (q @ k.transpose(2, 3)) * k.shape[-1]**-0.5 attn_scores = attn_scores.masked_fill(self.masked[:, :, :N, :N] == 0, float('-inf')) attn_weights = torch.softmax(attn_scores, dim=-1) attn_weights = self.dropout(attn_weights) context_vec = attn_weights @ v # Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out) out = context_vec.transpose(1, 2).contiguous().view(B, N, -1) out = self.W_o(out) return out Layer Normalization As you might recall, layer normalization improves the stability and efficiency of training. The input to this layer, $\\text{x}$, has a shape of $\\text{B} \\times \\text{N} \\times \\text{D}$ = [batch size, number of tokens, embedding dimension], and normalization is performed across the embedding dimension.\nclass LayerNorm(nn.Module): def __init__(self, emb_dim): super().__init__() self.eps = 1e-5 self.scale = nn.Parameter(torch.ones(emb_dim)) self.shift = nn.Parameter(torch.zeros(emb_dim)) def forward(self, x): mean = x.mean(dim=-1, keepdim=True) var = x.var(dim=-1, keepdim=True, unbiased=False) # Used biased variance estimation (without Bessel's correction) norm_x = (x - mean) / torch.sqrt(var + self.eps) return self.scale * norm_x + self.shift The variable eps is a small constant added to the variance to prevent division by zero. scale and shift are two trainable parameters that allow the model to learn optimal transformations. We set unbiased=False, which means we divide by $n$ instead of $n-1$, resulting in a slightly biased variance estimate (i.e., not using Bessel’s correction). However, this has negligible impact when the embedding dimension is large. Feed forward layer Next, we will implement a small neural network used as a part of the transformer block in LLMs.\nHistorically, the ReLU activation function has been widely used due to its simplicity and effectiveness. However, in modern LLMs, the Gaussian Error Linear Unit (GELU) is preferred for its improved performance.\nGELU vs ReLU activation functions\nGELU can be thought of as a smoother version of ReLU. Its smooth transitions allow for better optimization properties during training, leading to more nuanced parameter adjustments.\nclass GELU(nn.Module): def __init__(self): super().__init__() def forward(self, x): return 0.5 * x * (1 + torch.tanh( torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)) )) class MLP(nn.Module): def __init__(self, emb_dim): super().__init__() self.layer1 = nn.Linear(emb_dim, 4 * emb_dim) self.layer2 = nn.Linear(4 * emb_dim, emb_dim) self.gelu = GELU() def forward(self, x): x = self.gelu(self.layer1(x)) x = self.layer2(x) return x Decoder block Now, we assemble all the components we’ve built so far into a Transformer decoder block. This block forms the foundation of GPT models and is repeated multiple times throughout the architecture.\nclass DecoderBlock(nn.Module): def __init__(self, d_model, context_length, dropout, num_heads): super().__init__() self.masked_multi_head_sa = MultiHeadAttention(d_in=d_model, d_out=d_model, context_length=context_length, att_dropout=dropout, num_heads=num_heads) self.ln1 = LayerNorm(d_model) self.ff = MLP(emb_dim=d_model) self.ln2 = LayerNorm(d_model) self.residual_dropout = nn.Dropout(dropout) def forward(self, x): attn_output = self.residual_dropout(self.masked_multi_head_sa(x)) x = self.ln1(x + attn_output) ff_output = self.residual_dropout(self.ff(x)) x = self.ln2(x + ff_output) return x GPT-1 model With the decoder block implemented, we now have all the necessary components to build the GPT-1 architecture.\nclass GPT1(nn.Module): def __init__(self, D_embd, vocab_size, context_length, num_blocks, num_heads, dropout): super().__init__() # Token Embeddings - Convert integer word tokens (vocab) to a D-dimensional embedding vector self.token_embedding_table = nn.Embedding(vocab_size, D_embd) # Position Encoding - Encode each position to a D-dimensional embedding vector self.position_embedding_table = nn.Embedding(context_length, D_embd) # Embedding dropout self.embd_dropout = nn.Dropout(dropout) # Define multiple Decoder blocks self.blocks = nn.Sequential(*[DecoderBlock(d_model=D_embd, context_length=context_length, dropout=dropout, num_heads=num_heads) for _ in range(num_blocks)]) # Final FC layer to project the D-dimensional vector back to vocab space self.lm_head = nn.Linear(D_embd, vocab_size, bias=False) def forward(self, x): B, N = x.size() token_emb = self.token_embedding_table(x) # output: (B, N, D) pos_emb = self.position_embedding_table(torch.arange(N, device=device)) # output: (N, D) x = token_emb + pos_emb # ouput: (B, N, D) x = self.embd_dropout(x) x = self.blocks(x) logits = self.lm_head(x) # ouput: (B, N, vocab_size) return logits Tokenized text is first converted into token embeddings, which are then augmented with positional embeddings. This combined representation forms a tensor that passes through a series of transformer blocks, outputting a tensor of the same dimensionality. The final language modeling head (a linear layer without bias) projects this output into the vocabulary space, generating logits for each token in the vocabulary.\nPre-Training Now, let’s initialize a small GPT model and perform a forward pass as a sanity check.\n# Define hyperparameters block_size = 32 batch_size = 16 model = GPT1(D_embd=64, vocab_size=vocab_size, context_length=block_size, num_heads=4, num_blocks=4, dropout=0.1).to(device) # print the number of parameters in the model print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters') # Forward pass with one example xb, yb = make_batch(\"train\", batch_size=batch_size, block_size=block_size) print(\"Input shape:\", xb.size()) print(\"Target shape:\", yb.size()) context, target = xb.to(device), yb.to(device) output = model(context) print(\"Model output shape:\", output.size()) loss = F.cross_entropy(output.view(-1, vocab_size), target.view(-1)) print(\"Initial Loss:\", loss.item()) 0.209601 M parameters Input shape: torch.Size([16, 32]) Target shape: torch.Size([16, 32]) Model output shape: torch.Size([16, 32, 65]) Initial Loss: 4.331784248352051 As you can see, the model outputs a tensor of shape [16, 32, 65], since we passed 16 input texts with 32 tokens each. The last dimension corresponds to the vocabulary size of the tokenizer. To compute the loss, we collapse the first two dimensions, concatenating all tokens together and averaging the loss over the batch.\nWith that, we’re ready to pre-train the model on our dataset. 🚀\nmax_iter = 5000 eval_iter = 200 # Define the loss function \u0026 optimizer criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) for iter in range(max_iter): model.train() # Sample a batch of data xb, yb = make_batch('train', batch_size=batch_size, block_size=block_size) context, target = xb.to(device), yb.to(device) # Forward pass and optimization output = model(context) loss = criterion(output.view(-1, vocab_size), target.view(-1)) loss.backward() optimizer.step() optimizer.zero_grad() # Evaluate the loss on train and val if iter%100 == 0 or iter==0 or iter==(max_iter-1): with torch.no_grad(): model.eval() out = {} for split in ['train', 'val']: running_loss = 0 for k in range(eval_iter): xb, yb = make_batch(split, batch_size=batch_size, block_size=block_size) context, target = xb.to(device), yb.to(device) output = model(context) loss = criterion(output.view(-1, vocab_size), target.view(-1)) running_loss += loss.item() out[split] = running_loss / eval_iter print('\\n Step:{}/{}, Train Loss:{:.4f}, Val Loss:{:.4f}'.format(iter, max_iter, out['train'], out['val'])) ... Step:4999/5000, Train Loss:1.7101, Val Loss:1.8699 Generating text The inference process involves generating new text based on patterns the model has learned from the training data. We start with an initial context, typically the integer token 0, which represents the newline character in our vocabulary. This serves as the seed input to the model.\nThe model then outputs a probability distribution over all tokens in the vocabulary. We sample from this distribution to generate the next token. This newly generated token is appended to the existing context and fed back into the model to produce the next prediction. By iterating this process, the model generates coherent text that follows the structure and style of the training data—such as Shakespearean plays in the case of our Tiny Shakespeare dataset.\n# Generate text from the trained model context = torch.zeros((1, 1), dtype=torch.long, device=device) # Shape (B=1, N=1) max_new_tokens = 300 stored_context = context for _ in range(max_new_tokens): model.eval() with torch.no_grad(): output = model(context) # output: (B=1, N, vocab_size) output = output[:, -1, :] # take just the last time step (B, vocab_size) probs = torch.softmax(output, dim=-1) next_token = torch.multinomial(probs, num_samples=1) # Sample from the distribution (B=1, N) stored_context = torch.cat((stored_context, next_token), dim=1) # (B=1, N+1) context = stored_context[:, -block_size:] # Trim to lastest context print(''.join(decode(stored_context[0].tolist()))) PETES: For only royal is was the head Yourd Murden, the fay honour bend blood of the lord Here their enamen the all rant To to shall thy sunloldent will well with gene: All be dears in sraft'd were hine of thank cave so vinct there it, If your sry donance. I will your Bade my kills him, He firein, my, thou couldona wings; Ur to pear Than prition won out it. Since we sample at every step, text generation is stochastic, meaning that even with the same initial context, we may obtain different outputs. This ensures that the generated text is diverse rather than simply memorizing the training data.\nOur Tiny Shakespeare-trained model essentially acts as a text completer, generating text in the same style as Shakespeare when given a prompt. Similarly, a base LLM (large language model) functions as an autocompleter, predicting the next most probable tokens based on internet-scale training data.\nSummary of GPT-1 paper Here are some key implementation details of the GPT-1 model.\nTokenizer: Byte Pair Encoding (BPE) with a vocabulary size of 40,000 merges.\nContext length: $N = 512$ tokens.\nPosition Embeddings: Learned instead of the sinusoidal version proposed in Transformers.\nArchitecture hyperparameters:\nNumber of layers (decoder blocks): $L = 12$ Number of attention heads: $H = 12$ Embedding dimension: $d_{model} = 768$ MLP size: $d_{ff} = 4 \\times d_{model} = 3072$ Residual, embedding and attention dropout: $p = 0.1$ Pre-Training hyperparameters:\nInitialization: Since LayerNorm is used extensively throughout the model, weights were initialized from a zero-mean Gaussian distribution with a standard deviation of $0.02$. Optimizer: AdamW Batch size: 64 Weight decay: 0.01 Learning rate: Increased linearly from zero to a maximum value of $2.5 \\times 10^{-4}$ over the first 2000 updates, then annealed to zero using a cosine schedule. Number of epochs: 100 Fine-tuning hyperparameters:\nDropout in the classifier: $p = 0.1$ Learning rate: $6.25e^{-5}$ with a linear decay schedule and warmup over 0.2% of training steps. Batch size: 32 Number of epochs: 3 (sufficient for most cases). Weight decay: 0.5 References Medium Post on Most Successful Transformer Variants: Introducing BERT and GPT\nAndrej Karpathy’s video: Let’s build GPT: from scratch, in code, spelled out.\n[1] Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”, NAACL 2019.\n[2] Radford et al., “Improving Language Understanding by Generative Pre-Training”, OpenAI, 2018.\n",
  "wordCount" : "3965",
  "inLanguage": "en",
  "datePublished": "2025-01-27T00:00:00Z",
  "dateModified": "2025-01-27T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yugajmera.github.io/posts/08-gpt/post/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "YA's Almanac",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yugajmera.github.io/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yugajmera.github.io/" accesskey="h" title="YA&#39;s Almanac (Alt + H)">YA&#39;s Almanac</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yugajmera.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Introduction to LLMs: Coding GPT from scratch
    </h1>
    <div class="post-meta"><span title='2025-01-27 00:00:00 +0000 UTC'>January 27, 2025</span>&nbsp;·&nbsp;19 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#large-language-models-llms" aria-label="Large Language Models (LLMs)">Large Language Models (LLMs)</a><ul>
                        
                <li>
                    <a href="#types-of-llms" aria-label="Types of LLMs">Types of LLMs</a><ul>
                        
                <li>
                    <a href="#representation-models-encoder-only-models" aria-label="Representation Models: Encoder-Only Models">Representation Models: Encoder-Only Models</a></li>
                <li>
                    <a href="#generative-models-decoder-only-models" aria-label="Generative Models: Decoder-Only Models">Generative Models: Decoder-Only Models</a></li></ul>
                </li>
                <li>
                    <a href="#training-llms" aria-label="Training LLMs">Training LLMs</a><ul>
                        
                <li>
                    <a href="#1-unsupervised-pre-training" aria-label="1. Unsupervised Pre-training">1. Unsupervised Pre-training</a></li>
                <li>
                    <a href="#2-supervised-fine-tuning-sft" aria-label="2. Supervised Fine-Tuning (SFT)">2. Supervised Fine-Tuning (SFT)</a></li></ul>
                </li>
                <li>
                    <a href="#open-llms" aria-label="Open LLMs">Open LLMs</a></li></ul>
                </li>
                <li>
                    <a href="#coding-gpt-1-from-scratch" aria-label="Coding GPT-1 from scratch">Coding GPT-1 from scratch</a><ul>
                        
                <li>
                    <a href="#data-pre-processing" aria-label="Data pre-processing">Data pre-processing</a></li>
                <li>
                    <a href="#tokenization" aria-label="Tokenization">Tokenization</a></li>
                <li>
                    <a href="#create-batches" aria-label="Create batches">Create batches</a></li>
                <li>
                    <a href="#model-architecture" aria-label="Model Architecture">Model Architecture</a><ul>
                        
                <li>
                    <a href="#masked-multi-head-self-attention-layer" aria-label="Masked Multi-head Self-attention layer">Masked Multi-head Self-attention layer</a></li>
                <li>
                    <a href="#layer-normalization" aria-label="Layer Normalization">Layer Normalization</a></li>
                <li>
                    <a href="#feed-forward-layer" aria-label="Feed forward layer">Feed forward layer</a></li>
                <li>
                    <a href="#decoder-block" aria-label="Decoder block">Decoder block</a></li>
                <li>
                    <a href="#gpt-1-model" aria-label="GPT-1 model">GPT-1 model</a></li></ul>
                </li>
                <li>
                    <a href="#pre-training" aria-label="Pre-Training">Pre-Training</a></li>
                <li>
                    <a href="#generating-text" aria-label="Generating text">Generating text</a></li></ul>
                </li>
                <li>
                    <a href="#summary-of-gpt-1-paper" aria-label="Summary of GPT-1 paper">Summary of GPT-1 paper</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>By now, you&rsquo;ve probably used OpenAI&rsquo;s ChatGPT—a chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.</p>
<figure class="align-center ">
    <img loading="lazy" src="../chatgpt.png#center"
         alt="ChatGPT interface" width="600"/> <figcaption>
            <p>ChatGPT interface</p>
        </figcaption>
</figure>

<p>The unprecedented success of ChatGPT fueled further research into the technology behind it—Large Language Models (LLMs). Over the past two posts, we’ve built the theoretical foundation, and now it&rsquo;s time to get hands-on: coding a GPT-like LLM from the ground up.</p>
<h2 id="large-language-models-llms">Large Language Models (LLMs)<a hidden class="anchor" aria-hidden="true" href="#large-language-models-llms">#</a></h2>
<p>An LLM is a neural network designed to understand, generate, and interpret human language. As the name suggests, LLMs are simply language models with an extremely large number of parameters (billions or even trillions) trained on vast datasets—potentially encompassing all publicly available internet text.</p>
<p>While language modeling has been an area of research for decades (with RNNs, LSTMs, and Attention mechanisms), the introduction of Transformers in 2017 revolutionized the field.</p>
<figure class="align-center ">
    <img loading="lazy" src="../llm-roadmap.jpg#center"
         alt="Evolution of language models" width="550"/> <figcaption>
            <p>Evolution of language models</p>
        </figcaption>
</figure>

<p>With Transformers proving their effectiveness, the next logical step was scaling. This movement gained traction with Google&rsquo;s BERT and OpenAI’s GPT models in 2018. These two architectures define the major types of LLMs we see today.</p>
<h3 id="types-of-llms">Types of LLMs<a hidden class="anchor" aria-hidden="true" href="#types-of-llms">#</a></h3>
<p>Recalling the architecture of Transformers from the last post, it consists of two components: the encoder and the decoder. It was proposed as a replacement for the encoder-decoder structure based on RNN + Attention for Seq2Seq learning, where the encoder reads the input text and the decoder produces predictions for the task.</p>
<figure class="align-center ">
    <img loading="lazy" src="../transformer-full.png#center"
         alt="The Transformer Architecture." width="400"/> <figcaption>
            <p>The Transformer Architecture.</p>
        </figcaption>
</figure>

<p>Although the encoder-decoder architecture may seem natural for machine translation tasks (for which it was originally developed), it is not always necessary.</p>
<h4 id="representation-models-encoder-only-models">Representation Models: Encoder-Only Models<a hidden class="anchor" aria-hidden="true" href="#representation-models-encoder-only-models">#</a></h4>
<p>First school of thought: &ldquo;<em>Is a Decoder Necessary if I Only Want to Perform Data-to-Numbers Conversion?</em>&rdquo;</p>
<p>For example, in text classification tasks like sentiment analysis and spam detection or regression tasks like stock price prediction, where the goal is simply to convert data into categories or numerical values, the decoder can be omitted.</p>
<p>A good example of this approach is Bidirectional Encoder Representations from Transformers (BERT)<a href="#references">[1]</a>, an encoder-only Transformer model that uses bidirectional processing to understand the context and relationships of the input data.</p>
<p>The input is concatenated with a special token [CLS] (stands for classification) at the beginning of a sentence. Using the self-attention mechanism, this token aggregates information from all words, capturing the overall meaning of a sentence. The hidden state representation corresponding to it is then fed into an output layer for classification.</p>
<figure class="align-center ">
    <img loading="lazy" src="../bert-cls.png#center"
         alt="BERT for classification and regression tasks." width="600"/> <figcaption>
            <p>BERT for classification and regression tasks.</p>
        </figcaption>
</figure>

<h4 id="generative-models-decoder-only-models">Generative Models: Decoder-Only Models<a hidden class="anchor" aria-hidden="true" href="#generative-models-decoder-only-models">#</a></h4>
<p>Second school of thought: &ldquo;<em>Is an Encoder Necessary for Language Generation?</em>&rdquo;</p>
<p>Tasks like translation, summarization, and Q&amp;A involve transforming an input sequence into an output sequence. Traditionally, this process has been handled using a model composed of an encoder for understanding the input and a decoder for generating the output.</p>
<p>However, this process can be reframed—what if the input and output are treated as one continuous sequence? For example, a machine translation task—&ldquo;we are eating bread&rdquo; in English to &ldquo;estamos comiendo pan&rdquo; in Spanish—can be reframed as a language generation task: &ldquo;Translate English to Spanish: we are eating bread.&rdquo;</p>
<p>A language generation or text completion task is an autoregressive problem, which can be handled using a decoder-only model. One such example is the Generative Pre-trained Transformer (GPT)<a href="#references">[2]</a>.</p>
<figure class="align-center ">
    <img loading="lazy" src="../encoder-only.png#center"
         alt="GPT-1 Architecture." width="600"/> <figcaption>
            <p>GPT-1 Architecture.</p>
        </figcaption>
</figure>

<p>Since the decoder contains a &ldquo;masked&rdquo; self-attention layer, it attends only to previously generated tokens or the available context, allowing it to generate coherent sequences that follow the context.</p>
<p>Chronologically, GPT was first introduced in early 2018, followed by BERT later that year. Both architectures were implemented with distinct purposes but achieved great success due to their similar training approach.</p>
<h3 id="training-llms">Training LLMs<a hidden class="anchor" aria-hidden="true" href="#training-llms">#</a></h3>
<p>LLMs are trained in two stages using a semi-supervised learning procedure that combines unsupervised pre-training and supervised fine-tuning.</p>
<figure class="align-center ">
    <img loading="lazy" src="../training.png#center"
         alt="Training stages of an LLM."/> <figcaption>
            <p>Training stages of an LLM.</p>
        </figcaption>
</figure>

<h4 id="1-unsupervised-pre-training">1. Unsupervised Pre-training<a hidden class="anchor" aria-hidden="true" href="#1-unsupervised-pre-training">#</a></h4>
<p>In this first stage, an LLM is trained on vast amounts of raw, unlabeled data available on the internet. This allows the model to acquire broad world knowledge and develop an understanding of language semantics, including grammar and sentence structures.</p>
<p>This task-agnostic model is often referred to as a <em>base model</em> or a <em>foundation model</em>.</p>
<p>BERT pre-training uses the masked language model (MLM) objective, where random tokens in the input are masked, and the model is trained to predict them based on the surrounding context. This unique training strategy (masked word prediction) makes such models well-suited for text classification tasks.</p>
<p>For generative models like GPT, the language modeling objective is used—given a sequence of tokens, the model predicts the next token in the sequence, a simple next-word prediction task. This approach helps the model learn how words and phrases fit together naturally, making it capable of generating coherent and contextually relevant text based on the given context.</p>
<p>Since these objectives do not require labeled data, they enable training on massive, unlabeled text datasets. To capture a wide range of knowledge, the training data must be as diverse as possible.</p>
<p>Key points:</p>
<ul>
<li>Requires large datasets—pre-training involves downloading and processing vast internet text data.</li>
<li>High computational cost and time-intensive (very expensive).</li>
<li>Compresses internet knowledge and models language semantics.</li>
<li>A trained GPT model functions as a text completer.</li>
</ul>
<h4 id="2-supervised-fine-tuning-sft">2. <strong>Supervised Fine-Tuning (SFT)</strong><a hidden class="anchor" aria-hidden="true" href="#2-supervised-fine-tuning-sft">#</a></h4>
<p>The second stage involves adapting the pre-trained model to specific tasks using labeled data. Since task-specific datasets require manual annotation, they are significantly smaller compared to the massive datasets used in pre-training. This semi-supervised approach enables LLMs to adapt effectively to new tasks with relatively small amounts of labeled data.</p>
<p>Fine-tuning can be categorized into two types:</p>
<ol>
<li>
<p>Classification Fine-Tuning: Labeled data consists of text samples and associated class labels (e.g., emails labeled as &ldquo;spam&rdquo; or &ldquo;not spam&rdquo;).</p>
</li>
<li>
<p>Instruction Fine-Tuning: Labeled data consists of instruction-response pairs (e.g., &ldquo;Translate this sentence into Spanish: we are eating bread&rdquo; → &ldquo;estamos comiendo pan&rdquo;).</p>
</li>
</ol>
<p>Key points:</p>
<ul>
<li>Computationally cheaper than pre-training</li>
<li>Requires less data (fine-tuned on a narrower, manually labeled dataset).</li>
<li>Tailors the LLM to a specific task or domain.</li>
</ul>
<h3 id="open-llms">Open LLMs<a hidden class="anchor" aria-hidden="true" href="#open-llms">#</a></h3>
<p>Organizations developing open LLMs often share model weights and architectures with the public. Examples include Cohere’s Command R, Mistral models, Microsoft’s Phi, and Meta’s Llama models.</p>
<p>These organizations typically release two types of models:</p>
<ul>
<li>
<p><strong>Base Models</strong>: Unrefined models primarily used for fine-tuning, saving the cost of pre-training. These models function as text generators, producing content based on internet-derived knowledge.</p>
</li>
<li>
<p><strong>Assistant Models</strong>: Pre-fine-tuned models optimized for specific tasks like Q&amp;A, conversations, and assistance. They are often labeled as &ldquo;Instruct&rdquo; or &ldquo;SFT&rdquo; (Supervised Fine-Tuned) models.</p>
</li>
</ul>
<h2 id="coding-gpt-1-from-scratch">Coding GPT-1 from scratch<a hidden class="anchor" aria-hidden="true" href="#coding-gpt-1-from-scratch">#</a></h2>
<p>This section demonstrates how to pre-train a small-scale GPT-1 model for educational purposes. Large-scale pre-training requires significant computational resources (GPT-3 pretraining cost is estimated at $4.6 million), so the community typically uses pre-trained base models.</p>
<p>We&rsquo;ll train a character-level GPT-1 model using the Tiny Shakespeare dataset, containing 40,000 lines from Shakespeare&rsquo;s plays. The model will generate text character by character in an unsupervised setting, learning through next-character prediction.</p>
<p>In order to use text as input to our LLM, we first split it into individual characters, convert each character into integer tokens, and then transform these tokens into embedding vectors. These embeddings serve as numerical representations of the text, making it suitable for neural network processing.</p>
<h3 id="data-pre-processing">Data pre-processing<a hidden class="anchor" aria-hidden="true" href="#data-pre-processing">#</a></h3>
<p>First, we import necessary libraries and select the appropriate device.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Import functions</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt     <span style="color:#75715e"># for making figures</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">%</span>matplotlib inline
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;cuda&#39;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#39;cpu&#39;</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Using&#34;</span>, device)
</span></span></code></pre></div><p>Next, we download and inspect the dataset. The dataset contains 1.1 million characters.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Download the tiny shakespeare dataset</span>
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>wget https:<span style="color:#f92672">//</span>raw<span style="color:#f92672">.</span>githubusercontent<span style="color:#f92672">.</span>com<span style="color:#f92672">/</span>karpathy<span style="color:#f92672">/</span>char<span style="color:#f92672">-</span>rnn<span style="color:#f92672">/</span>master<span style="color:#f92672">/</span>data<span style="color:#f92672">/</span>tinyshakespeare<span style="color:#f92672">/</span>input<span style="color:#f92672">.</span>txt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;input.txt&#39;</span>, <span style="color:#e6db74">&#39;r&#39;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;utf-8&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    text <span style="color:#f92672">=</span> f<span style="color:#f92672">.</span>read()
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Length of the dataset:&#34;</span>, len(text))
</span></span><span style="display:flex;"><span>print(text[:<span style="color:#ae81ff">100</span>])
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span>Length of the dataset: <span style="color:#ae81ff">1115394</span>
</span></span><span style="display:flex;"><span>First Citizen:
</span></span><span style="display:flex;"><span>Before we proceed any further, hear me speak.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>All:
</span></span><span style="display:flex;"><span>Speak, speak.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>First Citizen:
</span></span><span style="display:flex;"><span>Yo
</span></span></code></pre></div><h3 id="tokenization">Tokenization<a hidden class="anchor" aria-hidden="true" href="#tokenization">#</a></h3>
<p>Tokenization is the process of breaking text into smaller units called tokens. These tokens can be individual words, characters, or special symbols. For example:</p>
<ul>
<li>Input: <code>&quot;Hello, world. Is this a test?&quot;</code></li>
<li>After tokenization: <code>['Hello', ',', 'world', '.', 'Is', 'this', 'a', 'test', '?']</code></li>
</ul>
<p>For our character-level model, each character itself will be treated as a token, meaning no further splitting is required. We first create a vocabulary, which is the set of all unique characters in our dataset. This defines all possible tokens our model can process as input and generate as output.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Create vocabulary</span>
</span></span><span style="display:flex;"><span>all_chars <span style="color:#f92672">=</span> sorted(list(set(text)))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;All characters that occur in the dataset:&#34;</span>, <span style="color:#e6db74">&#39;&#39;</span><span style="color:#f92672">.</span>join(all_chars))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vocab_size <span style="color:#f92672">=</span> len(all_chars)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Vocab size (number of unique characters in the dataset):&#34;</span>, vocab_size)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span>All characters that occur <span style="color:#66d9ef">in</span> the dataset: 
</span></span><span style="display:flex;"><span> !<span style="color:#960050;background-color:#1e0010">$</span>&amp;<span style="color:#960050;background-color:#1e0010">&#39;</span>,-.<span style="color:#ae81ff">3</span>:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
</span></span><span style="display:flex;"><span>Vocab size (number of unique characters <span style="color:#66d9ef">in</span> the dataset): <span style="color:#ae81ff">65</span>
</span></span></code></pre></div><p>We do not convert all text to lowercase because capitalization helps the model distinguish between proper and common nouns, understand sentence structure, and generate correctly capitalized text.</p>
<p>Next, we map each character in our vocabulary to an integer token ID (ranging from $0$ to $64$). This mapping allows us to later convert token IDs into embedding vectors.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Tokenize each character - convert the chars to ints</span>
</span></span><span style="display:flex;"><span>char_to_int <span style="color:#f92672">=</span> {all_chars[i]:i <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(all_chars))}
</span></span><span style="display:flex;"><span>int_to_char <span style="color:#f92672">=</span> {i:all_chars[i] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(all_chars))}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>encode <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> s: [char_to_int[i] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> s]
</span></span><span style="display:flex;"><span>decode <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> s: [int_to_char[i] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> s]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Tokenize entire dataset</span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(encode(text))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Tokenized first 10 characters:&#34;</span>)
</span></span><span style="display:flex;"><span>print(data[:<span style="color:#ae81ff">10</span>]<span style="color:#f92672">.</span>tolist())
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span>Tokenized first <span style="color:#ae81ff">10</span> characters:
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">[18, 47, 56, 57, 58, 1, 15, 47, 58, 47]</span>
</span></span></code></pre></div><p>The <code>encode</code> function maps characters to token IDs, while <code>decode</code> reverses the process to reconstruct text.</p>
<p>In summary,</p>
<ol>
<li>Tokenization breaks down the input text (training data) into individual tokens.</li>
<li>We build a vocabulary out of all unique tokens.</li>
<li>Each token is mapped to a unique interger ID.</li>
</ol>
<h3 id="create-batches">Create batches<a hidden class="anchor" aria-hidden="true" href="#create-batches">#</a></h3>
<p>The next step is to generate input-target pairs for training our language model.</p>
<p>Since training on the entire dataset at once is computationally expensive, we sample small chunks of the dataset (called block data) and train on them instead. The maximum size of these chunks is fixed for each task and is referred to as the block size or context length.</p>
<p>The context length is a vital parameter in LLMs, as it determines the maximum number of tokens the model can process in a single pass. A larger context window allows the model to capture longer dependencies and even process entire documents, but it also comes with increased computational cost.</p>
<p>Let&rsquo;s take an example to understand this better. It is intuitive to assume that $\text{x}$ represents the input tokens, while $\text{y}$ contains the target tokens, which are simply the inputs shifted by one position. This setup aligns with the next-token prediction task that our model will be trained on.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>block_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">8</span>              <span style="color:#75715e"># what is the maximum context length for predictions?</span>
</span></span><span style="display:flex;"><span>block_data <span style="color:#f92672">=</span> data[:block_size<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> data[:block_size]
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> data[<span style="color:#ae81ff">1</span>:block_size<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Training dataset chunck:&#34;</span>, block_data<span style="color:#f92672">.</span>tolist())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;x:&#34;</span>, x<span style="color:#f92672">.</span>tolist())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;y:&#34;</span>, y<span style="color:#f92672">.</span>tolist())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># How training occurs</span>
</span></span><span style="display:flex;"><span>context <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(block_size):
</span></span><span style="display:flex;"><span>    context<span style="color:#f92672">.</span>append(x[i]<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>    target <span style="color:#f92672">=</span> y[i]
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Context: </span><span style="color:#e6db74">{</span>context<span style="color:#e6db74">}</span><span style="color:#e6db74">, Target: </span><span style="color:#e6db74">{</span>target<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><pre tabindex="0"><code>Training dataset chunck: [18, 47, 56, 57, 58, 1, 15, 47, 58]
x: [18, 47, 56, 57, 58, 1, 15, 47]
y: [47, 56, 57, 58, 1, 15, 47, 58]
Context: [18], Target: 47
Context: [18, 47], Target: 56
Context: [18, 47, 56], Target: 57
Context: [18, 47, 56, 57], Target: 58
Context: [18, 47, 56, 57, 58], Target: 1
Context: [18, 47, 56, 57, 58, 1], Target: 15
Context: [18, 47, 56, 57, 58, 1, 15], Target: 47
Context: [18, 47, 56, 57, 58, 1, 15, 47], Target: 58
</code></pre><p>Here, the model iteratively builds context from previous tokens. At each step, it predicts the next token based on what it has seen so far.</p>
<p>This sliding context window ensures that:</p>
<ul>
<li>The model is trained on varied-length inputs.</li>
<li>It generalizes well to different sequence lengths.</li>
</ul>
<p>Now that we have understood the concept of block size, let&rsquo;s create batches.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Build the data loader (train/val split)</span>
</span></span><span style="display:flex;"><span>n <span style="color:#f92672">=</span> int(<span style="color:#ae81ff">0.9</span><span style="color:#f92672">*</span>len(data))
</span></span><span style="display:flex;"><span>train_data <span style="color:#f92672">=</span> data[:n]
</span></span><span style="display:flex;"><span>val_data <span style="color:#f92672">=</span> data[n:]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">make_batch</span>(split, block_size, batch_size):
</span></span><span style="display:flex;"><span>    data <span style="color:#f92672">=</span> train_data <span style="color:#66d9ef">if</span> split <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;train&#34;</span> <span style="color:#66d9ef">else</span> val_data
</span></span><span style="display:flex;"><span>    idx <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randint(len(data) <span style="color:#f92672">-</span> block_size, (batch_size,))
</span></span><span style="display:flex;"><span>    x, y <span style="color:#f92672">=</span> [], []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> idx:
</span></span><span style="display:flex;"><span>        x<span style="color:#f92672">.</span>append(data[i:block_size<span style="color:#f92672">+</span>i])
</span></span><span style="display:flex;"><span>        y<span style="color:#f92672">.</span>append(data[i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>:block_size<span style="color:#f92672">+</span>i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>stack(x), torch<span style="color:#f92672">.</span>stack(y)
</span></span></code></pre></div><p>In practice, input text can be longer than the model’s supported context length. In such cases, we truncate the text, keeping only the most recent tokens up to the maximum length.</p>
<h3 id="model-architecture">Model Architecture<a hidden class="anchor" aria-hidden="true" href="#model-architecture">#</a></h3>
<p>Let’s break down the key components of the GPT-1 architecture and implement a bare-bones version of the model:</p>
<ul>
<li>
<p><strong>Token Embeddings</strong>: Converts integer token IDs from the vocabulary into a $\text{D}$-dimensional embedding vector using a simple lookup table. In our case, each character token in the input sequence is represented as a vector.</p>
</li>
<li>
<p><strong>Positional Encoding</strong>: Since transformers do not inherently understand the order of tokens, we add a $\text{D}$-dimensional positional encoding to each token. GPT models use a learnable lookup table for this purpose.</p>
</li>
<li>
<p><strong>Input vectors</strong>: The final input vector is obtained by summing the token embeddings and positional encodings. This results in an input tensor of shape $\text{N} \times \text{D}$, where $\text{N}$ is the number of tokens (up to the context length) and $\text{D}$ is the embedding dimension.</p>
</li>
<li>
<p><strong>Model Architecture</strong> (Decoder-Only Transformer):</p>
<ul>
<li>The model consists of $\text{L}$ transformer blocks, each with:
<ul>
<li>Masked Multi-Head Self-Attention → Layer Norm → MLP → Layer Norm</li>
</ul>
</li>
<li>A fully connected layer (language modeling head) that projects the $\text{D}$-dimensional output back into the vocabulary space.</li>
<li>A softmax layer that converts these logits into probabilities for the next-token prediction.</li>
</ul>
</li>
</ul>
<figure class="align-center ">
    <img loading="lazy" src="../gpt1-architecture.png#center"
         alt="GPT-1 model" width="500"/> <figcaption>
            <p>GPT-1 model</p>
        </figcaption>
</figure>

<h4 id="masked-multi-head-self-attention-layer">Masked Multi-head Self-attention layer<a hidden class="anchor" aria-hidden="true" href="#masked-multi-head-self-attention-layer">#</a></h4>
<p>This follows the same mechanism as in the previous post, so I won’t go into detail here.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MultiHeadAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, d_in, d_out, context_length, attn_dropout, num_heads, qkv_bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_k <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_in, d_out, bias<span style="color:#f92672">=</span>qkv_bias)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_v <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_in, d_out, bias<span style="color:#f92672">=</span>qkv_bias)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_q <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_in, d_out, bias<span style="color:#f92672">=</span>qkv_bias)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W_o <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_out, d_out)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(attn_dropout)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>d_h <span style="color:#f92672">=</span> d_out <span style="color:#f92672">//</span> num_heads                    <span style="color:#75715e"># head dimension</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>num_heads <span style="color:#f92672">=</span> num_heads
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>register_buffer(<span style="color:#e6db74">&#34;masked&#34;</span>, torch<span style="color:#f92672">.</span>tril(torch<span style="color:#f92672">.</span>ones(context_length, context_length))<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, context_length, context_length))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Input vectors x - (B, N, d_in)</span>
</span></span><span style="display:flex;"><span>        B, N, d_in <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Obtain keys, values and queries - (B, N, d_out)</span>
</span></span><span style="display:flex;"><span>        keys <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_k(x)
</span></span><span style="display:flex;"><span>        values <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_v(x)
</span></span><span style="display:flex;"><span>        queries <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_q(x)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h)</span>
</span></span><span style="display:flex;"><span>        k <span style="color:#f92672">=</span> keys<span style="color:#f92672">.</span>view(B, N, self<span style="color:#f92672">.</span>num_heads, self<span style="color:#f92672">.</span>d_h)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> values<span style="color:#f92672">.</span>view(B, N, self<span style="color:#f92672">.</span>num_heads, self<span style="color:#f92672">.</span>d_h)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        q <span style="color:#f92672">=</span> queries<span style="color:#f92672">.</span>view(B, N, self<span style="color:#f92672">.</span>num_heads, self<span style="color:#f92672">.</span>d_h)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>) 
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Apply scaled dot-product attention with causal mask on each head</span>
</span></span><span style="display:flex;"><span>        attn_scores <span style="color:#f92672">=</span> (q <span style="color:#f92672">@</span> k<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>)) <span style="color:#f92672">*</span> k<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">**-</span><span style="color:#ae81ff">0.5</span>    
</span></span><span style="display:flex;"><span>        attn_scores <span style="color:#f92672">=</span> attn_scores<span style="color:#f92672">.</span>masked_fill(self<span style="color:#f92672">.</span>masked[:, :, :N, :N] <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, float(<span style="color:#e6db74">&#39;-inf&#39;</span>))
</span></span><span style="display:flex;"><span>        attn_weights <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(attn_scores, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)                             
</span></span><span style="display:flex;"><span>        attn_weights <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(attn_weights)
</span></span><span style="display:flex;"><span>        context_vec <span style="color:#f92672">=</span> attn_weights <span style="color:#f92672">@</span> v
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out)</span>
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> context_vec<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(B, N, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W_o(out)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span></code></pre></div><h4 id="layer-normalization">Layer Normalization<a hidden class="anchor" aria-hidden="true" href="#layer-normalization">#</a></h4>
<p>As you might recall, layer normalization improves the stability and efficiency of training. The input to this layer, $\text{x}$, has a shape of $\text{B} \times \text{N} \times \text{D}$ = [batch size, number of tokens, embedding dimension], and normalization is performed across the embedding dimension.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LayerNorm</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, emb_dim):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>eps <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-5</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>scale <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>ones(emb_dim))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>shift <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>zeros(emb_dim))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        mean <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        var <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>var(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, unbiased<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)       <span style="color:#75715e"># Used biased variance estimation (without Bessel&#39;s correction)</span>
</span></span><span style="display:flex;"><span>        norm_x <span style="color:#f92672">=</span> (x <span style="color:#f92672">-</span> mean) <span style="color:#f92672">/</span> torch<span style="color:#f92672">.</span>sqrt(var <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>eps)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>scale <span style="color:#f92672">*</span> norm_x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>shift
</span></span></code></pre></div><ul>
<li>The variable <code>eps</code> is a small constant added to the variance to prevent division by zero.</li>
<li><code>scale</code> and <code>shift</code> are two trainable parameters that allow the model to learn optimal transformations.</li>
<li>We set <code>unbiased=False</code>, which means we divide by $n$ instead of $n-1$, resulting in a slightly biased variance estimate (i.e., not using Bessel&rsquo;s correction). However, this has negligible impact when the embedding dimension is large.</li>
</ul>
<h4 id="feed-forward-layer">Feed forward layer<a hidden class="anchor" aria-hidden="true" href="#feed-forward-layer">#</a></h4>
<p>Next, we will implement a small neural network used as a part of the transformer block in LLMs.</p>
<p>Historically, the ReLU activation function has been widely used due to its simplicity and effectiveness. However, in modern LLMs, the Gaussian Error Linear Unit (GELU) is preferred for its improved performance.</p>
<figure class="align-center ">
    <img loading="lazy" src="../gelu.png#center"
         alt="GELU vs ReLU activation functions"/> <figcaption>
            <p>GELU vs ReLU activation functions</p>
        </figcaption>
</figure>

<p>GELU can be thought of as a smoother version of ReLU. Its smooth transitions allow for better optimization properties during training, leading to more nuanced parameter adjustments.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GELU</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> x <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>tanh(
</span></span><span style="display:flex;"><span>            torch<span style="color:#f92672">.</span>sqrt(torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2.0</span> <span style="color:#f92672">/</span> torch<span style="color:#f92672">.</span>pi)) <span style="color:#f92672">*</span>
</span></span><span style="display:flex;"><span>            (x <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.044715</span> <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>pow(x, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>        )) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MLP</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, emb_dim):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layer1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(emb_dim, <span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> emb_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>layer2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> emb_dim, emb_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gelu <span style="color:#f92672">=</span> GELU()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>gelu(self<span style="color:#f92672">.</span>layer1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>layer2(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><h4 id="decoder-block">Decoder block<a hidden class="anchor" aria-hidden="true" href="#decoder-block">#</a></h4>
<p>Now, we assemble all the components we’ve built so far into a Transformer decoder block. This block forms the foundation of GPT models and is repeated multiple times throughout the architecture.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DecoderBlock</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, d_model, context_length, dropout, num_heads):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>masked_multi_head_sa <span style="color:#f92672">=</span> MultiHeadAttention(d_in<span style="color:#f92672">=</span>d_model, d_out<span style="color:#f92672">=</span>d_model, context_length<span style="color:#f92672">=</span>context_length, att_dropout<span style="color:#f92672">=</span>dropout, num_heads<span style="color:#f92672">=</span>num_heads)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ln1 <span style="color:#f92672">=</span> LayerNorm(d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ff <span style="color:#f92672">=</span> MLP(emb_dim<span style="color:#f92672">=</span>d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ln2 <span style="color:#f92672">=</span> LayerNorm(d_model)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>residual_dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(dropout)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        attn_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>residual_dropout(self<span style="color:#f92672">.</span>masked_multi_head_sa(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ln1(x <span style="color:#f92672">+</span> attn_output)
</span></span><span style="display:flex;"><span>        ff_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>residual_dropout(self<span style="color:#f92672">.</span>ff(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ln2(x <span style="color:#f92672">+</span> ff_output)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><h4 id="gpt-1-model">GPT-1 model<a hidden class="anchor" aria-hidden="true" href="#gpt-1-model">#</a></h4>
<p>With the decoder block implemented, we now have all the necessary components to build the GPT-1 architecture.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPT1</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, D_embd, vocab_size, context_length, num_blocks, num_heads,  dropout):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Token Embeddings - Convert integer word tokens (vocab) to a D-dimensional embedding vector </span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>token_embedding_table <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(vocab_size, D_embd)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Position Encoding - Encode each position to a D-dimensional embedding vector</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>position_embedding_table <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(context_length, D_embd)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Embedding dropout</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>embd_dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(dropout)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Define multiple Decoder blocks</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>blocks <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>[DecoderBlock(d_model<span style="color:#f92672">=</span>D_embd, context_length<span style="color:#f92672">=</span>context_length, dropout<span style="color:#f92672">=</span>dropout, num_heads<span style="color:#f92672">=</span>num_heads) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(num_blocks)])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Final FC layer to project the D-dimensional vector back to vocab space</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lm_head <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(D_embd, vocab_size, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        B, N <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        token_emb <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>token_embedding_table(x)                               <span style="color:#75715e"># output: (B, N, D)</span>
</span></span><span style="display:flex;"><span>        pos_emb <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>position_embedding_table(torch<span style="color:#f92672">.</span>arange(N, device<span style="color:#f92672">=</span>device)) <span style="color:#75715e"># output: (N, D)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> token_emb <span style="color:#f92672">+</span> pos_emb                                               <span style="color:#75715e"># ouput: (B, N, D)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embd_dropout(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>blocks(x)
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lm_head(x)                                                 <span style="color:#75715e"># ouput: (B, N, vocab_size)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> logits
</span></span></code></pre></div><p>Tokenized text is first converted into token embeddings, which are then augmented with positional embeddings. This combined representation forms a tensor that passes through a series of transformer blocks, outputting a tensor of the same dimensionality. The final language modeling head (a linear layer without bias) projects this output into the vocabulary space, generating logits for each token in the vocabulary.</p>
<h3 id="pre-training">Pre-Training<a hidden class="anchor" aria-hidden="true" href="#pre-training">#</a></h3>
<p>Now, let&rsquo;s initialize a small GPT model and perform a forward pass as a sanity check.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Define hyperparameters</span>
</span></span><span style="display:flex;"><span>block_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>
</span></span><span style="display:flex;"><span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">16</span>              
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> GPT1(D_embd<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, vocab_size<span style="color:#f92672">=</span>vocab_size, context_length<span style="color:#f92672">=</span>block_size, num_heads<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, num_blocks<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, dropout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># print the number of parameters in the model</span>
</span></span><span style="display:flex;"><span>print(sum(p<span style="color:#f92672">.</span>numel() <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>parameters())<span style="color:#f92672">/</span><span style="color:#ae81ff">1e6</span>, <span style="color:#e6db74">&#39;M parameters&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Forward pass with one example</span>
</span></span><span style="display:flex;"><span>xb, yb <span style="color:#f92672">=</span> make_batch(<span style="color:#e6db74">&#34;train&#34;</span>, batch_size<span style="color:#f92672">=</span>batch_size, block_size<span style="color:#f92672">=</span>block_size)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Input shape:&#34;</span>, xb<span style="color:#f92672">.</span>size())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Target shape:&#34;</span>, yb<span style="color:#f92672">.</span>size())
</span></span><span style="display:flex;"><span>context, target <span style="color:#f92672">=</span> xb<span style="color:#f92672">.</span>to(device), yb<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>output <span style="color:#f92672">=</span> model(context)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Model output shape:&#34;</span>, output<span style="color:#f92672">.</span>size())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>cross_entropy(output<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, vocab_size), target<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Initial Loss:&#34;</span>, loss<span style="color:#f92672">.</span>item())
</span></span></code></pre></div><pre tabindex="0"><code>0.209601 M parameters
Input shape: torch.Size([16, 32])
Target shape: torch.Size([16, 32])
Model output shape: torch.Size([16, 32, 65])
Initial Loss: 4.331784248352051
</code></pre><p>As you can see, the model outputs a tensor of shape <code>[16, 32, 65]</code>, since we passed 16 input texts with 32 tokens each. The last dimension corresponds to the vocabulary size of the tokenizer. To compute the loss, we collapse the first two dimensions, concatenating all tokens together and averaging the loss over the batch.</p>
<p>With that, we’re ready to pre-train the model on our dataset. 🚀</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>max_iter <span style="color:#f92672">=</span> <span style="color:#ae81ff">5000</span>
</span></span><span style="display:flex;"><span>eval_iter <span style="color:#f92672">=</span> <span style="color:#ae81ff">200</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define the loss function &amp; optimizer</span>
</span></span><span style="display:flex;"><span>criterion <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>CrossEntropyLoss()
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>AdamW(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> iter <span style="color:#f92672">in</span> range(max_iter):
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Sample a batch of data</span>
</span></span><span style="display:flex;"><span>    xb, yb <span style="color:#f92672">=</span> make_batch(<span style="color:#e6db74">&#39;train&#39;</span>, batch_size<span style="color:#f92672">=</span>batch_size, block_size<span style="color:#f92672">=</span>block_size)
</span></span><span style="display:flex;"><span>    context, target <span style="color:#f92672">=</span> xb<span style="color:#f92672">.</span>to(device), yb<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Forward pass and optimization</span>
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> model(context)
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> criterion(output<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, vocab_size), target<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Evaluate the loss on train and val</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> iter<span style="color:#f92672">%</span><span style="color:#ae81ff">100</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">or</span> iter<span style="color:#f92672">==</span><span style="color:#ae81ff">0</span> <span style="color:#f92672">or</span> iter<span style="color:#f92672">==</span>(max_iter<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>            model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>            out <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> split <span style="color:#f92672">in</span> [<span style="color:#e6db74">&#39;train&#39;</span>, <span style="color:#e6db74">&#39;val&#39;</span>]:
</span></span><span style="display:flex;"><span>                running_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> range(eval_iter):
</span></span><span style="display:flex;"><span>                    xb, yb <span style="color:#f92672">=</span> make_batch(split, batch_size<span style="color:#f92672">=</span>batch_size, block_size<span style="color:#f92672">=</span>block_size)
</span></span><span style="display:flex;"><span>                    context, target <span style="color:#f92672">=</span> xb<span style="color:#f92672">.</span>to(device), yb<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>                    output <span style="color:#f92672">=</span> model(context)
</span></span><span style="display:flex;"><span>                    loss <span style="color:#f92672">=</span> criterion(output<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, vocab_size), target<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>                    running_loss <span style="color:#f92672">+=</span> loss<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>                out[split] <span style="color:#f92672">=</span> running_loss <span style="color:#f92672">/</span> eval_iter
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74"> Step:</span><span style="color:#e6db74">{}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{}</span><span style="color:#e6db74">, Train Loss:</span><span style="color:#e6db74">{:.4f}</span><span style="color:#e6db74">, Val Loss:</span><span style="color:#e6db74">{:.4f}</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">.</span>format(iter, max_iter, out[<span style="color:#e6db74">&#39;train&#39;</span>], out[<span style="color:#e6db74">&#39;val&#39;</span>]))
</span></span></code></pre></div><pre tabindex="0"><code>...
Step:4999/5000, Train Loss:1.7101, Val Loss:1.8699
</code></pre><h3 id="generating-text">Generating text<a hidden class="anchor" aria-hidden="true" href="#generating-text">#</a></h3>
<p>The inference process involves generating new text based on patterns the model has learned from the training data. We start with an initial context, typically the integer token <code>0</code>, which represents the newline character in our vocabulary. This serves as the seed input to the model.</p>
<p>The model then outputs a probability distribution over all tokens in the vocabulary. We sample from this distribution to generate the next token. This newly generated token is appended to the existing context and fed back into the model to produce the next prediction. By iterating this process, the model generates coherent text that follows the structure and style of the training data—such as Shakespearean plays in the case of our Tiny Shakespeare dataset.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Generate text from the trained model</span>
</span></span><span style="display:flex;"><span>context <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long, device<span style="color:#f92672">=</span>device)      <span style="color:#75715e"># Shape (B=1, N=1)</span>
</span></span><span style="display:flex;"><span>max_new_tokens <span style="color:#f92672">=</span> <span style="color:#ae81ff">300</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stored_context <span style="color:#f92672">=</span> context
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(max_new_tokens):
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> model(context)                                            <span style="color:#75715e"># output: (B=1, N, vocab_size)</span>
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> output[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, :]                                          <span style="color:#75715e"># take just the last time step (B, vocab_size)</span>
</span></span><span style="display:flex;"><span>        probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(output, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        next_token <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(probs, num_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)               <span style="color:#75715e"># Sample from the distribution (B=1, N)</span>
</span></span><span style="display:flex;"><span>        stored_context <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((stored_context, next_token), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)    <span style="color:#75715e"># (B=1, N+1)</span>
</span></span><span style="display:flex;"><span>        context <span style="color:#f92672">=</span> stored_context[:, <span style="color:#f92672">-</span>block_size:]                          <span style="color:#75715e"># Trim to lastest context</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;&#39;</span><span style="color:#f92672">.</span>join(decode(stored_context[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>tolist())))
</span></span></code></pre></div><pre tabindex="0"><code>PETES:
For only royal is was the head
Yourd
Murden, the fay honour bend blood of the
lord Here their enamen the all rant
To to shall thy sunloldent will well with gene:
All be dears in sraft&#39;d were hine of thank cave so vinct there it, If your sry donance.
I will your Bade my kills him,
He firein, my, thou couldona wings;
Ur to pear
Than prition won out it.
</code></pre><p>Since we sample at every step, text generation is stochastic, meaning that even with the same initial context, we may obtain different outputs. This ensures that the generated text is diverse rather than simply memorizing the training data.</p>
<p>Our Tiny Shakespeare-trained model essentially acts as a text completer, generating text in the same style as Shakespeare when given a prompt. Similarly, a base LLM (large language model) functions as an autocompleter, predicting the next most probable tokens based on internet-scale training data.</p>
<h2 id="summary-of-gpt-1-paper">Summary of GPT-1 paper<a hidden class="anchor" aria-hidden="true" href="#summary-of-gpt-1-paper">#</a></h2>
<p>Here are some key implementation details of the GPT-1 model.</p>
<ul>
<li>
<p><strong>Tokenizer</strong>: Byte Pair Encoding (BPE) with a vocabulary size of 40,000 merges.</p>
</li>
<li>
<p><strong>Context length</strong>: $N = 512$ tokens.</p>
</li>
<li>
<p><strong>Position Embeddings</strong>: Learned instead of the sinusoidal version proposed in Transformers.</p>
</li>
<li>
<p><strong>Architecture hyperparameters</strong>:</p>
<ul>
<li>Number of layers (decoder blocks): $L = 12$</li>
<li>Number of attention heads: $H = 12$</li>
<li>Embedding dimension: $d_{model} = 768$</li>
<li>MLP size: $d_{ff} = 4 \times d_{model} = 3072$</li>
<li>Residual, embedding and attention dropout: $p = 0.1$</li>
</ul>
</li>
<li>
<p><strong>Pre-Training hyperparameters</strong>:</p>
<ul>
<li><strong>Initialization</strong>: Since LayerNorm is used extensively throughout the model, weights were initialized from a zero-mean Gaussian distribution with a standard deviation of $0.02$.</li>
<li><strong>Optimizer</strong>: AdamW
<ul>
<li>Batch size: 64</li>
<li>Weight decay: 0.01</li>
</ul>
</li>
<li><strong>Learning rate</strong>:  Increased linearly from zero to a maximum value of $2.5 \times 10^{-4}$ over the first 2000 updates, then annealed to zero using a cosine schedule.</li>
<li><strong>Number of epochs</strong>: 100</li>
</ul>
</li>
<li>
<p><strong>Fine-tuning hyperparameters</strong>:</p>
<ul>
<li>Dropout in the classifier: $p = 0.1$</li>
<li>Learning rate: $6.25e^{-5}$ with a linear decay schedule and warmup over 0.2% of training steps.</li>
<li>Batch size: 32</li>
<li>Number of epochs: 3 (sufficient for most cases).</li>
<li>Weight decay: 0.5</li>
</ul>
</li>
</ul>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li>
<p><a href="https://medium.com/@hugmanskj/most-successful-transformer-variants-introducing-bert-and-gpt-59cfcb7bdf77">Medium Post on Most Successful Transformer Variants: Introducing BERT and GPT</a></p>
</li>
<li>
<p><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Andrej Karpathy&rsquo;s video: Let&rsquo;s build GPT: from scratch, in code, spelled out.</a></p>
</li>
<li>
<p>[1] Devlin et al., &ldquo;<a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>&rdquo;, NAACL 2019.</p>
</li>
<li>
<p>[2] Radford et al., &ldquo;<a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a>&rdquo;, OpenAI, 2018.</p>
</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://yugajmera.github.io/posts/07-transformer/post/">
    <span class="title">Next »</span>
    <br>
    <span>Generalizing Attention with Transformers</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://yugajmera.github.io/">YA&#39;s Almanac</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
