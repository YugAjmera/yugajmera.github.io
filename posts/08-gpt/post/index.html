<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Introduction to LLMs: Coding GPT from scratch | YA's Almanac</title>
<meta name=keywords content><meta name=description content="By now, you&rsquo;ve probably used OpenAI&rsquo;s ChatGPT—a chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.
ChatGPT interface
The unprecedented success of ChatGPT fueled further research into the technology behind it—Large Language Models (LLMs)."><meta name=author content><link rel=canonical href=https://yugajmera.github.io/posts/08-gpt/post/><meta name=google-site-verification content="G-S759YBMKJE"><link crossorigin=anonymous href=https://yugajmera.github.io/assets/css/stylesheet.74991b51f7611c2303d0b5649703d675530589621885eb78236e099c22aa93a5.css integrity="sha256-dJkbUfdhHCMD0LVklwPWdVMFiWIYhet4I24JnCKqk6U=" rel="preload stylesheet" as=style><link rel=icon href=https://yugajmera.github.io/assets/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yugajmera.github.io/assets/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yugajmera.github.io/assets/favicon-32x32.png><link rel=apple-touch-icon href=https://yugajmera.github.io/assets/apple-touch-icon.png><link rel=mask-icon href=https://yugajmera.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yugajmera.github.io/posts/08-gpt/post/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>mjx-container[display=true]{margin:1.5em 0!important}</style><meta property="og:title" content="Introduction to LLMs: Coding GPT from scratch"><meta property="og:description" content="By now, you&rsquo;ve probably used OpenAI&rsquo;s ChatGPT—a chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.
ChatGPT interface
The unprecedented success of ChatGPT fueled further research into the technology behind it—Large Language Models (LLMs)."><meta property="og:type" content="article"><meta property="og:url" content="https://yugajmera.github.io/posts/08-gpt/post/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-27T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-27T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Introduction to LLMs: Coding GPT from scratch"><meta name=twitter:description content="By now, you&rsquo;ve probably used OpenAI&rsquo;s ChatGPT—a chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.
ChatGPT interface
The unprecedented success of ChatGPT fueled further research into the technology behind it—Large Language Models (LLMs)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yugajmera.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Introduction to LLMs: Coding GPT from scratch","item":"https://yugajmera.github.io/posts/08-gpt/post/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introduction to LLMs: Coding GPT from scratch","name":"Introduction to LLMs: Coding GPT from scratch","description":"By now, you\u0026rsquo;ve probably used OpenAI\u0026rsquo;s ChatGPT—a chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.\nChatGPT interface\nThe unprecedented success of ChatGPT fueled further research into the technology behind it—Large Language Models (LLMs).","keywords":[],"articleBody":"By now, you’ve probably used OpenAI’s ChatGPT—a chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.\nChatGPT interface\nThe unprecedented success of ChatGPT fueled further research into the technology behind it—Large Language Models (LLMs). Over the past two posts, we’ve built the theoretical foundation, and now it’s time to get hands-on: coding a GPT-like LLM from the ground up.\nLarge Language Models (LLMs) An LLM is a neural network designed to understand, generate, and interpret human language. As the name suggests, LLMs are simply language models with an extremely large number of parameters (billions or even trillions) trained on vast datasets—potentially encompassing all publicly available internet text.\nWhile language modeling has been an area of research for decades (with RNNs, LSTMs, and Attention mechanisms), the introduction of Transformers in 2017 revolutionized the field.\nEvolution of language models\nWith Transformers proving their effectiveness, the next logical step was scaling. This movement gained traction with Google’s BERT and OpenAI’s GPT models in 2018. These two architectures define the major types of LLMs we see today.\nTypes of LLMs Recalling the architecture of Transformers from the last post, it consists of two components: the encoder and the decoder. It was proposed as a replacement for the encoder-decoder structure based on RNN + Attention for Seq2Seq learning, where the encoder reads the input text and the decoder produces predictions for the task.\nThe Transformer Architecture.\nAlthough the encoder-decoder architecture may seem natural for machine translation tasks (for which it was originally developed), it is not always necessary.\nRepresentation Models: Encoder-Only Models First school of thought: “Is a Decoder Necessary if I Only Want to Perform Data-to-Numbers Conversion?”\nFor example, in text classification tasks like sentiment analysis and spam detection or regression tasks like stock price prediction, where the goal is simply to convert data into categories or numerical values, the decoder can be omitted.\nA good example of this approach is Bidirectional Encoder Representations from Transformers (BERT)[1], an encoder-only Transformer model that uses bidirectional processing to understand the context and relationships of the input data.\nThe input is concatenated with a special token [CLS] (stands for classification) at the beginning of a sentence. Using the self-attention mechanism, this token aggregates information from all words, capturing the overall meaning of a sentence. The hidden state representation corresponding to it is then fed into an output layer for classification.\nBERT for classification and regression tasks.\nGenerative Models: Decoder-Only Models Second school of thought: “Is an Encoder Necessary for Language Generation?”\nTasks like translation, summarization, and Q\u0026A involve transforming an input sequence into an output sequence. Traditionally, this process has been handled using a model composed of an encoder for understanding the input and a decoder for generating the output.\nHowever, this process can be reframed—what if the input and output are treated as one continuous sequence? For example, a machine translation task—“we are eating bread” in English to “estamos comiendo pan” in Spanish—can be reframed as a language generation task: “Translate English to Spanish: we are eating bread.”\nA language generation or text completion task is an autoregressive problem, which can be handled using a decoder-only model. One such example is the Generative Pre-trained Transformer (GPT)[2].\nGPT-1 Architecture.\nSince the decoder contains a “masked” self-attention layer, it attends only to previously generated tokens or the available context, allowing it to generate coherent sequences that follow the context.\nChronologically, GPT was first introduced in early 2018, followed by BERT later that year. Both architectures were implemented with distinct purposes but achieved great success due to their similar training approach.\nTraining LLMs LLMs are trained in two stages using a semi-supervised learning procedure that combines unsupervised pre-training and supervised fine-tuning.\nTraining stages of an LLM.\n1. Unsupervised Pre-training In this first stage, an LLM is trained on vast amounts of raw, unlabeled data available on the internet. This allows the model to acquire broad world knowledge and develop an understanding of language semantics, including grammar and sentence structures.\nThis task-agnostic model is often referred to as a base model or a foundation model.\nBERT pre-training uses the masked language model (MLM) objective, where random tokens in the input are masked, and the model is trained to predict them based on the surrounding context. This unique training strategy (masked word prediction) makes such models well-suited for text classification tasks.\nFor generative models like GPT, the language modeling objective is used—given a sequence of tokens, the model predicts the next token in the sequence, a simple next-word prediction task. This approach helps the model learn how words and phrases fit together naturally, making it capable of generating coherent and contextually relevant text based on the given context.\nSince these objectives do not require labeled data, they enable training on massive, unlabeled text datasets. To capture a wide range of knowledge, the training data must be as diverse as possible.\nKey points:\nRequires large datasets—pre-training involves downloading and processing vast internet text data. High computational cost and time-intensive (very expensive). Compresses internet knowledge and models language semantics. A trained GPT model functions as a text completer. 2. Supervised Fine-Tuning (SFT) The second stage involves adapting the pre-trained model to specific tasks using labeled data. Since task-specific datasets require manual annotation, they are significantly smaller compared to the massive datasets used in pre-training. This semi-supervised approach enables LLMs to adapt effectively to new tasks with relatively small amounts of labeled data.\nFine-tuning can be categorized into two types:\nClassification Fine-Tuning: Labeled data consists of text samples and associated class labels (e.g., emails labeled as “spam” or “not spam”).\nInstruction Fine-Tuning: Labeled data consists of instruction-response pairs (e.g., “Translate this sentence into Spanish: we are eating bread” → “estamos comiendo pan”).\nKey points:\nComputationally cheaper than pre-training Requires less data (fine-tuned on a narrower, manually labeled dataset). Tailors the LLM to a specific task or domain. Open LLMs Organizations developing open LLMs often share model weights and architectures with the public. Examples include Cohere’s Command R, Mistral models, Microsoft’s Phi, and Meta’s Llama models.\nThese organizations typically release two types of models:\nBase Models: Unrefined models primarily used for fine-tuning, saving the cost of pre-training. These models function as text generators, producing content based on internet-derived knowledge.\nAssistant Models: Pre-fine-tuned models optimized for specific tasks like Q\u0026A, conversations, and assistance. They are often labeled as “Instruct” or “SFT” (Supervised Fine-Tuned) models.\nCoding GPT-1 from scratch This section demonstrates how to pre-train a small-scale GPT-1 model for educational purposes. Large-scale pre-training requires significant computational resources (GPT-3 pretraining cost is estimated at $4.6 million), so the community typically uses pre-trained base models.\nWe’ll train a character-level GPT-1 model using the Tiny Shakespeare dataset, containing 40,000 lines from Shakespeare’s plays. The model will generate text character by character in an unsupervised setting, learning through next-character prediction.\nIn order to use text as input to our LLM, we first split it into individual characters, convert each character into integer tokens, and then transform these tokens into embedding vectors. These embeddings serve as numerical representations of the text, making it suitable for neural network processing.\nData pre-processing First, we import necessary libraries and select the appropriate device.\n# Import functions import torch import torch.nn as nn device = 'cuda' if torch.cuda.is_available() else 'cpu' print(\"Using\", device) Next, we download and inspect the dataset. The dataset contains 1.1 million characters.\n# Download the tiny shakespeare dataset !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt with open('input.txt', 'r', encoding='utf-8') as f: text = f.read() print(\"Length of the dataset:\", len(text)) print(text[:100]) Length of the dataset: 1115394 First Citizen: Before we proceed any further, hear me speak. All: Speak, speak. First Citizen: Yo Tokenization Tokenization is the process of breaking text into smaller units called tokens. These tokens can be individual words, characters, or special symbols. For example:\nInput: \"Hello, world. Is this a test?\" After tokenization: ['Hello', ',', 'world', '.', 'Is', 'this', 'a', 'test', '?'] For our character-level model, each character itself will be treated as a token, meaning no further splitting is required. We first create a vocabulary, which is the set of all unique characters in our dataset. This defines all possible tokens our model can process as input and generate as output.\n# Create vocabulary all_chars = sorted(list(set(text))) print(\"All characters that occur in the dataset:\", ''.join(all_chars)) vocab_size = len(all_chars) print(\"Vocab size (number of unique characters in the dataset):\", vocab_size) All characters that occur in the dataset: !$\u0026',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz Vocab size (number of unique characters in the dataset): 65 We do not convert all text to lowercase because capitalization helps the model distinguish between proper and common nouns, understand sentence structure, and generate correctly capitalized text.\nNext, we map each character in our vocabulary to an integer token ID (ranging from $0$ to $64$). This mapping allows us to later convert token IDs into embedding vectors.\n# Tokenize each character - convert the chars to ints char_to_int = {all_chars[i]:i for i in range(len(all_chars))} int_to_char = {i:all_chars[i] for i in range(len(all_chars))} encode = lambda s: [char_to_int[i] for i in s] decode = lambda s: [int_to_char[i] for i in s] # Tokenize entire dataset data = torch.tensor(encode(text)) print(\"Tokenized first 10 characters:\") print(data[:10].tolist()) Tokenized first 10 characters: [18, 47, 56, 57, 58, 1, 15, 47, 58, 47] The encode function maps characters to token IDs, while decode reverses the process to reconstruct text.\nIn summary,\nTokenization breaks down the input text (training data) into individual tokens. We build a vocabulary out of all unique tokens. Each token is mapped to a unique interger ID. Create batches The next step is to generate input-target pairs for training our language model.\nSince training on the entire dataset at once is computationally expensive, we sample small chunks of the dataset (called block data) and train on them instead. The maximum size of these chunks is fixed for each task and is referred to as the block size or context length.\nThe context length is a vital parameter in LLMs, as it determines the maximum number of tokens the model can process in a single pass. A larger context window allows the model to capture longer dependencies and even process entire documents, but it also comes with increased computational cost.\nLet’s take an example to understand this better. It is intuitive to assume that $\\text{x}$ represents the input tokens, while $\\text{y}$ contains the target tokens, which are simply the inputs shifted by one position. This setup aligns with the next-token prediction task that our model will be trained on.\nblock_size = 8 # what is the maximum context length for predictions? block_data = data[:block_size+1] x = data[:block_size] y = data[1:block_size+1] print(\"Training dataset chunck:\", block_data.tolist()) print(\"x:\", x.tolist()) print(\"y:\", y.tolist()) # How training occurs context = [] for i in range(block_size): context.append(x[i].item()) target = y[i] print(f\"Context: {context}, Target: {target}\") Training dataset chunck: [18, 47, 56, 57, 58, 1, 15, 47, 58] x: [18, 47, 56, 57, 58, 1, 15, 47] y: [47, 56, 57, 58, 1, 15, 47, 58] Context: [18], Target: 47 Context: [18, 47], Target: 56 Context: [18, 47, 56], Target: 57 Context: [18, 47, 56, 57], Target: 58 Context: [18, 47, 56, 57, 58], Target: 1 Context: [18, 47, 56, 57, 58, 1], Target: 15 Context: [18, 47, 56, 57, 58, 1, 15], Target: 47 Context: [18, 47, 56, 57, 58, 1, 15, 47], Target: 58 Here, the model iteratively builds context from previous tokens. At each step, it predicts the next token based on what it has seen so far.\nThis sliding context window ensures that:\nThe model is trained on varied-length inputs. It generalizes well to different sequence lengths. Now that we have understood the concept of block size, let’s create batches.\n# Build the data loader (train/val split) n = int(0.9*len(data)) train_data = data[:n] val_data = data[n:] def make_batch(split, block_size, batch_size): data = train_data if split == \"train\" else val_data idx = torch.randint(len(data) - block_size, (batch_size,)) x, y = [], [] for i in idx: x.append(data[i:block_size+i]) y.append(data[i+1:block_size+i+1]) return torch.stack(x), torch.stack(y) In practice, input text can be longer than the model’s supported context length. In such cases, we truncate the text, keeping only the most recent tokens up to the maximum length.\nModel Architecture Let’s break down the key components of the GPT-1 architecture and implement a bare-bones version of the model:\nToken Embeddings: Converts integer token IDs from the vocabulary into a $\\text{D}$-dimensional embedding vector using a simple lookup table. In our case, each character token in the input sequence is represented as a vector.\nPositional Encoding: Since transformers do not inherently understand the order of tokens, we add a $\\text{D}$-dimensional positional encoding to each token. GPT models use a learnable lookup table for this purpose.\nInput vectors: The final input vector is obtained by summing the token embeddings and positional encodings. This results in an input tensor of shape $\\text{N} \\times \\text{D}$, where $\\text{N}$ is the number of tokens (up to the context length) and $\\text{D}$ is the embedding dimension.\nModel Architecture (Decoder-Only Transformer):\nThe model consists of $\\text{L}$ transformer blocks, each with: Masked Multi-Head Self-Attention → Layer Norm → MLP → Layer Norm A fully connected layer (language modeling head) that projects the $\\text{D}$-dimensional output back into the vocabulary space. A softmax layer that converts these logits into probabilities for the next-token prediction. GPT-1 architecture\nMasked Multi-head Self-attention layer This follows the same mechanism as in the previous post, so I won’t go into detail here.\nclass MultiHeadAttention(nn.Module): def __init__(self, d_in, d_out, context_length, attn_pdrop, num_heads, qkv_bias=False): super().__init__() self.W_qkv = nn.Linear(d_in, 3 * d_out, bias=qkv_bias) self.W_o = nn.Linear(d_out, d_out) self.attn_dropout = nn.Dropout(attn_pdrop) self.d_h = d_out // num_heads # head dimension self.num_heads = num_heads self.register_buffer(\"masked\", torch.tril(torch.ones(context_length, context_length)).view(1, 1, context_length, context_length)) def forward(self, x): # Input vectors x - (B, N, d_in) B, N, d_in = x.shape # Obtain keys, values and queries in one go - (B, N, d_out) qkv = self.W_qkv(x) queries, keys, values = qkv.chunk(3, dim=-1) # Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h) k = keys.view(B, N, self.num_heads, self.d_h).transpose(1, 2) v = values.view(B, N, self.num_heads, self.d_h).transpose(1, 2) q = queries.view(B, N, self.num_heads, self.d_h).transpose(1, 2) # Apply scaled dot-product attention with causal mask on each head attn_scores = (q @ k.transpose(2, 3)) * k.shape[-1]**-0.5 attn_scores = attn_scores.masked_fill(self.masked[:, :, :N, :N] == 0, float('-inf')) attn_weights = torch.softmax(attn_scores, dim=-1) attn_weights = self.attn_dropout(attn_weights) context_vec = attn_weights @ v # Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out) out = context_vec.transpose(1, 2).contiguous().view(B, N, -1) out = self.W_o(out) return out The only change I have made here is that I use a single linear layer to obtain the key, value, and query matrices in one go. Instead of applying three separate linear layers, we can use this trick to project the input into a single tensor with three times the output dimension and then split it along the last dimension. This reduces the number of matrix multiplications, making the computation more efficient.\nLayer Normalization As you might recall, layer normalization improves the stability and efficiency of training. The input to this layer, $\\text{x}$, has a shape of $\\text{B} \\times \\text{N} \\times \\text{D}$ = [batch size, number of tokens, embedding dimension], and normalization is performed across the embedding dimension.\nclass LayerNorm(nn.Module): def __init__(self, emb_dim): super().__init__() self.eps = 1e-5 self.scale = nn.Parameter(torch.ones(emb_dim)) self.shift = nn.Parameter(torch.zeros(emb_dim)) def forward(self, x): mean = x.mean(dim=-1, keepdim=True) var = x.var(dim=-1, keepdim=True, unbiased=False) # Used biased variance estimation (without Bessel's correction) norm_x = (x - mean) / torch.sqrt(var + self.eps) return self.scale * norm_x + self.shift The variable eps is a small constant added to the variance to prevent division by zero. scale and shift are two trainable parameters that allow the model to learn optimal transformations. We set unbiased=False, which means we divide by $n$ instead of $n-1$, resulting in a slightly biased variance estimate (i.e., not using Bessel’s correction), as done in GPT models. However, this has negligible impact when the embedding dimension is large. Feed forward layer Next, we will implement a small neural network used as a part of the transformer block in LLMs.\nHistorically, the ReLU activation function has been widely used due to its simplicity and effectiveness. However, in modern LLMs, the Gaussian Error Linear Unit (GELU) is preferred for its improved performance.\nGELU vs ReLU activation functions\nGELU can be thought of as a smoother version of ReLU. Its smooth transitions allow for better optimization properties during training, leading to more nuanced parameter adjustments.\nclass GELU(nn.Module): def __init__(self): super().__init__() def forward(self, x): return 0.5 * x * (1 + torch.tanh( torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)) )) class MLP(nn.Module): def __init__(self, emb_dim): super().__init__() self.layer1 = nn.Linear(emb_dim, 4 * emb_dim) self.layer2 = nn.Linear(4 * emb_dim, emb_dim) self.gelu = GELU() def forward(self, x): x = self.gelu(self.layer1(x)) x = self.layer2(x) return x Decoder block Now, we assemble all the components we’ve built so far into a Transformer decoder block. This block forms the foundation of GPT models and is repeated multiple times throughout the architecture.\nclass DecoderBlock(nn.Module): def __init__(self, d_model, context_length, num_heads, attn_pdrop, resid_pdrop): super().__init__() self.masked_multi_head_sa = MultiHeadAttention(d_in=d_model, d_out=d_model, context_length=context_length, attn_pdrop=attn_pdrop, num_heads=num_heads) self.ln1 = LayerNorm(d_model) self.mlp = MLP(emb_dim=d_model) self.ln2 = LayerNorm(d_model) self.resid_dropout = nn.Dropout(resid_pdrop) def forward(self, x): attn_output = self.resid_dropout(self.masked_multi_head_sa(x)) x = self.ln1(x + attn_output) mlp_output = self.resid_dropout(self.mlp(x)) x = self.ln2(x + mlp_output) return x GPT-1 model With the decoder block implemented, we now have all the necessary components to build the GPT-1 architecture.\nclass GPT1(nn.Module): def __init__(self, D_embd, vocab_size, context_length, num_blocks, num_heads, dropout): super().__init__() # Token Embeddings - Convert integer word tokens (vocab) to a D-dimensional embedding vector self.token_embedding_table = nn.Embedding(vocab_size, D_embd) # Position Encoding - Encode each position to a D-dimensional embedding vector self.position_embedding_table = nn.Embedding(context_length, D_embd) # Embedding dropout self.embd_dropout = nn.Dropout(dropout) # Define multiple Decoder blocks self.blocks = nn.Sequential(*[DecoderBlock(d_model=D_embd, context_length=context_length, dropout=dropout, num_heads=num_heads) for _ in range(num_blocks)]) # Final FC layer to project the D-dimensional vector back to vocab space self.lm_head = nn.Linear(D_embd, vocab_size, bias=False) def forward(self, x): B, N = x.size() token_emb = self.token_embedding_table(x) # output: (B, N, D) pos_emb = self.position_embedding_table(torch.arange(N, device=device)) # output: (N, D) x = token_emb + pos_emb # ouput: (B, N, D) x = self.embd_dropout(x) x = self.blocks(x) logits = self.lm_head(x) # ouput: (B, N, vocab_size) return logits Tokenized text is first converted into token embeddings, which are then augmented with positional embeddings. This combined representation forms a tensor that passes through a series of transformer blocks, outputting a tensor of the same dimensionality. The final language modeling head (a linear layer without bias) projects this output into the vocabulary space, generating logits for each token in the vocabulary.\nWeight initialization Since LayerNorm is used extensively throughout the model, weights were initialized from a zero-mean Gaussian distribution with a standard deviation of $0.02$, which the authors noted was sufficient.\n# Initialize weights as per GPT-1: Normal distribution with std=0.02 def gpt1_init(m): if isinstance(m, nn.Linear): # Applies to linear layers only nn.init.normal_(m.weight, mean=0.0, std=0.02) if m.bias is not None: nn.init.zeros_(m.bias) # Bias initialized to zero Pre-Training Now, let’s initialize a small GPT model and perform a forward pass as a sanity check.\n# Define hyperparameters block_size = 32 batch_size = 16 model = GPT1(D_embd=64, vocab_size=vocab_size, context_length=block_size, num_heads=4, num_blocks=4, dropout=0.1).to(device) # Apply initialization model = model.apply(gpt1_init) # print the number of parameters in the model print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters') # Forward pass with one example xb, yb = make_batch(\"train\", batch_size=batch_size, block_size=block_size) print(\"Input shape:\", xb.size()) print(\"Target shape:\", yb.size()) context, target = xb.to(device), yb.to(device) logits = model(context) print(\"Model output shape:\", logits.size()) loss = F.cross_entropy(logits.view(-1, vocab_size), target.view(-1)) print(\"Initial Loss:\", loss.item()) 0.209536 M parameters Input shape: torch.Size([16, 32]) Target shape: torch.Size([16, 32]) Model output shape: torch.Size([16, 32, 65]) Initial Loss: 4.262292861938477 As you can see, the model outputs a tensor of shape [16, 32, 65], since we passed 16 input texts with 32 tokens each. The last dimension corresponds to the vocabulary size of the tokenizer. To compute the loss, we collapse the first two dimensions, concatenating all tokens together and averaging the loss over the batch.\nAt initialization, all tokens in our vocabulary are equally likely, meaning each has a probability of $1/65$. If we manually compute the cross-entropy loss, we get: $$ \\text{loss} = -\\ln(1/65) = 4.174 $$ This is approximately the same as our computed value, indicating that we are on the right track.\nWeight decay The GPT-1 model used the Adam optimizer with a modified version of L2 regularization—known as the AdamW optimizer, applying a weight decay $0.01$ only to non-bias and non-gain (LayerNorm) weights, meaning only Linear layer weights were regularized.\nSince Linear weights are 2D tensors, while biases and LayerNorm weights are 1D, we can use this criterion to separate them into decay and non-decay parameter groups.\nHere’s how to implement it in PyTorch:\n# Separate parameters decay_params = [] no_decay_params = [] for name, param in model.named_parameters(): if param.requires_grad: if param.dim() \u003e= 2: decay_params.append(param) else: no_decay_params.append(param) # Define optimizer with different parameter groups optimizer = torch.optim.AdamW( [ {\"params\": decay_params, \"weight_decay\": 0.01}, # Linear weights {\"params\": no_decay_params, \"weight_decay\": 0.0} # Biases \u0026 LayerNorm weights ], lr=1e-3 ) With that, we’re ready to pre-train the model on our dataset. 🚀\nmax_iter = 5000 eval_iter = 200 # Define the loss function \u0026 optimizer criterion = torch.nn.CrossEntropyLoss() for iter in range(max_iter): model.train() # Sample a batch of data xb, yb = make_batch('train', batch_size=batch_size, block_size=block_size) context, target = xb.to(device), yb.to(device) # Forward pass and optimization optimizer.zero_grad() logits = model(context) loss = criterion(logits.view(-1, vocab_size), target.view(-1)) loss.backward() optimizer.step() # Evaluate the loss on train and val if iter%100 == 0 or iter==0 or iter==(max_iter-1): with torch.no_grad(): model.eval() out = {} for split in ['train', 'val']: running_loss = 0 for k in range(eval_iter): xb, yb = make_batch(split, batch_size=batch_size, block_size=block_size) context, target = xb.to(device), yb.to(device) logits = model(context) loss = criterion(logits.view(-1, vocab_size), target.view(-1)) running_loss += loss.item() out[split] = running_loss / eval_iter print('\\n Step:{}/{}, Train Loss:{:.4f}, Val Loss:{:.4f}'.format(iter, max_iter, out['train'], out['val'])) ... Step:4999/5000, Train Loss:1.7101, Val Loss:1.8699 Generating text The inference process involves generating new text based on patterns the model has learned from the training data. We start with an initial context, typically the integer token 0, which represents the newline character in our vocabulary. This serves as the seed input to the model.\nThe model then outputs a probability distribution over all tokens in the vocabulary. We sample from this distribution to generate the next token. This newly generated token is appended to the existing context and fed back into the model to produce the next prediction. By iterating this process, the model generates coherent text that follows the structure and style of the training data—such as Shakespearean plays in the case of our Tiny Shakespeare dataset.\n# Generate text from the trained model max_new_tokens = 300 context = torch.zeros((1, 1), dtype=torch.long, device=device) # Shape (B=1, N=1) stored_context = context for _ in range(max_new_tokens): model.eval() with torch.no_grad(): context = stored_context[:, -block_size:] # Trim the context to fit the model's context length logits = model(context) # logits: (B=1, N, vocab_size) logits = logits[:, -1, :] # take just the last time step (B, vocab_size) probs = torch.softmax(logits, dim=-1) next_token = torch.multinomial(probs, num_samples=1) # Sample from the distribution (B=1, N) stored_context = torch.cat((stored_context, next_token), dim=1) # (B=1, N+1) print(''.join(decode(stored_context[0].tolist()))) O this must says it toill dake to feare, What in, I reigreaton all him draster. he spit me with hermattan? COMANIUSIUS: As you my lastless; tell was with cantal you was? QUEEN ELIZABETH: Ay, forwas now can, and you fear'd, more my mune chamnot Moounce Let not the kise him? hy bu do to him, Unce I Since we sample at every step, text generation is stochastic, meaning that even with the same initial context, we may obtain different outputs. This ensures that the generated text is diverse rather than simply memorizing the training data.\nI know the generation is not very good since we’ve trained a character-level model, and it doesn’t grasp the meaning of words well. But still, it does a good job at generating Shakespeare-like text.\nSummary of GPT-1 paper Here are some key implementation details of the GPT-1 model.\nTokenizer: Byte Pair Encoding (BPE) with a vocabulary size of 40,000 merges. We’ll go into more detail on this in the next post.\nContext length: $N = 512$ tokens.\nPosition Embeddings: Learned instead of the sinusoidal version proposed in Transformers.\nArchitecture hyperparameters:\nNumber of layers (decoder blocks): $L = 12$ Number of attention heads: $H = 12$ Embedding dimension: $d_{model} = 768$ MLP size: $d_{ff} = 4 \\times d_{model} = 3072$ Residual, embedding and attention dropout: $p = 0.1$ Pre-Training hyperparameters:\nInitialization: Initialized from a zero-mean Gaussian distribution with a standard deviation of $0.02$. Optimizer: AdamW Batch size: 64 (randomly sampled) Weight decay: 0.01 Learning rate: Increased linearly from zero to a maximum value of $2.5 \\times 10^{-4}$ over the first 2000 updates, then annealed to zero using a cosine schedule. Number of epochs: 100 Fine-tuning hyperparameters:\nDropout in the classifier: $p = 0.1$ Learning rate: $6.25e^{-5}$ with a linear decay schedule and warmup over 0.2% of training steps. Batch size: 32 Number of epochs: 3 (sufficient for most cases). Weight decay: 0.5 References Medium Post on Most Successful Transformer Variants: Introducing BERT and GPT\nAndrej Karpathy’s video: Let’s build GPT: from scratch, in code, spelled out.\n[1] Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”, NAACL 2019.\n[2] Radford et al., “Improving Language Understanding by Generative Pre-Training”, OpenAI, 2018.\n","wordCount":"4251","inLanguage":"en","datePublished":"2025-01-27T00:00:00Z","dateModified":"2025-01-27T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://yugajmera.github.io/posts/08-gpt/post/"},"publisher":{"@type":"Organization","name":"YA's Almanac","logo":{"@type":"ImageObject","url":"https://yugajmera.github.io/assets/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yugajmera.github.io/ accesskey=h title="YA's Almanac (Alt + H)">YA's Almanac</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yugajmera.github.io/ title=Home><span>Home</span></a></li><li><a href=https://yugajmera.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://yugajmera.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Introduction to LLMs: Coding GPT from scratch</h1><div class=post-meta><span title='2025-01-27 00:00:00 +0000 UTC'>January 27, 2025</span>&nbsp;·&nbsp;20 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#large-language-models-llms>Large Language Models (LLMs)</a><ul><li><a href=#types-of-llms>Types of LLMs</a></li><li><a href=#training-llms>Training LLMs</a></li><li><a href=#open-llms>Open LLMs</a></li></ul></li><li><a href=#coding-gpt-1-from-scratch>Coding GPT-1 from scratch</a><ul><li><a href=#data-pre-processing>Data pre-processing</a></li><li><a href=#tokenization>Tokenization</a></li><li><a href=#create-batches>Create batches</a></li><li><a href=#model-architecture>Model Architecture</a></li><li><a href=#pre-training>Pre-Training</a></li><li><a href=#generating-text>Generating text</a></li></ul></li><li><a href=#summary-of-gpt-1-paper>Summary of GPT-1 paper</a></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p>By now, you&rsquo;ve probably used OpenAI&rsquo;s ChatGPT—a chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.</p><figure class=align-center><img loading=lazy src=../chatgpt.png#center alt="ChatGPT interface" width=600><figcaption><p>ChatGPT interface</p></figcaption></figure><p>The unprecedented success of ChatGPT fueled further research into the technology behind it—Large Language Models (LLMs). Over the past two posts, we’ve built the theoretical foundation, and now it&rsquo;s time to get hands-on: coding a GPT-like LLM from the ground up.</p><h2 id=large-language-models-llms>Large Language Models (LLMs)<a hidden class=anchor aria-hidden=true href=#large-language-models-llms>#</a></h2><p>An LLM is a neural network designed to understand, generate, and interpret human language. As the name suggests, LLMs are simply language models with an extremely large number of parameters (billions or even trillions) trained on vast datasets—potentially encompassing all publicly available internet text.</p><p>While language modeling has been an area of research for decades (with RNNs, LSTMs, and Attention mechanisms), the introduction of Transformers in 2017 revolutionized the field.</p><figure class=align-center><img loading=lazy src=../llm-roadmap.jpg#center alt="Evolution of language models" width=550><figcaption><p>Evolution of language models</p></figcaption></figure><p>With Transformers proving their effectiveness, the next logical step was scaling. This movement gained traction with Google&rsquo;s BERT and OpenAI’s GPT models in 2018. These two architectures define the major types of LLMs we see today.</p><h3 id=types-of-llms>Types of LLMs<a hidden class=anchor aria-hidden=true href=#types-of-llms>#</a></h3><p>Recalling the architecture of Transformers from the last post, it consists of two components: the encoder and the decoder. It was proposed as a replacement for the encoder-decoder structure based on RNN + Attention for Seq2Seq learning, where the encoder reads the input text and the decoder produces predictions for the task.</p><figure class=align-center><img loading=lazy src=../transformer-full.png#center alt="The Transformer Architecture." width=400><figcaption><p>The Transformer Architecture.</p></figcaption></figure><p>Although the encoder-decoder architecture may seem natural for machine translation tasks (for which it was originally developed), it is not always necessary.</p><h4 id=representation-models-encoder-only-models>Representation Models: Encoder-Only Models<a hidden class=anchor aria-hidden=true href=#representation-models-encoder-only-models>#</a></h4><p>First school of thought: &ldquo;<em>Is a Decoder Necessary if I Only Want to Perform Data-to-Numbers Conversion?</em>&rdquo;</p><p>For example, in text classification tasks like sentiment analysis and spam detection or regression tasks like stock price prediction, where the goal is simply to convert data into categories or numerical values, the decoder can be omitted.</p><p>A good example of this approach is Bidirectional Encoder Representations from Transformers (BERT)<a href=#references>[1]</a>, an encoder-only Transformer model that uses bidirectional processing to understand the context and relationships of the input data.</p><p>The input is concatenated with a special token [CLS] (stands for classification) at the beginning of a sentence. Using the self-attention mechanism, this token aggregates information from all words, capturing the overall meaning of a sentence. The hidden state representation corresponding to it is then fed into an output layer for classification.</p><figure class=align-center><img loading=lazy src=../bert-cls.png#center alt="BERT for classification and regression tasks." width=600><figcaption><p>BERT for classification and regression tasks.</p></figcaption></figure><h4 id=generative-models-decoder-only-models>Generative Models: Decoder-Only Models<a hidden class=anchor aria-hidden=true href=#generative-models-decoder-only-models>#</a></h4><p>Second school of thought: &ldquo;<em>Is an Encoder Necessary for Language Generation?</em>&rdquo;</p><p>Tasks like translation, summarization, and Q&amp;A involve transforming an input sequence into an output sequence. Traditionally, this process has been handled using a model composed of an encoder for understanding the input and a decoder for generating the output.</p><p>However, this process can be reframed—what if the input and output are treated as one continuous sequence? For example, a machine translation task—&ldquo;we are eating bread&rdquo; in English to &ldquo;estamos comiendo pan&rdquo; in Spanish—can be reframed as a language generation task: &ldquo;Translate English to Spanish: we are eating bread.&rdquo;</p><p>A language generation or text completion task is an autoregressive problem, which can be handled using a decoder-only model. One such example is the Generative Pre-trained Transformer (GPT)<a href=#references>[2]</a>.</p><figure class=align-center><img loading=lazy src=../encoder-only.png#center alt="GPT-1 Architecture." width=600><figcaption><p>GPT-1 Architecture.</p></figcaption></figure><p>Since the decoder contains a &ldquo;masked&rdquo; self-attention layer, it attends only to previously generated tokens or the available context, allowing it to generate coherent sequences that follow the context.</p><p>Chronologically, GPT was first introduced in early 2018, followed by BERT later that year. Both architectures were implemented with distinct purposes but achieved great success due to their similar training approach.</p><h3 id=training-llms>Training LLMs<a hidden class=anchor aria-hidden=true href=#training-llms>#</a></h3><p>LLMs are trained in two stages using a semi-supervised learning procedure that combines unsupervised pre-training and supervised fine-tuning.</p><figure class=align-center><img loading=lazy src=../training.png#center alt="Training stages of an LLM."><figcaption><p>Training stages of an LLM.</p></figcaption></figure><h4 id=1-unsupervised-pre-training>1. Unsupervised Pre-training<a hidden class=anchor aria-hidden=true href=#1-unsupervised-pre-training>#</a></h4><p>In this first stage, an LLM is trained on vast amounts of raw, unlabeled data available on the internet. This allows the model to acquire broad world knowledge and develop an understanding of language semantics, including grammar and sentence structures.</p><p>This task-agnostic model is often referred to as a <em>base model</em> or a <em>foundation model</em>.</p><p>BERT pre-training uses the masked language model (MLM) objective, where random tokens in the input are masked, and the model is trained to predict them based on the surrounding context. This unique training strategy (masked word prediction) makes such models well-suited for text classification tasks.</p><p>For generative models like GPT, the language modeling objective is used—given a sequence of tokens, the model predicts the next token in the sequence, a simple next-word prediction task. This approach helps the model learn how words and phrases fit together naturally, making it capable of generating coherent and contextually relevant text based on the given context.</p><p>Since these objectives do not require labeled data, they enable training on massive, unlabeled text datasets. To capture a wide range of knowledge, the training data must be as diverse as possible.</p><p>Key points:</p><ul><li>Requires large datasets—pre-training involves downloading and processing vast internet text data.</li><li>High computational cost and time-intensive (very expensive).</li><li>Compresses internet knowledge and models language semantics.</li><li>A trained GPT model functions as a text completer.</li></ul><h4 id=2-supervised-fine-tuning-sft>2. <strong>Supervised Fine-Tuning (SFT)</strong><a hidden class=anchor aria-hidden=true href=#2-supervised-fine-tuning-sft>#</a></h4><p>The second stage involves adapting the pre-trained model to specific tasks using labeled data. Since task-specific datasets require manual annotation, they are significantly smaller compared to the massive datasets used in pre-training. This semi-supervised approach enables LLMs to adapt effectively to new tasks with relatively small amounts of labeled data.</p><p>Fine-tuning can be categorized into two types:</p><ol><li><p>Classification Fine-Tuning: Labeled data consists of text samples and associated class labels (e.g., emails labeled as &ldquo;spam&rdquo; or &ldquo;not spam&rdquo;).</p></li><li><p>Instruction Fine-Tuning: Labeled data consists of instruction-response pairs (e.g., &ldquo;Translate this sentence into Spanish: we are eating bread&rdquo; → &ldquo;estamos comiendo pan&rdquo;).</p></li></ol><p>Key points:</p><ul><li>Computationally cheaper than pre-training</li><li>Requires less data (fine-tuned on a narrower, manually labeled dataset).</li><li>Tailors the LLM to a specific task or domain.</li></ul><h3 id=open-llms>Open LLMs<a hidden class=anchor aria-hidden=true href=#open-llms>#</a></h3><p>Organizations developing open LLMs often share model weights and architectures with the public. Examples include Cohere’s Command R, Mistral models, Microsoft’s Phi, and Meta’s Llama models.</p><p>These organizations typically release two types of models:</p><ul><li><p><strong>Base Models</strong>: Unrefined models primarily used for fine-tuning, saving the cost of pre-training. These models function as text generators, producing content based on internet-derived knowledge.</p></li><li><p><strong>Assistant Models</strong>: Pre-fine-tuned models optimized for specific tasks like Q&amp;A, conversations, and assistance. They are often labeled as &ldquo;Instruct&rdquo; or &ldquo;SFT&rdquo; (Supervised Fine-Tuned) models.</p></li></ul><h2 id=coding-gpt-1-from-scratch>Coding GPT-1 from scratch<a hidden class=anchor aria-hidden=true href=#coding-gpt-1-from-scratch>#</a></h2><p>This section demonstrates how to pre-train a small-scale GPT-1 model for educational purposes. Large-scale pre-training requires significant computational resources (GPT-3 pretraining cost is estimated at $4.6 million), so the community typically uses pre-trained base models.</p><p>We&rsquo;ll train a character-level GPT-1 model using the Tiny Shakespeare dataset, containing 40,000 lines from Shakespeare&rsquo;s plays. The model will generate text character by character in an unsupervised setting, learning through next-character prediction.</p><p>In order to use text as input to our LLM, we first split it into individual characters, convert each character into integer tokens, and then transform these tokens into embedding vectors. These embeddings serve as numerical representations of the text, making it suitable for neural network processing.</p><h3 id=data-pre-processing>Data pre-processing<a hidden class=anchor aria-hidden=true href=#data-pre-processing>#</a></h3><p>First, we import necessary libraries and select the appropriate device.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Import functions</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=s1>&#39;cuda&#39;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;cpu&#39;</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Using&#34;</span><span class=p>,</span> <span class=n>device</span><span class=p>)</span>
</span></span></code></pre></div><p>Next, we download and inspect the dataset. The dataset contains 1.1 million characters.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Download the tiny shakespeare dataset</span>
</span></span><span class=line><span class=cl><span class=err>!</span><span class=n>wget</span> <span class=n>https</span><span class=p>:</span><span class=o>//</span><span class=n>raw</span><span class=o>.</span><span class=n>githubusercontent</span><span class=o>.</span><span class=n>com</span><span class=o>/</span><span class=n>karpathy</span><span class=o>/</span><span class=n>char</span><span class=o>-</span><span class=n>rnn</span><span class=o>/</span><span class=n>master</span><span class=o>/</span><span class=n>data</span><span class=o>/</span><span class=n>tinyshakespeare</span><span class=o>/</span><span class=nb>input</span><span class=o>.</span><span class=n>txt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;input.txt&#39;</span><span class=p>,</span> <span class=s1>&#39;r&#39;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Length of the dataset:&#34;</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>text</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>text</span><span class=p>[:</span><span class=mi>100</span><span class=p>])</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>Length</span> <span class=n>of</span> <span class=n>the</span> <span class=n>dataset</span><span class=p>:</span> <span class=m>1115394</span>
</span></span><span class=line><span class=cl><span class=n>First</span> <span class=n>Citizen</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=n>Before</span> <span class=n>we</span> <span class=n>proceed</span> <span class=n>any</span> <span class=n>further</span><span class=p>,</span> <span class=n>hear</span> <span class=n>me</span> <span class=n>speak</span><span class=p>.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>All</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=n>Speak</span><span class=p>,</span> <span class=n>speak</span><span class=p>.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>First</span> <span class=n>Citizen</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=n>Yo</span>
</span></span></code></pre></div><h3 id=tokenization>Tokenization<a hidden class=anchor aria-hidden=true href=#tokenization>#</a></h3><p>Tokenization is the process of breaking text into smaller units called tokens. These tokens can be individual words, characters, or special symbols. For example:</p><ul><li>Input: <code>"Hello, world. Is this a test?"</code></li><li>After tokenization: <code>['Hello', ',', 'world', '.', 'Is', 'this', 'a', 'test', '?']</code></li></ul><p>For our character-level model, each character itself will be treated as a token, meaning no further splitting is required. We first create a vocabulary, which is the set of all unique characters in our dataset. This defines all possible tokens our model can process as input and generate as output.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Create vocabulary</span>
</span></span><span class=line><span class=cl><span class=n>all_chars</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=nb>set</span><span class=p>(</span><span class=n>text</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;All characters that occur in the dataset:&#34;</span><span class=p>,</span> <span class=s1>&#39;&#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>all_chars</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>vocab_size</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>all_chars</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Vocab size (number of unique characters in the dataset):&#34;</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>All</span> <span class=n>characters</span> <span class=n>that</span> <span class=n>occur</span> <span class=k>in</span> <span class=n>the</span> <span class=n>dataset</span><span class=p>:</span> 
</span></span><span class=line><span class=cl> <span class=p>!</span><span class=err>$</span><span class=p>&amp;</span><span class=err>&#39;</span><span class=p>,-.</span><span class=m>3</span><span class=p>:;?</span><span class=n>ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz</span>
</span></span><span class=line><span class=cl><span class=n>Vocab</span> <span class=n>size</span> <span class=p>(</span><span class=n>number</span> <span class=n>of</span> <span class=n>unique</span> <span class=n>characters</span> <span class=k>in</span> <span class=n>the</span> <span class=n>dataset</span><span class=p>):</span> <span class=m>65</span>
</span></span></code></pre></div><p>We do not convert all text to lowercase because capitalization helps the model distinguish between proper and common nouns, understand sentence structure, and generate correctly capitalized text.</p><p>Next, we map each character in our vocabulary to an integer token ID (ranging from $0$ to $64$). This mapping allows us to later convert token IDs into embedding vectors.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Tokenize each character - convert the chars to ints</span>
</span></span><span class=line><span class=cl><span class=n>char_to_int</span> <span class=o>=</span> <span class=p>{</span><span class=n>all_chars</span><span class=p>[</span><span class=n>i</span><span class=p>]:</span><span class=n>i</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>all_chars</span><span class=p>))}</span>
</span></span><span class=line><span class=cl><span class=n>int_to_char</span> <span class=o>=</span> <span class=p>{</span><span class=n>i</span><span class=p>:</span><span class=n>all_chars</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>all_chars</span><span class=p>))}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>encode</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>s</span><span class=p>:</span> <span class=p>[</span><span class=n>char_to_int</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>s</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>decode</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>s</span><span class=p>:</span> <span class=p>[</span><span class=n>int_to_char</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>s</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Tokenize entire dataset</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Tokenized first 10 characters:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>data</span><span class=p>[:</span><span class=mi>10</span><span class=p>]</span><span class=o>.</span><span class=n>tolist</span><span class=p>())</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>Tokenized</span> <span class=n>first</span> <span class=m>10</span> <span class=n>characters</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=na>[18, 47, 56, 57, 58, 1, 15, 47, 58, 47]</span>
</span></span></code></pre></div><p>The <code>encode</code> function maps characters to token IDs, while <code>decode</code> reverses the process to reconstruct text.</p><p>In summary,</p><ol><li>Tokenization breaks down the input text (training data) into individual tokens.</li><li>We build a vocabulary out of all unique tokens.</li><li>Each token is mapped to a unique interger ID.</li></ol><h3 id=create-batches>Create batches<a hidden class=anchor aria-hidden=true href=#create-batches>#</a></h3><p>The next step is to generate input-target pairs for training our language model.</p><p>Since training on the entire dataset at once is computationally expensive, we sample small chunks of the dataset (called block data) and train on them instead. The maximum size of these chunks is fixed for each task and is referred to as the block size or context length.</p><p>The context length is a vital parameter in LLMs, as it determines the maximum number of tokens the model can process in a single pass. A larger context window allows the model to capture longer dependencies and even process entire documents, but it also comes with increased computational cost.</p><p>Let&rsquo;s take an example to understand this better. It is intuitive to assume that $\text{x}$ represents the input tokens, while $\text{y}$ contains the target tokens, which are simply the inputs shifted by one position. This setup aligns with the next-token prediction task that our model will be trained on.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>block_size</span> <span class=o>=</span> <span class=mi>8</span>              <span class=c1># what is the maximum context length for predictions?</span>
</span></span><span class=line><span class=cl><span class=n>block_data</span> <span class=o>=</span> <span class=n>data</span><span class=p>[:</span><span class=n>block_size</span><span class=o>+</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>data</span><span class=p>[:</span><span class=n>block_size</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>data</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=n>block_size</span><span class=o>+</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Training dataset chunck:&#34;</span><span class=p>,</span> <span class=n>block_data</span><span class=o>.</span><span class=n>tolist</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;x:&#34;</span><span class=p>,</span> <span class=n>x</span><span class=o>.</span><span class=n>tolist</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;y:&#34;</span><span class=p>,</span> <span class=n>y</span><span class=o>.</span><span class=n>tolist</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># How training occurs</span>
</span></span><span class=line><span class=cl><span class=n>context</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>block_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>context</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=n>target</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Context: </span><span class=si>{</span><span class=n>context</span><span class=si>}</span><span class=s2>, Target: </span><span class=si>{</span><span class=n>target</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>Training dataset chunck: [18, 47, 56, 57, 58, 1, 15, 47, 58]
x: [18, 47, 56, 57, 58, 1, 15, 47]
y: [47, 56, 57, 58, 1, 15, 47, 58]
Context: [18], Target: 47
Context: [18, 47], Target: 56
Context: [18, 47, 56], Target: 57
Context: [18, 47, 56, 57], Target: 58
Context: [18, 47, 56, 57, 58], Target: 1
Context: [18, 47, 56, 57, 58, 1], Target: 15
Context: [18, 47, 56, 57, 58, 1, 15], Target: 47
Context: [18, 47, 56, 57, 58, 1, 15, 47], Target: 58
</code></pre><p>Here, the model iteratively builds context from previous tokens. At each step, it predicts the next token based on what it has seen so far.</p><p>This sliding context window ensures that:</p><ul><li>The model is trained on varied-length inputs.</li><li>It generalizes well to different sequence lengths.</li></ul><p>Now that we have understood the concept of block size, let&rsquo;s create batches.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Build the data loader (train/val split)</span>
</span></span><span class=line><span class=cl><span class=n>n</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=mf>0.9</span><span class=o>*</span><span class=nb>len</span><span class=p>(</span><span class=n>data</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>train_data</span> <span class=o>=</span> <span class=n>data</span><span class=p>[:</span><span class=n>n</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>val_data</span> <span class=o>=</span> <span class=n>data</span><span class=p>[</span><span class=n>n</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>make_batch</span><span class=p>(</span><span class=n>split</span><span class=p>,</span> <span class=n>block_size</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=n>train_data</span> <span class=k>if</span> <span class=n>split</span> <span class=o>==</span> <span class=s2>&#34;train&#34;</span> <span class=k>else</span> <span class=n>val_data</span>
</span></span><span class=line><span class=cl>    <span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>data</span><span class=p>)</span> <span class=o>-</span> <span class=n>block_size</span><span class=p>,</span> <span class=p>(</span><span class=n>batch_size</span><span class=p>,))</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>idx</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=n>i</span><span class=p>:</span><span class=n>block_size</span><span class=o>+</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>:</span><span class=n>block_size</span><span class=o>+</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>x</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span></code></pre></div><p>In practice, input text can be longer than the model’s supported context length. In such cases, we truncate the text, keeping only the most recent tokens up to the maximum length.</p><h3 id=model-architecture>Model Architecture<a hidden class=anchor aria-hidden=true href=#model-architecture>#</a></h3><p>Let’s break down the key components of the GPT-1 architecture and implement a bare-bones version of the model:</p><ul><li><p><strong>Token Embeddings</strong>: Converts integer token IDs from the vocabulary into a $\text{D}$-dimensional embedding vector using a simple lookup table. In our case, each character token in the input sequence is represented as a vector.</p></li><li><p><strong>Positional Encoding</strong>: Since transformers do not inherently understand the order of tokens, we add a $\text{D}$-dimensional positional encoding to each token. GPT models use a learnable lookup table for this purpose.</p></li><li><p><strong>Input vectors</strong>: The final input vector is obtained by summing the token embeddings and positional encodings. This results in an input tensor of shape $\text{N} \times \text{D}$, where $\text{N}$ is the number of tokens (up to the context length) and $\text{D}$ is the embedding dimension.</p></li><li><p><strong>Model Architecture</strong> (Decoder-Only Transformer):</p><ul><li>The model consists of $\text{L}$ transformer blocks, each with:<ul><li>Masked Multi-Head Self-Attention → Layer Norm → MLP → Layer Norm</li></ul></li><li>A fully connected layer (language modeling head) that projects the $\text{D}$-dimensional output back into the vocabulary space.</li><li>A softmax layer that converts these logits into probabilities for the next-token prediction.</li></ul></li></ul><figure class=align-center><img loading=lazy src=../gpt1-architecture.png#center alt="GPT-1 architecture" width=500><figcaption><p>GPT-1 architecture</p></figcaption></figure><h4 id=masked-multi-head-self-attention-layer>Masked Multi-head Self-attention layer<a hidden class=anchor aria-hidden=true href=#masked-multi-head-self-attention-layer>#</a></h4><p>This follows the same mechanism as in the previous post, so I won’t go into detail here.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span> <span class=n>attn_pdrop</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>qkv_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_qkv</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=mi>3</span> <span class=o>*</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_o</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_out</span><span class=p>,</span> <span class=n>d_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attn_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>attn_pdrop</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span> <span class=o>=</span> <span class=n>d_out</span> <span class=o>//</span> <span class=n>num_heads</span>                    <span class=c1># head dimension</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s2>&#34;masked&#34;</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>context_length</span><span class=p>,</span> <span class=n>context_length</span><span class=p>))</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span> <span class=n>context_length</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Input vectors x - (B, N, d_in)</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>d_in</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=c1># Obtain keys, values and queries in one go - (B, N, d_out)</span>
</span></span><span class=line><span class=cl>        <span class=n>qkv</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_qkv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>queries</span><span class=p>,</span> <span class=n>keys</span><span class=p>,</span> <span class=n>values</span> <span class=o>=</span> <span class=n>qkv</span><span class=o>.</span><span class=n>chunk</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=n>keys</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=n>values</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>queries</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Apply scaled dot-product attention with causal mask on each head</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_scores</span> <span class=o>=</span> <span class=p>(</span><span class=n>q</span> <span class=o>@</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span> <span class=o>*</span> <span class=n>k</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>**-</span><span class=mf>0.5</span>    
</span></span><span class=line><span class=cl>        <span class=n>attn_scores</span> <span class=o>=</span> <span class=n>attn_scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>masked</span><span class=p>[:,</span> <span class=p>:,</span> <span class=p>:</span><span class=n>N</span><span class=p>,</span> <span class=p>:</span><span class=n>N</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;-inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attn_scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>                             
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>attn_dropout</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>context_vec</span> <span class=o>=</span> <span class=n>attn_weights</span> <span class=o>@</span> <span class=n>v</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>context_vec</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_o</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span></code></pre></div><p>The only change I have made here is that I use a single linear layer to obtain the key, value, and query matrices in one go. Instead of applying three separate linear layers, we can use this trick to project the input into a single tensor with three times the output dimension and then split it along the last dimension. This reduces the number of matrix multiplications, making the computation more efficient.</p><h4 id=layer-normalization>Layer Normalization<a hidden class=anchor aria-hidden=true href=#layer-normalization>#</a></h4><p>As you might recall, layer normalization improves the stability and efficiency of training. The input to this layer, $\text{x}$, has a shape of $\text{B} \times \text{N} \times \text{D}$ = [batch size, number of tokens, embedding dimension], and normalization is performed across the embedding dimension.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>LayerNorm</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>eps</span> <span class=o>=</span> <span class=mf>1e-5</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>scale</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>emb_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>shift</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>emb_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>mean</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>var</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>var</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>unbiased</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>       <span class=c1># Used biased variance estimation (without Bessel&#39;s correction)</span>
</span></span><span class=line><span class=cl>        <span class=n>norm_x</span> <span class=o>=</span> <span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>mean</span><span class=p>)</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>var</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>scale</span> <span class=o>*</span> <span class=n>norm_x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>shift</span>
</span></span></code></pre></div><ul><li>The variable <code>eps</code> is a small constant added to the variance to prevent division by zero.</li><li><code>scale</code> and <code>shift</code> are two trainable parameters that allow the model to learn optimal transformations.</li><li>We set <code>unbiased=False</code>, which means we divide by $n$ instead of $n-1$, resulting in a slightly biased variance estimate (i.e., not using Bessel&rsquo;s correction), as done in GPT models. However, this has negligible impact when the embedding dimension is large.</li></ul><h4 id=feed-forward-layer>Feed forward layer<a hidden class=anchor aria-hidden=true href=#feed-forward-layer>#</a></h4><p>Next, we will implement a small neural network used as a part of the transformer block in LLMs.</p><p>Historically, the ReLU activation function has been widely used due to its simplicity and effectiveness. However, in modern LLMs, the Gaussian Error Linear Unit (GELU) is preferred for its improved performance.</p><figure class=align-center><img loading=lazy src=../gelu.png#center alt="GELU vs ReLU activation functions"><figcaption><p>GELU vs ReLU activation functions</p></figcaption></figure><p>GELU can be thought of as a smoother version of ReLU. Its smooth transitions allow for better optimization properties during training, leading to more nuanced parameter adjustments.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>GELU</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>x</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>2.0</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>pi</span><span class=p>))</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>            <span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=mf>0.044715</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=p>))</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MLP</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layer1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>emb_dim</span><span class=p>,</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>emb_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layer2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>4</span> <span class=o>*</span> <span class=n>emb_dim</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gelu</span> <span class=o>=</span> <span class=n>GELU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>gelu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layer1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><h4 id=decoder-block>Decoder block<a hidden class=anchor aria-hidden=true href=#decoder-block>#</a></h4><p>Now, we assemble all the components we’ve built so far into a Transformer decoder block. This block forms the foundation of GPT models and is repeated multiple times throughout the architecture.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>DecoderBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>attn_pdrop</span><span class=p>,</span> <span class=n>resid_pdrop</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>masked_multi_head_sa</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_in</span><span class=o>=</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_out</span><span class=o>=</span><span class=n>d_model</span><span class=p>,</span> <span class=n>context_length</span><span class=o>=</span><span class=n>context_length</span><span class=p>,</span> <span class=n>attn_pdrop</span><span class=o>=</span><span class=n>attn_pdrop</span><span class=p>,</span> <span class=n>num_heads</span><span class=o>=</span><span class=n>num_heads</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ln1</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span> <span class=o>=</span> <span class=n>MLP</span><span class=p>(</span><span class=n>emb_dim</span><span class=o>=</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ln2</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>resid_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>resid_pdrop</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>resid_dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>masked_multi_head_sa</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ln1</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=n>attn_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mlp_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>resid_dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>mlp</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ln2</span><span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=n>mlp_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><h4 id=gpt-1-model>GPT-1 model<a hidden class=anchor aria-hidden=true href=#gpt-1-model>#</a></h4><p>With the decoder block implemented, we now have all the necessary components to build the GPT-1 architecture.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>GPT1</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>D_embd</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span> <span class=n>num_blocks</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Token Embeddings - Convert integer word tokens (vocab) to a D-dimensional embedding vector </span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>token_embedding_table</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>D_embd</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Position Encoding - Encode each position to a D-dimensional embedding vector</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>position_embedding_table</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>context_length</span><span class=p>,</span> <span class=n>D_embd</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Embedding dropout</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embd_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Define multiple Decoder blocks</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>blocks</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=p>[</span><span class=n>DecoderBlock</span><span class=p>(</span><span class=n>d_model</span><span class=o>=</span><span class=n>D_embd</span><span class=p>,</span> <span class=n>context_length</span><span class=o>=</span><span class=n>context_length</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=n>dropout</span><span class=p>,</span> <span class=n>num_heads</span><span class=o>=</span><span class=n>num_heads</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_blocks</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Final FC layer to project the D-dimensional vector back to vocab space</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>D_embd</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>N</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>token_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>token_embedding_table</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>                               <span class=c1># output: (B, N, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>pos_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>position_embedding_table</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>N</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>))</span> <span class=c1># output: (N, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>token_emb</span> <span class=o>+</span> <span class=n>pos_emb</span>                                                 <span class=c1># ouput: (B, N, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embd_dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>blocks</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>                                                <span class=c1># ouput: (B, N, vocab_size)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>logits</span>
</span></span></code></pre></div><p>Tokenized text is first converted into token embeddings, which are then augmented with positional embeddings. This combined representation forms a tensor that passes through a series of transformer blocks, outputting a tensor of the same dimensionality. The final language modeling head (a linear layer without bias) projects this output into the vocabulary space, generating logits for each token in the vocabulary.</p><h4 id=weight-initialization>Weight initialization<a hidden class=anchor aria-hidden=true href=#weight-initialization>#</a></h4><p>Since LayerNorm is used extensively throughout the model, weights were initialized from a zero-mean Gaussian distribution with a standard deviation of $0.02$, which the authors noted was sufficient.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Initialize weights as per GPT-1: Normal distribution with std=0.02</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>gpt1_init</span><span class=p>(</span><span class=n>m</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>m</span><span class=p>,</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>):</span>  <span class=c1># Applies to linear layers only</span>
</span></span><span class=line><span class=cl>        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>normal_</span><span class=p>(</span><span class=n>m</span><span class=o>.</span><span class=n>weight</span><span class=p>,</span> <span class=n>mean</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span> <span class=n>std</span><span class=o>=</span><span class=mf>0.02</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>m</span><span class=o>.</span><span class=n>bias</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>zeros_</span><span class=p>(</span><span class=n>m</span><span class=o>.</span><span class=n>bias</span><span class=p>)</span>  <span class=c1># Bias initialized to zero</span>
</span></span></code></pre></div><h3 id=pre-training>Pre-Training<a hidden class=anchor aria-hidden=true href=#pre-training>#</a></h3><p>Now, let&rsquo;s initialize a small GPT model and perform a forward pass as a sanity check.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Define hyperparameters</span>
</span></span><span class=line><span class=cl><span class=n>block_size</span> <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>16</span>              
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>GPT1</span><span class=p>(</span><span class=n>D_embd</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>vocab_size</span><span class=o>=</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>context_length</span><span class=o>=</span><span class=n>block_size</span><span class=p>,</span> <span class=n>num_heads</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>num_blocks</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Apply initialization</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>gpt1_init</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># print the number of parameters in the model</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span><span class=o>/</span><span class=mf>1e6</span><span class=p>,</span> <span class=s1>&#39;M parameters&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Forward pass with one example</span>
</span></span><span class=line><span class=cl><span class=n>xb</span><span class=p>,</span> <span class=n>yb</span> <span class=o>=</span> <span class=n>make_batch</span><span class=p>(</span><span class=s2>&#34;train&#34;</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>block_size</span><span class=o>=</span><span class=n>block_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Input shape:&#34;</span><span class=p>,</span> <span class=n>xb</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Target shape:&#34;</span><span class=p>,</span> <span class=n>yb</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>context</span><span class=p>,</span> <span class=n>target</span> <span class=o>=</span> <span class=n>xb</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>yb</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>context</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Model output shape:&#34;</span><span class=p>,</span> <span class=n>logits</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>),</span> <span class=n>target</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Initial Loss:&#34;</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span></code></pre></div><pre tabindex=0><code>0.209536 M parameters
Input shape: torch.Size([16, 32])
Target shape: torch.Size([16, 32])
Model output shape: torch.Size([16, 32, 65])
Initial Loss: 4.262292861938477
</code></pre><p>As you can see, the model outputs a tensor of shape <code>[16, 32, 65]</code>, since we passed 16 input texts with 32 tokens each. The last dimension corresponds to the vocabulary size of the tokenizer. To compute the loss, we collapse the first two dimensions, concatenating all tokens together and averaging the loss over the batch.</p><p>At initialization, all tokens in our vocabulary are equally likely, meaning each has a probability of $1/65$. If we manually compute the cross-entropy loss, we get:
$$
\text{loss} = -\ln(1/65) = 4.174
$$
This is approximately the same as our computed value, indicating that we are on the right track.</p><h4 id=weight-decay>Weight decay<a hidden class=anchor aria-hidden=true href=#weight-decay>#</a></h4><p>The GPT-1 model used the Adam optimizer with a modified version of L2 regularization—known as the AdamW optimizer, applying a weight decay $0.01$ only to non-bias and non-gain (LayerNorm) weights, meaning only Linear layer weights were regularized.</p><p>Since Linear weights are 2D tensors, while biases and LayerNorm weights are 1D, we can use this criterion to separate them into decay and non-decay parameter groups.</p><p>Here’s how to implement it in PyTorch:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Separate parameters</span>
</span></span><span class=line><span class=cl><span class=n>decay_params</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>no_decay_params</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>named_parameters</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>param</span><span class=o>.</span><span class=n>requires_grad</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>param</span><span class=o>.</span><span class=n>dim</span><span class=p>()</span> <span class=o>&gt;=</span> <span class=mi>2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>decay_params</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>param</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>no_decay_params</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>param</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define optimizer with different parameter groups</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;params&#34;</span><span class=p>:</span> <span class=n>decay_params</span><span class=p>,</span> <span class=s2>&#34;weight_decay&#34;</span><span class=p>:</span> <span class=mf>0.01</span><span class=p>},</span>   <span class=c1># Linear weights</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span><span class=s2>&#34;params&#34;</span><span class=p>:</span> <span class=n>no_decay_params</span><span class=p>,</span> <span class=s2>&#34;weight_decay&#34;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>}</span>  <span class=c1># Biases &amp; LayerNorm weights</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>lr</span><span class=o>=</span><span class=mf>1e-3</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>With that, we’re ready to pre-train the model on our dataset. 🚀</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>max_iter</span> <span class=o>=</span> <span class=mi>5000</span>
</span></span><span class=line><span class=cl><span class=n>eval_iter</span> <span class=o>=</span> <span class=mi>200</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define the loss function &amp; optimizer</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=nb>iter</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_iter</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Sample a batch of data</span>
</span></span><span class=line><span class=cl>    <span class=n>xb</span><span class=p>,</span> <span class=n>yb</span> <span class=o>=</span> <span class=n>make_batch</span><span class=p>(</span><span class=s1>&#39;train&#39;</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>block_size</span><span class=o>=</span><span class=n>block_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>context</span><span class=p>,</span> <span class=n>target</span> <span class=o>=</span> <span class=n>xb</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>yb</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Forward pass and optimization</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>context</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>logits</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>),</span> <span class=n>target</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Evaluate the loss on train and val</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>iter</span><span class=o>%</span><span class=mi>100</span> <span class=o>==</span> <span class=mi>0</span> <span class=ow>or</span> <span class=nb>iter</span><span class=o>==</span><span class=mi>0</span> <span class=ow>or</span> <span class=nb>iter</span><span class=o>==</span><span class=p>(</span><span class=n>max_iter</span><span class=o>-</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>out</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>split</span> <span class=ow>in</span> <span class=p>[</span><span class=s1>&#39;train&#39;</span><span class=p>,</span> <span class=s1>&#39;val&#39;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>                <span class=n>running_loss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>                <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>eval_iter</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                    <span class=n>xb</span><span class=p>,</span> <span class=n>yb</span> <span class=o>=</span> <span class=n>make_batch</span><span class=p>(</span><span class=n>split</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>block_size</span><span class=o>=</span><span class=n>block_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=n>context</span><span class=p>,</span> <span class=n>target</span> <span class=o>=</span> <span class=n>xb</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>yb</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>context</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>logits</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>),</span> <span class=n>target</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>                    <span class=n>running_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                <span class=n>out</span><span class=p>[</span><span class=n>split</span><span class=p>]</span> <span class=o>=</span> <span class=n>running_loss</span> <span class=o>/</span> <span class=n>eval_iter</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;</span><span class=se>\n</span><span class=s1> Step:</span><span class=si>{}</span><span class=s1>/</span><span class=si>{}</span><span class=s1>, Train Loss:</span><span class=si>{:.4f}</span><span class=s1>, Val Loss:</span><span class=si>{:.4f}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=nb>iter</span><span class=p>,</span> <span class=n>max_iter</span><span class=p>,</span> <span class=n>out</span><span class=p>[</span><span class=s1>&#39;train&#39;</span><span class=p>],</span> <span class=n>out</span><span class=p>[</span><span class=s1>&#39;val&#39;</span><span class=p>]))</span>
</span></span></code></pre></div><pre tabindex=0><code>...
Step:4999/5000, Train Loss:1.7101, Val Loss:1.8699
</code></pre><h3 id=generating-text>Generating text<a hidden class=anchor aria-hidden=true href=#generating-text>#</a></h3><p>The inference process involves generating new text based on patterns the model has learned from the training data. We start with an initial context, typically the integer token <code>0</code>, which represents the newline character in our vocabulary. This serves as the seed input to the model.</p><p>The model then outputs a probability distribution over all tokens in the vocabulary. We sample from this distribution to generate the next token. This newly generated token is appended to the existing context and fed back into the model to produce the next prediction. By iterating this process, the model generates coherent text that follows the structure and style of the training data—such as Shakespearean plays in the case of our Tiny Shakespeare dataset.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Generate text from the trained model</span>
</span></span><span class=line><span class=cl><span class=n>max_new_tokens</span> <span class=o>=</span> <span class=mi>300</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>context</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>          <span class=c1># Shape (B=1, N=1)</span>
</span></span><span class=line><span class=cl><span class=n>stored_context</span> <span class=o>=</span> <span class=n>context</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_new_tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>context</span> <span class=o>=</span> <span class=n>stored_context</span><span class=p>[:,</span> <span class=o>-</span><span class=n>block_size</span><span class=p>:]</span>                          <span class=c1># Trim the context to fit the model&#39;s context length</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>context</span><span class=p>)</span>                                            <span class=c1># logits: (B=1, N, vocab_size)</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>                                          <span class=c1># take just the last time step (B, vocab_size)</span>
</span></span><span class=line><span class=cl>        <span class=n>probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>next_token</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>multinomial</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>               <span class=c1># Sample from the distribution (B=1, N)</span>
</span></span><span class=line><span class=cl>        <span class=n>stored_context</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>stored_context</span><span class=p>,</span> <span class=n>next_token</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>    <span class=c1># (B=1, N+1)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s1>&#39;&#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>decode</span><span class=p>(</span><span class=n>stored_context</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>tolist</span><span class=p>())))</span>
</span></span></code></pre></div><pre tabindex=0><code>O this must says it toill dake to feare,
What in, I reigreaton all him draster. he spit me with hermattan?

COMANIUSIUS:
As you
my lastless; tell was with cantal you was?

QUEEN ELIZABETH:
Ay, forwas now can, and you fear&#39;d, more my mune chamnot Moounce
Let not the kise him? hy bu do to him,
Unce I 
</code></pre><p>Since we sample at every step, text generation is stochastic, meaning that even with the same initial context, we may obtain different outputs. This ensures that the generated text is diverse rather than simply memorizing the training data.</p><p>I know the generation is not very good since we&rsquo;ve trained a character-level model, and it doesn&rsquo;t grasp the meaning of words well. But still, it does a good job at generating Shakespeare-like text.</p><h2 id=summary-of-gpt-1-paper>Summary of GPT-1 paper<a hidden class=anchor aria-hidden=true href=#summary-of-gpt-1-paper>#</a></h2><p>Here are some key implementation details of the GPT-1 model.</p><ul><li><p><strong>Tokenizer</strong>: Byte Pair Encoding (BPE) with a vocabulary size of 40,000 merges. We&rsquo;ll go into more detail on this in the next post.</p></li><li><p><strong>Context length</strong>: $N = 512$ tokens.</p></li><li><p><strong>Position Embeddings</strong>: Learned instead of the sinusoidal version proposed in Transformers.</p></li><li><p><strong>Architecture hyperparameters</strong>:</p><ul><li>Number of layers (decoder blocks): $L = 12$</li><li>Number of attention heads: $H = 12$</li><li>Embedding dimension: $d_{model} = 768$</li><li>MLP size: $d_{ff} = 4 \times d_{model} = 3072$</li><li>Residual, embedding and attention dropout: $p = 0.1$</li></ul></li><li><p><strong>Pre-Training hyperparameters</strong>:</p><ul><li><strong>Initialization</strong>: Initialized from a zero-mean Gaussian distribution with a standard deviation of $0.02$.</li><li><strong>Optimizer</strong>: AdamW<ul><li>Batch size: 64 (randomly sampled)</li><li>Weight decay: 0.01</li></ul></li><li><strong>Learning rate</strong>: Increased linearly from zero to a maximum value of $2.5 \times 10^{-4}$ over the first 2000 updates, then annealed to zero using a cosine schedule.</li><li><strong>Number of epochs</strong>: 100</li></ul></li><li><p><strong>Fine-tuning hyperparameters</strong>:</p><ul><li>Dropout in the classifier: $p = 0.1$</li><li>Learning rate: $6.25e^{-5}$ with a linear decay schedule and warmup over 0.2% of training steps.</li><li>Batch size: 32</li><li>Number of epochs: 3 (sufficient for most cases).</li><li>Weight decay: 0.5</li></ul></li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li><p><a href=https://medium.com/@hugmanskj/most-successful-transformer-variants-introducing-bert-and-gpt-59cfcb7bdf77>Medium Post on Most Successful Transformer Variants: Introducing BERT and GPT</a></p></li><li><p><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Andrej Karpathy&rsquo;s video: Let&rsquo;s build GPT: from scratch, in code, spelled out.</a></p></li><li><p>[1] Devlin et al., &ldquo;<a href=https://arxiv.org/abs/1810.04805>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>&rdquo;, NAACL 2019.</p></li><li><p>[2] Radford et al., &ldquo;<a href=https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf>Improving Language Understanding by Generative Pre-Training</a>&rdquo;, OpenAI, 2018.</p></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://yugajmera.github.io/posts/09-gpt2.1/post/><span class=title>« Prev</span><br><span>Looking into GPT-2 Part 1: BPE Tokenizer</span>
</a><a class=next href=https://yugajmera.github.io/posts/07-transformer/post/><span class=title>Next »</span><br><span>Generalizing Attention with Transformers</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://yugajmera.github.io/>YA's Almanac</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>