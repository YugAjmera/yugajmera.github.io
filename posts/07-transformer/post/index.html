<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Generalizing Attention with Transformers | YA&#39;s Almanac</title>
<meta name="keywords" content="">
<meta name="description" content="In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to &ldquo;attend&rdquo; to different parts of the input at each time step while generating the output sequence.
Attention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption.">
<meta name="author" content="">
<link rel="canonical" href="https://yugajmera.github.io/posts/07-transformer/post/">
<link crossorigin="anonymous" href="https://yugajmera.github.io/assets/css/stylesheet.334cb677465a1d1e9767dd05b097c98de3a5b4085e0d43c708104df17bc49585.css" integrity="sha256-M0y2d0ZaHR6XZ90FsJfJjeOltAheDUPHCBBN8XvElYU=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://yugajmera.github.io/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yugajmera.github.io/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yugajmera.github.io/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yugajmera.github.io/assets/apple-touch-icon.png">
<link rel="mask-icon" href="https://yugajmera.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://yugajmera.github.io/posts/07-transformer/post/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
<style>
   mjx-container[display="true"] {
       margin: 1.5em 0 ! important
   }
</style>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-S759YBMKJE', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Generalizing Attention with Transformers" />
<meta property="og:description" content="In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to &ldquo;attend&rdquo; to different parts of the input at each time step while generating the output sequence.
Attention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yugajmera.github.io/posts/07-transformer/post/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-11-18T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-11-18T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Generalizing Attention with Transformers"/>
<meta name="twitter:description" content="In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to &ldquo;attend&rdquo; to different parts of the input at each time step while generating the output sequence.
Attention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yugajmera.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Generalizing Attention with Transformers",
      "item": "https://yugajmera.github.io/posts/07-transformer/post/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Generalizing Attention with Transformers",
  "name": "Generalizing Attention with Transformers",
  "description": "In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to \u0026ldquo;attend\u0026rdquo; to different parts of the input at each time step while generating the output sequence.\nAttention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption.",
  "keywords": [
    
  ],
  "articleBody": "In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to “attend” to different parts of the input at each time step while generating the output sequence.\nAttention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption.\nImage captioning task with an RNN decoder with Attention.\nGiven its usefulness, let’s abstract the attention mechanism from sequence modeling and generalize it into a layer that can be inserted into any network.\nGeneral Attention Layer (left) Attention mechanism in the image captioning task. (right) Generalized attention mechanism represented with vectors.\nRecall that in the attention mechanism:\nInput:\nFeatures: $\\mathbf{z}$ (Shape: $\\text{H} \\times \\text{W} \\times \\text{D}$) Hidden state: $\\mathbf{h}$ (Shape: $\\text{D}$) Similarity function: $f_{\\text{att}}$ Operations:\nAlignment: $e_{i, j} = f_{\\text{att}} (\\mathbf{h}, \\mathbf{z}_{i,j})$ Attention: $a = \\text{softmax} (e) $ Output:\nContext vector: $\\mathbf{c} = \\sum_{i, j} \\text{ } a_{i,j} \\text{ } \\mathbf{z}_{i,j}$ (Shape: $\\text{D}$) This mechanism can be generalized to operate on any set of vectors, making it more broadly applicable in deep learning.\nTo formalize this generalization of the attention mechanism, let’s redefine its components:\nThe input features are now represented as a set of vectors, $\\mathbf{x}$, with shape $(\\text{N} \\times \\text{D})$, where $\\text{N} = \\text{H} \\times \\text{W}$. These vectors are the elements we want to attend over.\nThe hidden state of the decoder is renamed as a query vector, $\\mathbf{q}$ (Shape: $\\text{D}$).\nThe similarity function $f_{\\text{att}}$, typically implemented as a Multi-Layer Perceptron (MLP), compares the query vector to each input vector.\nThe output context vector is denoted as $\\mathbf{y}$.\nOur general attention mechanism now looks like this:\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}$) Query vector: $\\mathbf{q}$ (Shape: $\\text{D}$) Similarity function: $f_{\\text{att}}$ Operations:\nAlignment: $e_{j} = f_{\\text{att}} (\\mathbf{q}, \\mathbf{x}_{j})$ Attention: $a = \\text{softmax} (e) $ Output:\nContext vector: $\\mathbf{y} = \\sum_j a_{j} \\text{ } \\mathbf{x}_{j}$ (Shape: $\\text{D}$) Modifications Scaled dot product for similarity function The similarity function we used earlier is called additive attention, where a feed-forward network with a single hidden layer computes the compatibility function between query vectors and input vectors.\nA more commonly used alternative is dot-product (multiplicative) attention, where the query vector and input vectors are combined using a dot product. While both methods have similar theoretical complexity, dot-product attention is computationally more efficient as it can be implemented using optimized matrix multiplication code.\nHowever, when the vector dimension $\\text{D}$ is large, the resulting alignment scores can have high magnitudes. Since the softmax function normalizes these scores to compute attention weights, large magnitudes can cause softmax to saturate, leading to vanishing gradients during backpropagation.\nTo mitigate this, we scale the dot product by $\\frac{1}{\\sqrt{\\text{D}}}$. This adjustment reduces the impact of large vector magnitudes, similar to initialization techniques like Xavier or Kaiming. This approach, known as scaled dot-product attention, is widely used in modern models.\nMultiple Query vectors At each time step of the decoder, we use one query vector (i.e., one hidden state) to compute a probability distribution over the inputs, producing one context vector.\nWe can generalize this concept to handle multiple query vectors simultaneously, each generating a corresponding output vector. This allows us to compute multiple attention context vectors in parallel.\n(left) A general attention layer. (right) The same layer with multiple query vectors.\nWith this modification, the attention layer includes:\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}$) Query vectors: $\\mathbf{q}$ (Shape: $\\text{M} \\times \\text{D}$) Similarity function: scaled dot product Operations:\nAlignment: $e_{i, j} = (\\mathbf{q}_i \\cdot \\mathbf{x}_{j} )$ / $\\sqrt{\\text{D}}$ (Shape: $\\text{M} \\times \\text{N}$) Attention: $a = \\text{softmax} (e) $ Context vectors: $\\mathbf{y}_i = \\sum_j a_{i,j} \\text{ } \\mathbf{x}_{j}$ Output:\nOutput vectors: $\\mathbf{y}$ (Shape: $\\text{M} \\times \\text{D}$) Key and Value vectors In the attention mechanism, we use the input vectors in two different ways:\nTo generate the alignment scores that compare the input vectors with each query vector via the similarity function.\nTo compute the output context vectors by taking a weighted sum of input vectors and the attention weights.\nTo handle these roles effectively, the input vectors are separated into key vectors ($\\mathbf{k}$) and value vectors ($\\mathbf{v}$). Both are derived using learnable projection matrices applied to the input vectors:\nKeys are used to compute alignment scores with the query. Values are used to construct the context vector. The separation of keys and values enables the model to use input vectors differently for comparison and retrieval. For example:\nQuery: Google search: “How tall is the Empire State Building?” Keys: Google compares the query with a set of webpages that may contain the answer. Values: Returns the webpage saying, “At its top floor, the Empire State Building stands 1,250 feet (380 meters) tall.” Since the information used for matching (keys) is different from the information returned (values), we separate them into two distinct vectors. The key determines relevance or alignment, whereas the value is used to retrieve the actual information. This gives the model flexibility in handling different types of information.\n(left) General attention layer with multiple query vectors. (right) The same layer with separate key and value vectors.\nWith these modifications, the attention layer is described as follows:\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}$) Query vectors: $\\mathbf{q}$ (Shape: $\\text{M} \\times \\color{blue}{\\text{D}_k}$) Similarity function: scaled dot product Operations:\nKey vectors: ${\\color{blue}{\\mathbf{k}}} = \\mathbf{x} {\\color{blue}{W_\\mathbf{k}}}$ (Shape: $\\text{N} \\times \\color{blue}{\\text{D}_k}$) Value vectors: ${\\color{orange}{\\mathbf{v}}} = \\mathbf{x} {\\color{orange}{W_\\mathbf{v}}}$ (Shape: $\\text{N} \\times {\\color{orange}{\\text{D}_v}}$) Alignment: $e_{i, j} = (\\mathbf{q}_i \\cdot {\\color{blue}{\\mathbf{k}_j}} )$ / $\\sqrt{\\color{blue}{\\text{D}_k}}$ Attention: $a = \\text{softmax} (e) $ Context vectors: $\\mathbf{y}_i = \\sum_j a_{i,j} \\text{ } {\\color{orange}{\\mathbf{v}_j}}$ Output:\nOutput vectors: $\\mathbf{y}$ (Shape: $\\text{M} \\times {\\color{orange}{\\text{D}_v}}$) Matrix Representation The entire attention mechanism can be expressed compactly in matrix form:\n\\begin{align} \\text{Attention} (\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax} (\\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{\\text{D}_k}}) \\mathbf{V} \\end{align}\nWhere:\n$\\mathbf{Q}$: Matrix of query vectors (Shape: $\\text{M} \\times \\text{D}_k$) $\\mathbf{K}$: Matrix of key vectors (Shape: $\\text{N} \\times \\text{D}_k$) $\\mathbf{V}$: Matrix of value vectors (Shape: $\\text{N} \\times \\text{D}_v$) This is the most common representation of attention that we’ve derived so far. Self-Attention layer One special case of the attention layer is the self-attention layer, where we only have input vectors and no explicit query vectors. In this case, we use a query matrix to derive the query vectors from our input vectors. Since each input vector serves as its own query, we end up comparing each vector in the input set with every other vector.\nThis design leverages the power of attention while eliminating the need for external query vectors, enabling the model to capture relationships between different parts of the input.\nA self-attention layer\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}$) Similarity function: scaled dot product Operations:\nKey vectors: ${\\color{blue}{\\mathbf{k}}} = \\mathbf{x} {\\color{blue}{W_\\mathbf{k}}}$\nValue vectors: ${\\color{orange}{\\mathbf{v}}} = \\mathbf{x} {\\color{orange}{W_\\mathbf{v}}}$\nQuery vectors: ${\\color{green}{\\mathbf{q}}} = \\mathbf{x} {\\color{green}{W_\\mathbf{q}}}$\nAlignment: $e_{i, j} = ({\\color{green}{\\mathbf{q}_i}} \\cdot {\\color{blue}{\\mathbf{k}_j}} ) / \\sqrt{\\color{blue}{\\text{D}_k}}$\nAttention: $a = \\text{softmax} (e) $\nContext vectors: $\\mathbf{y}_i = \\sum_j a_{i,j} \\text{ } {\\color{orange}{\\mathbf{v}_j}}$\nOutput:\nOutput vectors: $\\mathbf{y}$ (Shape: $\\text{N} \\times {\\color{orange}{\\text{D}_v}}$) This forms a new type of neural network layer, where we input a set of vectors and output another set of vectors, effectively allowing the model to attend to different parts of its own input.\nPositional encoding A key consideration with the self-attention layer is that it is permutation equivariant. This means that if we change the order of the input vectors, we would still compute the same key, value, and query vectors, but they would be permuted in the same way the input vectors were permuted. As a result, the set of output vectors would remain the same, but their order would change.\nSelf-attention layer is permutation equivariant $f(s(x)) = s(f(x))$.\nThe self-attention layer does not inherently account for the order of the input vectors; it processes them as a set, irrespective of their sequence. While this property works well for certain tasks, it poses a challenge for tasks like machine translation or text generation, where the order of tokens is crucial.\nTo make the layer position-aware, positional encodings are added to the input vectors. These encodings capture the position of each element in the sequence. A function ${\\color{purple}{pos}}: \\text{D} \\rightarrow \\mathbb{R}^d$ transforms each input vector at position $j$ into a unique, $d$-dimensional positional vector.\nConcatenate special positional encoding $\\color{purple}{p_j}$ to each input vector $\\mathbf{x}_j$\nThere are two common ways to obtain the positional encoding function $p_j = pos(j)$:\nLearnable Lookup Table:\nA lookup table is learned during training that assigns a unique encoding to each position. This approach learns parameters for each position $t \\in [0, T)$, where $T$ is the maximum sequence length, leading to a lookup table of size $T \\times d$. Fixed Function:\nA fixed function is designed that outputs a unique, deterministic encoding for each position. This approach doesn’t require any learnable parameters. Masked Self-Attention layer A variant of the self-attention layer, called masked self-attention, is used for tasks like language modeling, where the goal is to predict the next word given the previous words. This is similar to a decoder RNN, where new words in the output sequence are generated one by one, with previously generated words providing context.\nWith the standard self-attention layer, the model can attend to all input tokens at once, which isn’t ideal for such tasks. To make the model “look back” at previous words while generating the next one, we mask future tokens. This prevents the model from attending to words that come after the current position in the sequence.\nTo achieve this, we set the attention weights for all future words to zero, ensuring that the model can only attend to past tokens. This is particularly useful in decoders, where sequences are generated step-by-step.\nA Masked self-attention layer.\nMulti-Head Self-Attention layer Instead of performing a single attention function over the entire input vector space, the multi-head self-attention mechanism splits the input into multiple subspaces and performs attention in parallel across these subspaces. This allows the model to attend to different aspects of the input simultaneously.\nIn this case, we divide each input vector into $\\text{H}$ chunks of equal size and feed them into several parallel attention layers. The input to each head has dimension $\\text{d}_{in} = \\frac{\\text{D}}{\\text{H}}$, and the key and value vectors have dimensions $\\text{d}_k$ and $\\text{d}_v$ respectively.\nThe output from each attention head is then concatenated (dimension $\\text{H} \\cdot \\text{d}_v$) and passed through a linear layer to produce the final output of a desired dimension $\\text{D}_{out}$.\nA Multi-head self-attention layer\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}$) Similarity function: scaled dot product For each head $h$:\nSplit Input vectors: $\\text{X}^h$ (Shape: $\\text{N} \\times \\text{d}_{in}$)\nKey vectors: $\\mathbf{k} = \\text{X}^h W_\\mathbf{k}^h$\nValue vectors: $\\mathbf{v} = \\text{X}^h W_\\mathbf{v}^h$\nQuery vectors: $\\mathbf{q} = \\text{X}^h W_\\mathbf{q}^h$\nAlignment: $e_{i, j} = (\\mathbf{q}_i \\cdot \\mathbf{k}_j ) / \\sqrt{\\text{d}_k}$\nAttention: $a = \\text{softmax} (e) $\nContext vectors: ${\\mathbf{y}_i} = \\sum_j a_{i,j} \\text{ } \\mathbf{v}_j$\nSplit Output vectors: $\\text{Y}^h$ (Shape: $\\text{N} \\times \\text{d}_v$)\nOutput:\nOutput vectors: $\\mathbf{y} = \\text{Concat} (\\text{Y}^0, \\cdots, \\text{Y}^{\\text{H} - 1}) W_o$ (Shape: $\\text{N} \\times \\text{D}_{out}$) Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, the attention mechanism would average over these subspaces, losing valuable information.\nTransformer The Transformer [1] was the first sequence model that relies solely on self-attention to compute representations of its input and output without using any recurrent units. This innovation marked a turning point for natural language processing (NLP), outperforming all state-of-the-art models of its time. It is often referred to as the “ImageNet moment” for NLP.\nLet’s take a closer look at the complete architecture of the Transformer.\nEmbeddings and Positional Encoding The input and output word tokens are converted into vectors of dimension $\\text{D} = d_{\\text{model}} = 512$ by learned embeddings.\nNext, positional encodings are added to input embedding vectors to inject information about the position of the tokens in the sequence. A fixed sinusoidal function is used to compute the positional encodings, which are of the same dimension $d_\\text{model}$ as the embeddings, so that the two can be summed together.\nEncoder block The encoder is composed of a stack of $N = 6$ identical layers, each consisting of two sub-layers:\nMulti-Head Self-Attention: Each output from this layer depends on every input, allowing for interactions between all vectors in the input sequence. Since the inputs come from the previous encoder layer, each position in the encoder can attend to all positions in the previous layer.\n$\\text{H} = 8$ $d_k = d_v = d_{in} = d_{\\text{model}}/H = 64$ $\\text{D}_{out} = 512$ Feed-forward network: Each input vector passes through an MLP independently, consisting of two linear layers and a ReLU activation in between.\nLinear 1 $(512, 2048)$ -\u003e ReLU -\u003e Linear 2 $(2048, 512)$ Additionally:\nResidual connection: A residual connection is used around each of the two sub-layers to improve the gradient flow through the model.\nLayer Normalization: Each sub-layer is followed by layer normalization to aid optimization (similar to BatchNorm in convolutional layers). Note that LayerNorm acts on each vector independently.\nThe Encoder of the Transformer.\nInput: A set of vectors $\\mathbf{x}$ (Shape: $\\text{N} \\times 512$) Output: A set of vectors $\\mathbf{y}$ (Shape: $\\text{N} \\times 512$) The interaction between vectors occurs only in the self-attention layer. LayerNorm and MLP operate on each input vector independently.\nDecoder block The decoder consists of $N = 6$ identical layers, each with three sub-layers:\nMasked Multi-Head Self-Attention: To prevent the decoder from attending to future tokens in the output sequence, we use masked self-attention, which ensures that each position in the decoder can only attend to past and current tokens.\nOperates with the same configuration: $\\text{H} = 8$, $d_{in}= 64$, $\\text{D}_{out} = 512$. Multi-Head Attention over Encoder outputs: This layer allows each position in the decoder to attend to all positions in the input sequence, passing relevant context from the encoder. This mimics the traditional encoder-decoder attention mechanism used in seq2seq models.\nSame configuration: $\\text{H} = 8$, $d_{in}= 64$, $\\text{D}_{out} = 512$. Feed-forward network: Same structure as in the encoder:\nLinear 1 (512, 2048) -\u003e ReLU -\u003e Linear 2 (2048, 512) Similar to the encoder, residual connections are added around each of the sub-layers, followed by layer normalization acting on each vector independently.\nThe Decoder of the Transformer.\nInput: Decoder sequence: A set of vectors $\\mathbf{x}$ (Shape: $\\text{M} \\times 512$) Encoder context: A set of context vectors $\\mathbf{c}$ (Shape: $\\text{N} \\times 512$) Output: A set of vectors $\\mathbf{y}$ (Shape: $\\text{M} \\times 512$) The masked self-attention sub-layer ensures autoregressive behavior by restricting attention to past inputs. The multi-head attention over encoder outputs bridges the encoder and decoder, allowing the decoder to focus on relevant parts of the input sequence.\nThe decoder block is followed by a linear layer and softmax function to convert the decoder output into predicted next-token probabilities.\nKey characteristics Parallel computation: Unlike RNNs, the Transformer processes entire sequences simultaneously, allowing all alignment and attention scores for all inputs to be computed in parallel, significantly improving efficiency on large datasets.\nFlexibility of inputs: It can effectively handle both unordered sets and ordered sequences (with positional encodings).\nGlobal context: Self-attention captures long-range dependencies across the sequence.\nThe Transformer’s innovative architecture has become the foundation for many advancements in NLP, inspiring models like BERT and GPT.\nReferences [1] Vaswani et al, “Attention is all you need”, NeurIPS 2017.\n",
  "wordCount" : "2554",
  "inLanguage": "en",
  "datePublished": "2024-11-18T00:00:00Z",
  "dateModified": "2024-11-18T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yugajmera.github.io/posts/07-transformer/post/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "YA's Almanac",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yugajmera.github.io/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yugajmera.github.io/" accesskey="h" title="YA&#39;s Almanac (Alt + H)">YA&#39;s Almanac</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yugajmera.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Generalizing Attention with Transformers
    </h1>
    <div class="post-meta"><span title='2024-11-18 00:00:00 +0000 UTC'>November 18, 2024</span>&nbsp;·&nbsp;12 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#general-attention-layer" aria-label="General Attention Layer">General Attention Layer</a><ul>
                        
                <li>
                    <a href="#modifications" aria-label="Modifications">Modifications</a><ul>
                        
                <li>
                    <a href="#scaled-dot-product-for-similarity-function" aria-label="Scaled dot product for similarity function">Scaled dot product for similarity function</a></li>
                <li>
                    <a href="#multiple-query-vectors" aria-label="Multiple Query vectors">Multiple Query vectors</a></li>
                <li>
                    <a href="#key-and-value-vectors" aria-label="Key and Value vectors">Key and Value vectors</a></li></ul>
                </li>
                <li>
                    <a href="#matrix-representation" aria-label="Matrix Representation">Matrix Representation</a></li></ul>
                </li>
                <li>
                    <a href="#self-attention-layer" aria-label="Self-Attention layer">Self-Attention layer</a><ul>
                        
                <li>
                    <a href="#positional-encoding" aria-label="Positional encoding">Positional encoding</a></li>
                <li>
                    <a href="#masked-self-attention-layer" aria-label="Masked Self-Attention layer">Masked Self-Attention layer</a></li>
                <li>
                    <a href="#multi-head-self-attention-layer" aria-label="Multi-Head Self-Attention layer">Multi-Head Self-Attention layer</a></li></ul>
                </li>
                <li>
                    <a href="#transformer" aria-label="Transformer">Transformer</a><ul>
                        
                <li>
                    <a href="#embeddings-and-positional-encoding" aria-label="Embeddings and Positional Encoding">Embeddings and Positional Encoding</a></li>
                <li>
                    <a href="#encoder-block" aria-label="Encoder block">Encoder block</a></li>
                <li>
                    <a href="#decoder-block" aria-label="Decoder block">Decoder block</a></li>
                <li>
                    <a href="#key-characteristics" aria-label="Key characteristics">Key characteristics</a></li></ul>
                </li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to &ldquo;attend&rdquo; to different parts of the input at each time step while generating the output sequence.</p>
<p>Attention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption.</p>
<figure class="align-center ">
    <img loading="lazy" src="../image-captioning.png#center"
         alt="Image captioning task with an RNN decoder with Attention."/> <figcaption>
            <p>Image captioning task with an RNN decoder with Attention.</p>
        </figcaption>
</figure>

<p>Given its usefulness, let&rsquo;s abstract the attention mechanism from sequence modeling and generalize it into a layer that can be inserted into any network.</p>
<h2 id="general-attention-layer">General Attention Layer<a hidden class="anchor" aria-hidden="true" href="#general-attention-layer">#</a></h2>
<figure class="align-center ">
    <img loading="lazy" src="../attention-layer-1.png#center"
         alt="(left) Attention mechanism in the image captioning task. (right) Generalized attention mechanism represented with vectors."/> <figcaption>
            <p>(left) Attention mechanism in the image captioning task. (right) Generalized attention mechanism represented with vectors.</p>
        </figcaption>
</figure>

<p>Recall that in the attention mechanism:</p>
<ul>
<li>
<p>Input:</p>
<ul>
<li>Features: $\mathbf{z}$ (Shape: $\text{H} \times \text{W} \times \text{D}$)</li>
<li>Hidden state: $\mathbf{h}$    (Shape: $\text{D}$)</li>
<li>Similarity function: $f_{\text{att}}$</li>
</ul>
</li>
<li>
<p>Operations:</p>
<ul>
<li>Alignment: $e_{i, j} = f_{\text{att}} (\mathbf{h}, \mathbf{z}_{i,j})$</li>
<li>Attention: $a =  \text{softmax} (e) $</li>
</ul>
</li>
<li>
<p>Output:</p>
<ul>
<li>Context vector: $\mathbf{c} = \sum_{i, j} \text{ } a_{i,j} \text{ } \mathbf{z}_{i,j}$ (Shape: $\text{D}$)</li>
</ul>
</li>
</ul>
<p>This mechanism can be generalized to operate on any set of vectors, making it more broadly applicable in deep learning.</p>
<p>To formalize this generalization of the attention mechanism, let’s redefine its components:</p>
<ol>
<li>
<p>The input features are now represented as a set of vectors, $\mathbf{x}$, with shape $(\text{N} \times \text{D})$, where $\text{N} = \text{H} \times \text{W}$. These vectors are the elements we want to attend over.</p>
</li>
<li>
<p>The hidden state of the decoder is renamed as a query vector, $\mathbf{q}$ (Shape: $\text{D}$).</p>
</li>
<li>
<p>The similarity function $f_{\text{att}}$, typically implemented as a Multi-Layer Perceptron (MLP), compares the query vector to each input vector.</p>
</li>
<li>
<p>The output context vector is denoted as $\mathbf{y}$.</p>
</li>
</ol>
<p>Our general attention mechanism now looks like this:</p>
<ul>
<li>
<p>Input:</p>
<ul>
<li>Input vectors: $\mathbf{x}$               (Shape: $\text{N} \times \text{D}$)</li>
<li>Query vector: $\mathbf{q}$                (Shape: $\text{D}$)</li>
<li>Similarity function: $f_{\text{att}}$</li>
</ul>
</li>
<li>
<p>Operations:</p>
<ul>
<li>Alignment: $e_{j} = f_{\text{att}} (\mathbf{q}, \mathbf{x}_{j})$</li>
<li>Attention: $a =  \text{softmax} (e) $</li>
</ul>
</li>
<li>
<p>Output:</p>
<ul>
<li>Context vector: $\mathbf{y} = \sum_j a_{j} \text{ } \mathbf{x}_{j}$ (Shape: $\text{D}$)</li>
</ul>
</li>
</ul>
<h3 id="modifications">Modifications<a hidden class="anchor" aria-hidden="true" href="#modifications">#</a></h3>
<h4 id="scaled-dot-product-for-similarity-function">Scaled dot product for similarity function<a hidden class="anchor" aria-hidden="true" href="#scaled-dot-product-for-similarity-function">#</a></h4>
<p>The similarity function we used earlier is called additive attention, where a feed-forward network with a single hidden layer computes the compatibility function between query vectors and input vectors.</p>
<p>A more commonly used alternative is dot-product (multiplicative) attention, where the query vector and input vectors are combined using a dot product. While both methods have similar theoretical complexity, dot-product attention is computationally more efficient as it can be implemented using optimized matrix multiplication code.</p>
<p>However, when the vector dimension $\text{D}$ is large, the resulting alignment scores can have high magnitudes. Since the softmax function normalizes these scores to compute attention weights, large magnitudes can cause softmax to saturate, leading to vanishing gradients during backpropagation.</p>
<p>To mitigate this, we scale the dot product by $\frac{1}{\sqrt{\text{D}}}$. This adjustment reduces the impact of large vector magnitudes, similar to initialization techniques like Xavier or Kaiming. This approach, known as scaled dot-product attention, is widely used in modern models.</p>
<h4 id="multiple-query-vectors">Multiple Query vectors<a hidden class="anchor" aria-hidden="true" href="#multiple-query-vectors">#</a></h4>
<p>At each time step of the decoder, we use one query vector (i.e., one hidden state) to compute a probability distribution over the inputs, producing one context vector.</p>
<p>We can generalize this concept to handle multiple query vectors simultaneously, each generating a corresponding output vector. This allows us to compute multiple attention context vectors in parallel.</p>
<figure class="align-center ">
    <img loading="lazy" src="../attention-layer-2.png#center"
         alt="(left) A general attention layer. (right) The same layer with multiple query vectors."/> <figcaption>
            <p>(left) A general attention layer. (right) The same layer with multiple query vectors.</p>
        </figcaption>
</figure>

<p>With this modification, the attention layer includes:</p>
<ul>
<li>
<p>Input:</p>
<ul>
<li>Input vectors: $\mathbf{x}$  (Shape: $\text{N} \times \text{D}$)</li>
<li>Query vectors: $\mathbf{q}$   (Shape: $\text{M} \times \text{D}$)</li>
<li>Similarity function: scaled dot product</li>
</ul>
</li>
<li>
<p>Operations:</p>
<ul>
<li>Alignment: $e_{i, j} = (\mathbf{q}_i \cdot \mathbf{x}_{j} )$ / $\sqrt{\text{D}}$  (Shape: $\text{M} \times \text{N}$)</li>
<li>Attention: $a =  \text{softmax} (e) $</li>
<li>Context vectors: $\mathbf{y}_i = \sum_j a_{i,j} \text{ } \mathbf{x}_{j}$</li>
</ul>
</li>
<li>
<p>Output:</p>
<ul>
<li>Output vectors: $\mathbf{y}$ (Shape: $\text{M} \times \text{D}$)</li>
</ul>
</li>
</ul>
<h4 id="key-and-value-vectors">Key and Value vectors<a hidden class="anchor" aria-hidden="true" href="#key-and-value-vectors">#</a></h4>
<p>In the attention mechanism, we use the input vectors in two different ways:</p>
<ol>
<li>
<p>To generate the alignment scores that compare the input vectors with each query vector via the similarity function.</p>
</li>
<li>
<p>To compute the output context vectors by taking a weighted sum of input vectors and the attention weights.</p>
</li>
</ol>
<p>To handle these roles effectively, the input vectors are separated into key vectors ($\mathbf{k}$) and value vectors ($\mathbf{v}$). Both are derived using learnable projection matrices applied to the input vectors:</p>
<ul>
<li>Keys are used to compute alignment scores with the query.</li>
<li>Values are used to construct the context vector.</li>
</ul>
<p>The separation of keys and values enables the model to use input vectors differently for comparison and retrieval. For example:</p>
<ul>
<li>Query: Google search: &ldquo;How tall is the Empire State Building?&rdquo;</li>
<li>Keys: Google compares the query with a set of webpages that may contain the answer.</li>
<li>Values: Returns the webpage saying, “At its top floor, the Empire State Building stands 1,250 feet (380 meters) tall.”</li>
</ul>
<p>Since the information used for matching (keys) is different from the information returned (values), we separate them into two distinct vectors. The key determines relevance or alignment, whereas the value is used to retrieve the actual information. This gives the model flexibility in handling different types of information.</p>
<figure class="align-center ">
    <img loading="lazy" src="../attention-layer-3.png#center"
         alt="(left) General attention layer with multiple query vectors. (right) The same layer with separate key and value vectors."/> <figcaption>
            <p>(left) General attention layer with multiple query vectors. (right) The same layer with separate key and value vectors.</p>
        </figcaption>
</figure>

<p>With these modifications, the attention layer is described as follows:</p>
<ul>
<li>
<p>Input:</p>
<ul>
<li>Input vectors: $\mathbf{x}$  (Shape: $\text{N} \times \text{D}$)</li>
<li>Query vectors: $\mathbf{q}$   (Shape: $\text{M} \times \color{blue}{\text{D}_k}$)</li>
<li>Similarity function: scaled dot product</li>
</ul>
</li>
<li>
<p>Operations:</p>
<ul>
<li>Key vectors: ${\color{blue}{\mathbf{k}}} = \mathbf{x} {\color{blue}{W_\mathbf{k}}}$ (Shape: $\text{N} \times \color{blue}{\text{D}_k}$)</li>
<li>Value vectors: ${\color{orange}{\mathbf{v}}} = \mathbf{x} {\color{orange}{W_\mathbf{v}}}$ (Shape: $\text{N} \times {\color{orange}{\text{D}_v}}$)</li>
<li>Alignment: $e_{i, j} = (\mathbf{q}_i  \cdot {\color{blue}{\mathbf{k}_j}} )$ / $\sqrt{\color{blue}{\text{D}_k}}$</li>
<li>Attention: $a =  \text{softmax} (e) $</li>
<li>Context vectors: $\mathbf{y}_i = \sum_j a_{i,j} \text{ } {\color{orange}{\mathbf{v}_j}}$</li>
</ul>
</li>
<li>
<p>Output:</p>
<ul>
<li>Output vectors: $\mathbf{y}$ (Shape: $\text{M} \times {\color{orange}{\text{D}_v}}$)</li>
</ul>
</li>
</ul>
<h3 id="matrix-representation">Matrix Representation<a hidden class="anchor" aria-hidden="true" href="#matrix-representation">#</a></h3>
<p>The entire attention mechanism can be expressed compactly in matrix form:</p>
<p>\begin{align}
\text{Attention} (\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax} (\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{\text{D}_k}}) \mathbf{V}
\end{align}</p>
<p>Where:</p>
<ul>
<li>$\mathbf{Q}$: Matrix of query vectors (Shape: $\text{M} \times \text{D}_k$)</li>
<li>$\mathbf{K}$: Matrix of key vectors (Shape: $\text{N} \times \text{D}_k$)</li>
<li>$\mathbf{V}$: Matrix of value vectors (Shape: $\text{N} \times \text{D}_v$)
This is the most common representation of attention that we’ve derived so far.</li>
</ul>
<h2 id="self-attention-layer">Self-Attention layer<a hidden class="anchor" aria-hidden="true" href="#self-attention-layer">#</a></h2>
<p>One special case of the attention layer is the self-attention layer, where we only have input vectors and no explicit query vectors. In this case, we use a query matrix to derive the query vectors from our input vectors. Since each input vector serves as its own query, we end up comparing each vector in the input set with every other vector.</p>
<p>This design leverages the power of attention while eliminating the need for external query vectors, enabling the model to capture relationships between different parts of the input.</p>
<figure class="align-center ">
    <img loading="lazy" src="../self-attention-layer.png#center"
         alt="A self-attention layer" width="350"/> <figcaption>
            <p>A self-attention layer</p>
        </figcaption>
</figure>

<ul>
<li>
<p>Input:</p>
<ul>
<li>Input vectors: $\mathbf{x}$  (Shape: $\text{N} \times \text{D}$)</li>
<li>Similarity function: scaled dot product</li>
</ul>
</li>
<li>
<p>Operations:</p>
<ul>
<li>
<p>Key vectors: ${\color{blue}{\mathbf{k}}} = \mathbf{x} {\color{blue}{W_\mathbf{k}}}$</p>
</li>
<li>
<p>Value vectors: ${\color{orange}{\mathbf{v}}} = \mathbf{x} {\color{orange}{W_\mathbf{v}}}$</p>
</li>
<li>
<p>Query vectors: ${\color{green}{\mathbf{q}}} = \mathbf{x} {\color{green}{W_\mathbf{q}}}$</p>
</li>
<li>
<p>Alignment: $e_{i, j} = ({\color{green}{\mathbf{q}_i}}  \cdot {\color{blue}{\mathbf{k}_j}} ) / \sqrt{\color{blue}{\text{D}_k}}$</p>
</li>
<li>
<p>Attention: $a =  \text{softmax} (e) $</p>
</li>
<li>
<p>Context vectors: $\mathbf{y}_i = \sum_j a_{i,j} \text{ } {\color{orange}{\mathbf{v}_j}}$</p>
</li>
</ul>
</li>
<li>
<p>Output:</p>
<ul>
<li>Output vectors: $\mathbf{y}$ (Shape: $\text{N} \times {\color{orange}{\text{D}_v}}$)</li>
</ul>
</li>
</ul>
<p>This forms a new type of neural network layer, where we input a set of vectors and output another set of vectors, effectively allowing the model to attend to different parts of its own input.</p>
<h3 id="positional-encoding">Positional encoding<a hidden class="anchor" aria-hidden="true" href="#positional-encoding">#</a></h3>
<p>A key consideration with the self-attention layer is that it is permutation equivariant. This means that if we change the order of the input vectors, we would still compute the same key, value, and query vectors, but they would be permuted in the same way the input vectors were permuted. As a result, the set of output vectors would remain the same, but their order would change.</p>
<figure class="align-center ">
    <img loading="lazy" src="../position-permutation.png#center"
         alt="Self-attention layer is permutation equivariant $f(s(x)) = s(f(x))$." width="600"/> <figcaption>
            <p>Self-attention layer is permutation equivariant $f(s(x)) = s(f(x))$.</p>
        </figcaption>
</figure>

<p>The self-attention layer does not inherently account for the order of the input vectors; it processes them as a set, irrespective of their sequence. While this property works well for certain tasks, it poses a challenge for tasks like machine translation or text generation, where the order of tokens is crucial.</p>
<p>To make the layer position-aware, positional encodings are added to the input vectors. These encodings capture the position of each element in the sequence. A function ${\color{purple}{pos}}: \text{D} \rightarrow \mathbb{R}^d$ transforms each input vector at position $j$ into a unique, $d$-dimensional positional vector.</p>
<figure class="align-center ">
    <img loading="lazy" src="../positional-encoding.png#center"
         alt="Concatenate special positional encoding $\color{purple}{p_j}$ to each input vector $\mathbf{x}_j$" width="250"/> <figcaption>
            <p>Concatenate special positional encoding $\color{purple}{p_j}$ to each input vector $\mathbf{x}_j$</p>
        </figcaption>
</figure>

<p>There are two common ways to obtain the positional encoding function $p_j = pos(j)$:</p>
<ol>
<li>
<p>Learnable Lookup Table:</p>
<ul>
<li>A lookup table is learned during training that assigns a unique encoding to each position.</li>
<li>This approach learns parameters for each position $t \in [0, T)$, where $T$ is the maximum sequence length, leading to a lookup table of size $T \times d$.</li>
</ul>
</li>
<li>
<p>Fixed Function:</p>
<ul>
<li>A fixed function is designed that outputs a unique, deterministic encoding for each position.</li>
<li>This approach doesn’t require any learnable parameters.</li>
</ul>
</li>
</ol>
<h3 id="masked-self-attention-layer">Masked Self-Attention layer<a hidden class="anchor" aria-hidden="true" href="#masked-self-attention-layer">#</a></h3>
<p>A variant of the self-attention layer, called masked self-attention, is used for tasks like language modeling, where the goal is to predict the next word given the previous words. This is similar to a decoder RNN, where new words in the output sequence are generated one by one, with previously generated words providing context.</p>
<p>With the standard self-attention layer, the model can attend to all input tokens at once, which isn&rsquo;t ideal for such tasks. To make the model &ldquo;look back&rdquo; at previous words while generating the next one, we mask future tokens. This prevents the model from attending to words that come after the current position in the sequence.</p>
<p>To achieve this, we set the attention weights for all future words to zero, ensuring that the model can only attend to past tokens. This is particularly useful in decoders, where sequences are generated step-by-step.</p>
<figure class="align-center ">
    <img loading="lazy" src="../masked-self-attention.png#center"
         alt="A Masked self-attention layer." width="350"/> <figcaption>
            <p>A Masked self-attention layer.</p>
        </figcaption>
</figure>

<h3 id="multi-head-self-attention-layer">Multi-Head Self-Attention layer<a hidden class="anchor" aria-hidden="true" href="#multi-head-self-attention-layer">#</a></h3>
<p>Instead of performing a single attention function over the entire input vector space, the multi-head self-attention mechanism splits the input into multiple subspaces and performs attention in parallel across these subspaces. This allows the model to attend to different aspects of the input simultaneously.</p>
<p>In this case, we divide each input vector into $\text{H}$ chunks of equal size and feed them into several parallel attention layers. The input to each head has dimension $\text{d}_{in} = \frac{\text{D}}{\text{H}}$, and the key and value vectors have dimensions $\text{d}_k$ and $\text{d}_v$ respectively.</p>
<p>The output from each attention head is then concatenated (dimension $\text{H} \cdot \text{d}_v$) and passed through a linear layer to produce the final output of a desired dimension $\text{D}_{out}$.</p>
<figure class="align-center ">
    <img loading="lazy" src="../multi-head-self-attention-layer.png#center"
         alt="A Multi-head self-attention layer" width="600"/> <figcaption>
            <p>A Multi-head self-attention layer</p>
        </figcaption>
</figure>

<ul>
<li>
<p>Input:</p>
<ul>
<li>Input vectors: $\mathbf{x}$  (Shape: $\text{N} \times \text{D}$)</li>
<li>Similarity function: scaled dot product</li>
</ul>
</li>
<li>
<p>For each head $h$:</p>
<ul>
<li>
<p>Split Input vectors: $\text{X}^h$  (Shape: $\text{N} \times \text{d}_{in}$)</p>
</li>
<li>
<p>Key vectors: $\mathbf{k} = \text{X}^h W_\mathbf{k}^h$</p>
</li>
<li>
<p>Value vectors: $\mathbf{v} = \text{X}^h W_\mathbf{v}^h$</p>
</li>
<li>
<p>Query vectors: $\mathbf{q} = \text{X}^h W_\mathbf{q}^h$</p>
</li>
<li>
<p>Alignment: $e_{i, j} = (\mathbf{q}_i  \cdot \mathbf{k}_j ) / \sqrt{\text{d}_k}$</p>
</li>
<li>
<p>Attention: $a =  \text{softmax} (e) $</p>
</li>
<li>
<p>Context vectors: ${\mathbf{y}_i} = \sum_j a_{i,j} \text{ } \mathbf{v}_j$</p>
</li>
<li>
<p>Split Output vectors: $\text{Y}^h$ (Shape: $\text{N} \times \text{d}_v$)</p>
</li>
</ul>
</li>
<li>
<p>Output:</p>
<ul>
<li>Output vectors: $\mathbf{y} = \text{Concat} (\text{Y}^0, \cdots, \text{Y}^{\text{H} - 1}) W_o$ (Shape: $\text{N} \times \text{D}_{out}$)</li>
</ul>
</li>
</ul>
<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, the attention mechanism would average over these subspaces, losing valuable information.</p>
<h2 id="transformer">Transformer<a hidden class="anchor" aria-hidden="true" href="#transformer">#</a></h2>
<p>The Transformer <a href="#references">[1]</a> was the first sequence model that relies solely on self-attention to compute representations of its input and output without using any recurrent units. This innovation marked a turning point for natural language processing (NLP), outperforming all state-of-the-art models of its time. It is often referred to as the &ldquo;ImageNet moment&rdquo; for NLP.</p>
<p>Let&rsquo;s take a closer look at the complete architecture of the Transformer.</p>
<h3 id="embeddings-and-positional-encoding">Embeddings and Positional Encoding<a hidden class="anchor" aria-hidden="true" href="#embeddings-and-positional-encoding">#</a></h3>
<p>The input and output word tokens are converted into vectors of dimension $\text{D} = d_{\text{model}} = 512$ by learned embeddings.</p>
<p>Next, positional encodings are added to input embedding vectors to inject information about the position of the tokens in the sequence. A fixed sinusoidal function is used to compute the positional encodings, which are of the same dimension $d_\text{model}$ as the embeddings, so that the two can be summed together.</p>
<figure class="align-center ">
    <img loading="lazy" src="../sin-cos-encoding.png#center" width="400"/> 
</figure>

<h3 id="encoder-block">Encoder block<a hidden class="anchor" aria-hidden="true" href="#encoder-block">#</a></h3>
<p>The encoder is composed of a stack of $N = 6$ identical layers, each consisting of two sub-layers:</p>
<ol>
<li>
<p><strong>Multi-Head Self-Attention</strong>: Each output from this layer depends on every input, allowing for interactions between all vectors in the input sequence. Since the inputs come from the previous encoder layer, each position in the encoder can attend to all positions in the previous layer.</p>
<ul>
<li>$\text{H} = 8$</li>
<li>$d_k = d_v = d_{in} = d_{\text{model}}/H = 64$</li>
<li>$\text{D}_{out} = 512$</li>
</ul>
</li>
<li>
<p><strong>Feed-forward network</strong>: Each input vector passes through an MLP independently, consisting of two linear layers and a ReLU activation in between.</p>
<ul>
<li>Linear 1 $(512, 2048)$ -&gt; ReLU -&gt; Linear 2 $(2048, 512)$</li>
</ul>
</li>
</ol>
<p>Additionally:</p>
<ul>
<li>
<p><strong>Residual connection</strong>: A residual connection is used around each of the two sub-layers to improve the gradient flow through the model.</p>
</li>
<li>
<p><strong>Layer Normalization</strong>: Each sub-layer is followed by layer normalization to aid optimization (similar to BatchNorm in convolutional layers). Note that LayerNorm acts on each vector independently.</p>
</li>
</ul>
<figure class="align-center ">
    <img loading="lazy" src="../transformer-encoder.png#center"
         alt="The Encoder of the Transformer." width="600"/> <figcaption>
            <p>The Encoder of the Transformer.</p>
        </figcaption>
</figure>

<ul>
<li>Input: A set of vectors $\mathbf{x}$  (Shape: $\text{N} \times 512$)</li>
<li>Output: A set of vectors $\mathbf{y}$ (Shape: $\text{N} \times 512$)</li>
</ul>
<p>The interaction between vectors occurs only in the self-attention layer. LayerNorm and MLP operate on each input vector independently.</p>
<h3 id="decoder-block">Decoder block<a hidden class="anchor" aria-hidden="true" href="#decoder-block">#</a></h3>
<p>The decoder consists of $N = 6$ identical layers, each with three sub-layers:</p>
<ol>
<li>
<p><strong>Masked Multi-Head Self-Attention</strong>: To prevent the decoder from attending to future tokens in the output sequence, we use masked self-attention, which ensures that each position in the decoder can only attend to past and current tokens.</p>
<ul>
<li>Operates with the same configuration: $\text{H} = 8$, $d_{in}= 64$, $\text{D}_{out} = 512$.</li>
</ul>
</li>
<li>
<p><strong>Multi-Head Attention over Encoder outputs</strong>: This layer allows each position in the decoder to attend to all positions in the input sequence, passing relevant context from the encoder. This mimics the traditional encoder-decoder attention mechanism used in seq2seq models.</p>
<ul>
<li>Same configuration: $\text{H} = 8$, $d_{in}= 64$, $\text{D}_{out} = 512$.</li>
</ul>
</li>
<li>
<p><strong>Feed-forward network</strong>: Same structure as in the encoder:</p>
<ul>
<li>Linear 1 (512, 2048) -&gt; ReLU -&gt; Linear 2 (2048, 512)</li>
</ul>
</li>
</ol>
<p>Similar to the encoder, residual connections are added around each of the sub-layers, followed by layer normalization acting on each vector independently.</p>
<figure class="align-center ">
    <img loading="lazy" src="../transformer-decoder.png#center"
         alt="The Decoder of the Transformer." width="700"/> <figcaption>
            <p>The Decoder of the Transformer.</p>
        </figcaption>
</figure>

<ul>
<li>Input:
<ul>
<li>Decoder sequence: A set of vectors $\mathbf{x}$  (Shape: $\text{M} \times 512$)</li>
<li>Encoder context: A set of context vectors $\mathbf{c}$  (Shape: $\text{N} \times 512$)</li>
</ul>
</li>
<li>Output: A set of vectors $\mathbf{y}$ (Shape: $\text{M} \times 512$)</li>
</ul>
<p>The masked self-attention sub-layer ensures autoregressive behavior by restricting attention to past inputs. The multi-head attention over encoder outputs bridges the encoder and decoder, allowing the decoder to focus on relevant parts of the input sequence.</p>
<p>The decoder block is followed by a linear layer and softmax function to convert the decoder output into predicted next-token probabilities.</p>
<h3 id="key-characteristics">Key characteristics<a hidden class="anchor" aria-hidden="true" href="#key-characteristics">#</a></h3>
<ul>
<li>
<p><strong>Parallel computation</strong>: Unlike RNNs, the Transformer processes entire sequences simultaneously, allowing all alignment and attention scores for all inputs to be computed in parallel, significantly improving efficiency on large datasets.</p>
</li>
<li>
<p><strong>Flexibility of inputs</strong>: It can effectively handle both unordered sets and ordered sequences (with positional encodings).</p>
</li>
<li>
<p><strong>Global context</strong>: Self-attention captures long-range dependencies across the sequence.</p>
</li>
</ul>
<p>The Transformer&rsquo;s innovative architecture has become the foundation for many advancements in NLP, inspiring models like BERT and GPT.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1] Vaswani et al, &ldquo;<a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>&rdquo;, NeurIPS 2017.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://yugajmera.github.io/posts/06-attention/post/">
    <span class="title">Next »</span>
    <br>
    <span>Sequence Modeling with Recurrent Neural Networks and Attention</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://yugajmera.github.io/">YA&#39;s Almanac</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
