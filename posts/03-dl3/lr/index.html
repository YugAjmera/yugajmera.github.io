<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Deep Learning Part 3: The Cherry on Top | YA&#39;s Almanac</title>
<meta name="keywords" content="">
<meta name="description" content="We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.
Regularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/03-dl3/lr/">
<link crossorigin="anonymous" href="http://localhost:1313/assets/css/stylesheet.334cb677465a1d1e9767dd05b097c98de3a5b4085e0d43c708104df17bc49585.css" integrity="sha256-M0y2d0ZaHR6XZ90FsJfJjeOltAheDUPHCBBN8XvElYU=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/03-dl3/lr/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
<style>
   mjx-container[display="true"] {
       margin: 1.5em 0 ! important
   }
</style>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-S759YBMKJE', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Deep Learning Part 3: The Cherry on Top" />
<meta property="og:description" content="We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.
Regularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/03-dl3/lr/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-10-08T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-10-08T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deep Learning Part 3: The Cherry on Top"/>
<meta name="twitter:description" content="We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.
Regularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Deep Learning Part 3: The Cherry on Top",
      "item": "http://localhost:1313/posts/03-dl3/lr/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Deep Learning Part 3: The Cherry on Top",
  "name": "Deep Learning Part 3: The Cherry on Top",
  "description": "We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.\nRegularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set.",
  "keywords": [
    
  ],
  "articleBody": "We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.\nRegularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set. But in practice, this doesn’t always work as expected! When we focus solely on minimizing loss on the training data—essentially trying to fit it perfectly—we run the risk of not generalizing well to the test data, a phenomenon known as overfitting.\nTo illustrate this, let’s consider an example. The blue dots in the figure below represent the training data. We fit two models: $m_1$ (a polynomial model) and $m_2$ (a linear model). Model $m_2$ has more layers, leading to a more complex decision boundary that fits the training data almost perfectly.\nIt’s important to remember that both the training and test sets are assumed to be sampled from a common dataset, represented as $p_{\\text{data}}$. Our goal should be to fit this probability distribution and not just the training data alone, as this ensures good performance on unseen test data. Now, let’s examine the test set, represented by the yellow points in the next figure.\nWhile $m_2$ fits the training data perfectly, it struggles with the test data because it overfits the training set. Model $m_1$, despite being simpler, performs better on the test set as it generalizes better to the underlying distribution $p_{\\text{data}}$.\nIf a simpler model like $m_1$ performs better on the test set, you might wonder why we don’t always use smaller models. The reason is that a smaller model runs the risk of underfitting, where it fails to capture the complexities of the data.\nSince the true distribution $p_{\\text{data}}$ is unknown to us, our best approach is to use a larger, more flexible model and apply techniques to help it generalize well—this is where regularization comes in. Regularization helps ensure that our model doesn’t overfit by encouraging it to find simpler decision boundaries.\nThere are several ways to regularize a model, and we’ll dive into each of these methods in detail next.\nWeight Decay The simplest way to make a deeper model behave more like a shallow one is to effectively reduce the influence of some weights by shrinking them. To discourage the model from becoming overly complex, we add a penalty to the loss function based on the magnitude of the model’s weights.\n$$ L(\\mathbf{W}) = \\frac{1}{N} \\sum_{i = 1}^N L_i(x_i, y_i, \\mathbf{W}) + \\lambda R(\\mathbf{W}) $$\nwhere $\\lambda$ is the hyperparameter that controls the regularization strength, i.e., how much to shrink the weights. Since we minimize this loss function during optimization, the added term helps decay some weights in the model. $R(\\mathbf{W})$, the regularization term, varies depending on the type of weight decay:\n\\begin{align} \\text{L2: } \u0026 R(\\mathbf{W}) = \\sum_{k} \\sum_{l} \\mathbf{W}_{k,l}^2 \\\\ \\text{L1: } \u0026 R(\\mathbf{W}) = \\sum_k \\sum_l |\\mathbf{W}_{k,l}| \\\\ \\text{Elastic Net (L1 + L2): } \u0026 R(\\mathbf{W}) = \\sum_k \\sum_l \\beta \\mathbf{W}_{k,l}^2 + |\\mathbf{W}_{k,l}| \\end{align}\nL2 regularization shrinks weights in proportion to their size, meaning larger weights get shrunk more (since we add their square to the loss function). This type of regularization prefers to keep all available features but with smaller weight values.\nL1 regularization shrinks all the weights by the same amount, with the smaller weights getting zeroed out. The intuition here is that small weights indicate that the feature associated with that weight is less important, so zeroing it out won’t significantly affect the output.\nLet’s take a simple example to understand this better: \\begin{align} \\mathbf{x} \u0026= [1, 1, 1, 1] \\\\ \\mathbf{w}_1 \u0026= [1, 0, 0, 0] \\\\ \\mathbf{w}_2 \u0026= [0.25, 0.25, 0.25, .0.25] \\end{align} Both weight vectors produce the same output, $\\mathbf{w}_1^T \\mathbf{x} = \\mathbf{w}_2^T \\mathbf{x}$. However, L1 regularization would prefer $\\mathbf{w}_1$ because it sparsifies the weights (i.e., zeros out some weights), while L2 regularization would prefer $\\mathbf{w}_2$, since it retains all the features but with smaller values. Since we usually want to keep all features active, L2 regularization is the more commonly used approach. A typical value for the L2 regularization parameter $\\lambda$ is 1e-4.\nLet’s now look at the mechanics of weight decay. When we apply L2 regularization, it modifies the gradient update step as follows: \\begin{align} L(\\mathbf{w}) \u0026= L_{\\text{data}}(\\mathbf{w}) + \\color{blue} \\lambda |\\mathbf{w}|^2 \\\\ g_t \u0026= \\nabla L_{\\text{data}}(\\mathbf{w}_t) + \\color{blue} 2 \\lambda \\mathbf{w}_t \\\\ s_t \u0026= \\text{optimizer} (g_t) = \\text{optimizer} (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ \\mathbf{w}_{t+1} \u0026= \\mathbf{w}_t - \\eta s_t \\end{align}\nThe blue term represents the weight decay and it can directly be incorporated into our optimizer. This can be done in PyTorch using the weight_decay argument.\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-4) While L2 regularization and weight decay are often used interchangeably for SGD and SGD with momentum, they diverge in behavior when applied to adaptive methods like AdaGrad, RMSProp, and Adam. Let’s explore why.\nDecoupled Weight decay Consider adding L2 regularization to Adam:\n\\begin{align} m_1 \u0026= \\beta_1 m_1 + (1 - \\beta_1) (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ m_2 \u0026= \\beta_2 m_2 + (1 - \\beta_2) (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ {m_1}_{\\text{unbias}} \u0026= m_1 / (1 - \\beta_1^t) \\\\ {m_2}_{\\text{unbias}} \u0026= m_2 / (1 - \\beta_2^t) \\\\ \\mathbf{w}_{t+1} \u0026= \\mathbf{w}_{t} - \\eta * \\frac{{m_1}_{\\text{unbias}}}{\\sqrt{{m_2}_{\\text{unbias}} + \\epsilon}} \\end{align}\nBy substituting the unbiased value of $m_1$ into the update step, we get:\n\\begin{align} \\mathbf{w}_{t+1} \u0026= \\mathbf{w}_{t} - \\eta * \\frac{\\beta_1 m_1 + (1 - \\beta_1) (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} )}{(\\sqrt{{m_2}_{\\text{unbias}} + \\epsilon}) ((1 - \\beta_1^t))} \\end{align}\nNotice how the blue term is normalized by the square root of the unbiased second moment, which tracks the sum of squared gradients. As a result, if the gradients for a particular weight are large, that weight will be regularized less than weights with smaller gradients. This means that the effect of regularization becomes dependent not only on the size of the weights but also on the size of the gradients.\nThis leads to unintended behavior and making L2 regularization less effective than intended. In contrast, with SGD and SGD with Momentum, L2 regularization works as expected: \\begin{align} \\text{SGD: } \u0026 \\Delta \\mathbf{w}^{t} = - \\eta * (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ \\text{SGD + Momentum: } \u0026 \\Delta \\mathbf{w}^{t} = - \\eta * (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) + m * \\Delta \\mathbf{w}^{t-1} \\end{align}\nTo fix this issue in Adam, a variant called AdamW [1] was introduced. In this variant, the weight decay term is decoupled from the optimizer and added directly to the update step.\n\\begin{align} L(\\mathbf{w}) \u0026= L_{\\text{data}}(\\mathbf{w}) \\\\ \\mathbf{dw} = g_t \u0026= \\nabla L_{\\text{data}}(\\mathbf{w}_t) \\\\ s_t \u0026= \\text{optimizer} (g_t) \\\\ \\mathbf{w}_{t+1} \u0026= \\mathbf{w}_t - \\eta (s_t + {\\color{orange} 2 \\lambda \\mathbf{w}_t} ) \\end{align}\nThis adjustment ensures that the weight decay term remains unaffected by the moving averages, keeping it directly proportional to the weights themselves.\nAdamW has been shown to significantly improve the generalization performance of the standard Adam optimizer. It is always the preferred choice when we want effective regularization with adaptive optimizers.\nDropout Another powerful regularization technique is Dropout [2], where we randomly set a fraction of neurons in a layer to zero during each forward pass. The dropout rate (probability of dropping a neuron) is a hyperparameter, typically set to 0.5.\nBy zeroing out neurons, the network is forced to avoid relying too heavily on any individual neuron or set of neurons, encouraging the model to distribute learning across different parts of the network. This approach effectively simulates training a large number of different sub-networks, each with a unique structure but sharing weights.\nIn terms of interpretation, dropout creates a more robust network by introducing redundancy in the learned features. Since neurons are randomly excluded during each forward pass, the model must learn multiple independent representations of the data. This redundancy helps prevent overfitting, as the model doesn’t rely on specific patterns that might only exist in the training data, generalizing better on unseen data.\nInverted dropout Dropout introduces randomness into the model during training, which can make prediction outputs non-deterministic. However, during inference, we want consistent predictions. To handle this, we use inverted dropout, that rescales the output of active neurons during training, to adjust for the dropped ones.\nWhen neurons are randomly dropped, the outputs of the remaining neurons are scaled up by the inverse of the dropout rate to maintain the overall magnitude of activations. For instance, if half of the neurons are dropped (dropout rate = 0.5), we double the outputs of the remaining neurons during training to compensate.\nDropout is most commonly applied to fully connected layers, as these layers tend to contain the majority of learnable parameters, making them more prone to overfitting.\nEarly Stopping Another way to prevent overfiting is to stop training when the validation accuracy starts to decrease. While training, we typically plot three curves:\nTraining loss vs Number of iterations Training accuracy vs Number of iterations Validation accuracy vs Number of iterations These curves provide insight into the “health” of the model. Ideally, training loss should decay exponentially with each iteration, while both training and validation accuracy should increase. A large gap between the training and validation accuracy suggests overfitting, whereas little to no gap indicates underfitting.\nAt each iteration, we save the model’s weights as checkpoints and later select the checkpoint where the validation accuracy is at its maximum. Training beyond this point might increase the training accuracy (as the model starts overfitting), but it could perform poorly on the test set because validation accuracy reflects the model’s ability to generalize.\nThis technique is called early stopping because it prevents the model from training all the way to the full number of iterations (or num_steps hyperparameter), stopping earlier to capture the best version of the model without overfitting.\nData Manipulation The next topping is data manipulation, where we explore how to process and modify the training dataset to make it more suitable for efficient learning.\nPreprocessing At every step of the optimization algorithm, the loss function is computed on a mini-batch of the training data. Therefore, the shape of the loss landscape, which the optimization algorithm navigates, depends directly on how the training data is structured. To make this landscape more favorable for optimization, we often preprocess the data before feeding it into the neural network.\nA common preprocessing step is normalizing the data, which zero-centers the data and ensures all features have the same variance. This step helps smooth out the optimization process by making the model less sensitive to small weight changes. The same mean and standard deviation used to normalize the training data is also applied to the test set to maintain consistency.\nOther standard preprocessing techniques for image data include:\nSubtracting the mean image Subtracting per-channel mean (mean along each color channel) Subtracting per-channel mean and divide by the per-channel standard deviation Augmentation Data augmentation is a widely-used technique that increases both the size and diversity of the training data by applying transformations to the input images. Not only does this artificially expand the dataset, but it also adds regularization by introducing noise in the training data, which helps prevent overfitting.\nIn computer vision, augmentations like flipping, rotating, scaling, blurring, jittering, and cropping are widely used as an implicit form of regularization. Augmentations also encode invariances into the model—this means we teach the model to learn that certain transformations shouldn’t change the output.\nThink about your specific task: what transformations should not change the desired network output? The answer will vary depending on the problem. For instance, a vertical flip might make sense for images of white blood cells but not for car images (as cars are never upside down in real-world scenarios).\nLearning Rate Schedules All the variants of gradient descent, such as Momentum, Adagrad, RMSProp, and Adam, use a learning rate as a hyperparameter for global minimum search. Different learning rates produce different learning behaviors (refer to the Figure below), so it is essential to set a good learning rate, and we prefer to choose the red one.\nBut it is not always possible to come up with one “perfect” learning rate by trial and error. So what if don’t keep the learning rate fixed, and change it during the training process?\nWe can choose a high learning rate to allow our optimization to make quick progress in the initial iterations of training and then decay it over time. This would speed up our algorithm and result in better performance characteristics. This mechanism of changing the learning rates over the training process is called learning rate schedules. Let’s see some commonly used learning rate schedulers,\nStep schedule - We start with a high learning rate (like the green one). When the curve starts flattening, we reduce the learning rate and continue the same process until convergence. E.g., for ResNets, multiplying LR by 0.1 after epochs 30, 60, and 90 would result in a learning curve as shown beside. But this kind of scheduling introduces a lot of new hyperparameters - at what fixed points should we decay the LR, and to what value? Tuning them usually takes a lot of time.\nDecay functions - Instead of explicitly choosing fixed time points, we use a function that determines the learning rate at each epoch, hence it has no additional hyperparameters. We start with some initial learning rate and then decay it over the training process with a function to zero.\nCosine scheduling uses a cosine function. It is a very popularly used learning rate scheduler in computer vision problems. We can also linearly decay the learning rate over time. Such kind of scheduling is used commonly for language tasks. The Transformers paper (Attention is all you need) used an inverse square root decay function. The learning rate at epoch $t$ in each of these cases is given as, \\begin{align*} \\text{Cosine: } \u0026 \\eta_t = \\frac{\\eta_0}{2} ( 1 + cos \\frac{t \\pi}{T}) \\\\ \\text{Linear: } \u0026 \\eta_t = \\eta_0 (1 - \\frac{t}{T}) \\\\ \\text{Inverse Sqrt: } \u0026 \\eta_t = \\frac{\\eta_0}{ \\sqrt{t} }\\\\ \\end{align*} where $\\eta_0$ is the initial learning rate and $T$ is the total number of epochs.\nLR schedules are an optional technique that can be implemented to boost learning. It is important to note that using LR schedules with SGD + Momentum is fairly important. Still, if we are using one of the more complicated optimization method, such as Adam, then it often works well even with a constant learning rate. In fact, SGD + Momentum can outperform Adam but may require more tuning of LR and schedule.\nRecall the training block code of the previous post. Pytorch’s optim package can be used to implement these LR schedulers. After each epoch in the training loop, we use .step() function to update the learning rate as per our schedule.\nmodel = torch.nn.Sequential(torch.nn.Linear(3072, 100), torch.nn.ReLU(), torch.nn.Linear(100, 10)) optimizer = torch.optim.SGD(model.parameters(), lr=initial_learning_rate) scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_steps) mini_batches = torch.utils.data.DataLoader(data, batch_size=batch_size) for t in range(num_steps): for X_batch, y_batch in mini_batches: y_pred = model(X_batch) loss = loss_fn(y_pred, y_batch) loss.backward() optimizer.step() optimizer.zero_grad() scheduler.step() References [1] Loshchilov and Hutter, “Decoupled Weight Decay Regularization”, ICLR 2019.\n[2] Srivastava et al, “Dropout: A simple way to prevent neural networks from overfitting”, JMLR 2014.\n",
  "wordCount" : "2579",
  "inLanguage": "en",
  "datePublished": "2024-10-08T00:00:00Z",
  "dateModified": "2024-10-08T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/03-dl3/lr/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "YA's Almanac",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="YA&#39;s Almanac (Alt + H)">YA&#39;s Almanac</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Deep Learning Part 3: The Cherry on Top
    </h1>
    <div class="post-meta"><span title='2024-10-08 00:00:00 +0000 UTC'>October 8, 2024</span>&nbsp;·&nbsp;13 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#regularization" aria-label="Regularization">Regularization</a><ul>
                        
                <li>
                    <a href="#weight-decay" aria-label="Weight Decay">Weight Decay</a><ul>
                        
                <li>
                    <a href="#decoupled-weight-decay" aria-label="Decoupled Weight decay">Decoupled Weight decay</a></li></ul>
                </li>
                <li>
                    <a href="#dropout" aria-label="Dropout">Dropout</a><ul>
                        
                <li>
                    <a href="#inverted-dropout" aria-label="Inverted dropout">Inverted dropout</a></li></ul>
                </li>
                <li>
                    <a href="#early-stopping" aria-label="Early Stopping">Early Stopping</a></li></ul>
                </li>
                <li>
                    <a href="#data-manipulation" aria-label="Data Manipulation">Data Manipulation</a><ul>
                        
                <li>
                    <a href="#preprocessing" aria-label="Preprocessing">Preprocessing</a></li>
                <li>
                    <a href="#augmentation" aria-label="Augmentation">Augmentation</a></li></ul>
                </li>
                <li>
                    <a href="#learning-rate-schedules" aria-label="Learning Rate Schedules">Learning Rate Schedules</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.</p>
<h2 id="regularization">Regularization<a hidden class="anchor" aria-hidden="true" href="#regularization">#</a></h2>
<p>When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set. But in practice, this doesn’t always work as expected! When we focus solely on minimizing loss on the training data—essentially trying to fit it perfectly—we run the risk of not generalizing well to the test data, a phenomenon known as overfitting.</p>
<p>To illustrate this, let’s consider an example. The blue dots in the figure below represent the training data. We fit two models: $m_1$ (a polynomial model) and $m_2$ (a linear model). Model $m_2$ has more layers, leading to a more complex decision boundary that fits the training data almost perfectly.</p>
<figure class="align-center ">
    <img loading="lazy" src="http://localhost:1313/posts/03-DL3/overfit1.png#center"/> 
</figure>

<p>It&rsquo;s important to remember that both the training and test sets are assumed to be sampled from a common dataset, represented as $p_{\text{data}}$. Our goal should be to fit this probability distribution and not just the training data alone, as this ensures good performance on unseen test data. Now, let’s examine the test set, represented by the yellow points in the next figure.</p>
<figure class="align-center ">
    <img loading="lazy" src="http://localhost:1313/posts/03-DL3/overfit2.png#center"/> 
</figure>

<p>While $m_2$ fits the training data perfectly, it struggles with the test data because it overfits the training set.  Model $m_1$, despite being simpler, performs better on the test set as it generalizes better to the underlying distribution $p_{\text{data}}$.</p>
<p>If a simpler model like $m_1$ performs better on the test set, you might wonder why we don’t always use smaller models. The reason is that a smaller model runs the risk of underfitting, where it fails to capture the complexities of the data.</p>
<p>Since the true distribution $p_{\text{data}}$ is unknown to us, our best approach is to use a larger, more flexible model and apply techniques to help it generalize well—this is where regularization comes in. Regularization helps ensure that our model doesn’t overfit by encouraging it to find simpler decision boundaries.</p>
<p>There are several ways to regularize a model, and we’ll dive into each of these methods in detail next.</p>
<h3 id="weight-decay">Weight Decay<a hidden class="anchor" aria-hidden="true" href="#weight-decay">#</a></h3>
<p>The simplest way to make a deeper model behave more like a shallow one is to effectively reduce the influence of some weights by shrinking them. To discourage the model from becoming overly complex, we add a penalty to the loss function based on the magnitude of the model’s weights.</p>
<p>$$
L(\mathbf{W}) = \frac{1}{N} \sum_{i = 1}^N L_i(x_i, y_i, \mathbf{W}) + \lambda R(\mathbf{W})
$$</p>
<p>where $\lambda$ is the hyperparameter that controls the regularization strength, i.e., how much to shrink the weights. Since we minimize this loss function during optimization, the added term helps decay some weights in the model. $R(\mathbf{W})$, the regularization term, varies depending on the type of weight decay:</p>
<p>\begin{align}
\text{L2: } &amp; R(\mathbf{W}) = \sum_{k} \sum_{l} \mathbf{W}_{k,l}^2 \\
\text{L1: } &amp; R(\mathbf{W}) = \sum_k \sum_l |\mathbf{W}_{k,l}| \\
\text{Elastic Net (L1 + L2): } &amp; R(\mathbf{W}) = \sum_k \sum_l \beta \mathbf{W}_{k,l}^2 + |\mathbf{W}_{k,l}|
\end{align}</p>
<p>L2 regularization shrinks weights in proportion to their size, meaning larger weights get shrunk more (since we add their square to the loss function). This type of regularization prefers to keep all available features but with smaller weight values.</p>
<p>L1 regularization shrinks all the weights by the same amount, with the smaller weights getting zeroed out. The intuition here is that small weights indicate that the feature associated with that weight is less important, so zeroing it out won’t significantly affect the output.</p>
<p>Let&rsquo;s take a simple example to understand this better:
\begin{align}
\mathbf{x} &amp;= [1, 1, 1, 1] \\
\mathbf{w}_1 &amp;= [1, 0, 0, 0] \\
\mathbf{w}_2 &amp;= [0.25, 0.25, 0.25, .0.25]
\end{align}
Both weight vectors produce the same output, $\mathbf{w}_1^T \mathbf{x} = \mathbf{w}_2^T \mathbf{x}$. However, L1 regularization would prefer $\mathbf{w}_1$ because it sparsifies the weights (i.e., zeros out some weights), while L2 regularization would prefer $\mathbf{w}_2$, since it retains all the features but with smaller values. Since we usually want to keep all features active, L2 regularization is the more commonly used approach. A typical value for the L2 regularization parameter $\lambda$ is 1e-4.</p>
<p>Let’s now look at the mechanics of weight decay. When we apply L2 regularization, it modifies the gradient update step as follows:
\begin{align}
L(\mathbf{w}) &amp;= L_{\text{data}}(\mathbf{w}) + \color{blue} \lambda |\mathbf{w}|^2 \\
g_t &amp;= \nabla L_{\text{data}}(\mathbf{w}_t) + \color{blue} 2 \lambda \mathbf{w}_t \\
s_t &amp;= \text{optimizer} (g_t) = \text{optimizer} (\mathbf{dw} + {\color{blue} 2 \lambda \mathbf{w}_t} ) \\
\mathbf{w}_{t+1} &amp;= \mathbf{w}_t -  \eta s_t
\end{align}</p>
<p>The blue term represents the weight decay and it can directly be incorporated into our optimizer. This can be done in PyTorch using the <code>weight_decay</code> argument.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>SGD(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-3</span>, weight_decay<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-4</span>)
</span></span></code></pre></div><p>While L2 regularization and weight decay are often used interchangeably for SGD and SGD with momentum, they diverge in behavior when applied to adaptive methods like AdaGrad, RMSProp, and Adam. Let’s explore why.</p>
<h4 id="decoupled-weight-decay">Decoupled Weight decay<a hidden class="anchor" aria-hidden="true" href="#decoupled-weight-decay">#</a></h4>
<p>Consider adding L2 regularization to Adam:</p>
<p>\begin{align}
m_1 &amp;= \beta_1 m_1 + (1 - \beta_1) (\mathbf{dw} + {\color{blue} 2 \lambda \mathbf{w}_t} ) \\
m_2 &amp;= \beta_2 m_2 + (1 - \beta_2) (\mathbf{dw} + {\color{blue} 2 \lambda \mathbf{w}_t} ) \\
{m_1}_{\text{unbias}} &amp;= m_1 / (1 - \beta_1^t) \\
{m_2}_{\text{unbias}} &amp;= m_2 / (1 - \beta_2^t) \\
\mathbf{w}_{t+1} &amp;= \mathbf{w}_{t} - \eta * \frac{{m_1}_{\text{unbias}}}{\sqrt{{m_2}_{\text{unbias}} + \epsilon}}
\end{align}</p>
<p>By substituting the unbiased value of $m_1$ into the update step, we get:</p>
<p>\begin{align}
\mathbf{w}_{t+1} &amp;= \mathbf{w}_{t} - \eta * \frac{\beta_1 m_1 + (1 - \beta_1) (\mathbf{dw} + {\color{blue} 2 \lambda \mathbf{w}_t} )}{(\sqrt{{m_2}_{\text{unbias}} + \epsilon}) ((1 - \beta_1^t))}
\end{align}</p>
<p>Notice how the blue term is normalized by the square root of the unbiased second moment, which tracks the sum of squared gradients. As a result, if the gradients for a particular weight are large, that weight will be regularized less than weights with smaller gradients. This means that the effect of regularization becomes dependent not only on the size of the weights but also on the size of the gradients.</p>
<p>This leads to unintended behavior and making L2 regularization less effective than intended. In contrast, with SGD and SGD with Momentum, L2 regularization works as expected:
\begin{align}
\text{SGD: } &amp; \Delta \mathbf{w}^{t} = - \eta * (\mathbf{dw} + {\color{blue} 2 \lambda \mathbf{w}_t} ) \\
\text{SGD + Momentum: } &amp; \Delta \mathbf{w}^{t} = - \eta * (\mathbf{dw} + {\color{blue} 2 \lambda \mathbf{w}_t} ) + m * \Delta \mathbf{w}^{t-1}
\end{align}</p>
<p>To fix this issue in Adam, a variant called AdamW [1] was introduced. In this variant, the weight decay term is decoupled from the optimizer and added directly to the update step.</p>
<p>\begin{align}
L(\mathbf{w}) &amp;= L_{\text{data}}(\mathbf{w}) \\
\mathbf{dw} = g_t &amp;= \nabla L_{\text{data}}(\mathbf{w}_t) \\
s_t &amp;= \text{optimizer} (g_t) \\
\mathbf{w}_{t+1} &amp;= \mathbf{w}_t -  \eta (s_t + {\color{orange} 2 \lambda \mathbf{w}_t} )
\end{align}</p>
<p>This adjustment ensures that the weight decay term remains unaffected by the moving averages, keeping it directly proportional to the weights themselves.</p>
<p>AdamW has been shown to significantly improve the generalization performance of the standard Adam optimizer. It is always the preferred choice when we want effective regularization with adaptive optimizers.</p>
<h3 id="dropout">Dropout<a hidden class="anchor" aria-hidden="true" href="#dropout">#</a></h3>
<p>Another powerful regularization technique is Dropout [<a href="#references">2</a>], where we randomly set a fraction of neurons in a layer to zero during each forward pass. The dropout rate (probability of dropping a neuron) is a hyperparameter, typically set to 0.5.</p>
<figure class="align-center ">
    <img loading="lazy" src="http://localhost:1313/posts/03-DL3/dropout.png#center" width="500"/> 
</figure>

<p>By zeroing out neurons, the network is forced to avoid relying too heavily on any individual neuron or set of neurons, encouraging the model to distribute learning across different parts of the network. This approach effectively simulates training a large number of different sub-networks, each with a unique structure but sharing weights.</p>
<p>In terms of interpretation, dropout creates a more robust network by introducing redundancy in the learned features. Since neurons are randomly excluded during each forward pass, the model must learn multiple independent representations of the data.  This redundancy helps prevent overfitting, as the model doesn&rsquo;t rely on specific patterns that might only exist in the training data, generalizing better on unseen data.</p>
<h4 id="inverted-dropout">Inverted dropout<a hidden class="anchor" aria-hidden="true" href="#inverted-dropout">#</a></h4>
<p>Dropout introduces randomness into the model during training, which can make prediction outputs non-deterministic. However, during inference, we want consistent predictions. To handle this, we use inverted dropout, that rescales the output of active neurons during training, to adjust for the dropped ones.</p>
<p>When neurons are randomly dropped, the outputs of the remaining neurons are scaled up by the inverse of the dropout rate to maintain the overall magnitude of activations. For instance, if half of the neurons are dropped (dropout rate = 0.5), we double the outputs of the remaining neurons during training to compensate.</p>
<p>Dropout is most commonly applied to fully connected layers, as these layers tend to contain the majority of learnable parameters, making them more prone to overfitting.</p>
<h3 id="early-stopping">Early Stopping<a hidden class="anchor" aria-hidden="true" href="#early-stopping">#</a></h3>
<p>Another way to prevent overfiting is to stop training when the validation accuracy starts to decrease. While training, we typically plot three curves:</p>
<ul>
<li>Training loss vs Number of iterations</li>
<li>Training accuracy vs Number of iterations</li>
<li>Validation accuracy vs Number of iterations</li>
</ul>
<figure class="align-center ">
    <img loading="lazy" src="http://localhost:1313/posts/03-DL3/early-stopping.png#center"/> 
</figure>

<p>These curves provide insight into the &ldquo;health&rdquo; of the model. Ideally, training loss should decay exponentially with each iteration, while both training and validation accuracy should increase. A large gap between the training and validation accuracy suggests overfitting, whereas little to no gap indicates underfitting.</p>
<p>At each iteration, we save the model&rsquo;s weights as checkpoints and later select the checkpoint where the validation accuracy is at its maximum. Training beyond this point might increase the training accuracy (as the model starts overfitting), but it could perform poorly on the test set because validation accuracy reflects the model&rsquo;s ability to generalize.</p>
<p>This technique is called early stopping because it prevents the model from training all the way to the full number of iterations (or <code>num_steps</code> hyperparameter), stopping earlier to capture the best version of the model without overfitting.</p>
<h2 id="data-manipulation">Data Manipulation<a hidden class="anchor" aria-hidden="true" href="#data-manipulation">#</a></h2>
<p>The next topping is data manipulation, where we explore how to process and modify the training dataset to make it more suitable for efficient learning.</p>
<h3 id="preprocessing">Preprocessing<a hidden class="anchor" aria-hidden="true" href="#preprocessing">#</a></h3>
<p>At every step of the optimization algorithm, the loss function is computed on a mini-batch of the training data. Therefore, the shape of the loss landscape, which the optimization algorithm navigates, depends directly on how the training data is structured. To make this landscape more favorable for optimization, we often preprocess the data before feeding it into the neural network.</p>
<figure class="align-center ">
    <img loading="lazy" src="http://localhost:1313/posts/03-DL3/preprocessing.png#center"/> 
</figure>

<p>A common preprocessing step is normalizing the data, which zero-centers the data and ensures all features have the same variance. This step helps smooth out the optimization process by making the model less sensitive to small weight changes. The same mean and standard deviation used to normalize the training data is also applied to the test set to maintain consistency.</p>
<p>Other standard preprocessing techniques for image data include:</p>
<ul>
<li>Subtracting the mean image</li>
<li>Subtracting per-channel mean (mean along each color channel)</li>
<li>Subtracting per-channel mean and divide by the per-channel standard deviation</li>
</ul>
<h3 id="augmentation">Augmentation<a hidden class="anchor" aria-hidden="true" href="#augmentation">#</a></h3>
<p>Data augmentation is a widely-used technique that increases both the size and diversity of the training data by applying transformations to the input images. Not only does this artificially expand the dataset, but it also adds regularization by introducing noise in the training data, which helps prevent overfitting.</p>
<figure class="align-center ">
    <img loading="lazy" src="http://localhost:1313/posts/03-DL3/augmentation.jpg#center"/> 
</figure>

<p>In computer vision, augmentations like flipping, rotating, scaling, blurring, jittering, and cropping are widely used as an implicit form of regularization. Augmentations also encode invariances into the model—this means we teach the model to learn that certain transformations shouldn&rsquo;t change the output.</p>
<p>Think about your specific task: what transformations should not change the desired network output? The answer will vary depending on the problem. For instance, a vertical flip might make sense for images of white blood cells but not for car images (as cars are never upside down in real-world scenarios).</p>
<h2 id="learning-rate-schedules">Learning Rate Schedules<a hidden class="anchor" aria-hidden="true" href="#learning-rate-schedules">#</a></h2>
<p>All the variants of gradient descent, such as Momentum, Adagrad, RMSProp, and Adam, use a learning rate as a hyperparameter for global minimum search. Different learning rates produce different learning behaviors (refer to the Figure below), so it is essential to set a good learning rate, and we prefer to choose the red one.</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXNNfTiRA2iliVkXZxJ65jD-u8O2JsMA_XEHv0V_eVC4g2IpIrVO049HrOv7ser_ZkoBtR46H1x0z56ebhkvPqRFQyxQsXnzgm2neeLqG9IpGT7UlimdRxdtT3ursUnf0Q-6V8ckZj2xQPRYJRCe1Ku3J1u7SAYIXYKTWAllb26AyYUcYRPbxszhULuA/s459/lr.png"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhXNNfTiRA2iliVkXZxJ65jD-u8O2JsMA_XEHv0V_eVC4g2IpIrVO049HrOv7ser_ZkoBtR46H1x0z56ebhkvPqRFQyxQsXnzgm2neeLqG9IpGT7UlimdRxdtT3ursUnf0Q-6V8ckZj2xQPRYJRCe1Ku3J1u7SAYIXYKTWAllb26AyYUcYRPbxszhULuA/w320-h289/lr.png" alt=""  />
</a></p>
<p>But it is not always possible to come up with one &ldquo;perfect&rdquo; learning rate by trial and error. So what if don&rsquo;t keep the learning rate fixed, and change it during the training process?</p>
<p>We can choose a high learning rate to allow our optimization to make quick progress in the initial iterations of training and then decay it over time. This would speed up our algorithm and result in better performance characteristics. This mechanism of changing the learning rates over the training process is called learning rate schedules. Let&rsquo;s see some commonly used learning rate schedulers,</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7tehAOY81rvA-JHph9iDivax7cOSEO9aIpXbSEVeE6lk8bHb01GgGSOEXfEVQ-wzkV5dXfcVoByw3d-PqhM33cQYe-3FZpt6UUGvN9CKxqCrdh4tEjkxLcpg-GG0Tl6hgNpcfovbEAYuXKZ3QKcUhSjtfmgEgUjarKgUwovL1Vkplr39uib6NVtPQhw/s546/step.PNG"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7tehAOY81rvA-JHph9iDivax7cOSEO9aIpXbSEVeE6lk8bHb01GgGSOEXfEVQ-wzkV5dXfcVoByw3d-PqhM33cQYe-3FZpt6UUGvN9CKxqCrdh4tEjkxLcpg-GG0Tl6hgNpcfovbEAYuXKZ3QKcUhSjtfmgEgUjarKgUwovL1Vkplr39uib6NVtPQhw/s320/step.PNG" alt=""  />
</a></p>
<p><strong>Step schedule</strong> - We start with a high learning rate (like the green one). When the curve starts flattening, we reduce the learning rate and continue the same process until convergence. E.g., for ResNets, multiplying LR by 0.1 after epochs 30, 60, and 90 would result in a learning curve as shown beside. But this kind of scheduling introduces a lot of new hyperparameters - at what fixed points should we decay the LR, and to what value? Tuning them usually takes a lot of time.</p>
<p><strong>Decay functions</strong> - Instead of explicitly choosing fixed time points, we use a function that determines the learning rate at each epoch, hence it has no additional hyperparameters. We start with some initial learning rate and then decay it over the training process with a function to zero.</p>
<p>Cosine scheduling uses a cosine function. It is a very popularly used learning rate scheduler in computer vision problems. We can also linearly decay the learning rate over time. Such kind of scheduling is used commonly for language tasks. The Transformers paper (Attention is all you need) used an inverse square root decay function. The learning rate at epoch $t$ in each of these cases is given as, \begin{align*} \text{Cosine: } &amp; \eta_t = \frac{\eta_0}{2} ( 1 + cos \frac{t \pi}{T}) \\ \text{Linear: } &amp; \eta_t = \eta_0 (1 - \frac{t}{T}) \\ \text{Inverse Sqrt: } &amp; \eta_t = \frac{\eta_0}{ \sqrt{t} }\\ \end{align*} where $\eta_0$ is the initial learning rate and $T$ is the total number of epochs.</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp87qXRkyMXD0gamrY3g9iE9Y7bZljLWn3TNE-VTMD99amnFGoJilda0Bj8ZUXLYOYSjFddDhgvmjhL4Ve5IGgOLaXJp59GQpXL1-Ij1cFfpQZatXng8X8ka7UDiH3VU9jNtIMiXML8uiLtrZAT6PiNqRbQRKgvwDvbSFOm7oXfdeM0OdLUzYNUEFALA/s1578/cosine.PNG"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgp87qXRkyMXD0gamrY3g9iE9Y7bZljLWn3TNE-VTMD99amnFGoJilda0Bj8ZUXLYOYSjFddDhgvmjhL4Ve5IGgOLaXJp59GQpXL1-Ij1cFfpQZatXng8X8ka7UDiH3VU9jNtIMiXML8uiLtrZAT6PiNqRbQRKgvwDvbSFOm7oXfdeM0OdLUzYNUEFALA/w640-h166/cosine.PNG" alt=""  />
</a></p>
<p>LR schedules are an optional technique that can be implemented to boost learning. It is important to note that using LR schedules with SGD + Momentum is fairly important. Still, if we are using one of the more complicated optimization method, such as Adam, then it often works well even with a constant learning rate. In fact, SGD + Momentum can outperform Adam but may require more tuning of LR and schedule.</p>
<p>Recall the training block code of the previous post. Pytorch&rsquo;s <code>optim</code> package can be used to implement these LR schedulers. After each epoch in the training loop, we use <code>.step()</code> function to update the learning rate as per our schedule.</p>
<pre><code>model = torch.nn.Sequential(torch.nn.Linear(3072, 100),
  torch.nn.ReLU(),
        torch.nn.Linear(100, 10))
optimizer = torch.optim.SGD(model.parameters(), lr=initial_learning_rate)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, num_steps)
mini_batches = torch.utils.data.DataLoader(data, batch_size=batch_size)
for t in range(num_steps):
  for X_batch, y_batch in mini_batches:
    y_pred = model(X_batch)     
    loss = loss_fn(y_pred, y_batch)
      loss.backward()
    optimizer.step()
    optimizer.zero_grad()
  scheduler.step()
</code></pre>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1] Loshchilov and Hutter, “Decoupled Weight Decay Regularization”, ICLR 2019.</p>
<p>[2] Srivastava et al, “Dropout: A simple way to prevent neural networks from overfitting”, JMLR 2014.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="http://localhost:1313/posts/02-dl2/post/">
    <span class="title">Next »</span>
    <br>
    <span>Deep Learning Part 2: The Icing</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="http://localhost:1313/">YA&#39;s Almanac</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
