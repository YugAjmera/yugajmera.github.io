<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Deep Learning Basics Part 3: The Cherry on Top | YA's Almanac</title>
<meta name=keywords content><meta name=description content="We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.
Regularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set."><meta name=author content><link rel=canonical href=http://localhost:1313/posts/03-dl3/post/><link crossorigin=anonymous href=http://localhost:1313/assets/css/stylesheet.74991b51f7611c2303d0b5649703d675530589621885eb78236e099c22aa93a5.css integrity="sha256-dJkbUfdhHCMD0LVklwPWdVMFiWIYhet4I24JnCKqk6U=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/assets/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/assets/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/assets/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/assets/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/03-dl3/post/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>mjx-container[display=true]{margin:1.5em 0!important}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S759YBMKJE",{anonymize_ip:!1})}</script><meta property="og:title" content="Deep Learning Basics Part 3: The Cherry on Top"><meta property="og:description" content="We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.
Regularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/03-dl3/post/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-21T00:00:00+00:00"><meta property="article:modified_time" content="2022-08-21T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Deep Learning Basics Part 3: The Cherry on Top"><meta name=twitter:description content="We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.
Regularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Deep Learning Basics Part 3: The Cherry on Top","item":"http://localhost:1313/posts/03-dl3/post/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Deep Learning Basics Part 3: The Cherry on Top","name":"Deep Learning Basics Part 3: The Cherry on Top","description":"We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.\nRegularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set.","keywords":[],"articleBody":"We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.\nRegularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set. But in practice, this doesn’t always work as expected! When we focus solely on minimizing loss on the training data—essentially trying to fit it perfectly—we run the risk of not generalizing well to the test data, a phenomenon known as overfitting.\nTo illustrate this, let’s consider an example. The blue dots in the figure below represent the training data. We fit two models: $m_1$ (a polynomial model) and $m_2$ (a linear model). Model $m_2$ has more layers, leading to a more complex decision boundary that fits the training data almost perfectly.\nIt’s important to remember that both the training and test sets are assumed to be sampled from a common dataset, represented as $p_{\\text{data}}$. Our goal should be to fit this probability distribution and not just the training data alone, as this ensures good performance on unseen test data. Now, let’s examine the test set, represented by the yellow points in the next figure.\nWhile $m_2$ fits the training data perfectly, it struggles with the test data because it overfits the training set. Model $m_1$, despite being simpler, performs better on the test set as it generalizes better to the underlying distribution $p_{\\text{data}}$.\nIf a simpler model like $m_1$ performs better on the test set, you might wonder why we don’t always use smaller models. The reason is that a smaller model runs the risk of underfitting, where it fails to capture the complexities of the data.\nSince the true distribution $p_{\\text{data}}$ is unknown to us, our best approach is to use a larger, more flexible model and apply techniques to help it generalize well—this is where regularization comes in. Regularization helps ensure that our model doesn’t overfit by encouraging it to find simpler decision boundaries.\nThere are several ways to regularize a model, and we’ll dive into each of these methods in detail next.\nWeight Decay The simplest way to make a deeper model behave more like a shallow one is to effectively reduce the influence of some weights by shrinking them. To discourage the model from becoming overly complex, we add a penalty to the loss function based on the magnitude of the model’s weights.\n$$ L(\\mathbf{W}) = \\frac{1}{N} \\sum_{i = 1}^N L_i(x_i, y_i, \\mathbf{W}) + \\lambda R(\\mathbf{W}) $$\nwhere $\\lambda$ is the hyperparameter that controls the regularization strength, i.e., how much to shrink the weights. Since we minimize this loss function during optimization, the added term helps decay some weights in the model. $R(\\mathbf{W})$, the regularization term, varies depending on the type of weight decay:\n\\begin{align} \\text{L2: } \u0026 R(\\mathbf{W}) = \\sum_{k} \\sum_{l} \\mathbf{W}_{k,l}^2 \\\\ \\text{L1: } \u0026 R(\\mathbf{W}) = \\sum_k \\sum_l |\\mathbf{W}_{k,l}| \\\\ \\text{Elastic Net (L1 + L2): } \u0026 R(\\mathbf{W}) = \\sum_k \\sum_l \\beta \\mathbf{W}_{k,l}^2 + |\\mathbf{W}_{k,l}| \\end{align}\nL2 regularization shrinks weights in proportion to their size, meaning larger weights get shrunk more (since we add their square to the loss function). This type of regularization prefers to keep all available features but with smaller weight values.\nL1 regularization shrinks all the weights by the same amount, with the smaller weights getting zeroed out. The intuition here is that small weights indicate that the feature associated with that weight is less important, so zeroing it out won’t significantly affect the output.\nLet’s take a simple example to understand this better: \\begin{align} \\mathbf{x} \u0026= [1, 1, 1, 1] \\\\ \\mathbf{w}_1 \u0026= [1, 0, 0, 0] \\\\ \\mathbf{w}_2 \u0026= [0.25, 0.25, 0.25, .0.25] \\end{align} Both weight vectors produce the same output, $\\mathbf{w}_1^T \\mathbf{x} = \\mathbf{w}_2^T \\mathbf{x}$. However, L1 regularization would prefer $\\mathbf{w}_1$ because it sparsifies the weights (i.e., zeros out some weights), while L2 regularization would prefer $\\mathbf{w}_2$, since it retains all the features but with smaller values. Since we usually want to keep all features active, L2 regularization is the more commonly used approach. A typical value for the L2 regularization parameter $\\lambda$ is 1e-4.\nLet’s now look at the mechanics of weight decay. When we apply L2 regularization, it modifies the gradient update step as follows: \\begin{align} L(\\mathbf{w}) \u0026= L_{\\text{data}}(\\mathbf{w}) + \\color{blue} \\lambda |\\mathbf{w}|^2 \\\\ g_t \u0026= \\nabla L_{\\text{data}}(\\mathbf{w}_t) + \\color{blue} 2 \\lambda \\mathbf{w}_t \\\\ s_t \u0026= \\text{optimizer} (g_t) = \\text{optimizer} (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ \\mathbf{w}_{t+1} \u0026= \\mathbf{w}_t - \\eta s_t \\end{align}\nThe blue term represents the weight decay and it can directly be incorporated into our optimizer. This can be done in PyTorch using the weight_decay argument.\n# Define the optimizer with L2 regularization optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-4) While L2 regularization and weight decay are often used interchangeably for SGD and SGD with momentum, they diverge in behavior when applied to adaptive methods like AdaGrad, RMSProp, and Adam. Let’s explore why.\nDecoupled Weight decay Consider adding L2 regularization to Adam:\n\\begin{align} m_1 \u0026= \\beta_1 m_1 + (1 - \\beta_1) (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ m_2 \u0026= \\beta_2 m_2 + (1 - \\beta_2) (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ {m_1}_{\\text{unbias}} \u0026= m_1 / (1 - \\beta_1^t) \\\\ {m_2}_{\\text{unbias}} \u0026= m_2 / (1 - \\beta_2^t) \\\\ \\mathbf{w}_{t+1} \u0026= \\mathbf{w}_{t} - \\eta * \\frac{{m_1}_{\\text{unbias}}}{\\sqrt{{m_2}_{\\text{unbias}} + \\epsilon}} \\end{align}\nBy substituting the unbiased value of $m_1$ into the update step, we get:\n\\begin{align} \\mathbf{w}_{t+1} \u0026= \\mathbf{w}_{t} - \\eta * \\frac{\\beta_1 m_1 + (1 - \\beta_1) (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} )}{(\\sqrt{{m_2}_{\\text{unbias}} + \\epsilon}) ((1 - \\beta_1^t))} \\end{align}\nNotice how the blue term is normalized by the square root of the unbiased second moment, which tracks the sum of squared gradients. As a result, if the gradients for a particular weight are large, that weight will be regularized less than weights with smaller gradients. This means that the effect of regularization becomes dependent not only on the size of the weights but also on the size of the gradients.\nThis leads to unintended behavior and making L2 regularization less effective than intended. In contrast, with SGD and SGD with Momentum, L2 regularization works as expected: \\begin{align} \\text{SGD: } \u0026 \\Delta \\mathbf{w}^{t} = - \\eta * (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) \\\\ \\text{SGD + Momentum: } \u0026 \\Delta \\mathbf{w}^{t} = - \\eta * (\\mathbf{dw} + {\\color{blue} 2 \\lambda \\mathbf{w}_t} ) + m * \\Delta \\mathbf{w}^{t-1} \\end{align}\nTo fix this issue in Adam, a variant called AdamW [1] was introduced. In this variant, the weight decay term is decoupled from the optimizer and added directly to the update step.\n\\begin{align} L(\\mathbf{w}) \u0026= L_{\\text{data}}(\\mathbf{w}) \\\\ \\mathbf{dw} = g_t \u0026= \\nabla L_{\\text{data}}(\\mathbf{w}_t) \\\\ s_t \u0026= \\text{optimizer} (g_t) \\\\ \\mathbf{w}_{t+1} \u0026= \\mathbf{w}_t - \\eta (s_t + {\\color{orange} 2 \\lambda \\mathbf{w}_t} ) \\end{align}\nThis adjustment ensures that the weight decay term remains unaffected by the moving averages, keeping it directly proportional to the weights themselves.\nAdamW has been shown to significantly improve the generalization performance of the standard Adam optimizer. It is always the preferred choice when we want effective regularization with adaptive optimizers.\nDropout Another powerful regularization technique is Dropout [2], where we randomly set a fraction of neurons in a layer to zero during each forward pass. The dropout rate (probability of dropping a neuron) is a hyperparameter, typically set to 0.5.\nBy zeroing out neurons, the network is forced to avoid relying too heavily on any individual neuron or set of neurons, encouraging the model to distribute learning across different parts of the network. This approach effectively simulates training a large number of different sub-networks, each with a unique structure but sharing weights.\nIn terms of interpretation, dropout creates a more robust network by introducing redundancy in the learned features. Since neurons are randomly excluded during each forward pass, the model must learn multiple independent representations of the data. This redundancy helps prevent overfitting, as the model doesn’t rely on specific patterns that might only exist in the training data, generalizing better on unseen data.\nInverted dropout Dropout introduces randomness into the model during training, which can make prediction outputs non-deterministic. However, during inference, we want consistent predictions. To handle this, we use inverted dropout, that rescales the output of active neurons during training, to adjust for the dropped ones.\nWhen neurons are randomly dropped, the outputs of the remaining neurons are scaled up by the inverse of the dropout rate to maintain the overall magnitude of activations. For instance, if half of the neurons are dropped (dropout rate = 0.5), we double the outputs of the remaining neurons during training to compensate.\nDropout is most commonly applied to fully connected layers, as these layers tend to contain the majority of learnable parameters, making them more prone to overfitting. Here’s how you can add it to your network:\nclass TwoLayerNet(torch.nn.Module): def __init__(self): super(TwoLayerNet, self).__init__() self.fc1 = torch.nn.Linear(28 * 28 * 1, 512) self.fc2 = torch.nn.Linear(512, 10) self.dropout = torch.nn.Dropout(p=0.5) # Add dropout self.relu = torch.nn.ReLU() def forward(self, x): x = self.relu(self.fc1(x)) x = self.dropout(x) x = self.fc2(x) return x Early Stopping Another way to prevent overfiting is to stop training when the validation accuracy starts to decrease. While training, we typically plot three curves:\nTraining loss vs Number of iterations Training accuracy vs Number of iterations Validation accuracy vs Number of iterations These curves provide insight into the “health” of the model. Ideally, training loss should decay exponentially with each iteration, while both training and validation accuracy should increase. A large gap between the training and validation accuracy suggests overfitting, whereas little to no gap indicates underfitting.\nAt each iteration, we save the model’s weights as checkpoints and later select the checkpoint where the validation accuracy is at its maximum. Training beyond this point might increase the training accuracy (as the model starts overfitting), but it could perform poorly on the test set because validation accuracy reflects the model’s ability to generalize.\nThis technique is called early stopping because it prevents the model from training all the way to the full number of iterations (or num_epochs hyperparameter), stopping earlier to capture the best version of the model without overfitting.\nData Manipulation The next topping is data manipulation, where we explore how to process and modify the training dataset to make it more suitable for efficient learning.\nPreprocessing At every step of the optimization algorithm, the loss function is computed on a mini-batch of the training data. Therefore, the shape of the loss landscape, which the optimization algorithm navigates, depends directly on how the training data is structured. To make this landscape more favorable for optimization, we often preprocess the data before feeding it into the neural network.\nA common preprocessing step is normalizing the data, which zero-centers the data and ensures all features have the same variance. This step helps smooth out the optimization process by making the model less sensitive to small weight changes.\nThe pixel values range from [0, 255], and are divided by 255 on conversion to tensors, resulting in in values in range [0, 1]. We then normalize these values with a mean of 0.5 and a standard deviation of 0.5 to obtain pixel values in the range [-1, 1]. This can be included in the list of transforms in PyTorch:\n# Apply transforms to dataset transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), # Convert image to Tensor torchvision.transforms.Normalize((0.5,), (0.5,)) # Normalize with mean=0.5, std=0.5 ]) The same transform is also applied to the test set to maintain consistency.\nOther standard preprocessing techniques for image data include:\nSubtracting the mean image Subtracting per-channel mean (mean along each color channel) Subtracting per-channel mean and divide by the per-channel standard deviation Augmentation Data augmentation is a widely-used technique that increases both the size and diversity of the training data by applying label-preserving transformations to the input images. Not only does this artificially expand the dataset, but it also adds regularization by introducing noise in the training data, which helps prevent overfitting.\nIn computer vision, augmentations like flipping, rotating, scaling, blurring, jittering, and cropping are widely used as an implicit form of regularization. Augmentations also encode invariances into the model—this means we teach the model to learn that certain transformations shouldn’t change the output.\nThink about your specific task: what transformations should not change the desired network output? The answer will vary depending on the problem. For instance, a vertical flip might make sense for images of white blood cells but not for car images (as cars are never upside down in real-world scenarios).\nAugmentations can also be included in the transforms list,\n# Apply transforms to dataset transform = torchvision.transforms.Compose([ torchvision.transforms.RandomRotation(degrees=15), # Randomly rotate by +/- 15 degrees torchvision.transforms.ToTensor(), # Convert image to Tensor torchvision.transforms.Normalize((0.5,), (0.5,)) # Normalize with mean=0.5, std=0.5 ]) Note that augmentations are typically excluded from the test set to ensure the model is evaluated on the original data distribution.\nLearning Rate Schedules All the optimizers we’ve discussed so far—SGD, SGD with Momentum, Adagrad, RMSProp, and Adam—use a fixed learning rate as a hyperparameter to guide the search for the global minimum. As you may have learned by now, this learning rate is a crucial variable in our learning process.\nDifferent learning rates produce different learning behaviors, as shown in the figure below. Therefore, it’s essential to choose an appropriate learning rate, ideally the red one. However, finding that one “perfect” learning rate through trial and error is not always feasible.\nWhat if we don’t keep the learning rate fixed and instead change it during the training process? We can start with a high learning rate to allow our optimization to make quick progress in the initial iterations and then gradually decay it over time, ensuring that the model converges to a lower loss at the end. This would lead to faster convergence and better performance characteristics.\nThis mechanism of changing the learning rate over the course of training is called learning rate scheduling. Let’s look at some commonly used learning rate schedules.\nStep Schedule In a step schedule, we start with a high learning rate (similar to the green curve in the figure), and when the loss curve starts to plateau, we decrease the learning rate. This process continues until convergence.\nFor example, we might reduce the learning rate by a factor of 0.1 after epochs 30, 60, and 90. The learning curve would then look something like this.\nSince we’re decaying the learning rate at arbitrary points during training, this schedule introduces additional hyperparameters, like the number of steps and when to decay. A common approach is to monitor the loss and decay the learning rate whenever the loss plateaus.\nDecay Schedule Instead of selecting fixed points to adjust the learning rate, we can define a function that dictates how the learning rate should decay over time. This eliminates the need for extra hyperparameters. Starting with an initial rate, these functions gradually reduce the it over time.\nHere are some commonly used decay functions:\nCosine schedule: This is a popular choice for computer vision problems. The learning rate follows a cosine function that smoothly decays over time. If $\\eta_0$ is the initial learning rate and $T$ is the total number of epochs, the learning rate at epoch $t$ is given by: $$ \\eta_t = \\frac{\\eta_0}{2} ( 1 + cos \\frac{t \\pi}{T}) $$\nLinear schedule: In this approach, the learning rate decays linearly over time, which is shown to work well for language models. $$ \\eta_t = \\eta_0 (1 - \\frac{t}{T}) $$\nInverse Square root: Commonly used in training Transformer models, this schedule follows an inverse square root decay. One drawback is that the model spends very little time at the higher learning rate. $$ \\eta_t = \\frac{\\eta_0}{ \\sqrt{t} } $$\nCyclic Schedule In addition to decaying the learning rate monotonically, we can adopt a cyclic learning rate schedule, which alternate between high and low learning rates during training. These schedules help prevent the optimization process from getting stuck in local minima.\nIn this approach, the learning rate decreases smoothly within each cycle, following a cosine decay curve. Once the cycle completes, the learning rate “warms up” by resetting to a higher value. This method, also known as Warm Restarts [3], allows the optimizer to periodically explore different regions of the loss landscape.\nCyclic schedules are particularly useful for training models on large datasets, as the periodic warm restarts enhance exploration, reducing the chances of premature convergence to suboptimal solutions.\nUpdating the code Let’s modify the training loop to include a learning rate scheduler: cosine decay.\n# Define the loss function criterion = torch.nn.CrossEntropyLoss() # Define the optimizer optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) # Define the cosine annealing learning rate scheduler num_epochs = 10 scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs) for epoch in range(num_epochs): # Training loop for (image, label) in train_loader: # Send the batch of images and labels to the GPU image, label = image.to(device), label.to(device) # Flatten the image image = image.view(image.shape[0], -1) # Forward pass and optimization output = model(image) loss = criterion(output, label) loss.backward() optimizer.step() optimizer.zero_grad() # Update the learning rate at the end of each epoch scheduler.step() The .step() function updates the learning rate based on the cosine decay schedule at the end of each epoch.\nUsing learning rate schedules with optimizers like SGD or SGD with Momentum is highly recommended for improving training efficiency and model performance. However, for adaptive methods like Adam, a constant learning rate often works well, as these methods automatically adjust learning rates on a per-parameter basis.\nOur deep learning cake is now complete, it’s time to dig in and savor the delicious knowledge we’ve baked together—enjoy every slice as you continue your journey in this exciting field!\nReferences [1] Loshchilov and Hutter, “Decoupled Weight Decay Regularization”, ICLR 2019.\n[2] Srivastava et al, “Dropout: A simple way to prevent neural networks from overfitting”, JMLR 2014.\n[3] Loshchilov and Hutter, “SGDR: Stochastic Gradient Descent with Warm Restarts”, ICLR 2017.\n","wordCount":"3025","inLanguage":"en","datePublished":"2022-08-21T00:00:00Z","dateModified":"2022-08-21T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/03-dl3/post/"},"publisher":{"@type":"Organization","name":"YA's Almanac","logo":{"@type":"ImageObject","url":"http://localhost:1313/assets/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="YA's Almanac (Alt + H)">YA's Almanac</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Home><span>Home</span></a></li><li><a href=http://localhost:1313/archives/ title=Archives><span>Archives</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Deep Learning Basics Part 3: The Cherry on Top</h1><div class=post-meta><span title='2022-08-21 00:00:00 +0000 UTC'>August 21, 2022</span>&nbsp;·&nbsp;15 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#regularization aria-label=Regularization>Regularization</a><ul><li><a href=#weight-decay aria-label="Weight Decay">Weight Decay</a><ul><li><a href=#decoupled-weight-decay aria-label="Decoupled Weight decay">Decoupled Weight decay</a></li></ul></li><li><a href=#dropout aria-label=Dropout>Dropout</a><ul><li><a href=#inverted-dropout aria-label="Inverted dropout">Inverted dropout</a></li></ul></li><li><a href=#early-stopping aria-label="Early Stopping">Early Stopping</a></li></ul></li><li><a href=#data-manipulation aria-label="Data Manipulation">Data Manipulation</a><ul><li><a href=#preprocessing aria-label=Preprocessing>Preprocessing</a></li><li><a href=#augmentation aria-label=Augmentation>Augmentation</a></li></ul></li><li><a href=#learning-rate-schedules aria-label="Learning Rate Schedules">Learning Rate Schedules</a><ul><li><a href=#step-schedule aria-label="Step Schedule">Step Schedule</a></li><li><a href=#decay-schedule aria-label="Decay Schedule">Decay Schedule</a></li><li><a href=#cyclic-schedule aria-label="Cyclic Schedule">Cyclic Schedule</a></li><li><a href=#updating-the-code aria-label="Updating the code">Updating the code</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.</p><h2 id=regularization>Regularization<a hidden class=anchor aria-hidden=true href=#regularization>#</a></h2><p>When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set. But in practice, this doesn’t always work as expected! When we focus solely on minimizing loss on the training data—essentially trying to fit it perfectly—we run the risk of not generalizing well to the test data, a phenomenon known as overfitting.</p><p>To illustrate this, let’s consider an example. The blue dots in the figure below represent the training data. We fit two models: $m_1$ (a polynomial model) and $m_2$ (a linear model). Model $m_2$ has more layers, leading to a more complex decision boundary that fits the training data almost perfectly.</p><figure class=align-center><img loading=lazy src=../overfit1.png#center></figure><p>It&rsquo;s important to remember that both the training and test sets are assumed to be sampled from a common dataset, represented as $p_{\text{data}}$. Our goal should be to fit this probability distribution and not just the training data alone, as this ensures good performance on unseen test data. Now, let’s examine the test set, represented by the yellow points in the next figure.</p><figure class=align-center><img loading=lazy src=../overfit2.png#center></figure><p>While $m_2$ fits the training data perfectly, it struggles with the test data because it overfits the training set. Model $m_1$, despite being simpler, performs better on the test set as it generalizes better to the underlying distribution $p_{\text{data}}$.</p><p>If a simpler model like $m_1$ performs better on the test set, you might wonder why we don’t always use smaller models. The reason is that a smaller model runs the risk of underfitting, where it fails to capture the complexities of the data.</p><p>Since the true distribution $p_{\text{data}}$ is unknown to us, our best approach is to use a larger, more flexible model and apply techniques to help it generalize well—this is where regularization comes in. Regularization helps ensure that our model doesn’t overfit by encouraging it to find simpler decision boundaries.</p><p>There are several ways to regularize a model, and we’ll dive into each of these methods in detail next.</p><h3 id=weight-decay>Weight Decay<a hidden class=anchor aria-hidden=true href=#weight-decay>#</a></h3><p>The simplest way to make a deeper model behave more like a shallow one is to effectively reduce the influence of some weights by shrinking them. To discourage the model from becoming overly complex, we add a penalty to the loss function based on the magnitude of the model’s weights.</p><p>$$
L(\mathbf{W}) = \frac{1}{N} \sum_{i = 1}^N L_i(x_i, y_i, \mathbf{W}) + \lambda R(\mathbf{W})
$$</p><p>where $\lambda$ is the hyperparameter that controls the regularization strength, i.e., how much to shrink the weights. Since we minimize this loss function during optimization, the added term helps decay some weights in the model. $R(\mathbf{W})$, the regularization term, varies depending on the type of weight decay:</p><p>\begin{align}
\text{L2: } & R(\mathbf{W}) = \sum_{k} \sum_{l} \mathbf{W}_{k,l}^2 \\
\text{L1: } & R(\mathbf{W}) = \sum_k \sum_l |\mathbf{W}_{k,l}| \\
\text{Elastic Net (L1 + L2): } & R(\mathbf{W}) = \sum_k \sum_l \beta \mathbf{W}_{k,l}^2 + |\mathbf{W}_{k,l}|
\end{align}</p><p>L2 regularization shrinks weights in proportion to their size, meaning larger weights get shrunk more (since we add their square to the loss function). This type of regularization prefers to keep all available features but with smaller weight values.</p><p>L1 regularization shrinks all the weights by the same amount, with the smaller weights getting zeroed out. The intuition here is that small weights indicate that the feature associated with that weight is less important, so zeroing it out won’t significantly affect the output.</p><p>Let&rsquo;s take a simple example to understand this better:
\begin{align}
\mathbf{x} &= [1, 1, 1, 1] \\
\mathbf{w}_1 &= [1, 0, 0, 0] \\
\mathbf{w}_2 &= [0.25, 0.25, 0.25, .0.25]
\end{align}
Both weight vectors produce the same output, $\mathbf{w}_1^T \mathbf{x} = \mathbf{w}_2^T \mathbf{x}$. However, L1 regularization would prefer $\mathbf{w}_1$ because it sparsifies the weights (i.e., zeros out some weights), while L2 regularization would prefer $\mathbf{w}_2$, since it retains all the features but with smaller values. Since we usually want to keep all features active, L2 regularization is the more commonly used approach. A typical value for the L2 regularization parameter $\lambda$ is 1e-4.</p><p>Let’s now look at the mechanics of weight decay. When we apply L2 regularization, it modifies the gradient update step as follows:
\begin{align}
L(\mathbf{w}) &= L_{\text{data}}(\mathbf{w}) + \color{blue} \lambda |\mathbf{w}|^2 \\
g_t &= \nabla L_{\text{data}}(\mathbf{w}_t) + \color{blue} 2 \lambda \mathbf{w}_t \\
s_t &= \text{optimizer} (g_t) = \text{optimizer} (\mathbf{dw} + {\color{blue} 2 \lambda \mathbf{w}_t} ) \\
\mathbf{w}_{t+1} &= \mathbf{w}_t - \eta s_t
\end{align}</p><p>The blue term represents the weight decay and it can directly be incorporated into our optimizer. This can be done in PyTorch using the <code>weight_decay</code> argument.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Define the optimizer with L2 regularization</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-3</span><span class=p>,</span> <span class=n>weight_decay</span><span class=o>=</span><span class=mf>1e-4</span><span class=p>)</span>
</span></span></code></pre></div><p>While L2 regularization and weight decay are often used interchangeably for SGD and SGD with momentum, they diverge in behavior when applied to adaptive methods like AdaGrad, RMSProp, and Adam. Let’s explore why.</p><h4 id=decoupled-weight-decay>Decoupled Weight decay<a hidden class=anchor aria-hidden=true href=#decoupled-weight-decay>#</a></h4><p>Consider adding L2 regularization to Adam:</p><p>\begin{align}
m_1 &= \beta_1 m_1 + (1 - \beta_1) (\mathbf{dw} + {\color{blue} 2 \lambda \mathbf{w}_t} ) \\
m_2 &= \beta_2 m_2 + (1 - \beta_2) (\mathbf{dw} + {\color{blue} 2 \lambda \mathbf{w}_t} ) \\
{m_1}_{\text{unbias}} &= m_1 / (1 - \beta_1^t) \\
{m_2}_{\text{unbias}} &= m_2 / (1 - \beta_2^t) \\
\mathbf{w}_{t+1} &= \mathbf{w}_{t} - \eta * \frac{{m_1}_{\text{unbias}}}{\sqrt{{m_2}_{\text{unbias}} + \epsilon}}
\end{align}</p><p>By substituting the unbiased value of $m_1$ into the update step, we get:</p><p>\begin{align}
\mathbf{w}_{t+1} &= \mathbf{w}_{t} - \eta * \frac{\beta_1 m_1 + (1 - \beta_1) (\mathbf{dw} + {\color{blue} 2 \lambda \mathbf{w}_t} )}{(\sqrt{{m_2}_{\text{unbias}} + \epsilon}) ((1 - \beta_1^t))}
\end{align}</p><p>Notice how the blue term is normalized by the square root of the unbiased second moment, which tracks the sum of squared gradients. As a result, if the gradients for a particular weight are large, that weight will be regularized less than weights with smaller gradients. This means that the effect of regularization becomes dependent not only on the size of the weights but also on the size of the gradients.</p><p>This leads to unintended behavior and making L2 regularization less effective than intended. In contrast, with SGD and SGD with Momentum, L2 regularization works as expected:
\begin{align}
\text{SGD: } & \Delta \mathbf{w}^{t} = - \eta * (\mathbf{dw} + {\color{blue} 2 \lambda \mathbf{w}_t} ) \\
\text{SGD + Momentum: } & \Delta \mathbf{w}^{t} = - \eta * (\mathbf{dw} + {\color{blue} 2 \lambda \mathbf{w}_t} ) + m * \Delta \mathbf{w}^{t-1}
\end{align}</p><p>To fix this issue in Adam, a variant called AdamW <a href=#references>[1]</a> was introduced. In this variant, the weight decay term is decoupled from the optimizer and added directly to the update step.</p><p>\begin{align}
L(\mathbf{w}) &= L_{\text{data}}(\mathbf{w}) \\
\mathbf{dw} = g_t &= \nabla L_{\text{data}}(\mathbf{w}_t) \\
s_t &= \text{optimizer} (g_t) \\
\mathbf{w}_{t+1} &= \mathbf{w}_t - \eta (s_t + {\color{orange} 2 \lambda \mathbf{w}_t} )
\end{align}</p><p>This adjustment ensures that the weight decay term remains unaffected by the moving averages, keeping it directly proportional to the weights themselves.</p><p>AdamW has been shown to significantly improve the generalization performance of the standard Adam optimizer. It is always the preferred choice when we want effective regularization with adaptive optimizers.</p><h3 id=dropout>Dropout<a hidden class=anchor aria-hidden=true href=#dropout>#</a></h3><p>Another powerful regularization technique is Dropout <a href=#references>[2]</a>, where we randomly set a fraction of neurons in a layer to zero during each forward pass. The dropout rate (probability of dropping a neuron) is a hyperparameter, typically set to 0.5.</p><figure class=align-center><img loading=lazy src=../dropout.png#center width=500></figure><p>By zeroing out neurons, the network is forced to avoid relying too heavily on any individual neuron or set of neurons, encouraging the model to distribute learning across different parts of the network. This approach effectively simulates training a large number of different sub-networks, each with a unique structure but sharing weights.</p><p>In terms of interpretation, dropout creates a more robust network by introducing redundancy in the learned features. Since neurons are randomly excluded during each forward pass, the model must learn multiple independent representations of the data. This redundancy helps prevent overfitting, as the model doesn&rsquo;t rely on specific patterns that might only exist in the training data, generalizing better on unseen data.</p><h4 id=inverted-dropout>Inverted dropout<a hidden class=anchor aria-hidden=true href=#inverted-dropout>#</a></h4><p>Dropout introduces randomness into the model during training, which can make prediction outputs non-deterministic. However, during inference, we want consistent predictions. To handle this, we use inverted dropout, that rescales the output of active neurons during training, to adjust for the dropped ones.</p><p>When neurons are randomly dropped, the outputs of the remaining neurons are scaled up by the inverse of the dropout rate to maintain the overall magnitude of activations. For instance, if half of the neurons are dropped (dropout rate = 0.5), we double the outputs of the remaining neurons during training to compensate.</p><p>Dropout is most commonly applied to fully connected layers, as these layers tend to contain the majority of learnable parameters, making them more prone to overfitting. Here&rsquo;s how you can add it to your network:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>TwoLayerNet</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>TwoLayerNet</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>28</span> <span class=o>*</span> <span class=mi>28</span> <span class=o>*</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>512</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>   <span class=c1># Add dropout</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><h3 id=early-stopping>Early Stopping<a hidden class=anchor aria-hidden=true href=#early-stopping>#</a></h3><p>Another way to prevent overfiting is to stop training when the validation accuracy starts to decrease. While training, we typically plot three curves:</p><ul><li>Training loss vs Number of iterations</li><li>Training accuracy vs Number of iterations</li><li>Validation accuracy vs Number of iterations</li></ul><figure class=align-center><img loading=lazy src=../early-stopping.png#center></figure><p>These curves provide insight into the &ldquo;health&rdquo; of the model. Ideally, training loss should decay exponentially with each iteration, while both training and validation accuracy should increase. A large gap between the training and validation accuracy suggests overfitting, whereas little to no gap indicates underfitting.</p><p>At each iteration, we save the model&rsquo;s weights as checkpoints and later select the checkpoint where the validation accuracy is at its maximum. Training beyond this point might increase the training accuracy (as the model starts overfitting), but it could perform poorly on the test set because validation accuracy reflects the model&rsquo;s ability to generalize.</p><p>This technique is called early stopping because it prevents the model from training all the way to the full number of iterations (or <code>num_epochs</code> hyperparameter), stopping earlier to capture the best version of the model without overfitting.</p><h2 id=data-manipulation>Data Manipulation<a hidden class=anchor aria-hidden=true href=#data-manipulation>#</a></h2><p>The next topping is data manipulation, where we explore how to process and modify the training dataset to make it more suitable for efficient learning.</p><h3 id=preprocessing>Preprocessing<a hidden class=anchor aria-hidden=true href=#preprocessing>#</a></h3><p>At every step of the optimization algorithm, the loss function is computed on a mini-batch of the training data. Therefore, the shape of the loss landscape, which the optimization algorithm navigates, depends directly on how the training data is structured. To make this landscape more favorable for optimization, we often preprocess the data before feeding it into the neural network.</p><figure class=align-center><img loading=lazy src=../preprocessing.png#center></figure><p>A common preprocessing step is normalizing the data, which zero-centers the data and ensures all features have the same variance. This step helps smooth out the optimization process by making the model less sensitive to small weight changes.</p><p>The pixel values range from [0, 255], and are divided by 255 on conversion to tensors, resulting in in values in range [0, 1]. We then normalize these values with a mean of 0.5 and a standard deviation of 0.5 to obtain pixel values in the range [-1, 1]. This can be included in the list of transforms in PyTorch:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Apply transforms to dataset</span>
</span></span><span class=line><span class=cl><span class=n>transform</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>            <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>                <span class=c1># Convert image to Tensor</span>
</span></span><span class=line><span class=cl>            <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>((</span><span class=mf>0.5</span><span class=p>,),</span> <span class=p>(</span><span class=mf>0.5</span><span class=p>,))</span>  <span class=c1># Normalize with mean=0.5, std=0.5</span>
</span></span><span class=line><span class=cl>            <span class=p>])</span>
</span></span></code></pre></div><p>The same transform is also applied to the test set to maintain consistency.</p><p>Other standard preprocessing techniques for image data include:</p><ul><li>Subtracting the mean image</li><li>Subtracting per-channel mean (mean along each color channel)</li><li>Subtracting per-channel mean and divide by the per-channel standard deviation</li></ul><h3 id=augmentation>Augmentation<a hidden class=anchor aria-hidden=true href=#augmentation>#</a></h3><p>Data augmentation is a widely-used technique that increases both the size and diversity of the training data by applying label-preserving transformations to the input images. Not only does this artificially expand the dataset, but it also adds regularization by introducing noise in the training data, which helps prevent overfitting.</p><figure class=align-center><img loading=lazy src=../augmentation.jpg#center></figure><p>In computer vision, augmentations like flipping, rotating, scaling, blurring, jittering, and cropping are widely used as an implicit form of regularization. Augmentations also encode <strong>invariances</strong> into the model—this means we teach the model to learn that certain transformations shouldn&rsquo;t change the output.</p><p>Think about your specific task: what transformations should not change the desired network output? The answer will vary depending on the problem. For instance, a vertical flip might make sense for images of white blood cells but not for car images (as cars are never upside down in real-world scenarios).</p><p>Augmentations can also be included in the transforms list,</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Apply transforms to dataset</span>
</span></span><span class=line><span class=cl><span class=n>transform</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span>
</span></span><span class=line><span class=cl>            <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>RandomRotation</span><span class=p>(</span><span class=n>degrees</span><span class=o>=</span><span class=mi>15</span><span class=p>),</span> <span class=c1># Randomly rotate by +/- 15 degrees</span>
</span></span><span class=line><span class=cl>            <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>(),</span>                 <span class=c1># Convert image to Tensor</span>
</span></span><span class=line><span class=cl>            <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>Normalize</span><span class=p>((</span><span class=mf>0.5</span><span class=p>,),</span> <span class=p>(</span><span class=mf>0.5</span><span class=p>,))</span>   <span class=c1># Normalize with mean=0.5, std=0.5</span>
</span></span><span class=line><span class=cl>            <span class=p>])</span>
</span></span></code></pre></div><p>Note that augmentations are typically excluded from the test set to ensure the model is evaluated on the original data distribution.</p><h2 id=learning-rate-schedules>Learning Rate Schedules<a hidden class=anchor aria-hidden=true href=#learning-rate-schedules>#</a></h2><p>All the optimizers we’ve discussed so far—SGD, SGD with Momentum, Adagrad, RMSProp, and Adam—use a fixed learning rate as a hyperparameter to guide the search for the global minimum. As you may have learned by now, this learning rate is a crucial variable in our learning process.</p><p>Different learning rates produce different learning behaviors, as shown in the figure below. Therefore, it&rsquo;s essential to choose an appropriate learning rate, ideally the red one. However, finding that one &ldquo;perfect&rdquo; learning rate through trial and error is not always feasible.</p><figure class=align-center><img loading=lazy src=../lr.png#center width=400></figure><p>What if we don’t keep the learning rate fixed and instead change it during the training process? We can start with a high learning rate to allow our optimization to make quick progress in the initial iterations and then gradually decay it over time, ensuring that the model converges to a lower loss at the end. This would lead to faster convergence and better performance characteristics.</p><p>This mechanism of changing the learning rate over the course of training is called learning rate scheduling. Let’s look at some commonly used learning rate schedules.</p><h3 id=step-schedule>Step Schedule<a hidden class=anchor aria-hidden=true href=#step-schedule>#</a></h3><p>In a step schedule, we start with a high learning rate (similar to the green curve in the figure), and when the loss curve starts to plateau, we decrease the learning rate. This process continues until convergence.</p><p>For example, we might reduce the learning rate by a factor of 0.1 after epochs 30, 60, and 90. The learning curve would then look something like this.</p><figure class=align-center><img loading=lazy src=../lr2.png#center width=400></figure><p>Since we’re decaying the learning rate at arbitrary points during training, this schedule introduces additional hyperparameters, like the number of steps and when to decay. A common approach is to monitor the loss and decay the learning rate whenever the loss plateaus.</p><h3 id=decay-schedule>Decay Schedule<a hidden class=anchor aria-hidden=true href=#decay-schedule>#</a></h3><p>Instead of selecting fixed points to adjust the learning rate, we can define a function that dictates how the learning rate should decay over time. This eliminates the need for extra hyperparameters. Starting with an initial rate, these functions gradually reduce the it over time.</p><p>Here are some commonly used decay functions:</p><ul><li>Cosine schedule: This is a popular choice for computer vision problems. The learning rate follows a cosine function that smoothly decays over time. If $\eta_0$ is the initial learning rate and $T$ is the total number of epochs, the learning rate at epoch $t$ is given by:</li></ul><p>$$
\eta_t = \frac{\eta_0}{2} ( 1 + cos \frac{t \pi}{T})
$$</p><ul><li>Linear schedule: In this approach, the learning rate decays linearly over time, which is shown to work well for language models.</li></ul><p>$$
\eta_t = \eta_0 (1 - \frac{t}{T})
$$</p><ul><li>Inverse Square root: Commonly used in training Transformer models, this schedule follows an inverse square root decay. One drawback is that the model spends very little time at the higher learning rate.</li></ul><p>$$
\eta_t = \frac{\eta_0}{ \sqrt{t} }
$$</p><figure class=align-center><img loading=lazy src=../schedules.png#center></figure><h3 id=cyclic-schedule>Cyclic Schedule<a hidden class=anchor aria-hidden=true href=#cyclic-schedule>#</a></h3><p>In addition to decaying the learning rate monotonically, we can adopt a cyclic learning rate schedule, which alternate between high and low learning rates during training. These schedules help prevent the optimization process from getting stuck in local minima.</p><p>In this approach, the learning rate decreases smoothly within each cycle, following a cosine decay curve. Once the cycle completes, the learning rate &ldquo;warms up&rdquo; by resetting to a higher value. This method, also known as Warm Restarts <a href=#references>[3]</a>, allows the optimizer to periodically explore different regions of the loss landscape.</p><p>Cyclic schedules are particularly useful for training models on large datasets, as the periodic warm restarts enhance exploration, reducing the chances of premature convergence to suboptimal solutions.</p><figure class=align-center><img loading=lazy src=../warm-restarts.png#center width=350></figure><h3 id=updating-the-code>Updating the code<a hidden class=anchor aria-hidden=true href=#updating-the-code>#</a></h3><p>Let&rsquo;s modify the training loop to include a learning rate scheduler: cosine decay.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Define the loss function</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define the optimizer</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>  <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define the cosine annealing learning rate scheduler</span>
</span></span><span class=line><span class=cl><span class=n>num_epochs</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=n>scheduler</span> <span class=o>=</span> <span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>CosineAnnealingLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>T_max</span><span class=o>=</span><span class=n>num_epochs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Training loop</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>image</span><span class=p>,</span> <span class=n>label</span><span class=p>)</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Send the batch of images and labels to the GPU</span>
</span></span><span class=line><span class=cl>        <span class=n>image</span><span class=p>,</span> <span class=n>label</span> <span class=o>=</span> <span class=n>image</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>label</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Flatten the image</span>
</span></span><span class=line><span class=cl>        <span class=n>image</span> <span class=o>=</span> <span class=n>image</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>image</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Forward pass and optimization</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>image</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>label</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Update the learning rate at the end of each epoch</span>
</span></span><span class=line><span class=cl>    <span class=n>scheduler</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>  
</span></span></code></pre></div><p>The <code>.step()</code> function updates the learning rate based on the cosine decay schedule at the end of each epoch.</p><p>Using learning rate schedules with optimizers like SGD or SGD with Momentum is highly recommended for improving training efficiency and model performance. However, for adaptive methods like Adam, a constant learning rate often works well, as these methods automatically adjust learning rates on a per-parameter basis.</p><p>Our deep learning cake is now complete, it&rsquo;s time to dig in and savor the delicious knowledge we&rsquo;ve baked together—enjoy every slice as you continue your journey in this exciting field!</p><p> </p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Loshchilov and Hutter, “<a href=https://arxiv.org/abs/1711.05101>Decoupled Weight Decay Regularization</a>”, ICLR 2019.</p><p>[2] Srivastava et al, “<a href=https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf>Dropout: A simple way to prevent neural networks from overfitting</a>”, JMLR 2014.</p><p>[3] Loshchilov and Hutter, “<a href=https://arxiv.org/abs/1608.03983>SGDR: Stochastic Gradient Descent with Warm Restarts</a>”, ICLR 2017.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/04-cnn/post/><span class=title>« Prev</span><br><span>Convolutional Neural Networks: Deep Learning for Image Recognition</span>
</a><a class=next href=http://localhost:1313/posts/02-dl2/post/><span class=title>Next »</span><br><span>Deep Learning Basics Part 2: The Icing</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>YA's Almanac</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>