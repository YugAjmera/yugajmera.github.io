<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Looking into GPT-2 Part 2: Architecture and Sampling | YA&#39;s Almanac</title>
<meta name="keywords" content="">
<meta name="description" content="Building on our previous exploration of GPT-1, let&rsquo;s now modify its architecture to recreate the GPT-2 [1] small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.
One major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning.">
<meta name="author" content="">
<link rel="canonical" href="https://yugajmera.github.io/posts/10-gpt2.2/post/">
<link crossorigin="anonymous" href="https://yugajmera.github.io/assets/css/stylesheet.74991b51f7611c2303d0b5649703d675530589621885eb78236e099c22aa93a5.css" integrity="sha256-dJkbUfdhHCMD0LVklwPWdVMFiWIYhet4I24JnCKqk6U=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://yugajmera.github.io/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yugajmera.github.io/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yugajmera.github.io/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yugajmera.github.io/assets/apple-touch-icon.png">
<link rel="mask-icon" href="https://yugajmera.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://yugajmera.github.io/posts/10-gpt2.2/post/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
<style>
   mjx-container[display="true"] {
       margin: 1.5em 0 ! important
   }
</style>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-S759YBMKJE', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Looking into GPT-2 Part 2: Architecture and Sampling" />
<meta property="og:description" content="Building on our previous exploration of GPT-1, let&rsquo;s now modify its architecture to recreate the GPT-2 [1] small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.
One major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yugajmera.github.io/posts/10-gpt2.2/post/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-03-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-03-01T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Looking into GPT-2 Part 2: Architecture and Sampling"/>
<meta name="twitter:description" content="Building on our previous exploration of GPT-1, let&rsquo;s now modify its architecture to recreate the GPT-2 [1] small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.
One major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yugajmera.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Looking into GPT-2 Part 2: Architecture and Sampling",
      "item": "https://yugajmera.github.io/posts/10-gpt2.2/post/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Looking into GPT-2 Part 2: Architecture and Sampling",
  "name": "Looking into GPT-2 Part 2: Architecture and Sampling",
  "description": "Building on our previous exploration of GPT-1, let\u0026rsquo;s now modify its architecture to recreate the GPT-2 [1] small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.\nOne major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning.",
  "keywords": [
    
  ],
  "articleBody": "Building on our previous exploration of GPT-1, let’s now modify its architecture to recreate the GPT-2 [1] small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.\nOne major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning. Additionally, GPT-2 is an excellent choice for learning how to implement LLMs, as it can run on a single GPU or even a laptop—unlike GPT-3, which requires GPU clusters for training and inference.\nThe official model code is available here: https://github.com/openai/gpt-2/blob/master/src/model.py\nArchitecture Let’s start with our imports:\n# Import functions import torch import torch.nn as nn device = 'cuda' if torch.cuda.is_available() else 'cpu' print(\"Using\", device) Before implementing the model, let’s take a look at GPT-2’s pre-trained weights from Hugging Face’s transformers library. This will help us understand the naming conventions used in the architecture:\nfrom transformers import GPT2LMHeadModel # Load pre-trained GPT-2 (small) and retrieve its state dictionary model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device) # 124M param model model_hf_params = model_hf.state_dict() for name, param in model_hf_params.items(): print(name, param.shape) transformer.wte.weight torch.Size([50257, 768]) transformer.wpe.weight torch.Size([1024, 768]) transformer.h.0.ln_1.weight torch.Size([768]) transformer.h.0.ln_1.bias torch.Size([768]) transformer.h.0.attn.c_attn.weight torch.Size([768, 2304]) transformer.h.0.attn.c_attn.bias torch.Size([2304]) transformer.h.0.attn.c_proj.weight torch.Size([768, 768]) transformer.h.0.attn.c_proj.bias torch.Size([768]) transformer.h.0.ln_2.weight torch.Size([768]) transformer.h.0.ln_2.bias torch.Size([768]) transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072]) transformer.h.0.mlp.c_fc.bias torch.Size([3072]) transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768]) transformer.h.0.mlp.c_proj.bias torch.Size([768]) ... transformer.ln_f.weight torch.Size([768]) transformer.ln_f.bias torch.Size([768]) lm_head.weight torch.Size([50257, 768]) The output lists the layer names along with their corresponding tensor shapes, giving us insights into GPT-2’s layer structure:\nThe model is encapsulated under the namespace transformer. wte represents the token embedding weights. wpe represents the position embedding weights. h is a list of hidden decoder blocks. LayerNorms are denoted as ln_1 and ln_2 within each decoder block, with the final LayerNorm represented as ln_f. The scale and shift parameters are renamed to weight and bias, respectively. Our W_qkv weight matrix is renamed to c_attn (short for causal attention), while W_o is renamed to c_proj. The causal attention layer also includes bias tensors, meaning we will set qkv_bias=True. lm_head represents the final linear layer (language modeling head) and does not include a bias term. To ensure compatibility when loading OpenAI’s pre-trained weights, we will follow this naming convention while defining the subcomponents of our model. Let’s implement these next! 🚀\nclass MultiHeadAttention(nn.Module): def __init__(self, d_in, d_out, context_length, attn_pdrop, num_heads, qkv_bias=False): super().__init__() self.c_attn = nn.Linear(d_in, 3 * d_out, bias=qkv_bias) self.c_proj = nn.Linear(d_out, d_out) self.attn_dropout = nn.Dropout(attn_pdrop) self.d_h = d_out // num_heads # head dimension self.num_heads = num_heads self.register_buffer(\"masked\", torch.tril(torch.ones(context_length, context_length)).view(1, 1, context_length, context_length)) def forward(self, x): # Input vectors x - (B, N, d_in) B, N, d_in = x.shape # Obtain keys, values and queries in one go - (B, N, d_out) qkv = self.c_attn(x) queries, keys, values = qkv.chunk(3, dim=-1) # Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h) k = keys.view(B, N, self.num_heads, self.d_h).transpose(1, 2) v = values.view(B, N, self.num_heads, self.d_h).transpose(1, 2) q = queries.view(B, N, self.num_heads, self.d_h).transpose(1, 2) # Apply scaled dot-product attention with causal mask on each head attn_scores = (q @ k.transpose(2, 3)) * k.shape[-1]**-0.5 attn_scores = attn_scores.masked_fill(self.masked[:, :, :N, :N] == 0, float('-inf')) attn_weights = torch.softmax(attn_scores, dim=-1) attn_weights = self.attn_dropout(attn_weights) context_vec = attn_weights @ v # Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out) out = context_vec.transpose(1, 2).contiguous().view(B, N, -1) out = self.c_proj(out) return out class LayerNorm(nn.Module): def __init__(self, emb_dim): super().__init__() self.eps = 1e-5 self.weight = nn.Parameter(torch.ones(emb_dim)) self.bias = nn.Parameter(torch.zeros(emb_dim)) def forward(self, x): mean = x.mean(dim=-1, keepdim=True) var = x.var(dim=-1, keepdim=True, unbiased=False) # Used biased variance estimation (without Bessel's correction) norm_x = (x - mean) / torch.sqrt(var + self.eps) return self.weight * norm_x + self.bias class GELU(nn.Module): def __init__(self): super().__init__() def forward(self, x): return 0.5 * x * (1 + torch.tanh( torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)) )) class MLP(nn.Module): def __init__(self, emb_dim): super().__init__() self.c_fc = nn.Linear(emb_dim, 4 * emb_dim) self.c_proj = nn.Linear(4 * emb_dim, emb_dim) self.gelu = GELU() def forward(self, x): x = self.gelu(self.c_fc(x)) x = self.c_proj(x) return x Here, I’ve taken our GPT-1 implementation and adapted it to match GPT-2’s naming conventions.\nA key detail is the masked variable, which is defined in register_buffer. This automatically adds it to the model’s state dictionary, meaning you’ll find entries like transformer.h.0.attn.masked. However, since it’s a non-trainable tensor, it does not exist in Hugging Face’s state dictionary. So, when loading pre-trained weights, we’ll simply ignore it.\nThe GPT-2 decoder block closely follows the GPT-1 model we implemented earlier, with a few key modifications. Let’s break them down.\nPre-Norm Transformer The original Transformer (as well as our GPT-1 implementation) used Post-Norm residual connections, where LayerNorm is applied after the sublayer and residual addition.\nPre-Norm vs Post-Norm Transformer Layers\nIn this setup, the raw output of each sublayer (such as the self-attention or feedforward network) is directly added to the residual before normalization. This can lead to instability as the network depth increases because the residual sum $[x_l + \\text{Sublayer}(x_l)]$ accumulates unnormalized activations over layers:\nIf $\\text{Sublayer}(x_l)$ outputs are very small values, the residual connection mostly carries $x_l$ forward without significant updates, leading to vanishing gradients.\nIf $\\text{Sublayer}(x_l)$ has large activations, they keep adding up over layers. Although LayerNorm tries to regulate this after the accumulation, the activations have already influenced the residual pathway, potentially leading to exploding gradients.\nEssentially, LayerNorm in Post-Norm setups tries to fix activations at the last moment—after instability has already propagated.\nTo address this, Xiong et al. [2] proposed Pre-Norm residual units, where LayerNorm is applied before the sublayer. This ensures each sublayer receives well-scaled inputs, improving gradient flow and training stability.\nMost modern Large Language Models, including GPT-2, have adopted this Pre-Norm approach. The key changes are:\nLayer Normalization is moved to the input of each sub-block, ensuring that each sublayer receives normalized inputs.\nAn additional Layer Normalization is added after the final self-attention block, further improving stability.\nWith these changes, the GPT-2 architecture now looks like this: GPT-2 architecture\nclass DecoderBlock(nn.Module): def __init__(self, cfg): super().__init__() self.ln_1 = LayerNorm(emb_dim=cfg['emb_dim']) self.attn = MultiHeadAttention(d_in=cfg['emb_dim'], d_out=cfg['emb_dim'], context_length=cfg['context_length'], attn_pdrop=cfg['attn_pdrop'], num_heads=cfg['n_heads'], qkv_bias=cfg['qkv_bias']) self.ln_2 = LayerNorm(emb_dim=cfg['emb_dim']) self.mlp = MLP(emb_dim=cfg['emb_dim']) self.resid_dropout = nn.Dropout(cfg['resid_pdrop']) def forward(self, x): x = x + self.resid_dropout(self.attn(self.ln_1(x))) x = x + self.resid_dropout(self.mlp(self.ln_2(x))) return x And our GPT-2 model will be coded as:\nclass GPT2(nn.Module): def __init__(self, cfg): super().__init__() self.transformer = nn.ModuleDict(dict( wte = nn.Embedding(cfg['vocab_size'], cfg['emb_dim']), # Token Embeddings wpe = nn.Embedding(cfg['context_length'], cfg['emb_dim']), # Position Encoding embd_dropout = nn.Dropout(cfg['embd_pdrop']), # Embedding dropout h = nn.Sequential(*[DecoderBlock(cfg) for _ in range(cfg['n_layers'])]), # Multiple Decoder blocks ln_f = LayerNorm(cfg['emb_dim']), # Final layernorm )) self.lm_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False) # Language modelling head def forward(self, x): B, N = x.size() token_emb = self.transformer.wte(x) # output: (B, N, D) pos_emb = self.transformer.wpe(torch.arange(N, device=device)) # output: (N, D) x = token_emb + pos_emb # ouput: (B, N, D) x = self.transformer.h(x) x = self.transformer.ln_f(x) logits = self.lm_head(x) # ouput: (B, N, vocab_size) return logits The hyperparameters of our model are defined in a Python dictionary, cfg, which is passed when instantiating the model.\n# Define configuration dictionary GPT_CONFIG_124M = { \"vocab_size\": 50257, # 50,000 BPE merges + 256 byte tokens + 1 \u003c|endoftext|\u003e token \"context_length\": 1024, # Maximum sequence length \"emb_dim\": 768, # Embedding dimension \"n_heads\": 12, # Number of attention heads \"n_layers\": 12, # Number of transformer blocks \"attn_pdrop\": 0.1, # Dropout probability for attention layers \"embd_pdrop\": 0.1, # Dropout probability for embeddings \"resid_pdrop\": 0.1, # Dropout probability for residual connections \"qkv_bias\": True # Whether to use bias in QKV projection } model = GPT2(GPT_CONFIG_124M).to(device) # Print the number of parameters in the model print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters') 163.037184 M parameters Oops! Our model has 163M parameters, even though we aimed to replicate the 124M parameter version. What’s going on?\nActually, nothing is wrong! The discrepancy arises due to the weight tying scheme used in the GPT-2 model. Let’s dive into it in more detail.\nWeight tying The first layer of our architecture is the token embedding table, which maps each token in our vocabulary to an embedding vector. The last layer is the language modeling head, which reverses this process by mapping the embeddings back to vocabulary space.\nIf we inspect our model’s state dictionary, we find that both layers have the same shape:\ngpt2_model_params = model.state_dict() print(gpt2_model_params[\"transformer.wte.weight\"].shape) print(gpt2_model_params[\"lm_head.weight\"].shape) torch.Size([50257, 768]) torch.Size([50257, 768]) This occurs because of how nn.Embedding and nn.Linear store their weights internally:\nnn.Embedding(dim_in, dim_out) stores its lookup table as torch.Size([dim_in, dim_out]). nn.Linear(features_in, features_out) stores its weight matrix as torch.Size([features_out, features_in]) to perform y = Wx operations. This observation led to the practice of weight tying, where the token embedding and language modeling head share the same set of weights. This approach was also used in the original Transformer paper.\nBy enforcing weight sharing, we inject an inductive bias into the model, indicating that:\nThe way tokens are mapped to embeddings should be similar to how embeddings are mapped back to tokens. The model should maintain consistency between input and output representations. If we visualize the parameter distribution in our model (with just one transformer block), we get: Parameter sizes of our model with one transformer block\nClearly, the majority of parameters reside in the token embeddings and the language modeling layer. By making them share weights, we reduce the overall parameter count, leading to lower memory usage and faster convergence speed since fewer parameters need to be learned.\nWe can enable weight tying by directly assigning the same weight tensor:\n# Weight tying model.transformer.wte.weight = model.lm_head.weight # Print the number of parameters in the model print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters') 124.439808 M parameters Hurray! We now have our own GPT-2 124M model.\nNext, let’s explore the weight initialization scheme used in GPT-2. This is useful if you want to train the model from scratch on your own dataset. However, if you’re only interested in loading pre-trained weights, feel free to skip this section.\nWeight initialization The GPT-2 paper doesn’t explicitly mention the weight initialization used, but we can infer it from the model.py code. Here’s the initialization strategy:\nToken embeddings: Normal distribution with standard deviation of $0.02$. Position embeddings: Normal distribution with standard deviation of $0.01$. Linear layers: Weights: Normal distribution with standard deviation of $0.02$. Bias: Initialized to zero. LayerNorm: The scale and shift parameters are initialized to ones and zeros, respectively, similar to our implementation. Additionally, the GPT-2 paper mentions using a scaling factor of $1/\\sqrt{\\text{N}}$, where $\\text{N}$ is the number of residual layers, on the weights of residual layer at initialization. This is to account for the accumulation of activations on the residual path — the standard deviation of activations grows inside the residual stream due to repeated additions. However, the GPT-2 code doesn’t seem to implement this scaling, so we’ll skip it for now.\nHere’s how we can implement the weight initialization:\n# Initialize weights as per GPT-2 def gpt2_init(m): if isinstance(m, nn.Linear): # Applies to linear layers only nn.init.normal_(m.weight, mean=0.0, std=0.02) if m.bias is not None: nn.init.zeros_(m.bias) # Bias initialized to zero elif isinstance(m, nn.Embedding): # Applied to embedding layer only nn.init.normal_(m.weight, mean=0.0, std=0.02) # Apply initialization model.apply(gpt2_init) Note: I’ve used a standard deviation of 0.02 for the position embeddings as well because the difference doesn’t significantly impact the model, and it keeps the code simpler.\nLoading Weights To load weights from HuggingFace, we simply copy the tensor values over to our model. For this, the tensor shapes need to match exactly between the HuggingFace model and our custom model.\nLet’s compare the state dictionaries of both models side by side to identify any mismatches in tensor shapes:\nfor name, param in model_hf_params.items(): if param.shape != gpt2_model_params[name].shape: print(f\"Shape mismatch for parameter {name}: {param.shape} vs {gpt2_model_params[name].shape}\") Shape mismatch for parameter transformer.h.0.attn.c_attn.weight: torch.Size([768, 2304]) vs torch.Size([2304, 768]) Shape mismatch for parameter transformer.h.0.mlp.c_fc.weight: torch.Size([768, 3072]) vs torch.Size([3072, 768]) Shape mismatch for parameter transformer.h.0.mlp.c_proj.weight: torch.Size([3072, 768]) vs torch.Size([768, 3072]) Oops, why don’t the shapes match?\nOpenAI’s GPT-2 checkpoints use a Conv1d module on each linear layer in the GPT-2 architecture. This is why the tensors are transposed and do not match directly.\nAs a result, the layers c_attn, c_proj, c_fc and c_proj need to be handled differently. We transpose the weights before copying them to ensure they match the expected shapes and are copied correctly.\nHere’s how to load the weights:\n# Loading weights in your model transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight'] with torch.no_grad(): for name, param in model_hf_params.items(): # check if the parameter name matches if name in gpt2_model_params: # if the parameter has to be transposed if name.endswith(tuple(transposed)): gpt2_model_params[name].copy_(param.t()) # Tranpose the weights and then copy # if the parameter shape matches directly elif param.shape == gpt2_model_params[name].shape: gpt2_model_params[name].copy_(param) # copy the weights over else: print(f\"Shape mismatch for parameter {name}: {param.shape} vs {gpt2_model_params[name].shape}\") else: print(f\"Parameter {name} not found in your model\") print(\"Weights are loaded successfully!\") This code will load the pre-trained HuggingFace weights into our model, handling special treatment for the linear layers to ensure that everything is correctly aligned.\nGenerating text Now that we’ve successfully loaded the weights, it’s time to test our model by generating some text. We’ll use the tiktoken tokenizer to encode the initial prompt, “Hello, I’m a language model,” and pass it into the model to generate a continuation.\nHere’s a basic text generation loop:\n# Generate text from the trained model max_new_tokens = 20 context_length = 1024 import tiktoken enc = tiktoken.get_encoding(\"gpt2\") tokens = enc.encode(\"Hello, I'm a language model,\") context = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0) # Adds the batch dimension: (B=1, N) stored_context = context for _ in range(max_new_tokens): model.eval() with torch.no_grad(): context = stored_context[:, -context_length:] # Trim the context to fit the model's context length logits = model(context) # logits: (B=1, N, vocab_size) logits = logits[:, -1, :] # take just the last time step (B, vocab_size) probs = torch.softmax(logits, dim=-1) next_token = torch.argmax(probs, dim=-1. keepdim=True) # Select the most likely next token (B=1, N) stored_context = torch.cat((stored_context, next_token), dim=1) # (B=1, N+1) print(enc.decode(stored_context[0].tolist())) Hello, I'm a language model, not a programming language. I'm a language model. I'm a language model. I'm a The model gets stuck in a loop, generating repeated phrases like “I’m a language model.” This is because we are always selecting the token with the highest probability at each step, which limits the model’s creativity and causes repetition. To resolve this, we can explore probabilistic sampling methods.\nProbabilistic Sampling To introduce more variety and creativity in the decoding process, we replace the argmax function with multinomial. This method uses the probability distribution output by the model to sample the next token proportionally to its probability score:\nlogits = model(context) # logits: (B=1, N, vocab_size) logits = logits[:, -1, :] # take just the last time step (B, vocab_size) probs = torch.softmax(logits, dim=-1) next_token = torch.multinomial(probs, num_samples=1) # Sample from the distribution (B=1, N) By using probabilistic sampling, we can explore a range of potential next tokens, leading to more diverse and interesting text.\nWhile this is a good way to sample text, there are other decoding strategies that allow us to control the distribution and selection process to generate more original text.\nTemperature Scaling Let’s understand temperature scaling through an example:\nprint(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)) print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])/0.001, dim=-1)) tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872]) tensor([0., 0., 0., 0., 1.]) When the magnitudes of logits are large, the softmax output saturates and converges to a one-hot encoding. Temperature scaling works similarly—by dividing the logits by a number greater than zero:\nlogits = model(context) logits = logits[:, -1, :] / temperature probs = torch.softmax(logits, dim=-1) next_token = torch.multinomial(probs, num_samples=1) Temperature scaling allows us to control the randomness of the output:\nTemperature \u003c 1: Produces more confident (sharper) distributions, picking the most likely token almost always. Temperatures \u003e 1: Results in a more uniformly distributed token probabilties, where other tokens are selected more often. This can add more variety but may also produce nonsensical text. Temperature = 1: This is equivalent to not using any temperature scaling. Top-k sampling In top-k sampling, we restrict the sampling process to the top-k most likely tokens and exclude the rest by masking their probabilities. This ensures that we avoid sampling very rare tokens while still providing some diversity in the output.\nWe achieve this by setting the logits of non-selected tokens to negative infinity, so that their softmax probabilities become zero, and the remaining probabilities sum to 1. The implementation is as follows:\ndef top_k_logits(logits, k): if k == 0: return logits # No truncation values, _ = torch.topk(logits, k=k) # Get top-k values min_value = values[:, -1] # Minimum value in top-k return torch.where(logits \u003c min_value, torch.tensor(float('-inf')), logits) torch.topk retrieves the values of top-k logits in descending order, and the where function sets the logits of tokens below the lowest logit value to negative infinity. This ensures that only the top-k logits contribute to the probability distribution.\nTop-p (Nucleus Sampling) While top-k gives us the ability to select the top-k tokens to consider in the sampling process, top-p dynamically selects the top tokens whose cumulative probability exceeds a certain threshold, denoted by p. Instead of a fixed number k, it adapts based on the distribution.\nFor example, we first sort the tokens by probability:\nToken A: 0.40 Token B: 0.30 Token C: 0.20 Token D: 0.05 Token E: 0.05 Next, we compute the cumulative probability:\nToken A: 0.40 Token A + B: 0.70 Token A + B + C: 0.90 ✅ (stop here, because we reached p=0.9) Token A + B + C + D: 0.95 Token A + B + C + D + E: 1.00 We keep only tokens A, B, and C since their cumulative probability exceeds p=0.9. The rest are discarded.\nHere’s how it is implemented in code:\ndef top_p_logits(logits, p): # Nucleus Sampling sorted_logits, _ = torch.sort(logits, dim=-1, descending=True) # Sort logits in descending order cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1) # Compute cumulative probabilities # Determine number of indices to include, keeping at least one num_to_keep = torch.clamp((cumulative_probs \u003c= p).sum(dim=-1) - 1, min=0) min_value = sorted_logits[:, num_to_keep] return torch.where(logits \u003c min_value, torch.tensor(float('-inf')), logits) Sampling code Now, let’s integrate all the sampling strategies we’ve discussed and implement a sample() function, similar to the one in GPT-2. You can find the original GPT-2 implementation here: https://github.com/openai/gpt-2/blob/master/src/sample.py\ndef sample(max_new_tokens, context_length, start_token=None, context=None, temperature=1, top_k=0, top_p=1): if start_token is None: assert context is not None, 'Specify exactly one of start_token and context!' else: assert context is None, 'Specify exactly one of start_token and context!' context = torch.full((1, 1), start_token, dtype=torch.long, device=device) stored_context = context for _ in range(max_new_tokens): model.eval() with torch.no_grad(): context = stored_context[:, -context_length:] logits = model(context) # logits: (B=1, N, vocab_size) logits = logits[:, -1, :] / temperature # Scale logits by temperature logits = top_k_logits(logits, k=top_k) # Apply top-k filtering logits = top_p_logits(logits, p=top_p) # Apply top-p (nucleus) sampling probs = torch.softmax(logits, dim=-1) # Convert logits to probabilities next_token = torch.multinomial(probs, num_samples=1) # Sample from the distribution (B=1, N) stored_context = torch.cat((stored_context, next_token), dim=1) # (B=1, N+1) return stored_context Using this function, we can start generating text either using a start token or an initial prompt.\ntokens = enc.encode(\"Hello, I'm a language model,\") prompt = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0) # Adds the batch dimension(B=1, N) next_tokens = sample(context=prompt, max_new_tokens=20, context_length=GPT_CONFIG_124M[\"context_length\"], top_p=0.9, top_k=40, temperature=1.0) print(enc.decode(next_tokens[0].tolist())) Here’s the output:\nHello, I'm a language model, and I wanted to do something more powerful than just an English translation of an English sentence.\" And with that, we’ve explored some of the most exciting text generation techniques that can bring out the true creativity of language models. Now, it’s your turn to experiment and create your own stories! Happy coding!\nReferences Andrej Karpathy’s video: Let’s reproduce GPT-2 (124M) [1] Radford et al., “Language Models are Unsupervised Multitask Learners”, OpenAI 2019. [2] Xiong et al., “On Layer Normalization in the Transformer Architecture”, ICML 2020. ",
  "wordCount" : "3318",
  "inLanguage": "en",
  "datePublished": "2025-03-01T00:00:00Z",
  "dateModified": "2025-03-01T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yugajmera.github.io/posts/10-gpt2.2/post/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "YA's Almanac",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yugajmera.github.io/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yugajmera.github.io/" accesskey="h" title="YA&#39;s Almanac (Alt + H)">YA&#39;s Almanac</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yugajmera.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Looking into GPT-2 Part 2: Architecture and Sampling
    </h1>
    <div class="post-meta"><span title='2025-03-01 00:00:00 +0000 UTC'>March 1, 2025</span>&nbsp;·&nbsp;16 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#architecture" aria-label="Architecture">Architecture</a><ul>
                        
                <li>
                    <a href="#pre-norm-transformer" aria-label="Pre-Norm Transformer">Pre-Norm Transformer</a></li>
                <li>
                    <a href="#weight-tying" aria-label="Weight tying">Weight tying</a></li>
                <li>
                    <a href="#weight-initialization" aria-label="Weight initialization">Weight initialization</a></li></ul>
                </li>
                <li>
                    <a href="#loading-weights" aria-label="Loading Weights">Loading Weights</a></li>
                <li>
                    <a href="#generating-text" aria-label="Generating text">Generating text</a><ul>
                        
                <li>
                    <a href="#probabilistic-sampling" aria-label="Probabilistic Sampling">Probabilistic Sampling</a></li>
                <li>
                    <a href="#temperature-scaling" aria-label="Temperature Scaling">Temperature Scaling</a></li>
                <li>
                    <a href="#top-k-sampling" aria-label="Top-k sampling">Top-k sampling</a></li>
                <li>
                    <a href="#top-p-nucleus-sampling" aria-label="Top-p (Nucleus Sampling)">Top-p (Nucleus Sampling)</a></li>
                <li>
                    <a href="#sampling-code" aria-label="Sampling code">Sampling code</a></li></ul>
                </li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Building on our previous exploration of GPT-1, let&rsquo;s now modify its architecture to recreate the GPT-2 <a href="#references">[1]</a> small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.</p>
<p>One major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning. Additionally, GPT-2 is an excellent choice for learning how to implement LLMs, as it can run on a single GPU or even a laptop—unlike GPT-3, which requires GPU clusters for training and inference.</p>
<p>The official model code is available here: <a href="https://github.com/openai/gpt-2/blob/master/src/model.py">https://github.com/openai/gpt-2/blob/master/src/model.py</a></p>
<h2 id="architecture">Architecture<a hidden class="anchor" aria-hidden="true" href="#architecture">#</a></h2>
<p>Let&rsquo;s start with our imports:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Import functions</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;cuda&#39;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#39;cpu&#39;</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Using&#34;</span>, device)
</span></span></code></pre></div><p>Before implementing the model, let&rsquo;s take a look at GPT-2&rsquo;s pre-trained weights from Hugging Face’s <code>transformers</code> library. This will help us understand the naming conventions used in the architecture:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> GPT2LMHeadModel
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load pre-trained GPT-2 (small) and retrieve its state dictionary</span>
</span></span><span style="display:flex;"><span>model_hf <span style="color:#f92672">=</span> GPT2LMHeadModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;gpt2&#34;</span>)<span style="color:#f92672">.</span>to(device) <span style="color:#75715e"># 124M param model</span>
</span></span><span style="display:flex;"><span>model_hf_params <span style="color:#f92672">=</span> model_hf<span style="color:#f92672">.</span>state_dict()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> model_hf_params<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>    print(name, param<span style="color:#f92672">.</span>shape)
</span></span></code></pre></div><pre tabindex="0"><code>transformer.wte.weight torch.Size([50257, 768])
transformer.wpe.weight torch.Size([1024, 768])
transformer.h.0.ln_1.weight torch.Size([768])
transformer.h.0.ln_1.bias torch.Size([768])
transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])
transformer.h.0.attn.c_attn.bias torch.Size([2304])
transformer.h.0.attn.c_proj.weight torch.Size([768, 768])
transformer.h.0.attn.c_proj.bias torch.Size([768])
transformer.h.0.ln_2.weight torch.Size([768])
transformer.h.0.ln_2.bias torch.Size([768])
transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])
transformer.h.0.mlp.c_fc.bias torch.Size([3072])
transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])
transformer.h.0.mlp.c_proj.bias torch.Size([768])

...

transformer.ln_f.weight torch.Size([768])
transformer.ln_f.bias torch.Size([768])
lm_head.weight torch.Size([50257, 768])
</code></pre><p>The output lists the layer names along with their corresponding tensor shapes, giving us insights into GPT-2&rsquo;s layer structure:</p>
<ul>
<li>The model is encapsulated under the namespace <code>transformer</code>.</li>
<li><code>wte</code> represents the token embedding weights.</li>
<li><code>wpe</code> represents the position embedding weights.</li>
<li><code>h</code> is a list of hidden decoder blocks.</li>
<li>LayerNorms are denoted as <code>ln_1</code> and <code>ln_2</code> within each decoder block, with the final LayerNorm represented as <code>ln_f</code>. The <code>scale</code> and <code>shift</code> parameters are renamed to <code>weight</code> and <code>bias</code>, respectively.</li>
<li>Our <code>W_qkv</code> weight matrix is renamed to <code>c_attn</code> (short for causal attention), while <code>W_o</code> is renamed to <code>c_proj</code>.  The causal attention layer also includes bias tensors, meaning we will set <code>qkv_bias=True</code>.</li>
<li><code>lm_head</code> represents the final linear layer (language modeling head) and does not include a bias term.</li>
</ul>
<p>To ensure compatibility when loading OpenAI’s pre-trained weights, we will follow this naming convention while defining the subcomponents of our model. Let’s implement these next! 🚀</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MultiHeadAttention</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, d_in, d_out, context_length, attn_pdrop, num_heads, qkv_bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_attn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_in, <span style="color:#ae81ff">3</span> <span style="color:#f92672">*</span> d_out, bias<span style="color:#f92672">=</span>qkv_bias)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_proj <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_out, d_out)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>attn_dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(attn_pdrop)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>d_h <span style="color:#f92672">=</span> d_out <span style="color:#f92672">//</span> num_heads                    <span style="color:#75715e"># head dimension</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>num_heads <span style="color:#f92672">=</span> num_heads
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>register_buffer(<span style="color:#e6db74">&#34;masked&#34;</span>, torch<span style="color:#f92672">.</span>tril(torch<span style="color:#f92672">.</span>ones(context_length, context_length))<span style="color:#f92672">.</span>view(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, context_length, context_length))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Input vectors x - (B, N, d_in)</span>
</span></span><span style="display:flex;"><span>        B, N, d_in <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Obtain keys, values and queries in one go - (B, N, d_out)</span>
</span></span><span style="display:flex;"><span>        qkv <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_attn(x)
</span></span><span style="display:flex;"><span>        queries, keys, values <span style="color:#f92672">=</span> qkv<span style="color:#f92672">.</span>chunk(<span style="color:#ae81ff">3</span>, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h)</span>
</span></span><span style="display:flex;"><span>        k <span style="color:#f92672">=</span> keys<span style="color:#f92672">.</span>view(B, N, self<span style="color:#f92672">.</span>num_heads, self<span style="color:#f92672">.</span>d_h)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> values<span style="color:#f92672">.</span>view(B, N, self<span style="color:#f92672">.</span>num_heads, self<span style="color:#f92672">.</span>d_h)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        q <span style="color:#f92672">=</span> queries<span style="color:#f92672">.</span>view(B, N, self<span style="color:#f92672">.</span>num_heads, self<span style="color:#f92672">.</span>d_h)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>) 
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Apply scaled dot-product attention with causal mask on each head</span>
</span></span><span style="display:flex;"><span>        attn_scores <span style="color:#f92672">=</span> (q <span style="color:#f92672">@</span> k<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>)) <span style="color:#f92672">*</span> k<span style="color:#f92672">.</span>shape[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">**-</span><span style="color:#ae81ff">0.5</span>    
</span></span><span style="display:flex;"><span>        attn_scores <span style="color:#f92672">=</span> attn_scores<span style="color:#f92672">.</span>masked_fill(self<span style="color:#f92672">.</span>masked[:, :, :N, :N] <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, float(<span style="color:#e6db74">&#39;-inf&#39;</span>))
</span></span><span style="display:flex;"><span>        attn_weights <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(attn_scores, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)                             
</span></span><span style="display:flex;"><span>        attn_weights <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>attn_dropout(attn_weights)
</span></span><span style="display:flex;"><span>        context_vec <span style="color:#f92672">=</span> attn_weights <span style="color:#f92672">@</span> v
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out)</span>
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> context_vec<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(B, N, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_proj(out)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LayerNorm</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, emb_dim):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>eps <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-5</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>weight <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>ones(emb_dim))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bias <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>zeros(emb_dim))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        mean <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        var <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>var(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, unbiased<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)       <span style="color:#75715e"># Used biased variance estimation (without Bessel&#39;s correction)</span>
</span></span><span style="display:flex;"><span>        norm_x <span style="color:#f92672">=</span> (x <span style="color:#f92672">-</span> mean) <span style="color:#f92672">/</span> torch<span style="color:#f92672">.</span>sqrt(var <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>eps)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>weight <span style="color:#f92672">*</span> norm_x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>bias
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GELU</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> x <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>tanh(
</span></span><span style="display:flex;"><span>            torch<span style="color:#f92672">.</span>sqrt(torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2.0</span> <span style="color:#f92672">/</span> torch<span style="color:#f92672">.</span>pi)) <span style="color:#f92672">*</span>
</span></span><span style="display:flex;"><span>            (x <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.044715</span> <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>pow(x, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>        )) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MLP</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, emb_dim):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_fc <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(emb_dim, <span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> emb_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>c_proj <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> emb_dim, emb_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gelu <span style="color:#f92672">=</span> GELU()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>gelu(self<span style="color:#f92672">.</span>c_fc(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>c_proj(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>Here, I’ve taken our GPT-1 implementation and adapted it to match GPT-2’s naming conventions.</p>
<p>A key detail is the <code>masked</code> variable, which is defined in <code>register_buffer</code>. This automatically adds it to the model’s state dictionary, meaning you’ll find entries like <code>transformer.h.0.attn.masked</code>. However, since it’s a non-trainable tensor, it does not exist in Hugging Face’s state dictionary. So, when loading pre-trained weights, we’ll simply ignore it.</p>
<p>The GPT-2 decoder block closely follows the GPT-1 model we implemented earlier, with a few key modifications. Let’s break them down.</p>
<h3 id="pre-norm-transformer">Pre-Norm Transformer<a hidden class="anchor" aria-hidden="true" href="#pre-norm-transformer">#</a></h3>
<p>The original Transformer (as well as our GPT-1 implementation) used Post-Norm residual connections, where LayerNorm is applied <em>after</em> the sublayer and residual addition.</p>
<figure class="align-center ">
    <img loading="lazy" src="../pre-post-norm.png#center"
         alt="Pre-Norm vs Post-Norm Transformer Layers"/> <figcaption>
            <p>Pre-Norm vs Post-Norm Transformer Layers</p>
        </figcaption>
</figure>

<p>In this setup, the raw output of each sublayer (such as the self-attention or feedforward network) is directly added to the residual before normalization. This can lead to instability as the network depth increases because the residual sum $[x_l + \text{Sublayer}(x_l)]$ accumulates unnormalized activations over layers:</p>
<ul>
<li>
<p>If $\text{Sublayer}(x_l)$ outputs are very small values, the residual connection mostly carries $x_l$ forward without significant updates, leading to vanishing gradients.</p>
</li>
<li>
<p>If $\text{Sublayer}(x_l)$ has large activations, they keep adding up over layers. Although LayerNorm tries to regulate this after the accumulation, the activations have already influenced the residual pathway, potentially leading to exploding gradients.</p>
</li>
</ul>
<p>Essentially, LayerNorm in Post-Norm setups tries to fix activations at the last moment—after instability has already propagated.</p>
<p>To address this, Xiong et al. <a href="#references">[2]</a> proposed Pre-Norm residual units, where LayerNorm is applied <em>before</em> the sublayer. This ensures each sublayer receives well-scaled inputs, improving gradient flow and training stability.</p>
<p>Most modern Large Language Models, including GPT-2, have adopted this Pre-Norm approach. The key changes are:</p>
<ol>
<li>
<p>Layer Normalization is moved to the input of each sub-block, ensuring that each sublayer receives normalized inputs.</p>
</li>
<li>
<p>An additional Layer Normalization is added after the final self-attention block, further improving stability.</p>
</li>
</ol>
<p>With these changes, the GPT-2 architecture now looks like this:
<figure class="align-center ">
    <img loading="lazy" src="../gpt-2.png#center"
         alt="GPT-2 architecture" width="500"/> <figcaption>
            <p>GPT-2 architecture</p>
        </figcaption>
</figure>
</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DecoderBlock</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, cfg):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ln_1 <span style="color:#f92672">=</span> LayerNorm(emb_dim<span style="color:#f92672">=</span>cfg[<span style="color:#e6db74">&#39;emb_dim&#39;</span>])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>attn <span style="color:#f92672">=</span> MultiHeadAttention(d_in<span style="color:#f92672">=</span>cfg[<span style="color:#e6db74">&#39;emb_dim&#39;</span>], d_out<span style="color:#f92672">=</span>cfg[<span style="color:#e6db74">&#39;emb_dim&#39;</span>], context_length<span style="color:#f92672">=</span>cfg[<span style="color:#e6db74">&#39;context_length&#39;</span>], attn_pdrop<span style="color:#f92672">=</span>cfg[<span style="color:#e6db74">&#39;attn_pdrop&#39;</span>], num_heads<span style="color:#f92672">=</span>cfg[<span style="color:#e6db74">&#39;n_heads&#39;</span>], qkv_bias<span style="color:#f92672">=</span>cfg[<span style="color:#e6db74">&#39;qkv_bias&#39;</span>])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>ln_2 <span style="color:#f92672">=</span> LayerNorm(emb_dim<span style="color:#f92672">=</span>cfg[<span style="color:#e6db74">&#39;emb_dim&#39;</span>])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>mlp <span style="color:#f92672">=</span> MLP(emb_dim<span style="color:#f92672">=</span>cfg[<span style="color:#e6db74">&#39;emb_dim&#39;</span>])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>resid_dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(cfg[<span style="color:#e6db74">&#39;resid_pdrop&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>resid_dropout(self<span style="color:#f92672">.</span>attn(self<span style="color:#f92672">.</span>ln_1(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>resid_dropout(self<span style="color:#f92672">.</span>mlp(self<span style="color:#f92672">.</span>ln_2(x)))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>And our GPT-2 model will be coded as:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GPT2</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, cfg):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>transformer <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleDict(dict(
</span></span><span style="display:flex;"><span>            wte <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(cfg[<span style="color:#e6db74">&#39;vocab_size&#39;</span>], cfg[<span style="color:#e6db74">&#39;emb_dim&#39;</span>]),                      <span style="color:#75715e"># Token Embeddings</span>
</span></span><span style="display:flex;"><span>            wpe <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(cfg[<span style="color:#e6db74">&#39;context_length&#39;</span>], cfg[<span style="color:#e6db74">&#39;emb_dim&#39;</span>]),                  <span style="color:#75715e"># Position Encoding</span>
</span></span><span style="display:flex;"><span>            embd_dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(cfg[<span style="color:#e6db74">&#39;embd_pdrop&#39;</span>]),                               <span style="color:#75715e"># Embedding dropout</span>
</span></span><span style="display:flex;"><span>            h <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>[DecoderBlock(cfg) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(cfg[<span style="color:#e6db74">&#39;n_layers&#39;</span>])]),    <span style="color:#75715e"># Multiple Decoder blocks</span>
</span></span><span style="display:flex;"><span>            ln_f <span style="color:#f92672">=</span> LayerNorm(cfg[<span style="color:#e6db74">&#39;emb_dim&#39;</span>]),                                           <span style="color:#75715e"># Final layernorm</span>
</span></span><span style="display:flex;"><span>        ))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>lm_head <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(cfg[<span style="color:#e6db74">&#39;emb_dim&#39;</span>], cfg[<span style="color:#e6db74">&#39;vocab_size&#39;</span>], bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)         <span style="color:#75715e"># Language modelling head</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        B, N <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>size()
</span></span><span style="display:flex;"><span>        token_emb <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>wte(x)                                 <span style="color:#75715e"># output: (B, N, D)</span>
</span></span><span style="display:flex;"><span>        pos_emb <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>wpe(torch<span style="color:#f92672">.</span>arange(N, device<span style="color:#f92672">=</span>device))      <span style="color:#75715e"># output: (N, D)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> token_emb <span style="color:#f92672">+</span> pos_emb                                             <span style="color:#75715e"># ouput: (B, N, D)</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>h(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>ln_f(x)
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lm_head(x)                                            <span style="color:#75715e"># ouput: (B, N, vocab_size)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> logits
</span></span></code></pre></div><p>The hyperparameters of our model are defined in a Python dictionary, <code>cfg</code>, which is passed when instantiating the model.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Define configuration dictionary</span>
</span></span><span style="display:flex;"><span>GPT_CONFIG_124M <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;vocab_size&#34;</span>: <span style="color:#ae81ff">50257</span>,   <span style="color:#75715e"># 50,000 BPE merges + 256 byte tokens + 1 &lt;|endoftext|&gt; token</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;context_length&#34;</span>: <span style="color:#ae81ff">1024</span>, <span style="color:#75715e"># Maximum sequence length</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;emb_dim&#34;</span>: <span style="color:#ae81ff">768</span>,         <span style="color:#75715e"># Embedding dimension</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;n_heads&#34;</span>: <span style="color:#ae81ff">12</span>,          <span style="color:#75715e"># Number of attention heads</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;n_layers&#34;</span>: <span style="color:#ae81ff">12</span>,         <span style="color:#75715e"># Number of transformer blocks</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;attn_pdrop&#34;</span>: <span style="color:#ae81ff">0.1</span>,      <span style="color:#75715e"># Dropout probability for attention layers</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;embd_pdrop&#34;</span>: <span style="color:#ae81ff">0.1</span>,      <span style="color:#75715e"># Dropout probability for embeddings</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;resid_pdrop&#34;</span>: <span style="color:#ae81ff">0.1</span>,     <span style="color:#75715e"># Dropout probability for residual connections</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;qkv_bias&#34;</span>: <span style="color:#66d9ef">True</span>        <span style="color:#75715e"># Whether to use bias in QKV projection</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> GPT2(GPT_CONFIG_124M)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Print the number of parameters in the model</span>
</span></span><span style="display:flex;"><span>print(sum(p<span style="color:#f92672">.</span>numel() <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>parameters()) <span style="color:#f92672">/</span> <span style="color:#ae81ff">1e6</span>, <span style="color:#e6db74">&#39;M parameters&#39;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span><span style="color:#ae81ff">163.037184</span> M parameters
</span></span></code></pre></div><p>Oops! Our model has 163M parameters, even though we aimed to replicate the 124M parameter version. What&rsquo;s going on?</p>
<p>Actually, nothing is wrong! The discrepancy arises due to the weight tying scheme used in the GPT-2 model. Let’s dive into it in more detail.</p>
<h3 id="weight-tying">Weight tying<a hidden class="anchor" aria-hidden="true" href="#weight-tying">#</a></h3>
<p>The first layer of our architecture is the token embedding table, which maps each token in our vocabulary to an embedding vector. The last layer is the language modeling head, which reverses this process by mapping the embeddings back to vocabulary space.</p>
<p>If we inspect our model’s state dictionary, we find that both layers have the same shape:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>gpt2_model_params <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>state_dict()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(gpt2_model_params[<span style="color:#e6db74">&#34;transformer.wte.weight&#34;</span>]<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>print(gpt2_model_params[<span style="color:#e6db74">&#34;lm_head.weight&#34;</span>]<span style="color:#f92672">.</span>shape)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span>torch.Size([<span style="color:#ae81ff">50257</span>, <span style="color:#ae81ff">768</span>])
</span></span><span style="display:flex;"><span>torch.Size([<span style="color:#ae81ff">50257</span>, <span style="color:#ae81ff">768</span>])
</span></span></code></pre></div><p>This occurs because of how <code>nn.Embedding</code> and <code>nn.Linear</code> store their weights internally:</p>
<ul>
<li><code>nn.Embedding(dim_in, dim_out)</code> stores its lookup table as <code>torch.Size([dim_in, dim_out])</code>.</li>
<li><code>nn.Linear(features_in, features_out)</code> stores its weight matrix as <code>torch.Size([features_out, features_in])</code> to perform <code>y = Wx</code> operations.</li>
</ul>
<p>This observation led to the practice of weight tying, where the token embedding and language modeling head share the same set of weights. This approach was also used in the original Transformer paper.</p>
<p>By enforcing weight sharing, we inject an inductive bias into the model, indicating that:</p>
<ul>
<li>The way tokens are mapped to embeddings should be similar to how embeddings are mapped back to tokens.</li>
<li>The model should maintain consistency between input and output representations.</li>
</ul>
<p>If we visualize the parameter distribution in our model (with just one transformer block), we get:
<figure class="align-center ">
    <img loading="lazy" src="../output.png#center"
         alt="Parameter sizes of our model with one transformer block"/> <figcaption>
            <p>Parameter sizes of our model with one transformer block</p>
        </figcaption>
</figure>
</p>
<p>Clearly, the majority of parameters reside in the token embeddings and the language modeling layer. By making them share weights, we reduce the overall parameter count, leading to lower memory usage and faster convergence speed since fewer parameters need to be learned.</p>
<p>We can enable weight tying by directly assigning the same weight tensor:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Weight tying</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>transformer<span style="color:#f92672">.</span>wte<span style="color:#f92672">.</span>weight <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>lm_head<span style="color:#f92672">.</span>weight
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Print the number of parameters in the model</span>
</span></span><span style="display:flex;"><span>print(sum(p<span style="color:#f92672">.</span>numel() <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>parameters())<span style="color:#f92672">/</span><span style="color:#ae81ff">1e6</span>, <span style="color:#e6db74">&#39;M parameters&#39;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span><span style="color:#ae81ff">124.439808</span> M parameters
</span></span></code></pre></div><p>Hurray! We now have our own GPT-2 124M model.</p>
<p>Next, let’s explore the weight initialization scheme used in GPT-2. This is useful if you want to train the model from scratch on your own dataset. However, if you&rsquo;re only interested in loading pre-trained weights, feel free to skip this section.</p>
<h3 id="weight-initialization">Weight initialization<a hidden class="anchor" aria-hidden="true" href="#weight-initialization">#</a></h3>
<p>The GPT-2 paper doesn’t explicitly mention the weight initialization used, but we can infer it from the <code>model.py</code> code. Here’s the initialization strategy:</p>
<ul>
<li>Token embeddings: Normal distribution with standard deviation of $0.02$.</li>
<li>Position embeddings: Normal distribution with standard deviation of $0.01$.</li>
<li>Linear layers:
<ul>
<li>Weights: Normal distribution with standard deviation of $0.02$.</li>
<li>Bias: Initialized to zero.</li>
</ul>
</li>
<li>LayerNorm: The scale and shift parameters are initialized to ones and zeros, respectively, similar to our implementation.</li>
</ul>
<p>Additionally, the GPT-2 paper mentions using a scaling factor of $1/\sqrt{\text{N}}$, where $\text{N}$ is the number of residual layers, on the weights of residual layer at initialization. This is to account for the accumulation of activations on the residual path — the standard deviation of activations grows inside the residual stream due to repeated additions. However, the GPT-2 code doesn’t seem to implement this scaling, so we’ll skip it for now.</p>
<p>Here’s how we can implement the weight initialization:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Initialize weights as per GPT-2</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gpt2_init</span>(m):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> isinstance(m, nn<span style="color:#f92672">.</span>Linear):        <span style="color:#75715e"># Applies to linear layers only</span>
</span></span><span style="display:flex;"><span>        nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>normal_(m<span style="color:#f92672">.</span>weight, mean<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.02</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> m<span style="color:#f92672">.</span>bias <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>zeros_(m<span style="color:#f92672">.</span>bias)      <span style="color:#75715e"># Bias initialized to zero</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">elif</span> isinstance(m, nn<span style="color:#f92672">.</span>Embedding):   <span style="color:#75715e"># Applied to embedding layer only</span>
</span></span><span style="display:flex;"><span>        nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>normal_(m<span style="color:#f92672">.</span>weight, mean<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, std<span style="color:#f92672">=</span><span style="color:#ae81ff">0.02</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Apply initialization</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>apply(gpt2_init)
</span></span></code></pre></div><p>Note: I’ve used a standard deviation of 0.02 for the position embeddings as well because the difference doesn’t significantly impact the model, and it keeps the code simpler.</p>
<h2 id="loading-weights">Loading Weights<a hidden class="anchor" aria-hidden="true" href="#loading-weights">#</a></h2>
<p>To load weights from HuggingFace, we simply copy the tensor values over to our model. For this, the tensor shapes need to match exactly between the HuggingFace model and our custom model.</p>
<p>Let’s compare the state dictionaries of both models side by side to identify any mismatches in tensor shapes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> model_hf_params<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> param<span style="color:#f92672">.</span>shape <span style="color:#f92672">!=</span> gpt2_model_params[name]<span style="color:#f92672">.</span>shape:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Shape mismatch for parameter </span><span style="color:#e6db74">{</span>name<span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>param<span style="color:#f92672">.</span>shape<span style="color:#e6db74">}</span><span style="color:#e6db74"> vs </span><span style="color:#e6db74">{</span>gpt2_model_params[name]<span style="color:#f92672">.</span>shape<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><pre tabindex="0"><code>Shape mismatch for parameter transformer.h.0.attn.c_attn.weight: torch.Size([768, 2304]) vs torch.Size([2304, 768])
Shape mismatch for parameter transformer.h.0.mlp.c_fc.weight: torch.Size([768, 3072]) vs torch.Size([3072, 768])
Shape mismatch for parameter transformer.h.0.mlp.c_proj.weight: torch.Size([3072, 768]) vs torch.Size([768, 3072])
</code></pre><p>Oops, why don&rsquo;t the shapes match?</p>
<p>OpenAI’s GPT-2 checkpoints use a <code>Conv1d</code> module on each linear layer in the GPT-2 architecture. This is why the tensors are transposed and do not match directly.</p>
<p>As a result, the layers <code>c_attn</code>, <code>c_proj</code>, <code>c_fc</code> and <code>c_proj</code> need to be handled differently. We transpose the weights before copying them to ensure they match the expected shapes and are copied correctly.</p>
<p>Here’s how to load the weights:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Loading weights in your model</span>
</span></span><span style="display:flex;"><span>transposed <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;attn.c_attn.weight&#39;</span>, <span style="color:#e6db74">&#39;attn.c_proj.weight&#39;</span>, <span style="color:#e6db74">&#39;mlp.c_fc.weight&#39;</span>, <span style="color:#e6db74">&#39;mlp.c_proj.weight&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> model_hf_params<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># check if the parameter name matches</span>
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">if</span> name <span style="color:#f92672">in</span> gpt2_model_params:                
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># if the parameter has to be transposed</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> name<span style="color:#f92672">.</span>endswith(tuple(transposed)):       
</span></span><span style="display:flex;"><span>          gpt2_model_params[name]<span style="color:#f92672">.</span>copy_(param<span style="color:#f92672">.</span>t())  <span style="color:#75715e"># Tranpose the weights and then copy</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># if the parameter shape matches directly</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> param<span style="color:#f92672">.</span>shape <span style="color:#f92672">==</span> gpt2_model_params[name]<span style="color:#f92672">.</span>shape:
</span></span><span style="display:flex;"><span>              gpt2_model_params[name]<span style="color:#f92672">.</span>copy_(param)  <span style="color:#75715e"># copy the weights over</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Shape mismatch for parameter </span><span style="color:#e6db74">{</span>name<span style="color:#e6db74">}</span><span style="color:#e6db74">: </span><span style="color:#e6db74">{</span>param<span style="color:#f92672">.</span>shape<span style="color:#e6db74">}</span><span style="color:#e6db74"> vs </span><span style="color:#e6db74">{</span>gpt2_model_params[name]<span style="color:#f92672">.</span>shape<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>          print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Parameter </span><span style="color:#e6db74">{</span>name<span style="color:#e6db74">}</span><span style="color:#e6db74"> not found in your model&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Weights are loaded successfully!&#34;</span>)
</span></span></code></pre></div><p>This code will load the pre-trained HuggingFace weights into our model, handling special treatment for the linear layers to ensure that everything is correctly aligned.</p>
<h2 id="generating-text">Generating text<a hidden class="anchor" aria-hidden="true" href="#generating-text">#</a></h2>
<p>Now that we’ve successfully loaded the weights, it&rsquo;s time to test our model by generating some text. We’ll use the <code>tiktoken</code> tokenizer to encode the initial prompt, &ldquo;<em>Hello, I&rsquo;m a language model,</em>&rdquo; and pass it into the model to generate a continuation.</p>
<p>Here’s a basic text generation loop:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Generate text from the trained model</span>
</span></span><span style="display:flex;"><span>max_new_tokens <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>context_length <span style="color:#f92672">=</span> <span style="color:#ae81ff">1024</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tiktoken
</span></span><span style="display:flex;"><span>enc <span style="color:#f92672">=</span> tiktoken<span style="color:#f92672">.</span>get_encoding(<span style="color:#e6db74">&#34;gpt2&#34;</span>)
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> enc<span style="color:#f92672">.</span>encode(<span style="color:#e6db74">&#34;Hello, I&#39;m a language model,&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>context <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(tokens, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long, device<span style="color:#f92672">=</span>device)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)    <span style="color:#75715e"># Adds the batch dimension: (B=1, N)</span>
</span></span><span style="display:flex;"><span>stored_context <span style="color:#f92672">=</span> context
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(max_new_tokens):
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        context <span style="color:#f92672">=</span> stored_context[:, <span style="color:#f92672">-</span>context_length:]                      <span style="color:#75715e"># Trim the context to fit the model&#39;s context length</span>
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> model(context)                                            <span style="color:#75715e"># logits: (B=1, N, vocab_size)</span>
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> logits[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, :]                                          <span style="color:#75715e"># take just the last time step (B, vocab_size)</span>
</span></span><span style="display:flex;"><span>        probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        next_token <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(probs, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1.</span> keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)             <span style="color:#75715e"># Select the most likely next token (B=1, N)</span>
</span></span><span style="display:flex;"><span>        stored_context <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((stored_context, next_token), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)    <span style="color:#75715e"># (B=1, N+1)</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>print(enc<span style="color:#f92672">.</span>decode(stored_context[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>tolist()))                               
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span>Hello, I<span style="color:#960050;background-color:#1e0010">&#39;</span>m a language model, not a programming language. I<span style="color:#960050;background-color:#1e0010">&#39;</span>m a language model. I<span style="color:#960050;background-color:#1e0010">&#39;</span>m a language model. I<span style="color:#960050;background-color:#1e0010">&#39;</span>m a
</span></span></code></pre></div><p>The model gets stuck in a loop, generating repeated phrases like &ldquo;I&rsquo;m a language model.&rdquo; This is because we are always selecting the token with the highest probability at each step, which limits the model&rsquo;s creativity and causes repetition. To resolve this, we can explore probabilistic sampling methods.</p>
<h3 id="probabilistic-sampling">Probabilistic Sampling<a hidden class="anchor" aria-hidden="true" href="#probabilistic-sampling">#</a></h3>
<p>To introduce more variety and creativity in the decoding process, we replace the <code>argmax</code> function with <code>multinomial</code>. This method uses the probability distribution output by the model to sample the next token proportionally to its probability score:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>logits <span style="color:#f92672">=</span> model(context)                               <span style="color:#75715e"># logits: (B=1, N, vocab_size)</span>
</span></span><span style="display:flex;"><span>logits <span style="color:#f92672">=</span> logits[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, :]                             <span style="color:#75715e"># take just the last time step (B, vocab_size)</span>
</span></span><span style="display:flex;"><span>probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>next_token <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(probs, num_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># Sample from the distribution (B=1, N)</span>
</span></span></code></pre></div><p>By using probabilistic sampling, we can explore a range of potential next tokens, leading to more diverse and interesting text.</p>
<p>While this is a good way to sample text, there are other decoding strategies that allow us to control the distribution and selection process to generate more original text.</p>
<h3 id="temperature-scaling">Temperature Scaling<a hidden class="anchor" aria-hidden="true" href="#temperature-scaling">#</a></h3>
<p>Let’s understand temperature scaling through an example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(torch<span style="color:#f92672">.</span>softmax(torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">0.1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.3</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.5</span>]), dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>print(torch<span style="color:#f92672">.</span>softmax(torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">0.1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.3</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.5</span>])<span style="color:#f92672">/</span><span style="color:#ae81ff">0.001</span>, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span>tensor([<span style="color:#ae81ff">0.1925</span>, <span style="color:#ae81ff">0.1426</span>, <span style="color:#ae81ff">0.2351</span>, <span style="color:#ae81ff">0.1426</span>, <span style="color:#ae81ff">0.2872</span>])
</span></span><span style="display:flex;"><span>tensor([<span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">0.</span>, <span style="color:#ae81ff">1.</span>])
</span></span></code></pre></div><p>When the magnitudes of logits are large, the softmax output saturates and converges to a one-hot encoding. Temperature scaling works similarly—by dividing the logits by a number greater than zero:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>logits <span style="color:#f92672">=</span> model(context)                               
</span></span><span style="display:flex;"><span>logits <span style="color:#f92672">=</span> logits[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, :] <span style="color:#f92672">/</span> temperature                         
</span></span><span style="display:flex;"><span>probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>next_token <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(probs, num_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  
</span></span></code></pre></div><p>Temperature scaling allows us to control the randomness of the output:</p>
<ul>
<li>Temperature &lt; 1: Produces more confident (sharper) distributions, picking the most likely token almost always.</li>
<li>Temperatures &gt; 1: Results in a more uniformly distributed token probabilties, where other tokens are selected more often. This can add more variety but may also produce nonsensical text.</li>
<li>Temperature = 1: This is equivalent to not using any temperature scaling.</li>
</ul>
<h3 id="top-k-sampling">Top-k sampling<a hidden class="anchor" aria-hidden="true" href="#top-k-sampling">#</a></h3>
<p>In top-k sampling, we restrict the sampling process to the top-k most likely tokens and exclude the rest by masking their probabilities. This ensures that we avoid sampling very rare tokens while still providing some diversity in the output.</p>
<p>We achieve this by setting the logits of non-selected tokens to negative infinity, so that their softmax probabilities become zero, and the remaining probabilities sum to 1. The implementation is as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">top_k_logits</span>(logits, k):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> k <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> logits                   <span style="color:#75715e"># No truncation</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    values, _ <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(logits, k<span style="color:#f92672">=</span>k) <span style="color:#75715e"># Get top-k values</span>
</span></span><span style="display:flex;"><span>    min_value <span style="color:#f92672">=</span> values[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]           <span style="color:#75715e"># Minimum value in top-k</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>where(logits <span style="color:#f92672">&lt;</span> min_value, torch<span style="color:#f92672">.</span>tensor(float(<span style="color:#e6db74">&#39;-inf&#39;</span>)), logits)
</span></span></code></pre></div><p><code>torch.topk</code> retrieves the values of top-k logits in descending order, and the <code>where</code> function sets the logits of tokens below the lowest logit value to negative infinity. This ensures that only the top-k logits contribute to the probability distribution.</p>
<h3 id="top-p-nucleus-sampling">Top-p (Nucleus Sampling)<a hidden class="anchor" aria-hidden="true" href="#top-p-nucleus-sampling">#</a></h3>
<p>While top-k gives us the ability to select the top-k tokens to consider in the sampling process, top-p dynamically selects the top tokens whose cumulative probability exceeds a certain threshold, denoted by <code>p</code>. Instead of a fixed number <code>k</code>, it adapts based on the distribution.</p>
<p>For example, we first sort the tokens by probability:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span>Token A: <span style="color:#ae81ff">0.40</span>  
</span></span><span style="display:flex;"><span>Token B: <span style="color:#ae81ff">0.30</span>  
</span></span><span style="display:flex;"><span>Token C: <span style="color:#ae81ff">0.20</span>  
</span></span><span style="display:flex;"><span>Token D: <span style="color:#ae81ff">0.05</span>  
</span></span><span style="display:flex;"><span>Token E: <span style="color:#ae81ff">0.05</span>  
</span></span></code></pre></div><p>Next, we compute the cumulative probability:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-csharp" data-lang="csharp"><span style="display:flex;"><span>Token A: <span style="color:#ae81ff">0.40</span>  
</span></span><span style="display:flex;"><span>Token A + B: <span style="color:#ae81ff">0.70</span>  
</span></span><span style="display:flex;"><span>Token A + B + C: <span style="color:#ae81ff">0.90</span>  <span style="color:#960050;background-color:#1e0010">✅</span> (stop here, because we reached p=<span style="color:#ae81ff">0.9</span>)
</span></span><span style="display:flex;"><span>Token A + B + C + D: <span style="color:#ae81ff">0.95</span>  
</span></span><span style="display:flex;"><span>Token A + B + C + D + E: <span style="color:#ae81ff">1.00</span>  
</span></span></code></pre></div><p>We keep only tokens A, B, and C since their cumulative probability exceeds <code>p=0.9</code>. The rest are discarded.</p>
<p>Here’s how it is implemented in code:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">top_p_logits</span>(logits, p):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Nucleus Sampling</span>
</span></span><span style="display:flex;"><span>    sorted_logits, _ <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sort(logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, descending<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)                 <span style="color:#75715e"># Sort logits in descending order</span>
</span></span><span style="display:flex;"><span>    cumulative_probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cumsum(torch<span style="color:#f92672">.</span>softmax(sorted_logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>), dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># Compute cumulative probabilities</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Determine number of indices to include, keeping at least one</span>
</span></span><span style="display:flex;"><span>    num_to_keep <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>clamp((cumulative_probs <span style="color:#f92672">&lt;=</span> p)<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>, min<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    min_value <span style="color:#f92672">=</span> sorted_logits[:, num_to_keep]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>where(logits <span style="color:#f92672">&lt;</span> min_value, torch<span style="color:#f92672">.</span>tensor(float(<span style="color:#e6db74">&#39;-inf&#39;</span>)), logits)
</span></span></code></pre></div><h3 id="sampling-code">Sampling code<a hidden class="anchor" aria-hidden="true" href="#sampling-code">#</a></h3>
<p>Now, let’s integrate all the sampling strategies we’ve discussed and implement a <code>sample()</code> function, similar to the one in GPT-2. You can find the original GPT-2 implementation here: <a href="https://github.com/openai/gpt-2/blob/master/src/sample.py">https://github.com/openai/gpt-2/blob/master/src/sample.py</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sample</span>(max_new_tokens, context_length, start_token<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, context<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, top_k<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, top_p<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> start_token <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>       <span style="color:#66d9ef">assert</span> context <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>, <span style="color:#e6db74">&#39;Specify exactly one of start_token and context!&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">assert</span> context <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>, <span style="color:#e6db74">&#39;Specify exactly one of start_token and context!&#39;</span>
</span></span><span style="display:flex;"><span>        context <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>full((<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), start_token, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long, device<span style="color:#f92672">=</span>device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    stored_context <span style="color:#f92672">=</span> context
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(max_new_tokens):
</span></span><span style="display:flex;"><span>        model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>            context <span style="color:#f92672">=</span> stored_context[:, <span style="color:#f92672">-</span>context_length:]
</span></span><span style="display:flex;"><span>            logits <span style="color:#f92672">=</span> model(context)                                            <span style="color:#75715e"># logits: (B=1, N, vocab_size)</span>
</span></span><span style="display:flex;"><span>            logits <span style="color:#f92672">=</span> logits[:, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, :] <span style="color:#f92672">/</span> temperature                            <span style="color:#75715e"># Scale logits by temperature</span>
</span></span><span style="display:flex;"><span>            logits <span style="color:#f92672">=</span> top_k_logits(logits, k<span style="color:#f92672">=</span>top_k)                             <span style="color:#75715e"># Apply top-k filtering</span>
</span></span><span style="display:flex;"><span>            logits <span style="color:#f92672">=</span> top_p_logits(logits, p<span style="color:#f92672">=</span>top_p)                             <span style="color:#75715e"># Apply top-p (nucleus) sampling</span>
</span></span><span style="display:flex;"><span>            probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>softmax(logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)                              <span style="color:#75715e"># Convert logits to probabilities</span>
</span></span><span style="display:flex;"><span>            next_token <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(probs, num_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)               <span style="color:#75715e"># Sample from the distribution (B=1, N)</span>
</span></span><span style="display:flex;"><span>            stored_context <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat((stored_context, next_token), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)    <span style="color:#75715e"># (B=1, N+1)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> stored_context
</span></span></code></pre></div><p>Using this function, we can start generating text either using a start token or an initial prompt.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> enc<span style="color:#f92672">.</span>encode(<span style="color:#e6db74">&#34;Hello, I&#39;m a language model,&#34;</span>)
</span></span><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(tokens, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long, device<span style="color:#f92672">=</span>device)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)    <span style="color:#75715e"># Adds the batch dimension(B=1, N)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>next_tokens <span style="color:#f92672">=</span> sample(context<span style="color:#f92672">=</span>prompt, max_new_tokens<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>, context_length<span style="color:#f92672">=</span>GPT_CONFIG_124M[<span style="color:#e6db74">&#34;context_length&#34;</span>], top_p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, top_k<span style="color:#f92672">=</span><span style="color:#ae81ff">40</span>, temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>)
</span></span><span style="display:flex;"><span>print(enc<span style="color:#f92672">.</span>decode(next_tokens[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>tolist()))       
</span></span></code></pre></div><p>Here&rsquo;s the output:</p>
<pre tabindex="0"><code>Hello, I&#39;m a language model, and I wanted to do something more powerful than just an English translation of an English sentence.&#34;
</code></pre><p>And with that, we&rsquo;ve explored some of the most exciting text generation techniques that can bring out the true creativity of language models. Now, it&rsquo;s your turn to experiment and create your own stories! Happy coding!</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Andrej Karpathy&rsquo;s video: Let&rsquo;s reproduce GPT-2 (124M)</a></li>
<li>[1] Radford et al., &ldquo;<a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a>&rdquo;, OpenAI 2019.</li>
<li>[2] Xiong et al., &ldquo;<a href="https://arxiv.org/abs/2002.04745">On Layer Normalization in the Transformer Architecture</a>&rdquo;, ICML 2020.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://yugajmera.github.io/posts/09-gpt2.1/post/">
    <span class="title">Next »</span>
    <br>
    <span>Looking into GPT-2 Part 1: BPE Tokenizer</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://yugajmera.github.io/">YA&#39;s Almanac</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
