---
title: "Looking into GPT-2 architecture Part 2: Internals"
date: 2025-03-01
math: mathjax
showToc: true # show contents
TocOpen: false # open contents automantically
---
Let's modify the GPT-1 architecture from before to reproduce the GPT-2 small model, specifically the smallest version with 124 million parameters. Note that while the paper mentions this as 117 million parameters, this was later corrected.

Unlike other GPT models, OpenAI has released the weights of the pre-trained GPT-2 model, which we will load into our implementation. This allows us to have a sanity check of our architecture and allows us to skip the expensive pretraining stage and jump to fine-tuning it.  GPT-2 is also a better choice for learning how to implement LLMs, as it can be run on a single laptop computer, whereas GPT-3 requires a GPU cluster for training and inference.

The model code is released here: [https://github.com/openai/gpt-2/blob/master/src/model.py](https://github.com/openai/gpt-2/blob/master/src/model.py)


## Architecture
Let's get the import functions first,
```python
# Import functions
import torch
import torch.nn as nn

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print("Using", device)
```

Before writing our model, let's look at the GPT-2 weights from huggigface transformer code to understand the nomenclature. 

```python
from transformers import GPT2LMHeadModel

# Instantiate HuggingFace GPT-2 model and get its state dictionary
model_hf = GPT2LMHeadModel.from_pretrained("gpt2").to(device) #124M
model_hf_params = model_hf.state_dict()

for name, param in model_hf_params.items():
    print(name, param.shape)
```
```
transformer.wte.weight torch.Size([50257, 768])
transformer.wpe.weight torch.Size([1024, 768])
transformer.h.0.ln_1.weight torch.Size([768])
transformer.h.0.ln_1.bias torch.Size([768])
transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])
transformer.h.0.attn.c_attn.bias torch.Size([2304])
transformer.h.0.attn.c_proj.weight torch.Size([768, 768])
transformer.h.0.attn.c_proj.bias torch.Size([768])
transformer.h.0.ln_2.weight torch.Size([768])
transformer.h.0.ln_2.bias torch.Size([768])
transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])
transformer.h.0.mlp.c_fc.bias torch.Size([3072])
transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])
transformer.h.0.mlp.c_proj.bias torch.Size([768])

...

transformer.ln_f.weight torch.Size([768])
transformer.ln_f.bias torch.Size([768])
lm_head.weight torch.Size([50257, 768])
```
* The model is enclosed under the namespace `transformer`.
* `wte` is the wieghts of the token embeddings.
* `wpe` is the weights of the position embedding.
* `h` is the list of hidden decoder blocks.
* LayerNorms are represented as `ln_1` and `ln_2` in each decoder block and the final one as `ln_f`. The `scale` and `shift` parameters are renamed to `weight` and `bias`. 
* Our `W_qkv` weight matrix gets renames to `c_attn` (stands for causal attention) and `W_o` renamed to `c_proj`. The causal attention also has a bias tensors meaning, we will set `qkv_bias=True`. 
* `lm_head` is the weights of last linear layer, also called the language modelling head, without a bias term. 


We will follow the same naming convention and let's rewrite the subcomponenets of our model so that we can directly load these weights in our model.

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, attn_pdrop, num_heads, qkv_bias=False):
        super().__init__()
        self.c_attn = nn.Linear(d_in, 3 * d_out, bias=qkv_bias)
        self.c_proj = nn.Linear(d_out, d_out)
        self.attn_dropout = nn.Dropout(attn_pdrop)

        self.d_h = d_out // num_heads                    # head dimension
        self.num_heads = num_heads
        
        self.register_buffer("masked", torch.tril(torch.ones(context_length, context_length)).view(1, 1, context_length, context_length))
    
    def forward(self, x):
        # Input vectors x - (B, N, d_in)
        B, N, d_in = x.shape
        # Obtain keys, values and queries in one go - (B, N, d_out)
        qkv = self.c_attn(x)
        queries, keys, values = qkv.chunk(3, dim=-1)
        
        # Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h)
        k = keys.view(B, N, self.num_heads, self.d_h).transpose(1, 2)
        v = values.view(B, N, self.num_heads, self.d_h).transpose(1, 2)
        q = queries.view(B, N, self.num_heads, self.d_h).transpose(1, 2) 
        
        # Apply scaled dot-product attention with causal mask on each head
        attn_scores = (q @ k.transpose(2, 3)) * k.shape[-1]**-0.5    
        attn_scores = attn_scores.masked_fill(self.masked[:, :, :N, :N] == 0, float('-inf'))
        attn_weights = torch.softmax(attn_scores, dim=-1)                             
        attn_weights = self.attn_dropout(attn_weights)
        context_vec = attn_weights @ v

        # Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out)
        out = context_vec.transpose(1, 2).contiguous().view(B, N, -1)

        out = self.c_proj(out)
        return out

class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        )) 

class MLP(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.c_fc = nn.Linear(emb_dim, 4 * emb_dim)
        self.c_proj = nn.Linear(4 * emb_dim, emb_dim)
        self.gelu = GELU()

    def forward(self, x):
        x = self.gelu(self.c_fc(x))
        x = self.c_proj(x)
        return x

class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.weight = nn.Parameter(torch.ones(emb_dim))
        self.bias = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)       # Used biased variance estimation (without Bessel's correction)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.weight * norm_x + self.bias
```
Here I have simply renamed the weight matrices to match GPT-2 naming convention. The variable in `register_buffer` automatically gets added to the state dictionary of the model, which means you would see `transformer.h.0.attn.masked` in the state dictionary. Since it is a non-trainable tensor, it does not exist in huggingface state dict and we will ignore it. 

The GPT-2 architecture largely follows the GPT-1 model that we coded before with a few modifications. Before writing the decoder block, lets look at them.


### Pre-Norm Transformer

The original Transformer used Post-Norm residual units, where Layer Normalization is applied after the sublayer and residual addition. This setup was also used in the GPT-1 model.

{{< figure src="../pre-post-norm.png" align="center" caption="Pre-Norm vs Post-Norm Transformer Layers.">}}

However, if we examine this setup closely, we see that the raw output of each sublayer (such as the self-attention or feedforward network) is directly added to the residual before normalization. This can lead to instability as the network depth increases because the residual sum $[x_l + \text{Sublayer}(x_l)]$ accumulates unnormalized activations over layers.

* If $\text{Sublayer}(x_l)$ outputs are very small values, the residual connection mostly carries $x_l$ forward without significant updates, leading to vanishing gradients.

* If $\text{Sublayer}(x_l)$ has large activations, they keep adding up over layers. Although LayerNorm tries to regulate this after the accumulation, the activations have already influenced the residual pathway, potentially leading to exploding gradients.



Essentially, LayerNorm only fixes activations at the last momentâ€”after instability has already propagated.

To address this, Xiong et al. [2] proposed pre-norm residual units, where LayerNorm is applied before the sublayer. This ensures that each sublayer receives well-scaled inputs, leading to better gradient flow and more stable training.

Due to its advantages in training speed and stability, most modern Large Language Models now use Pre-Norm Transformers. The key changes are:

1. Layer Normalization is moved to the input of each sub-block, ensuring that each sublayer receives normalized inputs.
2. An additional Layer Normalization is added after the final self-attention block, further improving stability.

The architecture now looks like,
{{< figure src="../gpt-2.png" align="center" width="500" caption="GPT-2 architecture.">}}

```python
class DecoderBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.ln_1 = LayerNorm(emb_dim=cfg['emb_dim'])
        self.attn = MultiHeadAttention(d_in=cfg['emb_dim'], d_out=cfg['emb_dim'], context_length=cfg['context_length'], attn_pdrop=cfg['attn_pdrop'], num_heads=cfg['n_heads'], qkv_bias=cfg['qkv_bias'])
        self.ln_2 = LayerNorm(emb_dim=cfg['emb_dim'])
        self.mlp = MLP(emb_dim=cfg['emb_dim'])
        self.resid_dropout = nn.Dropout(cfg['resid_pdrop'])

    def forward(self, x):
        x = x + self.resid_dropout(self.attn(self.ln_1(x)))
        x = x + self.resid_dropout(self.mlp(self.ln_2(x)))
        return x
```

And our GPT-2 model will be coded as,
```python
class GPT2(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(cfg['vocab_size'], cfg['emb_dim']),                      # Token Embeddings
            wpe = nn.Embedding(cfg['context_length'], cfg['emb_dim']),                  # Position Encoding
            embd_dropout = nn.Dropout(cfg['embd_pdrop']),                               # Embedding dropout
            h = nn.Sequential(*[DecoderBlock(cfg) for _ in range(cfg['n_layers'])]),    # Multiple Decoder blocks
            ln_f = LayerNorm(cfg['emb_dim']),                                           # Final layernorm
        ))
        self.lm_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False)         # Language modelling head

    def forward(self, x):
        B, N = x.size()
        token_emb = self.transformer.wte(x)                                 # output: (B, N, D)
        pos_emb = self.transformer.wpe(torch.arange(N, device=device))      # output: (N, D)
        x = token_emb + pos_emb                                             # ouput: (B, N, D)
        x = self.transformer.h(x)
        x = self.transformer.ln_f(x)
        logits = self.lm_head(x)                                            # ouput: (B, N, vocab_size)
        return logits
```

The configuration of our model, i.e. the hyperparameters will be defined in a python dictionary `cfg` that is passed when we instatiate our model. 

```python
# define configuration dictionary
GPT_CONFIG_124M = {
    "vocab_size" : 50257,   # Number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token
    "context_length": 1024, # Context length
    "emb_dim": 768,         # Embedding dimension
    "n_heads": 12,          # Number of attention heads
    "n_layers": 12,         # Number of layers
    "attn_pdrop": 0.1,      # attention dropout probability
    "embd_pdrop": 0.1,      # embedding dropout probability
    "resid_pdrop": 0.1,     # residual dropout probability
    "qkv_bias": True        # causal attention bias
}

model = GPT2(GPT_CONFIG_124M).to(device)

# print the number of parameters in the model
print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')
```
```csharp
163.037184 M parameters
```
Oops! Our model has 163M parameters, but we had started to write the 124M parameter version. Whats wrong?

Nothing is wrong. This discrepancy is due to the weight tying scheme that GPT-2 model used. Let's look into it in detail. 


### Weight tying
The first layer of our architecture is the token embeedings table that maps each token in our vocabulary to embedding space. The last layer of our architecture is the language modelling layer that just does the reverse, maps embedding output back to vocabulary. 

If we were to look at the state dictionary of our model, we would find that both have the same shapes. 
```python
gpt2_model_params = model.state_dict()

print(gpt2_model_params["transformer.wte.weight"].shape)
print(gpt2_model_params["lm_head.weight"].shape)
```
```csharp
torch.Size([50257, 768])
torch.Size([50257, 768])
```
This is due to how `nn.Embedding` and `nn.Linear` store their weights internally. 
* For `nn.Embedding(dim_in, dim_out)`, the lookup table will stored as `torch.Size([dim_in, dim_out])`.
* For `nn.Linear(features_in, features_out)`, the weights matrix will have shape `torch.Size([features_out, features_in])`, so as to perform `y = Wx` operation. 

This observation led to a scheme called weight tying, the practice of setting the token embedding to share the same set of weights as the prediction head. The original Transformer paper also employed this scheme. We inject an inductive bias that tells the model that ..

If we were to plot the size of parameters of our model, it would look something like this. 
{{< figure src="../output.png" align="center" caption="Parameter sizes of our model, but with just 1 transformer block">}}

As you can clearly note majority of parameters lie in the two end layers, ie. token embeddings and language modelling layer. This means if were we make them share weights, it reduces the number of parameters in the model, leading to lower memory usage and faster computation speed. The model would also converge faster as the model now has to learn far less number of parameters during training. 

We can perform weight tying by directly equating the weights,
```python
# Weight tying
model.transformer.wte.weight = model.lm_head.weight

# print the number of parameters in the model
print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')
```
```csharp
124.439808 M parameters
```
Hurray! We have our own GPT-2 124M model now. 

Next, I want us to look at the weight initialization scheme mentioned in GPT-2 paper for the completeness of the architecture. This can be useful when you want to train the model on your own dataset. Feel free to skip this section if you are interested in only loading the weights directly and using them. 


### Weight initialization
The GPT-2 paper has no mention of the weight intialization used by them, but we can infer them from the `model.py` code. 

* Token embeddings: Normal distribution with standard deviation of $0.02$.
* Position embeddings: Normal distribution with standard deviation of $0.01$.
* Linear layers: 
    * Weights: Normal distribution with standard deviation of $0.02$.
    * Bias: zero
* LayerNorm: The scale and shift parameters are intialized with ones and zeroes, similar to what we have done in our implementation.

Moreover, GPT-2 mentions that they use a scaling factor of $1/\sqrt{\text{N}}$, where $\text{N}$ is the number of residual layers, on the weights of residual layer at initialization. This accounts for the accumalation of activations on the residual path-standard deviation grows inside the residual stream due to repeated additions. 

Weirdly, the GPT-2 code doesnt include this scaling :(, so lets skip this for now.

```python
# Initialize weights as per GPT-2
def gpt2_init(m):
    if isinstance(m, nn.Linear):        # Applies to linear layers only
        nn.init.normal_(m.weight, mean=0.0, std=0.02)
        if m.bias is not None:
            nn.init.zeros_(m.bias)      # Bias initialized to zero
    elif isinstance(m, nn.Embedding):   # Applied to embedding layer only
        nn.init.normal_(m.weight, mean=0.0, std=0.02)

# Apply initialization
model.apply(gpt2_init)
```
Note that I have used standard deviation $0.02$ for position embeddings as well as it does not make much of a difference and was making the code very complicated. 



## Loading weights
To load the huggingface weights into our model, we simply copy the tensor values over to our model. For us to copy them, they should match exactly in tensor shapes. 

Let's look at the state dictionaries of the huggingface model and our own model side by side for comparison. 

```python
for name, param in model_hf_params.items():
    if param.shape != gpt2_model_params[name].shape:
        print(f"Shape mismatch for parameter {name}: {param.shape} vs {gpt2_model_params[name].shape}")
```
```
Shape mismatch for parameter transformer.h.0.attn.c_attn.weight: torch.Size([768, 2304]) vs torch.Size([2304, 768])
Shape mismatch for parameter transformer.h.0.mlp.c_fc.weight: torch.Size([768, 3072]) vs torch.Size([3072, 768])
Shape mismatch for parameter transformer.h.0.mlp.c_proj.weight: torch.Size([3072, 768]) vs torch.Size([768, 3072])
```
Confused on why they are not matching?

OpenAI checkpoints use a Conv1d module on each linear layer of the GPT-2 architecture, which means on `c_attn`, `c_proj`, `c_fc` and `c_proj`. This is why you would see that the tensors are trasposed and do not match. To account for this, we simply transpose them back to match our tensor shapes and copy them over. 

Here is the code to load the weights,
```python
# Loading weights in your model
transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']

with torch.no_grad():
  for name, param in model_hf_params.items():
      # check if parameter match in names
      if name in gpt2_model_params:                
        
        # check if the parameter has to be transposed before copying
        if name.endswith(tuple(transposed)):       
          gpt2_model_params[name].copy_(param.t())  # Tranpose the weights and then copy
        
        # check if the parameter shape matches directly
        elif param.shape == gpt2_model_params[name].shape:
              gpt2_model_params[name].copy_(param)  # copy over
        else:
            print(f"Shape mismatch for parameter {name}: {param.shape} vs {gpt2_model_params[name].shape}")
      else:
          print(f"Parameter {name} not found in your model")
    print("Weights are loaded successfully!")
```


## Generating text
Now that we have loaded our weights, its time to test our model by generating text. We will be using the same code as before for this with `Hello, I'm a language model,` as the initial context. We use tiktoken tokenizer to encode this text into tokens and use it as model input. 

```python
# Generate text from the trained model
max_new_tokens = 30
block_size = 1024

import tiktoken
enc = tiktoken.get_encoding("gpt2")
tokens = enc.encode("Hello, I'm a language model,")

context = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)
stored_context = context

for _ in range(max_new_tokens):
    model.eval()
    with torch.no_grad():
        output = model(context)                                            # output: (B=1, N, vocab_size)
        output = output[:, -1, :]                                          # take just the last time step (B, vocab_size)
        probs = torch.softmax(output, dim=-1)
        next_token = torch.multinomial(probs, num_samples=1)               # Sample from the distribution (B=1, N)
        stored_context = torch.cat((stored_context, next_token), dim=1)    # (B=1, N+1)
        context = stored_context[:, -block_size:]                          # Trim to lastest context

print(enc.decode(stored_context[0].tolist()))
```
```csharp
Hello, I'm a language model, but now I think about languages of the sort that I can solve spelling problems on. It's a huge task. I can't get a match pair

```
Crazy right!?

Although is a straightforward way to sample new text, there are few ways with which we can control the randomness in the sampling process. 

### Top-k sampling
Here we take the probabilities and only keep the top k probabilities, i.e. clamp the others to zero and renormalize. We then essentially sample from only these top k classes. This makes sure that we are never sampling very rare tokens and helps keep the model on track with the generation process and sticks in the vicinity of likely tokens. 

### Top-p

### Temperature scaling







