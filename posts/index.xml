<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on YA&#39;s Almanac</title>
    <link>https://yugajmera.github.io/posts/</link>
    <description>Recent content in Posts on YA&#39;s Almanac</description>
    <generator>Hugo -- 0.124.1</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 14 Nov 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://yugajmera.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Olympics of DL: The ImageNet Challenge</title>
      <link>https://yugajmera.github.io/posts/05-imagenet/post/</link>
      <pubDate>Mon, 14 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://yugajmera.github.io/posts/05-imagenet/post/</guid>
      <description>The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was an annual competition that took place from 2010 to 2017, attracting teams from around the world to showcase their best-performing image classification models. This challenge became a crucial benchmark in the field, with its winners significantly influencing the landscape of image recognition and deep learning research.
The competition used a subset of the ImageNet dataset, containing 1.3M training examples across 1000 different classes, with 50k validation and 100k test examples.</description>
    </item>
    <item>
      <title>Convolutional Neural Networks (CNNs): DL for Images</title>
      <link>https://yugajmera.github.io/posts/04-cnn/post/</link>
      <pubDate>Sun, 18 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://yugajmera.github.io/posts/04-cnn/post/</guid>
      <description>Linear classifiers or MLPs that we have discussed so far don&amp;rsquo;t respect the 2D spatial structure of input images. These images are flattened into a 1D vector before passing them through the network which destroys the spatial structure of the image.
This creates a need for a new computational model that can operate on images while preserving spatial relationships — Convolutional Neural Networks (CNNs). Let&amp;rsquo;s understand the components of this CNN model.</description>
    </item>
    <item>
      <title>Deep Learning Basics Part 3: The Cherry on Top</title>
      <link>https://yugajmera.github.io/posts/03-dl3/post/</link>
      <pubDate>Sun, 21 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://yugajmera.github.io/posts/03-dl3/post/</guid>
      <description>We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.
Regularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set.</description>
    </item>
    <item>
      <title>Deep Learning Basics Part 2: The Icing</title>
      <link>https://yugajmera.github.io/posts/02-dl2/post/</link>
      <pubDate>Fri, 05 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://yugajmera.github.io/posts/02-dl2/post/</guid>
      <description>In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.
However, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable.</description>
    </item>
    <item>
      <title>Deep Learning Bascis Part 1: The Base  of the Cake</title>
      <link>https://yugajmera.github.io/posts/01-dl1/post/</link>
      <pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://yugajmera.github.io/posts/01-dl1/post/</guid>
      <description>While there is a wealth of deep learning content scattered across the internet, I wanted to create a one-stop solution where you can find all the fundamental concepts needed to write your own neural network from scratch.
This series is inspired by Justin Johnson&amp;rsquo;s course, and Stanford&amp;rsquo;s CS231n.
Image Classification Before diving into the details, let’s start with the basics: image classification. This is a core task in computer vision where a model takes an image as input and assigns it a category label from a fixed set of predefined categories.</description>
    </item>
  </channel>
</rss>
