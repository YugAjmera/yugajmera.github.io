<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on YA&#39;s Almanac</title>
    <link>https://yugajmera.github.io/posts/</link>
    <description>Recent content in Posts on YA&#39;s Almanac</description>
    <generator>Hugo -- 0.124.1</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 21 Aug 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://yugajmera.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deep Learning Basics Part 3: The Cherry on Top</title>
      <link>https://yugajmera.github.io/posts/03-dl3/post/</link>
      <pubDate>Sun, 21 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://yugajmera.github.io/posts/03-dl3/post/</guid>
      <description>We’ve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready—but we still need the toppings to make it more appealing. In this part, we’re going to discuss the available toppings—concepts that enhance optimization and help us reach a better final solution for the model’s weights. The most important topping among them is Regularization.
Regularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set.</description>
    </item>
    <item>
      <title>Deep Learning Basics Part 2: The Icing</title>
      <link>https://yugajmera.github.io/posts/02-dl2/post/</link>
      <pubDate>Fri, 05 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://yugajmera.github.io/posts/02-dl2/post/</guid>
      <description>In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.
However, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable.</description>
    </item>
    <item>
      <title>Deep Learning Bascis Part 1: The Base  of the Cake</title>
      <link>https://yugajmera.github.io/posts/01-dl1/post/</link>
      <pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://yugajmera.github.io/posts/01-dl1/post/</guid>
      <description>While there is a wealth of deep learning content scattered across the internet, I wanted to create a one-stop solution where you can find all the fundamental concepts needed to write your own neural network from scratch.
This series is inspired by Justin Johnson&amp;rsquo;s course, and Stanford&amp;rsquo;s CS231n.
Image Classification Before diving into the details, let’s start with the basics: image classification. This is a core task in computer vision where a model takes an image as input and assigns it a category label from a fixed set of predefined categories.</description>
    </item>
  </channel>
</rss>
