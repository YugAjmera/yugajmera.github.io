<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Neural Network for Images: Convolutional Neural Networks (CNNs) | YA&#39;s Almanac</title>
<meta name="keywords" content="">
<meta name="description" content="Linear Neural Networks that we have talked about till this point do not work when dealing with image data, as they don&rsquo;t respect the spatial structure of images. When a neural network is applied to a 2D image, it flattens it out in 1D and then uses it as input. This creates a need for a new computational node that operates on images - Convolutional Neural Networks (CNNs).
A convolution layer has a 3D image tensor (3 X H X W) as an input and a 3D filter (also called a kernel) that convolves over the image, i.">
<meta name="author" content="">
<link rel="canonical" href="https://yugajmera.github.io/posts/cnn/post/">
<link crossorigin="anonymous" href="https://yugajmera.github.io/assets/css/stylesheet.ab4ae6179ea15f8d40abe2eaf7686672c2efdd6c8ab9f32ba68b327655b638ab.css" integrity="sha256-q0rmF56hX41Aq&#43;Lq92hmcsLv3WyKufMrposydlW2OKs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://yugajmera.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yugajmera.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yugajmera.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yugajmera.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://yugajmera.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://yugajmera.github.io/posts/cnn/post/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
<style>
   mjx-container[display="true"] {
       margin: 1.5em 0 ! important
   }
</style>



  
    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-S759YBMKJE');
        }
      </script>
    
  

<meta property="og:title" content="Neural Network for Images: Convolutional Neural Networks (CNNs)" />
<meta property="og:description" content="Linear Neural Networks that we have talked about till this point do not work when dealing with image data, as they don&rsquo;t respect the spatial structure of images. When a neural network is applied to a 2D image, it flattens it out in 1D and then uses it as input. This creates a need for a new computational node that operates on images - Convolutional Neural Networks (CNNs).
A convolution layer has a 3D image tensor (3 X H X W) as an input and a 3D filter (also called a kernel) that convolves over the image, i." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yugajmera.github.io/posts/cnn/post/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-18T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-09-18T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Neural Network for Images: Convolutional Neural Networks (CNNs)"/>
<meta name="twitter:description" content="Linear Neural Networks that we have talked about till this point do not work when dealing with image data, as they don&rsquo;t respect the spatial structure of images. When a neural network is applied to a 2D image, it flattens it out in 1D and then uses it as input. This creates a need for a new computational node that operates on images - Convolutional Neural Networks (CNNs).
A convolution layer has a 3D image tensor (3 X H X W) as an input and a 3D filter (also called a kernel) that convolves over the image, i."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yugajmera.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Neural Network for Images: Convolutional Neural Networks (CNNs)",
      "item": "https://yugajmera.github.io/posts/cnn/post/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Neural Network for Images: Convolutional Neural Networks (CNNs)",
  "name": "Neural Network for Images: Convolutional Neural Networks (CNNs)",
  "description": "Linear Neural Networks that we have talked about till this point do not work when dealing with image data, as they don\u0026rsquo;t respect the spatial structure of images. When a neural network is applied to a 2D image, it flattens it out in 1D and then uses it as input. This creates a need for a new computational node that operates on images - Convolutional Neural Networks (CNNs).\nA convolution layer has a 3D image tensor (3 X H X W) as an input and a 3D filter (also called a kernel) that convolves over the image, i.",
  "keywords": [
    
  ],
  "articleBody": "Linear Neural Networks that we have talked about till this point do not work when dealing with image data, as they don’t respect the spatial structure of images. When a neural network is applied to a 2D image, it flattens it out in 1D and then uses it as input. This creates a need for a new computational node that operates on images - Convolutional Neural Networks (CNNs).\nA convolution layer has a 3D image tensor (3 X H X W) as an input and a 3D filter (also called a kernel) that convolves over the image, i.e. slides over the image spatially, computing dot products. It should be noted that since it a 2D operation, the number of depth channels (here its 3 RGB) always has to match the number of depth channels in the filter. The figure below shows an example of the convolution operation.\nThe values in the filter are the weights that are learned from training, and the same filter gets applied over all the positions of the image. Here is what it would look like in 3D.\nEach 3D filter gives a 2D output called an activation map. Having only one filter does not make much sense, so a convolution layer involves convolving the input with a bank of filters (different filters with different weight values). When we convolve these filters with the input, we get one 2D activation map for each filter. We stack all the 2D activation maps together to obtain a 3D output tensor with the number of depth channels equal to the number of filters in that layer. An example is shown below,\nAs you can clearly note that the size of the activation map is not equal to the size of the image (or the input). Actually, it depends on the size of the filter. Here’s how, \\begin{align*} \\underbrace{N \\times C_{in} \\times H \\times W}_{\\text{Input size}} + \\underbrace{C_{out} \\times C_{in} \\times K_w \\times K_h}_{\\text{Filters size}} \\Rightarrow \\underbrace{N \\times C_{out} \\times H’ \\times W’}_{\\text{Output size}} \\end{align*} where $N$ is the number of images in a single mini-batch, $C_{in}$ is the number of input channels and $C_{out}$ is the number of output channels or the number of filters in that convolution layer. Usually, $H = W$ (considering a square image) and $K_w = K_h = K$ (considering a square kernel), the output size $H’ = W’$ is given by, \\begin{align*} W’ = \\frac{(W - K + 2P)}{S} + 1 \\end{align*} where $P$ is the padding size and $S$ is the stride. Padding is a process of adding zeros around the input, applied so that the input size does not shrink after convolution operation. Stride is often used to downsample our input, where we modify the amount of movement of the filter over the input. So in a stride of 2, rather than placing our filter on every pixel of the image, we would place it on alternate pixels.\nJust like linear networks, stacking multiple convolution layers would result in one single big convolution. So, we need to have activations after each convolution. We even sometimes have $1 \\times 1$ convolution layer which gives MLP operating on each input pixel separately. Common choices:\n$P = (K-1)/2 \\hspace{1mm}$ gives “same” padding (output size = input size) $C_{in}$, $C_{out}$ = 32, 64, 128, 256 (in Powers of 2) K = 3, P = 1, S = 1 (Kernel = $3 \\times 3$) K = 5, P = 2, S = 1 (Kernel = $5 \\times 5$) K = 1, P = 0, S = 1 (Kernel = $1 \\times 1$) K = 3, P = 1, S = 2 (Downsample by 2) Pooling layers are another way (and much more preferred) to downsample a large input. The concept of the filter is the same as the convolution layer but instead of convolving, we take the maximum value of the elements in a kernel. Such a type of operation is called max-pooling and it takes only the kernel size and stride as hyperparameters. An example is shown below where the kernel size is 2 and the stride is 2.\nAnother common pooling operation is average pooling, where we take the average of all the elements in the kernel. Pooling layers do not have any learnable parameters so, during backpropagation, it simply points to the value that had maximum in the previous layer (where the value came from). It itself introduces non-linearity to our model, so any kind of activation is not required after pooling layers.\nIt also introduces translational invariance to our input, which means if a cat feature is present in the top-left corner of the filter or in the right-bottom, it will still give the same output as we are essentially performing the max over the kernel. The output remains the same even if the exact position of something in the image changes a little.\nIt should be noted that when we downsample our input using pooling or strided convolutions, the number of channels increases so that the total volume remains preserved.\n",
  "wordCount" : "838",
  "inLanguage": "en",
  "datePublished": "2022-09-18T00:00:00Z",
  "dateModified": "2022-09-18T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yugajmera.github.io/posts/cnn/post/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "YA's Almanac",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yugajmera.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yugajmera.github.io/" accesskey="h" title="YA&#39;s Almanac (Alt + H)">YA&#39;s Almanac</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yugajmera.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Neural Network for Images: Convolutional Neural Networks (CNNs)
    </h1>
    <div class="post-meta"><span title='2022-09-18 00:00:00 +0000 UTC'>September 18, 2022</span>&nbsp;·&nbsp;4 min

</div>
  </header> 

  <div class="post-content"><p>Linear Neural Networks that we have talked about till this point do not work when dealing with image data, as they don&rsquo;t respect the spatial structure of images. When a neural network is applied to a 2D image, it flattens it out in 1D and then uses it as input. This creates a need for a new computational node that operates on images - Convolutional Neural Networks (CNNs).</p>
<p>A convolution layer has a 3D image tensor (3 X H X W) as an input and a 3D filter (also called a kernel) that convolves over the image, i.e. slides over the image spatially, computing dot products. It should be noted that since it a 2D operation, the number of depth channels (here its 3 RGB) always has to match the number of depth channels in the filter. The figure below shows an example of the convolution operation.</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNUGCKyaxR58kuiO_WEEemHaxo7sC9Hfu6CZBsCA1v8OX5BvhRDRFTjAawl2ApdFU6JaxYUav7CvTyUfxwP-SjZkqSCcM6dZ5IYs5EIc3hjrvbqsujvHWDqMbAcqpd4TYHR92bKjpAC5w6k1sbilYL6RgrXhjebIQyD0bx3rT1rW4fj5fSBOCXMt-BhA/s945/img.gif"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiNUGCKyaxR58kuiO_WEEemHaxo7sC9Hfu6CZBsCA1v8OX5BvhRDRFTjAawl2ApdFU6JaxYUav7CvTyUfxwP-SjZkqSCcM6dZ5IYs5EIc3hjrvbqsujvHWDqMbAcqpd4TYHR92bKjpAC5w6k1sbilYL6RgrXhjebIQyD0bx3rT1rW4fj5fSBOCXMt-BhA/w640-h360/img.gif" alt=""  />
</a></p>
<p>The values in the filter are the weights that are learned from training, and the same filter gets applied over all the positions of the image. Here is what it would look like in 3D.</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWD2zfIapvf075hiFeRKWXc0kKTvl7t0EyMDsbK1raTaiCwpvCJft_iJR7b8QU8yHqrkvIK7asd2Sx5MsEiKaE6U1T9qYa3B2c9BOZi7ggP-sZL5tXIbhBfGR-WGE24Cy2KznZz2eP2brn0SEs-4Y_vPPd_J1SoZvCeDirdRS94c0wEZHhjOUlZExYCQ/s369/img2.gif"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiWD2zfIapvf075hiFeRKWXc0kKTvl7t0EyMDsbK1raTaiCwpvCJft_iJR7b8QU8yHqrkvIK7asd2Sx5MsEiKaE6U1T9qYa3B2c9BOZi7ggP-sZL5tXIbhBfGR-WGE24Cy2KznZz2eP2brn0SEs-4Y_vPPd_J1SoZvCeDirdRS94c0wEZHhjOUlZExYCQ/s320/img2.gif" alt=""  />
</a></p>
<p>Each 3D filter gives a 2D output called an activation map. Having only one filter does not make much sense, so a convolution layer involves convolving the input with a bank of filters (different filters with different weight values). When we convolve these filters with the input, we get one 2D activation map for each filter. We stack all the 2D activation maps together to obtain a 3D output tensor with the number of depth channels equal to the number of filters in that layer. An example is shown below,</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg9bymMVF-1rLa76ioYxgKwGQTqu9_sMRXAXYDMEtShZGAE3g51FGt761QFyvL6LMfol9Dk7VevyrYFFhwTFyyIpGqI0KwtwpFCJ1_gbEqRgYTwZBIDJxBGdLWw7gjgV6nOmdqZQOde3Jt8lqzcAPE5vALC64tWy35OA5gw4Jb_lBbEwbEPEjFRosjivA/s1102/Capture.PNG"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEg9bymMVF-1rLa76ioYxgKwGQTqu9_sMRXAXYDMEtShZGAE3g51FGt761QFyvL6LMfol9Dk7VevyrYFFhwTFyyIpGqI0KwtwpFCJ1_gbEqRgYTwZBIDJxBGdLWw7gjgV6nOmdqZQOde3Jt8lqzcAPE5vALC64tWy35OA5gw4Jb_lBbEwbEPEjFRosjivA/w640-h386/Capture.PNG" alt=""  />
</a></p>
<p>As you can clearly note that the size of the activation map is not equal to the size of the image (or the input). Actually, it depends on the size of the filter. Here&rsquo;s how, \begin{align*} \underbrace{N \times C_{in} \times H \times W}_{\text{Input size}} + \underbrace{C_{out} \times C_{in} \times K_w \times K_h}_{\text{Filters size}} \Rightarrow \underbrace{N \times C_{out} \times H&rsquo; \times W&rsquo;}_{\text{Output size}} \end{align*} where $N$ is the number of images in a single mini-batch, $C_{in}$ is the number of input channels and $C_{out}$ is the number of output channels or the number of filters in that convolution layer. Usually, $H = W$ (considering a square image) and $K_w = K_h = K$ (considering a square kernel), the output size $H&rsquo; = W&rsquo;$ is given by, \begin{align*} W&rsquo; = \frac{(W - K + 2P)}{S} + 1 \end{align*} where $P$ is the padding size and $S$ is the stride. Padding is a process of adding zeros around the input, applied so that the input size does not shrink after convolution operation. Stride is often used to downsample our input, where we modify the amount of movement of the filter over the input. So in a stride of 2, rather than placing our filter on every pixel of the image, we would place it on alternate pixels.</p>
<p>Just like linear networks, stacking multiple convolution layers would result in one single big convolution. So, we need to have activations after each convolution. We even sometimes have $1 \times 1$ convolution layer which gives MLP operating on each input pixel separately. Common choices:</p>
<ul>
<li>$P = (K-1)/2 \hspace{1mm}$ gives &ldquo;same&rdquo; padding (output size = input size)</li>
<li>$C_{in}$, $C_{out}$ = 32, 64, 128, 256 (in Powers of 2)</li>
<li>K = 3, P = 1, S = 1 (Kernel = $3 \times 3$)</li>
<li>K = 5, P = 2, S = 1 (Kernel = $5 \times 5$)</li>
<li>K = 1, P = 0, S = 1 (Kernel = $1 \times 1$)</li>
<li>K = 3, P = 1, S = 2 (Downsample by 2)</li>
</ul>
<p>Pooling layers are another way (and much more preferred) to downsample a large input. The concept of the filter is the same as the convolution layer but instead of convolving, we take the maximum value of the elements in a kernel. Such a type of operation is called max-pooling and it takes only the kernel size and stride as hyperparameters. An example is shown below where the kernel size is 2 and the stride is 2.</p>
<p><a href="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhap1FbVHXBO2OuVjnnQoDybsBj5Xc4f2xb3ce0ZyBBrAN8rpjhUVUyTlggD9SgtTFCOHZyndrFxZ2Dz6k9HlOUCxUfb7DKUWJADLfPbmKeXvMeHQgYoi482wVtGrRKL78QkzdCKSRTX1sydqst2mquDwYxUXmm6C7u_FjjlPanOhND4WY4EqEV0kR_w/s783/1_vOxthD0FpBR6fJcpPxq6Hg.gif"><img loading="lazy" src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjhap1FbVHXBO2OuVjnnQoDybsBj5Xc4f2xb3ce0ZyBBrAN8rpjhUVUyTlggD9SgtTFCOHZyndrFxZ2Dz6k9HlOUCxUfb7DKUWJADLfPbmKeXvMeHQgYoi482wVtGrRKL78QkzdCKSRTX1sydqst2mquDwYxUXmm6C7u_FjjlPanOhND4WY4EqEV0kR_w/w400-h199/1_vOxthD0FpBR6fJcpPxq6Hg.gif" alt=""  />
</a></p>
<p>Another common pooling operation is average pooling, where we take the average of all the elements in the kernel. Pooling layers do not have any learnable parameters so, during backpropagation, it simply points to the value that had maximum in the previous layer (where the value came from). It itself introduces non-linearity to our model, so any kind of activation is not required after pooling layers.</p>
<p>It also introduces <strong>translational invariance</strong> to our input, which means if a cat feature is present in the top-left corner of the filter or in the right-bottom, it will still give the same output as we are essentially performing the max over the kernel. The output remains the same even if the exact position of something in the image changes a little.</p>
<p>It should be noted that when we downsample our input using pooling or strided convolutions, the number of channels increases so that the total volume remains preserved.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://yugajmera.github.io/posts/img-classification-mnist/post/">
    <span class="title">« Prev</span>
    <br>
    <span>Image Classification using CNNs: MNIST dataset</span>
  </a>
  <a class="next" href="https://yugajmera.github.io/posts/batch-norm/post/">
    <span class="title">Next »</span>
    <br>
    <span>Understanding Batch Normalization</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://yugajmera.github.io/">YA&#39;s Almanac</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
