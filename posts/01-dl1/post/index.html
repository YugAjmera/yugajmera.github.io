<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Deep Learning Bascis Part 1: The Base  of the Cake | YA&#39;s Almanac</title>
<meta name="keywords" content="">
<meta name="description" content="While there is a wealth of deep learning content scattered across the internet, I wanted to create a one-stop solution where you can find all the fundamental concepts needed to write your own neural network from scratch.
This series is inspired by Justin Johnson&rsquo;s course, and Stanford&rsquo;s CS231n.
Image Classification Before diving into the details, let’s start with the basics: image classification. This is a core task in computer vision where a model takes an image as input and assigns it a category label from a fixed set of predefined categories.">
<meta name="author" content="">
<link rel="canonical" href="https://yugajmera.github.io/posts/01-dl1/post/">
<link crossorigin="anonymous" href="https://yugajmera.github.io/assets/css/stylesheet.334cb677465a1d1e9767dd05b097c98de3a5b4085e0d43c708104df17bc49585.css" integrity="sha256-M0y2d0ZaHR6XZ90FsJfJjeOltAheDUPHCBBN8XvElYU=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://yugajmera.github.io/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yugajmera.github.io/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yugajmera.github.io/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yugajmera.github.io/assets/apple-touch-icon.png">
<link rel="mask-icon" href="https://yugajmera.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://yugajmera.github.io/posts/01-dl1/post/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
<style>
   mjx-container[display="true"] {
       margin: 1.5em 0 ! important
   }
</style>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-S759YBMKJE', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Deep Learning Bascis Part 1: The Base  of the Cake" />
<meta property="og:description" content="While there is a wealth of deep learning content scattered across the internet, I wanted to create a one-stop solution where you can find all the fundamental concepts needed to write your own neural network from scratch.
This series is inspired by Justin Johnson&rsquo;s course, and Stanford&rsquo;s CS231n.
Image Classification Before diving into the details, let’s start with the basics: image classification. This is a core task in computer vision where a model takes an image as input and assigns it a category label from a fixed set of predefined categories." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yugajmera.github.io/posts/01-dl1/post/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-08-01T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deep Learning Bascis Part 1: The Base  of the Cake"/>
<meta name="twitter:description" content="While there is a wealth of deep learning content scattered across the internet, I wanted to create a one-stop solution where you can find all the fundamental concepts needed to write your own neural network from scratch.
This series is inspired by Justin Johnson&rsquo;s course, and Stanford&rsquo;s CS231n.
Image Classification Before diving into the details, let’s start with the basics: image classification. This is a core task in computer vision where a model takes an image as input and assigns it a category label from a fixed set of predefined categories."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yugajmera.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Deep Learning Bascis Part 1: The Base  of the Cake",
      "item": "https://yugajmera.github.io/posts/01-dl1/post/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Deep Learning Bascis Part 1: The Base  of the Cake",
  "name": "Deep Learning Bascis Part 1: The Base  of the Cake",
  "description": "While there is a wealth of deep learning content scattered across the internet, I wanted to create a one-stop solution where you can find all the fundamental concepts needed to write your own neural network from scratch.\nThis series is inspired by Justin Johnson\u0026rsquo;s course, and Stanford\u0026rsquo;s CS231n.\nImage Classification Before diving into the details, let’s start with the basics: image classification. This is a core task in computer vision where a model takes an image as input and assigns it a category label from a fixed set of predefined categories.",
  "keywords": [
    
  ],
  "articleBody": "While there is a wealth of deep learning content scattered across the internet, I wanted to create a one-stop solution where you can find all the fundamental concepts needed to write your own neural network from scratch.\nThis series is inspired by Justin Johnson’s course, and Stanford’s CS231n.\nImage Classification Before diving into the details, let’s start with the basics: image classification. This is a core task in computer vision where a model takes an image as input and assigns it a category label from a fixed set of predefined categories.\nFor us humans, this is a trivial task, but for computers, it’s much more complex. An image, represented as $(C, H, W)$, is essentially a grid of numbers ranging from 0 to 255, where each value defines the brightness of a pixel. Here, $H$ is the height, $W$ is the width, and $C$ is the number of channels, which is 3 for an RGB image. This makes it difficult for a computer to “see” the image in the same way we do.\nOne option is to encode our human knowledge about objects and patterns into the model to help recognize images. However, a more robust and scalable approach is data-driven: we train a model that learns how to recognize different types of objects in images from data. The pipeline looks something like this:\nCollect a dataset of images and their corresponding labels. Use deep learning (DL) to train a classifier. Evaluate the classifier on new, unseen images. Dataset The dataset is usually divided into 3 parts: training set, validation set and test set. We use the training set to train our model.\nHyperparameters are variables that must be set before training begins, and their values cannot be derived from the training data. Since these parameters are highly problem-dependent, they are usually selected through trial and error. After training, we evaluate our trained model on the validation set and use it to fine-tune the hyperparameters, selecting the values that yield the best performance on this set.\nOnce everything is finalized, we evaluate the model on the test set only once, at the very end. This gives us an unbiased estimate of how the model will perform on truly unseen data, helping us predict its real-world performance.\nLinear Classifier Now, let’s define our first and most basic model: a linear classifier (also known as a perceptron). Suppose we have a dataset, $\\{x_i, y_i\\}_{i=1}^N$, where $x_i$ is an RGB image of size $(3, 32, 32)$ and $y_i$ is its corresponding label (integer) across three categories: cat (label=0), car (label=1), and frog (label=2).\nWe feed the image to our model as a flattened vector of shape $(3072, 1)$. The linear classifier consists of a weight matrix $W$ and a bias $b$ (offset). In practice, we concatenate the input with 1s at the bottom row to combine the weights and bias into a single matrix, $\\mathbf{W}$, using a technique called the bias trick.\n$$ f(\\mathbf{x}, \\mathbf{W}) = W \\mathbf{x} + b = \\begin{bmatrix} \\mathbf{W} \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ 1 \\end{bmatrix} $$\nThe model outputs a score vector of size $(3, 1)$, which defines the class scores for each of the 3 predefined categories.\nThink of these scores as the model’s confidence levels—it’s basically saying how sure it is that the image belongs to each specific category. The class with the highest score becomes our predicted label $\\hat{y}_i$. The scores of the true class are highlighted in bold.\nClearly, the predictions of our classifier are not always correct. This brings us to our first essential concept in deep learning: the loss function.\nLoss function A loss function helps us quantify the performance of our model on the data. Naturally, lower the loss, the better our classifier is. It is also called an objective function or a cost function.\nWe compute the loss for each input image and simply take an average across the training set to compute the final loss value.\n$$ L(\\mathbf{W}) = \\frac{1}{N} \\sum_{i=1}^N L_i \\left( f(x_i, \\mathbf{W}), y_i \\right) $$\nSVM Loss The idea behind Support Vector Machine (SVM) loss is pretty straightforward: we want the score of the correct class to be significantly higher than the scores given to all the incorrect classes. Makes sense, right? We want our classifier to be confident in its predictions, meaning it should assign a higher score to the correct category, compared to the wrong ones, by a certain margin.\nThe SVM loss has the form, $$ L_i = \\sum_{j \\neq y_i} \\text{max}(0, \\underbrace{s_j}_{\\text{score of jth class}} - \\underbrace{s_{y_i}}_{\\text{score of true class}} + \\underbrace{1}_{\\text{margin}}) $$\nIt’s also called Hinge Loss because its shape looks just like a door hinge. When the score for the correct class is higher than all the other scores by a certain margin, the loss becomes zero. Otherwise, it increases linearly. A key point to note is that the loss is always computed for the incorrect categories, and for the correct class, it’s always zero.\nLet’s derive the SVM loss for our toy example. For the cat image, the score of the true class $s_{y_i} = 3.2$. The loss for the car and frog classes are calculated as, $$ L_1 = \\text{max}(0, 5.1 - 3.2 + 1) + \\text{max}(0, -1.7 - 3.2 + 1) = 2.9 + 0 = 2.9 $$\nSimilarly, for the other two training examples, \\begin{align} L_2 \u0026= \\text{max}(0, 1.3 - 4.9 + 1) + \\text{max}(0, 2 - 4.9 + 1) = 0 \\\\ L_3 \u0026= \\text{max}(0, 2.2 + 3.1 + 1) + \\text{max}(0, 2.5 + 3.1 + 1) = 6.3 + 6.6 = 12.9 \\end{align}\nThe final loss of our classifier is the average of all the losses: $L_{\\text{svm}} = 5.27$.\nCross-Entropy Loss Cross-Entropy Loss, also known as Logistic Loss, is one of the most commonly used loss functions in deep learning. It helps interpret raw classifier scores as probabilities, which makes sense because we want our model’s output to reflect how likely it thinks a certain class is correct.\nTo do this, we run the raw scores through an exponential function to ensure all the outputs are positive (since probabilities are always non-negative). After that, we normalize them so they sum to 1, giving us a valid probability distribution over all the categories. This entire transformation is known as the softmax function. The image below shows how the softmax function works on our cat image scores.\nThe term “softmax” comes from the fact that it’s a smooth, differentiable approximation to the max function. If we were to apply the max function directly to the raw scores—like in our cat image example—it would output something like $[0, 1, 0]$, which is a valid probability distribution. But the problem is, the max function isn’t differentiable, which means we can’t use it to train neural networks (you’ll see why shortly). This makes softmax a go-to tool in deep learning when you need to highlight the maximum value while ensuring the function remains differentiable.\nThe softmax function provides our estimated probability distribution, denoted by $q$. The true or desired distribution is one where all the probability mass is concentrated on the correct class, i.e., $p = [0, \\dots, 1, \\dots, 0]$, with a single 1 at the $y_i$th position (also known as a one-hot encoded vector). The difference between two probability distributions is measured using cross entropy. The cross-entropy between a true distribution $p$ and an estimated distribution $q$ is defined as: \\begin{equation} H(p,q) = - \\sum_x p(x) \\text{ log} (q(x)) \\end{equation}\nThus, the cross-entropy loss is simply the negative log of the predicted probability for the true class. For our cat image example, the loss would be $L_1 = -\\log(0.13) = 2.04$.\nThe cross-entropy loss has the form, \\begin{equation} L_i = -\\text{log} \\underbrace{\\left( \\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}} \\right)}_{\\text{softmax function}} \\end{equation}\nSome important things to note:\nThe loss can only reach zero if the output of softmax is a perfect one-hot vector. However, this situation is practically impossible because the exponential function only approaches zero when the score is negative infinity. Therefore, achieving $L_{\\text{ce}} = 0$ is not feasible. On initialization, when the weights are initialized with small random values (we will get to this in the next part), the loss would be $L_{\\text{ce}} = - \\log(\\frac{1}{c}) = \\log(c)$ where $c$ is the number of categories. This serves as a quick check: if you observe this loss value at the start of training, it’s a sign that everything is working as expected. The loss function tells us how well our current classifier is doing (with its existing weights) on the training data. Since we’re all a bit greedy, we aim to find that golden set of weights that minimizes this loss. This brings us to the second key concept in deep learning: optimization.\nOptimization Optimization is the process of utilizing the training data to search through all possible values of $\\mathbf{W}$ to find the best one that results in the minimum loss.\nIntuitively, it is like traversing a large high-dimensional landscape, where every point (x,y) on the ground represents the value of the weight matrix $\\mathbf{W}$ and the height of that point is the value of loss function $L(\\mathbf{W})$. Our goal is to navigate to the bottom-most point of this terrain, where we’ll find the weights that yield the minimal loss.\nSince we don’t have the exact equation of the landscape, computing the minimum of the curve is not straightforward (it’s not a convex optimization problem). Instead, we take iterative, small steps towards the direction of the local minimum, hoping to eventually reach the lowest point. The derivative of a function gives us the slope, so this direction of steepest descent will be the negative gradient of the loss function with respect to the weights. This is the famous gradient descent algorithm.\nGradient Descent In the algorithm below, commonly referred to as the vanilla version of gradient descent, we start by initializing the weights with some arbitrary values. Then, we loop for a fixed number of iterations, where for each iteration, we compute the gradient for our current weights and take a step in the direction of the negative gradient to update the weight values.\n# Vanilla Gradient Descent w = initialize_weights() for t in range(num_steps): dw = compute_gradient(loss_fn, data, w) w += - learning_rate * dw The size of each step is a hyperparameter that controls how much we move in the negative gradient direction during each iteration of the optimization process. In other words, it controls how fast our network learns, which is why it’s referred to as the learning rate.\nA higher learning rate means taking larger steps toward the negative gradient, which might lead to faster convergence but can also result in unstable behavior. On the other hand, lower learning rate is more stable but may take longer to converge. Therefore it is crucial to select an optimal learning rate.\nThe number of iterations determines how many times we run this algorithm for the model to converge. Generally, more iterations lead to better results, as we continuously approach the minimum, and so, this hyperparameter is constrained by computational resources and time.\nJust as we averaged the losses over all images in the training data to compute the overall loss, we also take the average of individual gradients when calculating the overall gradient.\n\\begin{align} L(\\mathbf{W}) \u0026= \\frac{1}{N} \\sum_{i = 1}^N L_i(x_i, y_i, \\mathbf{W}) \\\\ \\mathbf{dw} = \\frac{\\partial L}{ \\partial \\mathbf{W}} \u0026= \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial}{\\partial \\mathbf{W}} L_i(x_i, y_i, \\mathbf{W}) \\end{align}\nThis process becomes computationally expensive and slow when dealing with a huge training set ($N$ is very large), as just one step of gradient descent involves looping over the entire training dataset.\nIn practice, this version of gradient descent is not very feasible as we almost always have large datasets. Instead of computing the gradient over all examples, we approximate it’s value by computing it over small sub-samples of the training set. The intuition here is that we assume that the examples in the training data are correlated, so calculating the gradient over batches of the training data is a good approximation to the gradient of the full objective.\nWe split the training dataset into mini-batches, each containing $B$ examples, and use these mini-batches to perform weight update steps. This approach allows for much faster convergence because we can perform more frequent parameter updates using the mini-batch gradients. This variant is known as Mini-batch Stochastic Gradient Descent (SGD).\n# Mini-batch Gradient Descent w = initialize_weights() batches = split(data, batch_size) for t in range(num_steps): for mini_batch in batches: dw = compute_gradient(loss_fn, mini_batch, w) w += - learning_rate * dw The samples are drawn randomly, and the batch size is another hyperparameter often selected from values like 32, 64, 128, or 256—commonly powers of 2. This is because many vectorized operations perform more efficiently when their inputs are sized in powers of 2. A larger batch size provides a better estimate of the true gradient and is often chosen as large as possible until you run out of GPU memory.\nThe extreme case where the mini-batch contains only a single example ($B = 1$), represents a pure Stochastic Gradient Descent. In this approach, we update the parameters more frequently by taking one observation at each iteration, resulting in a much more erratic learning curve compared to the mini-batch version. This is why mini-batch SGD is generally preferred and is almost always used in practice.\nMini-batch SGD, while effective, does have some challenges and limitations:\nLoss Landscape Variation: If the loss landscape changes rapidly in one direction and slowly in another, a constant learning rate would cause the algorithm to progress slowly along the shallow direction while jittering along the steep direction. Although a smaller learning rate might stabilize the optimization in steeper regions, it would significantly reduce progress in the shallow regions, leading to slower overall convergence. Local Minima and Saddle Points: When the loss function has local minima or saddle points, the gradients at these locations are zero. As a result, gradient descent can get stuck and be unable to escape these points. Noisy Gradients: Since our gradients are computed from mini-batches, they can be noisy, which may affect the stability and convergence of the optimization process. A solution to these problems is a technique called Momentum [1], which draws inspiration from physics.\nMomentum Instead of relying solely on the gradient of the current step to guide the search, momentum also accumulates the gradients from past steps to determine the direction of movement. This helps the optimization process build speed in directions with consistent gradients while smoothing out updates. As a result, when the gradient updates ($\\mathbf{dw}$) consistently point in the same direction, we build momentum and quickly descend the surface, potentially escaping local minima or saddle points along the way.\n\\begin{align} \\text{SGD: } \u0026 \\Delta \\mathbf{w} = - \\eta * \\mathbf{dw} \\\\ \\text{SGD + Momentum: } \u0026 \\Delta \\mathbf{w}^{t} = - \\eta * \\mathbf{dw} + m * \\Delta \\mathbf{w}^{t-1} \\end{align}\nMomentum helps address the three challenges of mini-batch SGD:\nLoss Landscape Variation: With a smaller learning rate, we can avoid jittering in steep areas, while also maintaining a faster pace in shallow regions. As we progress through flatter areas, we build velocity, which speeds up convergence without increasing the learning rate.\nLocal Minima and Saddle Points: By accumulating previous gradients, we ensure there is always an update to the weights, even when gradients temporarily become small or zero. This enables the optimizer to push through saddle points or escape shallow local minima.\nNoisy Gradients: Momentum smooths out noisy gradient updates by averaging them over time, resulting in more stable updates. This reduces the erratic behavior caused by mini-batch noise and leads to smoother convergence overall.\n# SGD + Momentum v = 0 for t in range(num_steps): v = - (learning_rate * dw) + (m * v) w += v In the algorithm above, the variable $v$ accumulates velocity as a running mean of gradients, while $m$ serves as a friction or decay factor. To understand this concept better, consider two extreme cases:\nNo momentum ($m = 0$): The algorithm behaves exactly like standard gradient descent. Maximum momentum ($m = 1)$: The updates oscillate indefinitely, similar to a frictionless bowl, failing to converge. In practice, values for $m$ are typically set around 0.9 or 0.99, striking a balance between acceleration and stability.\nSince we build velocity over time, we risk overshooting even after reaching the minima, which can lead to jittering. Therefore, we need a way to decay the weight updates over time, and this is where Adagrad [2] comes in.\nAdaGrad Instead of keeping track of the sum of gradients like momentum, the Adaptive Gradient algorithm, or AdaGrad for short, maintains a running sum of squared gradients to have an adaptive learning rate.\n# Adagrad grad_squared = 0 for t in range(num_steps): grad_squared += dw * dw w += -learning_rate * dw / (grad_squared.sqrt() + 1e-7) In the direction of a steep descent (where $\\mathbf{dw}$ is high), AdaGrad dampens the progress, resulting in smaller update steps. Conversely, in shallow regions (where $\\mathbf{dw}$ is low), it allows for larger updates. This adaptive approach effectively addresses the problem of landscape variation.\nDuring the initial iterations, the sum of the squared gradients is small, leading to a higher learning rate. As we continue accumulating squared gradients, the learning rate gradually decays over time—a beneficial feature to have!\nSince the sum of squared gradients only increases over time, it can lead to situations where the learning rate becomes too low to make meaningful updates, potentially even before reaching the minimum. To overcome this limitation, we use a variant called RMSProp, which decays this running sum of squared gradients, ensuring that the learning rate does not become too small.\nRMSProp Root Mean Square Propagation, or RMSProp for short, is a leaky version of AdaGrad that introduces a decay rate to the running sum of squared gradients.\n# RMSProp grad_squared = 0 for t in range(num_steps): grad_squared = (decay_rate * grad_squared) + (1 - decay_rate) * dw * dw w += -learning_rate * dw / (grad_squared.sqrt() + 1e-7) The decay rate is a hyperparameter typically set to 0.9, while a common choice for the learning rate is 0.001. This mechanism allows RMSProp to effectively balance the adaptation of the learning rate, preventing the rapid decay issues encountered with AdaGrad.\nAdam Adaptive Moment Estimation, or Adam [3] for short, combines the strengths of both RMSProp and Momentum, utilizing a running sum of gradients as well as the squared gradients to optimize the learning process.\n# Adam m1 = 0 m2 = 0 for t in range(1, num_steps): m1 = (beta1 * m1) + (1 - beta1) * dw m2 = (beta2 * m1) + (1 - beta2) * dw * dw m1_unbias = m1 / (1 - beta1 **t) m2_unbias = m2 / (1 - beta2 **t) w += -learning_rate * m1_unbias / (m2_unbias.sqrt() + 1e-7) The sum of gradients and squared gradients are very small at the beginning of training (biased towards zero), which can lead to excessively large updates. To address this issue, Adam employs a technique called bias correction as shown in algorithm above.\nThis adjustment ensures that the updates remain stable and effective, especially in the early stages of training when the optimizer may be prone to making erratic updates due to the low initial values of the moments.\nBeta1 is the decay rate for the first moment (the sum of gradient, aka momentum), and it is commonly set to 0.9. Beta 2 is the decay rate for the second moment (the sum of gradient squared), typically set at 0.999. Adam has become a go-to optimizer for much of the deep learning community today. Learning rates of 1e-3, 5e-4, and 1e-4 serve as great starting points for most models.\nThe visualization below demonstrates how different optimizers behave on a loss landscape featuring both local and global minima, all starting from a common point.\nGradient Descent (cyan) vs. Momentum (magenta) with m = 0.99 vs. AdaGrad (white) vs RMSProp (green) with decay rate = 0.9 vs Adam (blue) with beta1 = 0.9 and beta2 = 0.999, on a surface with a global minimum (the left well) and local minimum (the right well). Source: visualization tool\nNow that we have prepared the base of our deep learning cake, it’s time to add the icing in the next part!\nReferences [1] Sutskever et al, “On the importance of initialization and momentum in deep learning”, ICML 2013.\n[2] Duchi et al, “Adaptive subgradient methods for online learning and stochastic optimization”, JMLR 2011.\n[3] Kingma and Ba, “Adam: A method for stochastic optimization”, ICLR 2015.\n",
  "wordCount" : "3427",
  "inLanguage": "en",
  "datePublished": "2022-08-01T00:00:00Z",
  "dateModified": "2022-08-01T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yugajmera.github.io/posts/01-dl1/post/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "YA's Almanac",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yugajmera.github.io/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yugajmera.github.io/" accesskey="h" title="YA&#39;s Almanac (Alt + H)">YA&#39;s Almanac</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yugajmera.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Deep Learning Bascis Part 1: The Base  of the Cake
    </h1>
    <div class="post-meta"><span title='2022-08-01 00:00:00 +0000 UTC'>August 1, 2022</span>&nbsp;·&nbsp;17 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#image-classification" aria-label="Image Classification">Image Classification</a><ul>
                        
                <li>
                    <a href="#dataset" aria-label="Dataset">Dataset</a></li>
                <li>
                    <a href="#linear-classifier" aria-label="Linear Classifier">Linear Classifier</a></li></ul>
                </li>
                <li>
                    <a href="#loss-function" aria-label="Loss function">Loss function</a><ul>
                        
                <li>
                    <a href="#svm-loss" aria-label="SVM Loss">SVM Loss</a></li>
                <li>
                    <a href="#cross-entropy-loss" aria-label="Cross-Entropy Loss">Cross-Entropy Loss</a></li></ul>
                </li>
                <li>
                    <a href="#optimization" aria-label="Optimization">Optimization</a><ul>
                        
                <li>
                    <a href="#gradient-descent" aria-label="Gradient Descent">Gradient Descent</a></li>
                <li>
                    <a href="#momentum" aria-label="Momentum">Momentum</a></li>
                <li>
                    <a href="#adagrad" aria-label="AdaGrad">AdaGrad</a></li>
                <li>
                    <a href="#rmsprop" aria-label="RMSProp">RMSProp</a></li>
                <li>
                    <a href="#adam" aria-label="Adam">Adam</a></li></ul>
                </li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>While there is a wealth of deep learning content scattered across the internet, I wanted to create a one-stop solution where you can find all the fundamental concepts needed to write your own neural network from scratch.</p>
<p>This series is inspired by <a href="https://www.youtube.com/watch?v=dJYGatp4SvA&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r">Justin Johnson&rsquo;s course</a>, and <a href="https://cs231n.stanford.edu/index.html">Stanford&rsquo;s CS231n</a>.</p>
<h2 id="image-classification">Image Classification<a hidden class="anchor" aria-hidden="true" href="#image-classification">#</a></h2>
<p>Before diving into the details, let’s start with the basics: image classification. This is a core task in computer vision where a model takes an image as input and assigns it a category label from a fixed set of predefined categories.</p>
<p>For us humans, this is a trivial task, but for computers, it’s much more complex. An image, represented as $(C, H, W)$, is essentially a grid of numbers ranging from 0 to 255, where each value defines the brightness of a pixel. Here, $H$ is the height, $W$ is the width, and $C$ is the number of channels, which is 3 for an RGB image. This makes it difficult for a computer to &ldquo;see&rdquo; the image in the same way we do.</p>
<p>One option is to encode our human knowledge about objects and patterns into the model to help recognize images. However, a more robust and scalable approach is data-driven: we train a model that <em>learns</em> how to recognize different types of objects in images from data. The pipeline looks something like this:</p>
<ol>
<li>Collect a dataset of images and their corresponding labels.</li>
<li>Use deep learning (DL) to train a classifier.</li>
<li>Evaluate the classifier on new, unseen images.</li>
</ol>
<h3 id="dataset">Dataset<a hidden class="anchor" aria-hidden="true" href="#dataset">#</a></h3>
<p>The dataset is usually divided into 3 parts: training set, validation set and test set. We use the training set to train our model.</p>
<p>Hyperparameters are variables that must be set before training begins, and their values cannot be derived from the training data. Since these parameters are highly problem-dependent, they are usually selected through trial and error. After training, we evaluate our trained model on the validation set and use it to fine-tune the hyperparameters, selecting the values that yield the best performance on this set.</p>
<figure class="align-center ">
    <img loading="lazy" src="../dataset.png#center"/> 
</figure>

<p>Once everything is finalized, we evaluate the model on the test set only once, at the very end. This gives us an unbiased estimate of how the model will perform on truly unseen data, helping us predict its real-world performance.</p>
<h3 id="linear-classifier">Linear Classifier<a hidden class="anchor" aria-hidden="true" href="#linear-classifier">#</a></h3>
<p>Now, let’s define our first and most basic model: a linear classifier (also known as a perceptron). Suppose we have a dataset, $\{x_i, y_i\}_{i=1}^N$, where $x_i$ is an RGB image of size $(3, 32, 32)$ and $y_i$ is its corresponding label (integer) across three categories: cat (label=0), car (label=1), and frog (label=2).</p>
<figure class="align-center ">
    <img loading="lazy" src="../linear_classifier.png#center"/> 
</figure>

<p>We feed the image to our model as a flattened vector of shape $(3072, 1)$. The linear classifier consists of a weight matrix $W$ and a bias $b$ (offset). In practice, we concatenate the input with 1s at the bottom row to combine the weights and bias into a single matrix, $\mathbf{W}$, using a technique called the bias trick.</p>
<p>$$
f(\mathbf{x}, \mathbf{W})
= W \mathbf{x} + b
= \begin{bmatrix} \mathbf{W} \end{bmatrix} \begin{bmatrix} \mathbf{x} \\ 1 \end{bmatrix}
$$</p>
<p>The model outputs a score vector of size $(3, 1)$, which defines the class scores for each of the 3 predefined categories.</p>
<figure class="align-center ">
    <img loading="lazy" src="../scores.png#center" width="400"/> 
</figure>

<p>Think of these scores as the model&rsquo;s confidence levels—it&rsquo;s basically saying how sure it is that the image belongs to each specific category. The class with the highest score becomes our predicted label $\hat{y}_i$. The scores of the true class are highlighted in bold.</p>
<p>Clearly, the predictions of our classifier are not always correct. This brings us to our first essential concept in deep learning: the loss function.</p>
<h2 id="loss-function">Loss function<a hidden class="anchor" aria-hidden="true" href="#loss-function">#</a></h2>
<p>A loss function helps us <em>quantify</em> the performance of our model on the data. Naturally, lower the loss, the better our classifier is. It is also called an objective function or a cost function.</p>
<figure class="align-center ">
    <img loading="lazy" src="../loss.png#center"/> 
</figure>

<p>We compute the loss for each input image and simply take an average across the training set to compute the final loss value.</p>
<p>$$
L(\mathbf{W}) = \frac{1}{N} \sum_{i=1}^N L_i \left( f(x_i, \mathbf{W}), y_i \right)
$$</p>
<h3 id="svm-loss">SVM Loss<a hidden class="anchor" aria-hidden="true" href="#svm-loss">#</a></h3>
<p>The idea behind Support Vector Machine (SVM) loss is pretty straightforward: we want the score of the correct class to be significantly higher than the scores given to all the incorrect classes. Makes sense, right? We want our classifier to be confident in its predictions, meaning it should assign a higher score to the correct category, compared to the wrong ones, by a certain margin.</p>
<p>The SVM loss has the form,
$$
L_i = \sum_{j \neq y_i} \text{max}(0,
\underbrace{s_j}_{\text{score of jth class}} - \underbrace{s_{y_i}}_{\text{score of true class}} + \underbrace{1}_{\text{margin}})
$$</p>
<figure class="align-center ">
    <img loading="lazy" src="../hinge.png#center" width="400"/> 
</figure>

<p>It’s also called Hinge Loss because its shape looks just like a door hinge. When the score for the correct class is higher than all the other scores by a certain margin, the loss becomes zero. Otherwise, it increases linearly. A key point to note is that the loss is always computed for the incorrect categories, and for the correct class, it&rsquo;s always zero.</p>
<figure class="align-center ">
    <img loading="lazy" src="../svm_scores.png#center" width="400"/> 
</figure>

<p>Let&rsquo;s derive the SVM loss for our toy example. For the cat image, the score of the true class $s_{y_i} = 3.2$. The loss for the car and frog classes are calculated as,
$$
L_1 = \text{max}(0, 5.1 - 3.2 + 1) + \text{max}(0, -1.7 - 3.2 + 1) = 2.9 + 0 = 2.9
$$</p>
<p>Similarly, for the other two training examples,
\begin{align}
L_2 &amp;= \text{max}(0, 1.3 - 4.9 + 1) + \text{max}(0, 2 - 4.9 + 1) = 0 \\
L_3 &amp;= \text{max}(0, 2.2 + 3.1 + 1) + \text{max}(0, 2.5 + 3.1 + 1) = 6.3 + 6.6 = 12.9
\end{align}</p>
<p>The final loss of our classifier is the average of all the losses: $L_{\text{svm}} = 5.27$.</p>
<h3 id="cross-entropy-loss">Cross-Entropy Loss<a hidden class="anchor" aria-hidden="true" href="#cross-entropy-loss">#</a></h3>
<p>Cross-Entropy Loss, also known as Logistic Loss, is one of the most commonly used loss functions in deep learning. It helps interpret raw classifier scores as probabilities, which makes sense because we want our model’s output to reflect how likely it thinks a certain class is correct.</p>
<p>To do this, we run the raw scores through an exponential function to ensure all the outputs are positive (since probabilities are always non-negative). After that, we normalize them so they sum to 1, giving us a valid probability distribution over all the categories. This entire transformation is known as the softmax function. The image below shows how the softmax function works on our cat image scores.</p>
<figure class="align-center ">
    <img loading="lazy" src="../ce_scores.png#center"/> 
</figure>

<p>The term &ldquo;softmax&rdquo; comes from the fact that it’s a smooth, differentiable approximation to the max function. If we were to apply the max function directly to the raw scores—like in our cat image example—it would output something like $[0, 1, 0]$, which is a valid probability distribution. But the problem is, the max function isn’t differentiable, which means we can’t use it to train neural networks (you’ll see why shortly). This makes softmax a go-to tool in deep learning when you need to highlight the maximum value while ensuring the function remains differentiable.</p>
<p>The softmax function provides our estimated probability distribution, denoted by $q$. The true or desired distribution is one where all the probability mass is concentrated on the correct class, i.e., $p = [0, \dots, 1, \dots, 0]$, with a single 1 at the $y_i$th position (also known as a one-hot encoded vector). The difference between two probability distributions is measured using cross entropy. The cross-entropy between a true distribution $p$ and an estimated distribution $q$ is defined as:
\begin{equation}
H(p,q) = - \sum_x p(x) \text{ log} (q(x))
\end{equation}</p>
<p>Thus, the cross-entropy loss is simply the negative log of the predicted probability for the true class. For our cat image example, the loss would be $L_1 = -\log(0.13) = 2.04$.</p>
<p>The cross-entropy loss has the form,
\begin{equation}
L_i = -\text{log} \underbrace{\left( \frac{e^{s_{y_i}}}{\sum_j e^{s_j}} \right)}_{\text{softmax function}}
\end{equation}</p>
<p>Some important things to note:</p>
<ul>
<li>The loss can only reach zero if the output of softmax is a perfect one-hot vector. However, this situation is practically impossible because the exponential function only approaches zero when the score is negative infinity. Therefore, achieving $L_{\text{ce}} = 0$  is not feasible.</li>
<li>On initialization, when the weights are initialized with small random values (we will get to this in the next part), the loss would be $L_{\text{ce}} = - \log(\frac{1}{c}) = \log(c)$ where $c$ is the number of categories.  This serves as a quick check: if you observe this loss value at the start of training, it’s a sign that everything is working as expected.</li>
</ul>
<p>The loss function tells us how well our current classifier is doing (with its existing weights) on the training data. Since we’re all a bit greedy, we aim to find that golden set of weights that minimizes this loss. This brings us to the second key concept in deep learning: optimization.</p>
<h2 id="optimization">Optimization<a hidden class="anchor" aria-hidden="true" href="#optimization">#</a></h2>
<p>Optimization is the process of utilizing the training data to search through all possible values of
$\mathbf{W}$ to find the best one that results in the minimum loss.</p>
<p>Intuitively, it is like traversing a large high-dimensional landscape, where every point (x,y) on the ground represents the value of the weight matrix $\mathbf{W}$ and the height of that point is the value of loss function $L(\mathbf{W})$. Our goal is to navigate to the bottom-most point of this terrain, where we&rsquo;ll find the weights that yield the minimal loss.</p>
<p>Since we don&rsquo;t have the exact equation of the landscape, computing the minimum of the curve is not straightforward (it&rsquo;s not a convex optimization problem). Instead, we take iterative, small steps towards the direction of the local minimum, hoping to eventually reach the lowest point. The derivative of a function gives us the slope, so this direction of steepest descent will be the negative gradient of the loss function with respect to the weights. This is the famous gradient descent algorithm.</p>
<h3 id="gradient-descent">Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#gradient-descent">#</a></h3>
<p>In the algorithm below, commonly referred to as the vanilla version of gradient descent, we start by initializing the weights with some arbitrary values. Then, we loop for a fixed number of iterations, where for each iteration, we compute the gradient for our current weights and take a step in the direction of the negative gradient to update the weight values.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Vanilla Gradient Descent</span>
</span></span><span style="display:flex;"><span>w <span style="color:#f92672">=</span> initialize_weights()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(num_steps):
</span></span><span style="display:flex;"><span>    dw <span style="color:#f92672">=</span> compute_gradient(loss_fn, data, w)
</span></span><span style="display:flex;"><span>    w <span style="color:#f92672">+=</span> <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> dw
</span></span></code></pre></div><p>The size of each step is a hyperparameter that controls how much we move in the negative gradient direction during each iteration of the optimization process. In other words, it controls how fast our network learns, which is why it&rsquo;s referred to as the learning rate.</p>
<p>A higher learning rate means taking larger steps toward the negative gradient, which might lead to faster convergence but can also result in unstable behavior. On the other hand, lower learning rate is more stable but may take longer to converge. Therefore it is crucial to select an optimal learning rate.</p>
<p>The number of iterations determines how many times we run this algorithm for the model to converge. Generally, more iterations lead to better results, as we continuously approach the minimum, and so, this hyperparameter is constrained by computational resources and time.</p>
<p>Just as we averaged the losses over all images in the training data to compute the overall loss, we also take the average of individual gradients when calculating the overall gradient.</p>
<p>\begin{align}
L(\mathbf{W}) &amp;= \frac{1}{N} \sum_{i = 1}^N L_i(x_i, y_i, \mathbf{W}) \\
\mathbf{dw} = \frac{\partial L}{ \partial \mathbf{W}}
&amp;= \frac{1}{N} \sum_{i=1}^N \frac{\partial}{\partial \mathbf{W}} L_i(x_i, y_i, \mathbf{W})
\end{align}</p>
<p>This process becomes computationally expensive and slow when dealing with a huge training set ($N$ is very large), as just one step of gradient descent involves looping over the entire training dataset.</p>
<p>In practice, this version of gradient descent is not very feasible as we almost always have large datasets. Instead of computing the gradient over all examples, we approximate it&rsquo;s value by computing it over small sub-samples of the training set. The intuition here is that we assume that the examples in the training data are correlated, so calculating the gradient over batches of the training data is a good approximation to the gradient of the full objective.</p>
<p>We split the training dataset into mini-batches, each containing $B$ examples, and use these mini-batches to perform weight update steps. This approach allows for much faster convergence because we can perform more frequent parameter updates using the mini-batch gradients. This variant is known as Mini-batch Stochastic Gradient Descent (SGD).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Mini-batch Gradient Descent</span>
</span></span><span style="display:flex;"><span>w <span style="color:#f92672">=</span> initialize_weights()
</span></span><span style="display:flex;"><span>batches <span style="color:#f92672">=</span> split(data, batch_size)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(num_steps):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> mini_batch <span style="color:#f92672">in</span> batches:
</span></span><span style="display:flex;"><span>        dw <span style="color:#f92672">=</span> compute_gradient(loss_fn, mini_batch, w)
</span></span><span style="display:flex;"><span>        w <span style="color:#f92672">+=</span> <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> dw
</span></span></code></pre></div><p>The samples are drawn randomly, and the batch size is another hyperparameter often selected from values like 32, 64, 128, or 256—commonly powers of 2. This is because many vectorized operations perform more efficiently when their inputs are sized in powers of 2. A larger batch size provides a better estimate of the true gradient and is often chosen as large as possible until you run out of GPU memory.</p>
<p>The extreme case where the mini-batch contains only a single example ($B = 1$), represents a pure Stochastic Gradient Descent. In this approach, we update the parameters more frequently by taking one observation at each iteration, resulting in a much more erratic learning curve compared to the mini-batch version. This is why mini-batch SGD is generally preferred and is almost always used in practice.</p>
<p>Mini-batch SGD, while effective, does have some challenges and limitations:</p>
<ol>
<li>Loss Landscape Variation: If the loss landscape changes rapidly in one direction and slowly in another, a constant learning rate would cause the algorithm to progress slowly along the shallow direction while jittering along the steep direction. Although a smaller learning rate might stabilize the optimization in steeper regions, it would significantly reduce progress in the shallow regions, leading to slower overall convergence.</li>
<li>Local Minima and Saddle Points: When the loss function has local minima or saddle points, the gradients at these locations are zero. As a result, gradient descent can get stuck and be unable to escape these points.</li>
<li>Noisy Gradients: Since our gradients are computed from mini-batches, they can be noisy, which may affect the stability and convergence of the optimization process.</li>
</ol>
<p>A solution to these problems is a technique called Momentum <a href="#references">[1]</a>, which draws inspiration from physics.</p>
<h3 id="momentum">Momentum<a hidden class="anchor" aria-hidden="true" href="#momentum">#</a></h3>
<p>Instead of relying solely on the gradient of the current step to guide the search, momentum also accumulates the gradients from past steps to determine the direction of movement. This helps the optimization process build speed in directions with consistent gradients while smoothing out updates. As a result, when the gradient updates ($\mathbf{dw}$) consistently point in the same direction, we build momentum and quickly descend the surface, potentially escaping local minima or saddle points along the way.</p>
<p>\begin{align}
\text{SGD: } &amp; \Delta \mathbf{w} = - \eta * \mathbf{dw} \\
\text{SGD + Momentum: } &amp; \Delta \mathbf{w}^{t} = - \eta * \mathbf{dw} + m * \Delta \mathbf{w}^{t-1}
\end{align}</p>
<p>Momentum helps address the three challenges of mini-batch SGD:</p>
<ol>
<li>
<p>Loss Landscape Variation: With a smaller learning rate, we can avoid jittering in steep areas, while also maintaining a faster pace in shallow regions. As we progress through flatter areas, we build velocity, which speeds up convergence without increasing the learning rate.</p>
</li>
<li>
<p>Local Minima and Saddle Points: By accumulating previous gradients, we ensure there is always an update to the weights, even when gradients temporarily become small or zero. This enables the optimizer to push through saddle points or escape shallow local minima.</p>
</li>
<li>
<p>Noisy Gradients: Momentum smooths out noisy gradient updates by averaging them over time, resulting in more stable updates. This reduces the erratic behavior caused by mini-batch noise and leads to smoother convergence overall.</p>
</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># SGD + Momentum</span>
</span></span><span style="display:flex;"><span>v <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(num_steps):
</span></span><span style="display:flex;"><span>    v <span style="color:#f92672">=</span>  <span style="color:#f92672">-</span> (learning_rate <span style="color:#f92672">*</span> dw) <span style="color:#f92672">+</span> (m <span style="color:#f92672">*</span> v)
</span></span><span style="display:flex;"><span>    w <span style="color:#f92672">+=</span> v
</span></span></code></pre></div><p>In the algorithm above, the variable $v$ accumulates velocity as a running mean of gradients, while $m$ serves as a friction or decay factor. To understand this concept better, consider two extreme cases:</p>
<ul>
<li>No momentum ($m = 0$): The algorithm behaves exactly like standard gradient descent.</li>
<li>Maximum momentum ($m = 1)$: The updates oscillate indefinitely, similar to a frictionless bowl, failing to converge.</li>
</ul>
<p>In practice, values for $m$ are typically set around 0.9 or 0.99, striking a balance between acceleration and stability.</p>
<p>Since we build velocity over time, we risk overshooting even after reaching the minima, which can lead to jittering. Therefore, we need a way to decay the weight updates over time, and this is where Adagrad <a href="#references">[2]</a> comes in.</p>
<h3 id="adagrad">AdaGrad<a hidden class="anchor" aria-hidden="true" href="#adagrad">#</a></h3>
<p>Instead of keeping track of the sum of gradients like momentum, the Adaptive Gradient algorithm, or AdaGrad for short, maintains a running sum of squared gradients to have an adaptive learning rate.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Adagrad</span>
</span></span><span style="display:flex;"><span>grad_squared <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(num_steps):
</span></span><span style="display:flex;"><span>    grad_squared <span style="color:#f92672">+=</span> dw <span style="color:#f92672">*</span> dw
</span></span><span style="display:flex;"><span>    w <span style="color:#f92672">+=</span> <span style="color:#f92672">-</span>learning_rate <span style="color:#f92672">*</span> dw <span style="color:#f92672">/</span> (grad_squared<span style="color:#f92672">.</span>sqrt() <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-7</span>)
</span></span></code></pre></div><p>In the direction of a steep descent (where $\mathbf{dw}$ is high), AdaGrad dampens the progress, resulting in smaller update steps. Conversely, in shallow regions (where $\mathbf{dw}$ is low), it allows for larger updates. This adaptive approach effectively addresses the problem of landscape variation.</p>
<p>During the initial iterations, the sum of the squared gradients is small, leading to a higher learning rate. As we continue accumulating squared gradients, the learning rate gradually decays over time—a beneficial feature to have!</p>
<p>Since the sum of squared gradients only increases over time, it can lead to situations where the learning rate becomes too low to make meaningful updates, potentially even before reaching the minimum. To overcome this limitation, we use a variant called RMSProp, which decays this running sum of squared gradients, ensuring that the learning rate does not become too small.</p>
<h3 id="rmsprop">RMSProp<a hidden class="anchor" aria-hidden="true" href="#rmsprop">#</a></h3>
<p>Root Mean Square Propagation, or RMSProp for short, is a leaky version of AdaGrad that introduces a decay rate to the running sum of squared gradients.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># RMSProp</span>
</span></span><span style="display:flex;"><span>grad_squared <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(num_steps):
</span></span><span style="display:flex;"><span>    grad_squared <span style="color:#f92672">=</span> (decay_rate <span style="color:#f92672">*</span> grad_squared) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> decay_rate) <span style="color:#f92672">*</span> dw <span style="color:#f92672">*</span> dw
</span></span><span style="display:flex;"><span>    w <span style="color:#f92672">+=</span> <span style="color:#f92672">-</span>learning_rate <span style="color:#f92672">*</span> dw <span style="color:#f92672">/</span> (grad_squared<span style="color:#f92672">.</span>sqrt() <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-7</span>)
</span></span></code></pre></div><p>The decay rate is a hyperparameter typically set to 0.9, while a common choice for the learning rate is 0.001. This mechanism allows RMSProp to effectively balance the adaptation of the learning rate, preventing the rapid decay issues encountered with AdaGrad.</p>
<h3 id="adam">Adam<a hidden class="anchor" aria-hidden="true" href="#adam">#</a></h3>
<p>Adaptive Moment Estimation, or Adam <a href="#references">[3]</a> for short, combines the strengths of both RMSProp and Momentum, utilizing a running sum of gradients as well as the squared gradients to optimize the learning process.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Adam</span>
</span></span><span style="display:flex;"><span>m1 <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>m2 <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, num_steps):
</span></span><span style="display:flex;"><span>   m1 <span style="color:#f92672">=</span> (beta1 <span style="color:#f92672">*</span> m1) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> beta1) <span style="color:#f92672">*</span> dw
</span></span><span style="display:flex;"><span>   m2 <span style="color:#f92672">=</span> (beta2 <span style="color:#f92672">*</span> m1) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> beta2) <span style="color:#f92672">*</span> dw <span style="color:#f92672">*</span> dw
</span></span><span style="display:flex;"><span>   m1_unbias <span style="color:#f92672">=</span> m1 <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> beta1 <span style="color:#f92672">**</span>t)
</span></span><span style="display:flex;"><span>   m2_unbias <span style="color:#f92672">=</span> m2 <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> beta2 <span style="color:#f92672">**</span>t)
</span></span><span style="display:flex;"><span>   w <span style="color:#f92672">+=</span> <span style="color:#f92672">-</span>learning_rate <span style="color:#f92672">*</span> m1_unbias <span style="color:#f92672">/</span> (m2_unbias<span style="color:#f92672">.</span>sqrt() <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-7</span>)
</span></span></code></pre></div><p>The sum of gradients and squared gradients are very small at the beginning of training (biased towards zero), which can lead to excessively large updates. To address this issue, Adam employs a technique called bias correction as shown in algorithm above.</p>
<p>This adjustment ensures that the updates remain stable and effective, especially in the early stages of training when the optimizer may be prone to making erratic updates due to the low initial values of the moments.</p>
<ul>
<li>Beta1 is the decay rate for the first moment (the sum of gradient, aka momentum), and it is commonly set to 0.9.</li>
<li>Beta 2 is the decay rate for the second moment (the sum of gradient squared), typically set at 0.999.</li>
</ul>
<p>Adam has become a go-to optimizer for much of the deep learning community today. Learning rates of 1e-3, 5e-4, and 1e-4 serve as great starting points for most models.</p>
<p>The visualization below demonstrates how different optimizers behave on a loss landscape featuring both local and global minima, all starting from a common point.</p>
<figure class="align-center ">
    <img loading="lazy" src="../viz.gif#center"
         alt="Gradient Descent (cyan) vs. Momentum (magenta) with m = 0.99 vs. AdaGrad (white) vs RMSProp (green) with decay rate = 0.9 vs Adam (blue) with beta1 = 0.9 and beta2 = 0.999, on a surface with a global minimum (the left well) and local minimum (the right well). Source: visualization tool"/> <figcaption>
            <p>Gradient Descent (cyan) vs. Momentum (magenta) with m = 0.99 vs. AdaGrad (white) vs RMSProp (green) with decay rate = 0.9 vs Adam (blue) with beta1 = 0.9 and beta2 = 0.999, on a surface with a global minimum (the left well) and local minimum (the right well). Source: <a href="https://github.com/lilipads/gradient_descent_viz">visualization tool</a></p>
        </figcaption>
</figure>

<p>Now that we have prepared the base of our deep learning cake, it’s time to add the icing in the next part!</p>
<p> </p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1] Sutskever et al, “<a href="https://proceedings.mlr.press/v28/sutskever13.pdf">On the importance of initialization and momentum in deep learning</a>”, ICML 2013.</p>
<p>[2] Duchi et al, “<a href="https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Adaptive subgradient methods for online learning and stochastic optimization</a>”, JMLR 2011.</p>
<p>[3] Kingma and Ba, “<a href="https://arxiv.org/abs/1412.6980">Adam: A method for stochastic optimization</a>”, ICLR 2015.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://yugajmera.github.io/posts/02-dl2/post/">
    <span class="title">« Prev</span>
    <br>
    <span>Deep Learning Basics Part 2: The Icing</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://yugajmera.github.io/">YA&#39;s Almanac</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
