<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Understanding Neural Radiance Fields (NeRFs) | YA Logs</title>
<meta name="keywords" content="">
<meta name="description" content="Imagine being able to generate photorealistic 3D models of objects and scenes that can be viewed from any angle, with details so realistic that they are indistinguishable from reality. That&rsquo;s what the Neural Radiance Fields (NeRF) is capable of doing and much more. With more than 50 papers related to NeRFs in the CVPR 2022, it is one of the most influential papers of all time.
Neural fields A neural field is a neural network that parametrizes a signal.">
<meta name="author" content="">
<link rel="canonical" href="https://yugajmera.github.io/posts/nerf/post/">
<link crossorigin="anonymous" href="https://yugajmera.github.io/assets/css/stylesheet.ab4ae6179ea15f8d40abe2eaf7686672c2efdd6c8ab9f32ba68b327655b638ab.css" integrity="sha256-q0rmF56hX41Aq&#43;Lq92hmcsLv3WyKufMrposydlW2OKs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://yugajmera.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yugajmera.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yugajmera.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yugajmera.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://yugajmera.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://yugajmera.github.io/posts/nerf/post/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



  

<meta property="og:title" content="Understanding Neural Radiance Fields (NeRFs)" />
<meta property="og:description" content="Imagine being able to generate photorealistic 3D models of objects and scenes that can be viewed from any angle, with details so realistic that they are indistinguishable from reality. That&rsquo;s what the Neural Radiance Fields (NeRF) is capable of doing and much more. With more than 50 papers related to NeRFs in the CVPR 2022, it is one of the most influential papers of all time.
Neural fields A neural field is a neural network that parametrizes a signal." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yugajmera.github.io/posts/nerf/post/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-03-05T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Understanding Neural Radiance Fields (NeRFs)"/>
<meta name="twitter:description" content="Imagine being able to generate photorealistic 3D models of objects and scenes that can be viewed from any angle, with details so realistic that they are indistinguishable from reality. That&rsquo;s what the Neural Radiance Fields (NeRF) is capable of doing and much more. With more than 50 papers related to NeRFs in the CVPR 2022, it is one of the most influential papers of all time.
Neural fields A neural field is a neural network that parametrizes a signal."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yugajmera.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Understanding Neural Radiance Fields (NeRFs)",
      "item": "https://yugajmera.github.io/posts/nerf/post/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Understanding Neural Radiance Fields (NeRFs)",
  "name": "Understanding Neural Radiance Fields (NeRFs)",
  "description": "Imagine being able to generate photorealistic 3D models of objects and scenes that can be viewed from any angle, with details so realistic that they are indistinguishable from reality. That\u0026rsquo;s what the Neural Radiance Fields (NeRF) is capable of doing and much more. With more than 50 papers related to NeRFs in the CVPR 2022, it is one of the most influential papers of all time.\nNeural fields A neural field is a neural network that parametrizes a signal.",
  "keywords": [
    
  ],
  "articleBody": "Imagine being able to generate photorealistic 3D models of objects and scenes that can be viewed from any angle, with details so realistic that they are indistinguishable from reality. That’s what the Neural Radiance Fields (NeRF) is capable of doing and much more. With more than 50 papers related to NeRFs in the CVPR 2022, it is one of the most influential papers of all time.\nNeural fields A neural field is a neural network that parametrizes a signal. In our case, this signal is either a single 3D scene or an object. It is important to note that a single network needs to be trained to encode (capture) a single scene. It is worth mentioning that, unlike standard machine learning, the objective is to overfit the neural network to a particular scene. Essentially, neural fields embed the scene into the weights of the network.\n3D scenes are usually stored in computer graphics using voxel grids or polygon meshes. However, voxels are costly to store and polygon meshes are limited to hard surfaces. Neural fields are gaining popularity as they are efficient and compact representations of objects or scenes that are differentiable, continuous, and can have arbitrary dimensions and resolutions. Neural radiance fields are a special case of Neural fields that solve the view synthesis problem.\nNeural Radiance Fields (NeRFs) NeRFs as proposed by Mildenhall et al accept a single continuous 5D coordinate as input, which consists of a spatial location $(x, y, z)$ and viewing direction $(\\theta, \\phi)$. This particular point of the object/scene is fed into an MLP, which outputs the corresponding color intensities $c = (r, g, b)$ and volume density $\\sigma$.\nThe network’s weights are optimized to encode the representation of the scene so that the model can easily render novel views seen from any point in space.\nRay Marching To gain a better grasp of the different stages in NeRF training, let’s use this 3D scene as an instance.\nThe training dataset includes images ($H \\times W$) captured from $n$ different viewpoints of the same scene. Each image’s camera viewpoint is stored as $x_c, y_c, z_c$ in the world space. Since the camera is always “aimed” at the object, we only need two more rotation parameters to fully describe the pose: the inclination and azimuth angles ($\\theta$, $\\phi$).\nNow, for each camera pose, we “shoot” a ray from the camera (or viewer’s eye), through every pixel of the image, resulting in $H \\times W$ rays per pose. Each ray is described by two vectors,\n$\\mathbf{o}$ : a vector denoting the origin of the ray. $\\mathbf{d}$ : a normalized vector denoting the direction of the ray. An arbitrary point on the ray can then be defined as $r(t) = \\mathbf{o} + t * \\mathbf{d}$.\nThis process of ray tracing is known as backward tracing, as it involves tracing the path of light rays from the camera to the object, as opposed to tracing from the light source to the object.\nInput: A set of camera poses $(x_c, , y_c, , z_c,, , \\theta, , \\phi)_n$\nOutput: A bundle of rays for every pose $(r_{\\mathbf{o}, \\mathbf{d}})_{H \\times W \\times n}$\nSampling Query points You may wonder, what is done with the rays? We trace them from the camera through the scene by adjusting the parameter $t$ until they intersect with some interesting location (object) in the scene. To find these locations, we incrementally step along the ray and sample points at regular intervals.\nBy querying a trained neural network at these 3D points along the viewing ray, we can determine if they belong to the object volume and obtain their visual properties to render an image. However, sampling points along a ray is challenging, as too many non-object points won’t provide useful information, and focusing only on high-density regions may miss interesting areas.\nIn our toy example, we uniformly sample along the ray by taking $m$ samples. However, to improve performance, the authors use “hierarchical volume sampling” to allocate samples proportionally to their expected impact on the final rendering.\nInput: A bundle of rays for every pose $(r_{\\mathbf{o}, \\mathbf{d}})_{H \\times W \\times n}$\nOutput: A set of 3D query points for every ray $(x_p, , y_p, , z_p)_{m \\times H \\times W \\times n}$\nPositional Encoding Once we collect the query points for every ray, we are potentially ready to feed them into our neural network. However, the authors found that resulting renderings perform poorly at representing high-frequency variations in color and geometry that make images perceptually sharp and vivid for the human eye.\nThis observation is consistent with Rahaman et al. who show that deep networks have a tendency to learn lower-frequency functions. They claim that mapping the inputs to a higher dimensional space using high-frequency functions before passing them to the network enables better fitting of data that contains high-frequency variation.\nThe authors use the positional encoding containing sine and cosine functions of varying frequencies.\n\\begin{equation*} \\gamma(p) = [sin (2^0 \\pi p) , cos (2^0 \\pi p) ; sin (2 \\pi p) , cos (2 \\pi p) ; … ; sin (2^{L-1} \\pi p) , cos (2^{L-1} \\pi p)] \\end{equation*}\nwhere $L$ is the number of dimensions in the positional encoding. The function $\\gamma(.)$ is applied separately to each of the three coordinate values in $\\mathbf{x}$ (which are normalized to lie in [−1, 1]). As viewing direction is also an input to our network, we embed it as well. The authors use $L = 10$ for embeding the 3D query points and $L = 4$ for embeding the viewing direction.\nInput: A set of 3D query points for every ray, $(\\mathbf{x} = x_p, , y_p, , z_p)_{m \\times H \\times W \\times n}$ and 3D viewing direction $(\\mathbf{d} = \\theta, \\phi, \\psi)_n$\nOutput: Embedings of query points $\\gamma(\\mathbf{x})_{m \\times H \\times W \\times n}$ and viewing direction $\\gamma(\\mathbf{d})_n$\nNeural Network inference To achieve multiview consistency in a neural network, we restrict the network to predict the volume density $\\sigma$ as a function of only the location in 3D space, while allowing the RGB color $c$ to be predicted as a function of both location and viewing direction.\nThe MLP architecture consists of 8 fully-connected layers, each with 256 channels and ReLU activations. A skip connection is included in the network that concatenates the input to the fifth layer’s activation. It takes the encoded query points $\\gamma(\\mathbf{x})$ as input and produces two outputs: the volume density $\\sigma$ and a 256-dimensional feature vector.\nThis feature vector is then concatenated with the embedded viewing direction $\\gamma(\\mathbf{d})$ and passed through another fully-connected layer with 128 channels and a ReLU activation to produce the view-dependent RGB color $c$ (tuple in the range from 0 to 1). The authors claim that a model trained without view dependence has difficulty representing specularities.\nBoth of these pieces of information combined allow us to compute the volume density profile for every ray as shown in the figure below.\nInput: Embedings of query points $\\gamma(\\mathbf{x})_{m \\times H \\times W \\times n}$ and viewing direction $\\gamma(\\mathbf{d})_n$\nOutput: RGB color and volume density for every query point $(c, , \\sigma)_{m \\times H \\times W \\times n}$\nVolume Rendering Now, it’s time to turn this volume density profile along a ray into an rgb pixel value. Once we have computed the pixel values for all the rays, we will have a full $H \\times W$ image for all the $n$ viewpoints.\nThe volume rendering process involves computing the accumulated radiance along the viewing ray as it passes through the neural radiance field. This is done by integrating the radiance values at each sampled point along the ray, weighted by the transmittance, or opacity of the medium at that point.\nThe expected color of a camera ray $r(t) = \\mathbf{o} + t * \\mathbf{d}$ with near and far bounds $t_n$ and $t_f$ is given as:\n\\begin{equation} C(r) = \\int_{t_n}^{t_f} T(t) ; \\sigma(r(t)) ; c(r(t), \\mathbf{d}) ; dt \\end{equation}\nwhere function $T(t)$ denotes the accumulated transmittance along the ray from $t_n$ to $t$, i.e., the probability that the ray travels from $t_n$ to $t$ without hitting any other particle.\n\\begin{equation*} T(t) = \\text{exp} \\left( - \\int_{t_n}^{t} \\sigma(r(s)) ; ds \\right) \\end{equation*}\nThese complex-looking integrals can be approximated via numerical quadrature. We use a stratified sampling approach where we select a random set of quadrature points ${t_i}_{i = 1}^N$ by uniformly drawing samples from $N$ evenly-spaced ray bins between $t_n$ and $t_f$.\nThe use of stratified sampling allows for a continuous representation of the scene, despite using a discrete set of samples to estimate the integral. This is because the MLP is evaluated at continuous positions during optimization.\nThe approximated color of each ray is computed as,\n\\begin{equation*} C(r) \\approx \\sum_{i = 1}^N T(t_i) ; \\alpha_i ; c_i \\end{equation*} where $\\alpha_i = (1 - \\text{exp}(-\\sigma_i , \\delta_i))$, which can be viewed as a measure of opacity and $\\delta_i = t_{i+1} - t_i$ is the distance between two quadrature points. The accumulated transmittance is then a cumulative product of all the transmittance (which is 1 - opacity) behind it,\n\\begin{equation*} T_i = \\prod_{j=1}^{i-1} (1 - \\alpha_j) \\end{equation*}\nInput: RGB color and volume density for every query point $(c, , \\sigma)_{m \\times H \\times W \\times n}$\nOutput: Rendered Images $(H \\times W)_{n}$\nComputing loss The final step is to calculate the loss between the rendered image and the ground truth for each viewpoint in the dataset. This loss function is a simple L2 loss between each pixel of the rendered image and the corresponding pixel of the ground truth image.\nCredits: While writing this blog, I referred to dtransposed’s blog post (the images are also from this post), and AI Summer’s post.\n",
  "wordCount" : "1610",
  "inLanguage": "en",
  "datePublished": "2023-03-05T00:00:00Z",
  "dateModified": "2023-03-05T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yugajmera.github.io/posts/nerf/post/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "YA Logs",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yugajmera.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yugajmera.github.io/" accesskey="h" title="YA Logs (Alt + H)">YA Logs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yugajmera.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Understanding Neural Radiance Fields (NeRFs)
    </h1>
    <div class="post-meta"><span title='2023-03-05 00:00:00 +0000 UTC'>March 5, 2023</span>&nbsp;·&nbsp;8 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#neural-fields" aria-label="Neural fields">Neural fields</a></li>
                <li>
                    <a href="#neural-radiance-fields-nerfs" aria-label="Neural Radiance Fields (NeRFs)">Neural Radiance Fields (NeRFs)</a><ul>
                        
                <li>
                    <a href="#ray-marching" aria-label="Ray Marching">Ray Marching</a></li>
                <li>
                    <a href="#sampling-query-points" aria-label="Sampling Query points">Sampling Query points</a></li>
                <li>
                    <a href="#positional-encoding" aria-label="Positional Encoding">Positional Encoding</a></li>
                <li>
                    <a href="#neural-network-inference" aria-label="Neural Network inference">Neural Network inference</a></li>
                <li>
                    <a href="#volume-rendering" aria-label="Volume Rendering">Volume Rendering</a></li>
                <li>
                    <a href="#computing-loss" aria-label="Computing loss">Computing loss</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Imagine being able to generate photorealistic 3D models of objects and scenes that can be viewed from any angle, with details so realistic that they are indistinguishable from reality. That&rsquo;s what the Neural Radiance Fields (NeRF) is capable of doing and much more. With more than 50 papers related to NeRFs in the CVPR 2022, it is one of the most influential papers of all time.</p>
<p> </p>
<h2 id="neural-fields">Neural fields<a hidden class="anchor" aria-hidden="true" href="#neural-fields">#</a></h2>
<p>A neural field is a neural network that parametrizes a signal. In our case, this signal is either a single 3D scene or an object. It is important to note that a single network needs to be trained to encode (capture) a single scene. It is worth mentioning that, unlike standard machine learning, the objective is to overfit the neural network to a particular scene. Essentially, neural fields embed the scene into the weights of the network.</p>
<p>3D scenes are usually stored in computer graphics using voxel grids or polygon meshes. However, voxels are costly to store and polygon meshes are limited to hard surfaces. Neural fields are gaining popularity as they are efficient and compact representations of objects or scenes that are differentiable, continuous, and can have arbitrary dimensions and resolutions. Neural radiance fields are a special case of Neural fields that solve the view synthesis problem.</p>
<p> </p>
<h2 id="neural-radiance-fields-nerfs">Neural Radiance Fields (NeRFs)<a hidden class="anchor" aria-hidden="true" href="#neural-radiance-fields-nerfs">#</a></h2>
<p>NeRFs as proposed by <a href="https://arxiv.org/abs/2003.08934">Mildenhall et al</a> accept a single continuous 5D coordinate as input, which consists of a spatial location $(x, y, z)$ and viewing direction $(\theta, \phi)$. This particular point of the object/scene is fed into an MLP, which outputs the corresponding color intensities $c = (r, g, b)$ and volume density $\sigma$.</p>
<p><img loading="lazy" src="https://yugajmera.github.io/posts/nerf/nerfs.png" alt=""  />
</p>
<p>The network’s weights are optimized to encode the representation of the scene so that the model can easily render novel views seen from any point in space.</p>
<p> </p>
<h3 id="ray-marching">Ray Marching<a hidden class="anchor" aria-hidden="true" href="#ray-marching">#</a></h3>
<p>To gain a better grasp of the different stages in NeRF training, let&rsquo;s use this 3D scene as an instance.</p>
<p><img loading="lazy" src="https://yugajmera.github.io/posts/nerf/figure1.png" alt=""  />
</p>
<p>The training dataset includes images ($H \times W$) captured from $n$ different viewpoints of the same scene. Each image&rsquo;s camera viewpoint is stored as $x_c, y_c, z_c$ in the world space. Since the camera is always &ldquo;aimed&rdquo; at the object, we only need two more rotation parameters to fully describe the pose: the inclination and azimuth angles ($\theta$, $\phi$).</p>
<p>Now, for each camera pose, we &ldquo;shoot&rdquo; a ray from the camera (or viewer&rsquo;s eye), through every pixel of the image, resulting in $H \times W$ rays per pose. Each ray is described by two vectors,</p>
<ul>
<li>$\mathbf{o}$ : a vector denoting the origin of the ray.</li>
<li>$\mathbf{d}$ : a normalized vector denoting the direction of the ray.</li>
</ul>
<p>An arbitrary point on the ray can then be defined as $r(t) = \mathbf{o} + t * \mathbf{d}$.</p>
<p><img loading="lazy" src="https://yugajmera.github.io/posts/nerf/figure2.png" alt=""  />
</p>
<p>This process of ray tracing is known as backward tracing, as it involves tracing the path of light rays from the camera to the object, as opposed to tracing from the light source to the object.</p>
<p><strong>Input</strong>: A set of camera poses $(x_c, , y_c, , z_c,, , \theta, , \phi)_n$</p>
<p><strong>Output</strong>: A bundle of rays for every pose $(r_{\mathbf{o}, \mathbf{d}})_{H \times W \times n}$</p>
<p> </p>
<h3 id="sampling-query-points">Sampling Query points<a hidden class="anchor" aria-hidden="true" href="#sampling-query-points">#</a></h3>
<p>You may wonder, what is done with the rays? We trace them from the camera through the scene by adjusting the parameter $t$ until they intersect with some interesting location (object) in the scene. To find these locations, we incrementally step along the ray and sample points at regular intervals.</p>
<p>By querying a trained neural network at these 3D points along the viewing ray, we can determine if they belong to the object volume and obtain their visual properties to render an image. However, sampling points along a ray is challenging, as too many non-object points won&rsquo;t provide useful information, and focusing only on high-density regions may miss interesting areas.</p>
<p><img loading="lazy" src="https://yugajmera.github.io/posts/nerf/figure3.png" alt=""  />
</p>
<p>In our toy example, we uniformly sample along the ray by taking $m$ samples. However, to improve performance, the authors use &ldquo;hierarchical volume sampling&rdquo; to allocate samples proportionally to their expected impact on the final rendering.</p>
<p><strong>Input</strong>: A bundle of rays for every pose $(r_{\mathbf{o}, \mathbf{d}})_{H \times W \times n}$</p>
<p><strong>Output</strong>: A set of 3D query points for every ray $(x_p, , y_p, , z_p)_{m \times H \times W \times n}$</p>
<p> </p>
<h3 id="positional-encoding">Positional Encoding<a hidden class="anchor" aria-hidden="true" href="#positional-encoding">#</a></h3>
<p>Once we collect the query points for every ray, we are potentially ready to feed them into our neural network. However, the authors found that resulting renderings perform poorly at representing high-frequency variations in color and geometry that make images perceptually sharp and vivid for the human eye.</p>
<p>This observation is consistent with <a href="https://arxiv.org/abs/1806.08734">Rahaman et al.</a> who show that deep networks have a tendency to learn lower-frequency functions. They claim that mapping the inputs to a higher dimensional space using high-frequency functions before passing them to the network enables better fitting of data that contains high-frequency variation.</p>
<p>The authors use the positional encoding containing sine and cosine functions of varying frequencies.</p>
<p>\begin{equation*}
\gamma(p) = [sin (2^0 \pi p) , cos (2^0 \pi p) ; sin (2 \pi p) , cos (2 \pi p) ; &hellip; ; sin (2^{L-1} \pi p) , cos (2^{L-1} \pi p)]
\end{equation*}</p>
<p>where $L$ is the number of dimensions in the positional encoding. <!-- raw HTML omitted --></p>
<p>The function $\gamma(.)$ is applied separately to each of the three coordinate values in $\mathbf{x}$ (which are normalized to lie in [−1, 1]). As viewing direction is also an input to our network, we embed it as well. The authors use $L = 10$ for embeding the 3D query points and $L = 4$ for embeding the viewing direction.</p>
<p><strong>Input</strong>: A set of 3D query points for every ray, $(\mathbf{x} = x_p, , y_p, , z_p)_{m \times H \times W \times n}$ and 3D viewing direction $(\mathbf{d} = \theta, \phi, \psi)_n$</p>
<p><strong>Output</strong>: Embedings of query points $\gamma(\mathbf{x})_{m \times H \times W \times n}$ and viewing direction $\gamma(\mathbf{d})_n$</p>
<p> </p>
<h3 id="neural-network-inference">Neural Network inference<a hidden class="anchor" aria-hidden="true" href="#neural-network-inference">#</a></h3>
<p>To achieve multiview consistency in a neural network, we restrict the network to predict the volume density $\sigma$ as a function of only the location in 3D space, while allowing the RGB color $c$ to be predicted as a function of both location and viewing direction.</p>
<p><img loading="lazy" src="https://yugajmera.github.io/posts/nerf/model.jpg" alt=""  />
</p>
<p>The MLP architecture consists of 8 fully-connected layers, each with 256 channels and ReLU activations. A skip connection is included in the network that concatenates the input to the fifth layer&rsquo;s activation. It takes the encoded query points $\gamma(\mathbf{x})$ as input and produces two outputs: the volume density $\sigma$ and a 256-dimensional feature vector.</p>
<p>This feature vector is then concatenated with the embedded viewing direction $\gamma(\mathbf{d})$ and passed through another fully-connected layer with 128 channels and a ReLU activation to produce the view-dependent RGB color $c$ (tuple in the range from 0 to 1). The authors claim that a model trained without view dependence has difficulty representing specularities.</p>
<p>Both of these pieces of information combined allow us to compute the volume density profile for every ray as shown in the figure below.</p>
<p><img loading="lazy" src="https://yugajmera.github.io/posts/nerf/profile.png" alt=""  />
</p>
<p><strong>Input</strong>: Embedings of query points $\gamma(\mathbf{x})_{m \times H \times W \times n}$ and viewing direction $\gamma(\mathbf{d})_n$</p>
<p><strong>Output</strong>: RGB color and volume density for every query point $(c, , \sigma)_{m \times H \times W \times n}$</p>
<p> </p>
<h3 id="volume-rendering">Volume Rendering<a hidden class="anchor" aria-hidden="true" href="#volume-rendering">#</a></h3>
<p>Now, it&rsquo;s time to turn this volume density profile along a ray into an rgb pixel value. Once we have computed the pixel values for all the rays, we will have a full $H \times W$ image for all the $n$ viewpoints.</p>
<p>The volume rendering process involves computing the accumulated radiance along the viewing ray as it passes through the neural radiance field. This is done by integrating the radiance values at each sampled point along the ray, weighted by the transmittance, or opacity of the medium at that point.</p>
<p>The expected color of a camera ray $r(t) = \mathbf{o} + t * \mathbf{d}$ with near and far bounds $t_n$ and $t_f$ is given as:</p>
<p>\begin{equation}
C(r) = \int_{t_n}^{t_f} T(t) ; \sigma(r(t)) ; c(r(t), \mathbf{d}) ; dt
\end{equation}</p>
<p>where function $T(t)$ denotes the accumulated transmittance along the ray from $t_n$ to $t$, i.e., the probability that the ray travels from $t_n$ to $t$ without hitting
any other particle.</p>
<p>\begin{equation*}
T(t) = \text{exp} \left( - \int_{t_n}^{t} \sigma(r(s)) ; ds  \right)
\end{equation*}</p>
<p>These complex-looking integrals can be approximated via numerical quadrature. We use a stratified sampling approach where we select a random set of quadrature points ${t_i}_{i = 1}^N$ by uniformly drawing samples from $N$ evenly-spaced ray bins between $t_n$ and $t_f$.</p>
<p><img loading="lazy" src="https://yugajmera.github.io/posts/nerf/rendering.png" alt=""  />
</p>
<p>The use of stratified sampling allows for a continuous representation of the scene, despite using a discrete set of samples to estimate the integral. This is because the MLP is evaluated at continuous positions during optimization.</p>
<p>The approximated color of each ray is computed as,</p>
<p>\begin{equation*}
C(r) \approx \sum_{i = 1}^N T(t_i) ; \alpha_i ; c_i
\end{equation*}
where $\alpha_i = (1 - \text{exp}(-\sigma_i , \delta_i))$, which can be viewed as a measure of opacity and $\delta_i = t_{i+1} - t_i$ is the distance between two quadrature points. The accumulated transmittance is then a cumulative product of all the transmittance (which is 1 - opacity) behind it,</p>
<p>\begin{equation*}
T_i =  \prod_{j=1}^{i-1} (1 - \alpha_j)
\end{equation*}</p>
<p><strong>Input</strong>: RGB color and volume density for every query point $(c, , \sigma)_{m \times H \times W \times n}$</p>
<p><strong>Output</strong>: Rendered Images $(H \times W)_{n}$</p>
<p> </p>
<h3 id="computing-loss">Computing loss<a hidden class="anchor" aria-hidden="true" href="#computing-loss">#</a></h3>
<p>The final step is to calculate the loss between the rendered image and the ground truth for each viewpoint in the dataset. This loss function is a simple L2 loss between each pixel of the rendered image and the corresponding pixel of the ground truth image.</p>
<p> 
 
 
 </p>
<p><em>Credits</em>: While writing this blog, I referred to <a href="https://dtransposed.github.io/blog/2022/08/06/NeRF/">dtransposed&rsquo;s blog post</a> (the images are also from this post), and <a href="https://theaisummer.com/nerf/">AI Summer&rsquo;s post</a>.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://yugajmera.github.io/posts/diffusion-models/post/">
    <span class="title">Next »</span>
    <br>
    <span>Decoding Diffusion Models</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://yugajmera.github.io/">YA Logs</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
