<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Loss functions | YA&#39;s Almanac</title>
<meta name="keywords" content="">
<meta name="description" content="While there are a ton of concepts related to Deep learning scrambled all around the internet, I thought why not have just one place where you can find all the fundamental concepts required to write your own neural network. But, if you prefer watching a video over reading a blog and have the time, I highly recommend Justin Johnson&rsquo;s course.
In this first part, I will discuss one of the most essential elements of deep learning - the loss function!">
<meta name="author" content="">
<link rel="canonical" href="https://yugajmera.github.io/posts/loss-function/post/">
<link crossorigin="anonymous" href="https://yugajmera.github.io/assets/css/stylesheet.ab4ae6179ea15f8d40abe2eaf7686672c2efdd6c8ab9f32ba68b327655b638ab.css" integrity="sha256-q0rmF56hX41Aq&#43;Lq92hmcsLv3WyKufMrposydlW2OKs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://yugajmera.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yugajmera.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yugajmera.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yugajmera.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://yugajmera.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://yugajmera.github.io/posts/loss-function/post/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
<style>
   mjx-container[display="true"] {
       margin: 1.5em 0 ! important
   }
</style>



  
    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-S759YBMKJE');
        }
      </script>
    
  

<meta property="og:title" content="Loss functions" />
<meta property="og:description" content="While there are a ton of concepts related to Deep learning scrambled all around the internet, I thought why not have just one place where you can find all the fundamental concepts required to write your own neural network. But, if you prefer watching a video over reading a blog and have the time, I highly recommend Justin Johnson&rsquo;s course.
In this first part, I will discuss one of the most essential elements of deep learning - the loss function!" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yugajmera.github.io/posts/loss-function/post/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-30T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-07-30T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Loss functions"/>
<meta name="twitter:description" content="While there are a ton of concepts related to Deep learning scrambled all around the internet, I thought why not have just one place where you can find all the fundamental concepts required to write your own neural network. But, if you prefer watching a video over reading a blog and have the time, I highly recommend Justin Johnson&rsquo;s course.
In this first part, I will discuss one of the most essential elements of deep learning - the loss function!"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yugajmera.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Loss functions",
      "item": "https://yugajmera.github.io/posts/loss-function/post/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Loss functions",
  "name": "Loss functions",
  "description": "While there are a ton of concepts related to Deep learning scrambled all around the internet, I thought why not have just one place where you can find all the fundamental concepts required to write your own neural network. But, if you prefer watching a video over reading a blog and have the time, I highly recommend Justin Johnson\u0026rsquo;s course.\nIn this first part, I will discuss one of the most essential elements of deep learning - the loss function!",
  "keywords": [
    
  ],
  "articleBody": "While there are a ton of concepts related to Deep learning scrambled all around the internet, I thought why not have just one place where you can find all the fundamental concepts required to write your own neural network. But, if you prefer watching a video over reading a blog and have the time, I highly recommend Justin Johnson’s course.\nIn this first part, I will discuss one of the most essential elements of deep learning - the loss function! I call it the “Oxygen of Deep Learning”, because, without a loss function, a neural network cannot be trained (so it would just be dead!).\nBefore we jump into the nitty-gritty, let’s lay down the basics: image classification. It’s a core task of computer vision, where our model takes an image as input and spits out a label from a predetermined set of categories. Easy peasy, right?\nNow let’s formulate this: Suppose we have an image $x_i$ and its true label $y_i$ from the training dataset. We feed $x_i$ into our model to obtain scores for each class. In the example below, we give an image of a cat, car, and frog as inputs to our model to generate these scores.\nThink of these scores as the model’s confidence levels—it’s basically saying how sure it is that the image belongs to each specific category. The class with the maximum score becomes our predicted label $\\hat{y}_i$. The scores of the true class are highlighted in bold.\nClearly, the predictions of our classifier is not always correct. A loss function helps us quantify the performance of our model on our data. So, naturally, the lower the loss, the better our model is. It is also called an objective function or a cost function. We compute a loss for each input image and simply take an average across the training set to compute the final loss value.\nSVM Loss The idea behind Support Vector Machine (SVM) loss is pretty straightforward: we want the score of the correct class to be significantly higher than the scores for all the incorrect classes. Makes sense, right? We want our classifier to be confident in its predictions by giving a higher score to the correct category in comparison to the wrong ones by some margin.\nThe SVM loss has the form, \\begin{align} L_i = \\sum_{j \\neq y_i} \\text{max}(0, \\underbrace{s_j}_{\\text{score of jth class}} - \\underbrace{s_{y_i}}_{\\text{score of true class}} + \\underbrace{1}_{\\text{margin}}) \\end{align}\nThe SVM loss is also called Hinge Loss because its shape looks just like a door hinge. When the score of the correct class is higher than all the other scores plus a certain margin, the loss is zero. Otherwise, it increases linearly. It is important to note that the loss for the true class is always zero.\nLet’s derive the SVM loss for our example. For the cat image, the true score $s_{y_i} = 3.2$. The loss is computed on the car and frog classes as, \\begin{equation*} L_1 = \\text{max}(0, 5.1 - 3.2 + 1) + \\text{max}(0, -1.7 - 3.2 + 1) = 2.9 + 0 = 2.9 \\end{equation*} Similarly for the other two training examples, \\begin{align*} L_2 \u0026= \\text{max}(0, 1.3 - 4.9 + 1) + \\text{max}(0, 2 - 4.9 + 1) = 0 \\\\ L_3 \u0026= \\text{max}(0, 2.2 + 3.1 + 1) + \\text{max}(0, 2.5 + 3.1 + 1) = 6.3 + 6.6 = 12.9 \\end{align*} The final loss of our classifier is the average of all the losses: $L_{\\text{svm}} = 5.27$.\nCross-Entropy Loss Cross-Entropy loss or Logistic Loss is one of the most commonly used loss functions, where raw classifier scores are interpreted as probabilities. We run the raw scores through an exponential function to ensure the outputs are positive. We then normalize them to obtain a probabilistic distribution over all the categories. This transformation is called the softmax function.\nThe term “softmax” comes from the fact that it is a differential approximation to the max function. If we were to directly apply the max function to the raw score vector of the cat image in our example, it would yield $[0, 1, 0]$. However, this function is non-differentiable, making it unsuitable for training neural networks. Softmax is therefore a go-to tool in deep learning when you need to compute the max of something while ensuring differentiability. The image above shows the softmax function applied to cat image scores.\nThe softmax function provides our estimated distribution (denoted by $q$). The true or desired distribution is such that all the probability mass is concentrated on the correct class, i.e. $p = [0, .. 1, .. , 0]$ (contains a single 1 at the $y_i$th position).\nThe difference between two probability distributions is measured by cross entropy. The cross-entropy between a “true” distribution $p$ and an estimated distribution $q$ is defined as: \\begin{equation*} H(p,q) = - \\sum_x p(x) \\text{ log} (q(x)) \\end{equation*}\nHence, the cross-entropy loss would be the negative log of the probability of true class. For the above cat image example, the loss would be $L_1 = -log(0.13) = 2.04$. The cross-entropy loss has the form, \\begin{equation} L_i = -\\text{log} \\underbrace{\\left( \\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}} \\right)}_{\\text{softmax function}} \\end{equation}\nWhile these are the two most commonly used loss functions, a complete list of options can be found here - Loss Functions\n",
  "wordCount" : "874",
  "inLanguage": "en",
  "datePublished": "2022-07-30T00:00:00Z",
  "dateModified": "2022-07-30T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yugajmera.github.io/posts/loss-function/post/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "YA's Almanac",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yugajmera.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yugajmera.github.io/" accesskey="h" title="YA&#39;s Almanac (Alt + H)">YA&#39;s Almanac</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yugajmera.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Loss functions
    </h1>
    <div class="post-meta"><span title='2022-07-30 00:00:00 +0000 UTC'>July 30, 2022</span>&nbsp;·&nbsp;5 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#svm-loss" aria-label="SVM Loss">SVM Loss</a></li>
                <li>
                    <a href="#cross-entropy-loss" aria-label="Cross-Entropy Loss">Cross-Entropy Loss</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>While there are a ton of concepts related to Deep learning scrambled all around the internet, I thought why not have just one place where you can find all the fundamental concepts required to write your own neural network. But, if you prefer watching a video over reading a blog and  have the time, I highly recommend <a href="https://www.youtube.com/watch?v=dJYGatp4SvA&amp;list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r">Justin Johnson&rsquo;s course</a>.</p>
<p>In this first part, I will discuss one of the most essential elements of deep learning - the loss function! I call it the &ldquo;Oxygen of Deep Learning&rdquo;, because, without a loss function, a neural network cannot be trained (so it would just be dead!).</p>
<p>Before we jump into the nitty-gritty, let&rsquo;s lay down the basics: image classification. It&rsquo;s a core task of computer vision, where our model takes an image as input and spits out a label from a predetermined set of categories. Easy peasy, right?</p>
<p>Now let&rsquo;s formulate this: Suppose we have an image $x_i$ and its true label $y_i$ from the training dataset. We feed $x_i$ into our model to obtain scores for each class. In the example below, we give an image of a cat, car, and frog as inputs to our model to generate these scores.</p>
<figure class="align-center ">
    <img loading="lazy" src="https://yugajmera.github.io/posts/loss-function/scores.png#center" width="500"/> 
</figure>

<p>Think of these scores as the model&rsquo;s confidence levels—it&rsquo;s basically saying how sure it is that the image belongs to each specific category. The class with the maximum score becomes our predicted label $\hat{y}_i$. The scores of the true class are highlighted in bold.</p>
<p>Clearly, the predictions of our classifier is not always correct. A loss function helps us quantify the performance of our model on our data. So, naturally, the lower the loss, the better our model is. It is also called an objective function or a cost function. We compute a loss for each input image and simply take an average across the training set to compute the final loss value.</p>
<figure class="align-center ">
    <img loading="lazy" src="https://yugajmera.github.io/posts/loss-function/loss.png#center"/> 
</figure>

<p> </p>
<h3 id="svm-loss">SVM Loss<a hidden class="anchor" aria-hidden="true" href="#svm-loss">#</a></h3>
<p>The idea behind Support Vector Machine (SVM) loss is pretty straightforward: we want the score of the correct class to be significantly higher than the scores for all the incorrect classes. Makes sense, right? We want our classifier to be confident in its predictions by giving a higher score to the correct category in comparison to the wrong ones by some margin.</p>
<p>The SVM loss has the form,
\begin{align}
L_i = \sum_{j \neq y_i} \text{max}(0, \underbrace{s_j}_{\text{score of jth class}} - \underbrace{s_{y_i}}_{\text{score of true class}} + \underbrace{1}_{\text{margin}})
\end{align}</p>
<figure class="align-center ">
    <img loading="lazy" src="https://yugajmera.github.io/posts/loss-function/hinge.png#center"/> 
</figure>

<p>The SVM loss is also called Hinge Loss because its shape looks just like a door hinge. When the score of the correct class is higher than all the other scores plus a certain margin, the loss is zero. Otherwise, it increases linearly. It is important to note that the loss for the true class is always zero.</p>
<figure class="align-center ">
    <img loading="lazy" src="https://yugajmera.github.io/posts/loss-function/svm_loss.png#center" width="450"/> 
</figure>

<p>Let&rsquo;s derive the SVM loss for our example. For the cat image, the true score $s_{y_i} = 3.2$. The loss is computed on the car and frog classes as,
\begin{equation*}
L_1 = \text{max}(0, 5.1 - 3.2 + 1) + \text{max}(0, -1.7 - 3.2 + 1) = 2.9 + 0 = 2.9
\end{equation*}
Similarly for the other two training examples,
\begin{align*}
L_2 &amp;= \text{max}(0, 1.3 - 4.9 + 1) + \text{max}(0, 2 - 4.9 + 1) = 0 \\ L_3 &amp;= \text{max}(0, 2.2 + 3.1 + 1) + \text{max}(0, 2.5 + 3.1 + 1) = 6.3 + 6.6 = 12.9
\end{align*}
The final loss of our classifier is the average of all the losses: $L_{\text{svm}} = 5.27$.</p>
<p> </p>
<h3 id="cross-entropy-loss">Cross-Entropy Loss<a hidden class="anchor" aria-hidden="true" href="#cross-entropy-loss">#</a></h3>
<p>Cross-Entropy loss or Logistic Loss is one of the most commonly used loss functions, where raw classifier scores are interpreted as probabilities. We run the raw scores through an exponential function to ensure the outputs are positive. We then normalize them to obtain a probabilistic distribution over all the categories. This transformation is called the softmax function.</p>
<figure class="align-center ">
    <img loading="lazy" src="https://yugajmera.github.io/posts/loss-function/ce_loss.png#center"/> 
</figure>

<p>The term &ldquo;softmax&rdquo; comes from the fact that it is a differential approximation to the max function. If we were to directly apply the max function to the raw score vector of the cat image in our example, it would yield $[0, 1, 0]$. However, this function is non-differentiable, making it unsuitable for training neural networks. Softmax is therefore a go-to tool in deep learning when you need to compute the max of something while ensuring differentiability. The image above shows the softmax function applied to cat image scores.</p>
<p>The softmax function provides our estimated distribution (denoted by $q$). The true or desired distribution is such that all the probability mass is concentrated on the correct class, i.e. $p = [0, .. 1, .. , 0]$ (contains a single 1 at the $y_i$th position).</p>
<p>The difference between two probability distributions is measured by cross entropy. The cross-entropy between a &ldquo;true&rdquo; distribution $p$ and an estimated distribution $q$ is defined as:
\begin{equation*}
H(p,q) = - \sum_x p(x) \text{ log} (q(x))
\end{equation*}</p>
<p>Hence, the cross-entropy loss would be the negative log of the probability of true class. For the above cat image example, the loss would be $L_1 = -log(0.13) = 2.04$. The cross-entropy loss has the form, \begin{equation} L_i = -\text{log} \underbrace{\left( \frac{e^{s_{y_i}}}{\sum_j e^{s_j}} \right)}_{\text{softmax function}} \end{equation}</p>
<p> <br>
While these are the two most commonly used loss functions, a complete list of options can be found here - <a href="https://pytorch.org/docs/stable/nn.functional.html#loss-functions">Loss Functions</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://yugajmera.github.io/posts/optimization/post/">
    <span class="title">« Prev</span>
    <br>
    <span>Optimization Methods</span>
  </a>
  <a class="next" href="https://yugajmera.github.io/posts/sample_post/">
    <span class="title">Next »</span>
    <br>
    <span>My 1st post</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://yugajmera.github.io/">YA&#39;s Almanac</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
