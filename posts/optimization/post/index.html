<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Optimization Methods | YA&#39;s Almanac</title>
<meta name="keywords" content="">
<meta name="description" content="The loss function tells us how good our current classifier, with its current weights, is performing on the training data. Because we&rsquo;re a bunch of greedy people, we want to find those golden set of weights that incurs the minimum loss on our training set, squeezing out every ounce of accuracy we can to ace the test set.
We generally initialize our model with some weights and then use the training set to search over all the possible values, aiming to arrive at the one that optimally fits our data.">
<meta name="author" content="">
<link rel="canonical" href="https://yugajmera.github.io/posts/optimization/post/">
<link crossorigin="anonymous" href="https://yugajmera.github.io/assets/css/stylesheet.ab4ae6179ea15f8d40abe2eaf7686672c2efdd6c8ab9f32ba68b327655b638ab.css" integrity="sha256-q0rmF56hX41Aq&#43;Lq92hmcsLv3WyKufMrposydlW2OKs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://yugajmera.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yugajmera.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yugajmera.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yugajmera.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://yugajmera.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://yugajmera.github.io/posts/optimization/post/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
<style>
   mjx-container[display="true"] {
       margin: 1.5em 0 ! important
   }
</style>



  
    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-S759YBMKJE');
        }
      </script>
    
  

<meta property="og:title" content="Optimization Methods" />
<meta property="og:description" content="The loss function tells us how good our current classifier, with its current weights, is performing on the training data. Because we&rsquo;re a bunch of greedy people, we want to find those golden set of weights that incurs the minimum loss on our training set, squeezing out every ounce of accuracy we can to ace the test set.
We generally initialize our model with some weights and then use the training set to search over all the possible values, aiming to arrive at the one that optimally fits our data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yugajmera.github.io/posts/optimization/post/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-08-01T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Optimization Methods"/>
<meta name="twitter:description" content="The loss function tells us how good our current classifier, with its current weights, is performing on the training data. Because we&rsquo;re a bunch of greedy people, we want to find those golden set of weights that incurs the minimum loss on our training set, squeezing out every ounce of accuracy we can to ace the test set.
We generally initialize our model with some weights and then use the training set to search over all the possible values, aiming to arrive at the one that optimally fits our data."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yugajmera.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Optimization Methods",
      "item": "https://yugajmera.github.io/posts/optimization/post/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Optimization Methods",
  "name": "Optimization Methods",
  "description": "The loss function tells us how good our current classifier, with its current weights, is performing on the training data. Because we\u0026rsquo;re a bunch of greedy people, we want to find those golden set of weights that incurs the minimum loss on our training set, squeezing out every ounce of accuracy we can to ace the test set.\nWe generally initialize our model with some weights and then use the training set to search over all the possible values, aiming to arrive at the one that optimally fits our data.",
  "keywords": [
    
  ],
  "articleBody": "The loss function tells us how good our current classifier, with its current weights, is performing on the training data. Because we’re a bunch of greedy people, we want to find those golden set of weights that incurs the minimum loss on our training set, squeezing out every ounce of accuracy we can to ace the test set.\nWe generally initialize our model with some weights and then use the training set to search over all the possible values, aiming to arrive at the one that optimally fits our data. Optimization is the process of finding the set of parameters that minimizes the loss function.\nImagine a landscape, where every point (x,y) represents a weight value and the height at that point is the loss function. Our goal is to navigate to the bottom-most point of this terrain, where we’ll find the weights that yield the minimal loss.\nSince we don’t have the exact equation of the landscape, computing the minimum of the curve is not straightforward (it’s not a convex optimization problem). So, instead, we take iterative, small steps towards the direction of the local minimum, hoping to eventually reach at the lowest point. This direction of the local steepest descent is essentially the negative gradient of the loss function with respect to the weights.\nGradient Descent In the algorithm below, also known as the vanilla version of gradient descent, we begin by initializing the weights with some arbitary values. Then, we loop for a fixed number of iterations, where for each iteration, we compute the gradient for our current weights and take a step in the negative gradient direction to update the weight values.\n# Vanilla Gradient Descent w = initialize_weights() for t in range(num_steps): dw = compute_gradient(loss_fn, data, w) w += - learning_rate * dw The size of each step is determined by a hyperparameter known as the learning rate. This parameter controls how much we move in the negative gradient direction during each iteration of the optimization process. A higher learning rate means taking larger steps toward the negative gradient, which might lead to faster convergence but can also result in unstable behavior. On the other hand, lower learning rates are more stable but may take longer to converge. Therefore it is crucial to select an optimal learning rate.\nThe number of iterations is a also hyperparameter that determines the finite number of steps to run this algorithm for the model to converge. This value depends on the time at hand and computation resources. We usually stop when the model converges.\nJust as we averaged the losses over all images in the training data to compute the overall loss, we also take the average of individual gradients when calculating the overall gradient.\n\\begin{align*} L \u0026= \\frac{1}{N} \\sum_{i = 1}^N L_i(x_i, y_i, W) \\\\ \\mathbf{dw} = \\frac{\\partial L}{ \\partial W} \u0026= \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial}{\\partial W} L_i(x_i, y_i, W) \\end{align*}\nHowever, this process becomes computationally expensive and slow when dealing with a huge training set ($N$ is very large), as just one step of gradient descent involves looping over the entire training data.\nIn practice, this version of gradient descent is not very feasible. But fear not! Here’s a solution: Instead of using the entire training set, we split it into mini-batches, each containing $B$ examples. We can now use this mini-batch to perform a parameter update. This variant called Mini-batch Gradient Descent.\nThe intuition behind this solution lies in the assumption that the examples in the training data are correlated. Therefore, computing the gradient over batches of the training data is a good approximation to the gradient of the full objective. This allows us to achieve much faster convergence by evaluating the mini-batch gradients to perform more frequent parameter updates.\n# Mini-batch Gradient Descent w = initialize_weights() batches = split(data, batch_size) for t in range(num_steps): for mini_batch in batches: dw = compute_gradient(loss_fn, mini_batch, w) w += - learning_rate * dw The batch size is another hyperparameter often chosen from values like 32, 64, 128, or 256, which you might notice are powers of 2. This is because many vectorized operations perform more efficiently when their inputs are sized in powers of 2. We select a batch size as large as possible that still fits comfortably within the memory constraints of the GPU.\nThe extreme case is a setting where the mini-batch contains only one single example $B = 1$. This variant is called Stochastic Gradient Descent (SGD), where we update the parameters by taking one observation at each iteration.\n# Stochastic Gradient Descent (SGD) w = initialize_weights() for t in range(num_steps): for example in data: dw = compute_gradient(loss_fn, example, w) w += - learning_rate * dw Since we are updating the weights at each iteration, the learning curve tends to be erratic in SGD as compared to the mini-batch version. This is why the latter is prefered. Both versions are commonly referred to as SGD in practice, as the distinguishing factor is simply the batch size used during training.\nThere are a couple of challenges with SGD that needs to be addressed before applying it to optimize our models,\nIf the loss landscape contains a local minimum or a saddle point, the SGD algorithm might get trapped in it. If the loss landscape varies rapidly in one direction and slowly in another, then with a constant learning rate, it would progress slowly along the shallow direction and jitter along the steep direction. While choosing a small learning rate may work in this case, it would lead to very slow convergence. Momentum The gradient descent with momentum algorithm, or simply Momentum, draws inspiration from physics. Instead of relying solely on the gradient of the current step to guide the search, momentum also accumulates the gradients from past steps to determine the direction of movement. By doing so, it ensures that we keep moving consistently in the same direction.\nAs a result, if the updates of weights (dw) has been constantly high, we build momentum and quickly descend the surface while jumping out of the local minimums along the way (solving Problem 1).\n# SGD + Momentum v = 0 for t in range(num_steps): v = (m * v) - learning_rate * dw w += v Let’s consider two extreme cases to understand this concept better. If $m = 0$ (no momentum), then the algorithm behaves exactly like gradient descent. On the other hand, if $m = 1$, it rocks back and forth endlessly like a frictionless bowl, never converging. Typically the value of $m$ is set to 0.9 or 0.99.\n\\begin{align*} \\text{SGD: } \u0026 \\Delta w = - \\eta * dw \\\\ \\text{SGD + Momentum: } \u0026 \\Delta w^{t} = - \\eta * dw + m * \\Delta w^{t-1} \\end{align*}\nWhen the change in weights is large in the past iterations, i.e., the landscape is steep, we take larger steps towards the minimum (with high momentum) and avoid getting trapped in any local minimas. Whereas, as we approach the minimum, the change in weights becomes small, we add a small momentum to our gradient step, maintaining (some) stability for convergence. The illustration below visually depicts this concept.\nMomentum (magenta) with m = 0.99 vs. Gradient Descent (cyan) on a surface with a global minimum (the left well) and local minimum (the right well)\nAdaGrad Instead of keeping track of the sum of gradient-like momentum, the Adaptive Gradient algorithm, or AdaGrad for short, keeps track of the sum of gradient squared and uses that to have an adaptive learning rate.\n# Adagrad grad_squared = 0 for t in range(num_steps): grad_squared += dw * dw w += -learning_rate * dw / (grad_squared.sqrt() + 1e-7) In the direction of a steep descent (dw = high), the learning rate would be damped, and we would take smaller update steps, whereas, in the shallow direction (dw = low), we would take larger steps (solves Problem 2). Furthermore, in the initial iterations, the sum of gradient squares would be small, so the learning rate would be high, and as we keep accumulating gradient squares, the learning rate decays over time (a good feature to have to speed up the learning process!).\nHowever, Adagrad might decay the learning rate even before reaching the minimum as the sum of gradient squared only grows and never shrinks! If the sum of gradient squares becomes too big, the learning rate would be too low to update the weights. To overcome this problem, we use a variant called RMSProp.\nRMSProp RMSProp (short for Root Mean Square Propagation) is a leaky version of AdaGrad that decays the running sum of square gradients and ensures that the learning rate does not become too small.\n# RMSProp grad_squared = 0 for t in range(num_steps): grad_squared = (decay_rate * grad_squared) + (1 - decay_rate) * dw * dw w += -learning_rate * dw / (grad_squared.sqrt() + 1e-7) The decay rate is a hyperparameter usually set to 0.9, and the typical value of the learning rate = 0.001.\nThe behavior of these algorithms can be visualized below. AdaGrad (white) and RMSProp (green) are both faster and more stable than Momentum (as it chooses a better path), but they get stuck in a local minimum (since they solve Problem 2 and not Problem 1).\nMomentum (magenta) with m = 0.99 vs. Gradient Descent (cyan) vs. AdaGrad (white) vs RMSProp (green) with decay rate = 0.9 on a surface with a global minimum (the left well) and local minimum (the right well)\nAdam Adaptive Moment Estimation (Adam) combines RMSProp and Momentum (getting the best of two worlds!). In AdaGrad and RMSProp, with a decay rate closer to one, the sum of squared gradients would be very small in the initial iterations (the moments are biased towards zero), leading to a very high learning rate at the beginning. We overcome this problem using bias correction.\n# Adam m1 = 0 m2 = 0 for t in range(num_steps): m1 = (beta1 * m1) + (1 - beta1) * dw m2 = (beta2 * m1) + (1 - beta2) * dw * dw m1_unbias = m1 / (1 - beta1 ** t) m2_unbias = m2 / (1 - beta2 **t) w += -learning_rate * m1_unbias / (m2_unbias.sqrt() + 1e-7) Beta1 is the decay rate for the first moment, the sum of gradient (aka momentum), commonly set at 0.9. Beta 2 is the decay rate for the second moment, the sum of gradient squared, and it is commonly set at 0.999.\nAdam has become a go-to optimizer for most of the deep learning community today. Learning rates = 1e-3, 5e-4, and 1e-4 can be a great starting point for most models.\nMomentum (magenta) with m = 0.99 vs. Gradient Descent (cyan) vs. AdaGrad (white) vs RMSProp (green) with decay rate = 0.9 vs Adam (blue) with beta1 = 0.9 and beta2 = 0.999 on a surface with a global minimum (the left well) and local minimum (the right well)\nCredits: I have created these visualizations using this visualization tool.\nYou can also look at this cool visualization I came across on Emilien Dupont’s blog post, where you can click anywhere on the loss profile to see how different methods converge from that starting point.\n",
  "wordCount" : "1864",
  "inLanguage": "en",
  "datePublished": "2022-08-01T00:00:00Z",
  "dateModified": "2022-08-01T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yugajmera.github.io/posts/optimization/post/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "YA's Almanac",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yugajmera.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yugajmera.github.io/" accesskey="h" title="YA&#39;s Almanac (Alt + H)">YA&#39;s Almanac</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yugajmera.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Optimization Methods
    </h1>
    <div class="post-meta"><span title='2022-08-01 00:00:00 +0000 UTC'>August 1, 2022</span>&nbsp;·&nbsp;9 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#gradient-descent" aria-label="Gradient Descent">Gradient Descent</a></li>
                <li>
                    <a href="#momentum" aria-label="Momentum">Momentum</a></li>
                <li>
                    <a href="#adagrad" aria-label="AdaGrad">AdaGrad</a></li>
                <li>
                    <a href="#rmsprop" aria-label="RMSProp">RMSProp</a></li>
                <li>
                    <a href="#adam" aria-label="Adam">Adam</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>The loss function tells us how good our current classifier, with its current weights, is performing on the training data. Because we&rsquo;re a bunch of greedy people, we want to find those golden set of weights that incurs the minimum loss on our training set, squeezing out every ounce of accuracy we can to ace the test set.</p>
<p>We generally initialize our model with some weights and then use the training set to search over all the possible values, aiming to arrive at the one that optimally fits our data. Optimization is the process of finding the set of parameters that minimizes the loss function.</p>
<p>Imagine a landscape, where every point (x,y) represents a weight value and the height at that point is the loss function. Our goal is to navigate to the bottom-most point of this terrain, where we&rsquo;ll find the weights that yield the minimal loss.</p>
<p>Since we don&rsquo;t have the exact equation of the landscape, computing the minimum of the curve is not straightforward (it&rsquo;s not a convex optimization problem). So, instead, we take iterative, small steps towards the direction of the local minimum, hoping to eventually reach at the lowest point. This direction of the local steepest descent is essentially the negative gradient of the loss function with respect to the weights.</p>
<p> </p>
<h3 id="gradient-descent">Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#gradient-descent">#</a></h3>
<p>In the algorithm below, also known as the vanilla version of gradient descent, we begin by initializing the weights with some arbitary values. Then, we loop for a fixed number of iterations, where for each iteration, we compute the gradient for our current weights and take a step in the negative gradient direction to update the weight values.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Vanilla Gradient Descent</span>
</span></span><span style="display:flex;"><span>w <span style="color:#f92672">=</span> initialize_weights()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(num_steps):
</span></span><span style="display:flex;"><span>    dw <span style="color:#f92672">=</span> compute_gradient(loss_fn, data, w)
</span></span><span style="display:flex;"><span>    w <span style="color:#f92672">+=</span> <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> dw
</span></span></code></pre></div><p>The size of each step is determined by a hyperparameter known as the learning rate. This parameter controls how much we move in the negative gradient direction during each iteration of the optimization process. A higher learning rate means taking larger steps toward the negative gradient, which might lead to faster convergence but can also result in unstable behavior. On the other hand, lower learning rates are more stable but may take longer to converge. Therefore it is crucial to select an optimal learning rate.</p>
<p>The number of iterations is a also hyperparameter that determines the finite number of steps to run this algorithm for the model to converge. This value depends on the time at hand and computation resources. We usually stop when the model converges.</p>
<p>Just as we averaged the losses over all images in the training data to compute the overall loss, we also take the average of individual gradients when calculating the overall gradient.</p>
<p>\begin{align*}
L &amp;= \frac{1}{N} \sum_{i = 1}^N L_i(x_i, y_i, W) \\
\mathbf{dw} = \frac{\partial L}{ \partial W} &amp;= \frac{1}{N} \sum_{i=1}^N \frac{\partial}{\partial W} L_i(x_i, y_i, W)
\end{align*}</p>
<p>However, this process becomes computationally expensive and slow when dealing with a huge training set ($N$ is very large), as just one step of gradient descent involves looping over the entire training data.</p>
<p>In practice, this version of gradient descent is not very feasible. But fear not! Here&rsquo;s a solution: Instead of using the entire training set, we split it into mini-batches, each containing $B$ examples. We can now use this mini-batch to perform a parameter update. This variant called Mini-batch Gradient Descent.</p>
<p>The intuition behind this solution lies in the assumption that the examples in the training data are correlated. Therefore, computing the gradient over batches of the training data is a good approximation to the gradient of the full objective. This allows us to achieve much faster convergence by evaluating the mini-batch gradients to perform more frequent parameter updates.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Mini-batch Gradient Descent</span>
</span></span><span style="display:flex;"><span>w <span style="color:#f92672">=</span> initialize_weights()
</span></span><span style="display:flex;"><span>batches <span style="color:#f92672">=</span> split(data, batch_size)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(num_steps):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> mini_batch <span style="color:#f92672">in</span> batches:
</span></span><span style="display:flex;"><span>        dw <span style="color:#f92672">=</span> compute_gradient(loss_fn, mini_batch, w)
</span></span><span style="display:flex;"><span>        w <span style="color:#f92672">+=</span> <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> dw
</span></span></code></pre></div><p>The batch size is another hyperparameter often chosen from values like 32, 64, 128, or 256, which you might notice are powers of 2. This is because many vectorized operations perform more efficiently when their inputs are sized in powers of 2. We select a batch size as large as possible that still fits comfortably within the memory constraints of the GPU.</p>
<p>The extreme case is a setting where the mini-batch contains only one single example $B = 1$. This variant is called Stochastic Gradient Descent (SGD), where we update the parameters by taking one observation at each iteration.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Stochastic Gradient Descent (SGD)</span>
</span></span><span style="display:flex;"><span>w <span style="color:#f92672">=</span> initialize_weights()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(num_steps):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> example <span style="color:#f92672">in</span> data:
</span></span><span style="display:flex;"><span>        dw <span style="color:#f92672">=</span> compute_gradient(loss_fn, example, w)
</span></span><span style="display:flex;"><span>      	w <span style="color:#f92672">+=</span> <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> dw
</span></span></code></pre></div><p>Since we are updating the weights at each iteration, the learning curve tends to be erratic in SGD as compared to the mini-batch version. This is why the latter is prefered. Both versions are commonly referred to as SGD in practice, as the distinguishing factor is simply the batch size used during training.</p>
<p>There are a couple of challenges with SGD that needs to be addressed before applying it to optimize our models,</p>
<ol>
<li>If the loss landscape contains a local minimum or a saddle point, the SGD algorithm might get trapped in it.</li>
<li>If the loss landscape varies rapidly in one direction and slowly in another, then with a constant learning rate, it would progress slowly along the shallow direction and jitter along the steep direction. While choosing a small learning rate may work in this case, it would lead to very slow convergence.</li>
</ol>
<p> </p>
<h3 id="momentum">Momentum<a hidden class="anchor" aria-hidden="true" href="#momentum">#</a></h3>
<p>The gradient descent with momentum algorithm, or simply Momentum, draws inspiration from physics. Instead of relying solely on the gradient of the current step to guide the search, momentum also accumulates the gradients from past steps to determine the direction of movement. By doing so, it ensures that we keep moving consistently in the same direction.</p>
<p>As a result, if the updates of weights (dw) has been constantly high, we build momentum and quickly descend the surface while jumping out of the local minimums along the way (solving Problem 1).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># SGD + Momentum</span>
</span></span><span style="display:flex;"><span>v <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(num_steps):
</span></span><span style="display:flex;"><span>    v <span style="color:#f92672">=</span>  (m <span style="color:#f92672">*</span> v) <span style="color:#f92672">-</span> learning_rate <span style="color:#f92672">*</span> dw
</span></span><span style="display:flex;"><span>    w <span style="color:#f92672">+=</span> v
</span></span></code></pre></div><p>Let’s consider two extreme cases to understand this concept better. If $m = 0$ (no momentum), then the algorithm behaves exactly like gradient descent. On the other hand, if $m = 1$, it rocks back and forth endlessly like a frictionless bowl, never converging. Typically the value of $m$ is set to 0.9 or 0.99.</p>
<p>\begin{align*}
\text{SGD: } &amp; \Delta w = - \eta * dw \\
\text{SGD + Momentum: } &amp; \Delta w^{t} = - \eta * dw + m * \Delta w^{t-1}
\end{align*}</p>
<p>When the change in weights is large in the past iterations, i.e., the landscape is steep, we take larger steps towards the minimum (with high momentum) and avoid getting trapped in any local minimas. Whereas, as we approach the minimum, the change in weights becomes small, we add a small momentum to our gradient step, maintaining (some) stability for convergence. The illustration below visually depicts this concept.</p>
<figure class="align-center ">
    <img loading="lazy" src="https://yugajmera.github.io/posts/optimization/momentum.gif#center"
         alt="Momentum (magenta) with m = 0.99 vs. Gradient Descent (cyan) on a surface with a global minimum (the left well) and local minimum (the right well)"/> <figcaption>
            <p>Momentum (magenta) with m = 0.99 vs. Gradient Descent (cyan) on a surface with a global minimum (the left well) and local minimum (the right well)</p>
        </figcaption>
</figure>

<p> </p>
<h3 id="adagrad">AdaGrad<a hidden class="anchor" aria-hidden="true" href="#adagrad">#</a></h3>
<p>Instead of keeping track of the sum of gradient-like momentum, the Adaptive Gradient algorithm, or AdaGrad for short, keeps track of the sum of gradient squared and uses that to have an adaptive learning rate.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Adagrad</span>
</span></span><span style="display:flex;"><span>grad_squared <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(num_steps):
</span></span><span style="display:flex;"><span>    grad_squared <span style="color:#f92672">+=</span> dw <span style="color:#f92672">*</span> dw
</span></span><span style="display:flex;"><span>    w <span style="color:#f92672">+=</span> <span style="color:#f92672">-</span>learning_rate <span style="color:#f92672">*</span> dw <span style="color:#f92672">/</span> (grad_squared<span style="color:#f92672">.</span>sqrt() <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-7</span>)
</span></span></code></pre></div><p>In the direction of a steep descent (dw = high), the learning rate would be damped, and we would take smaller update steps, whereas, in the shallow direction (dw = low), we would take larger steps (solves Problem 2). Furthermore, in the initial iterations, the sum of gradient squares would be small, so the learning rate would be high, and as we keep accumulating gradient squares, the learning rate decays over time (a good feature to have to speed up the learning process!).</p>
<p>However, Adagrad might decay the learning rate even before reaching the minimum as the sum of gradient squared only grows and never shrinks! If the sum of gradient squares becomes too big, the learning rate would be too low to update the weights. To overcome this problem, we use a variant called RMSProp.</p>
<p> </p>
<h3 id="rmsprop">RMSProp<a hidden class="anchor" aria-hidden="true" href="#rmsprop">#</a></h3>
<p>RMSProp (short for Root Mean Square Propagation) is a leaky version of AdaGrad that decays the running sum of square gradients and ensures that the learning rate does not become too small.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># RMSProp</span>
</span></span><span style="display:flex;"><span>grad_squared <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(num_steps):
</span></span><span style="display:flex;"><span>    grad_squared <span style="color:#f92672">=</span> (decay_rate <span style="color:#f92672">*</span> grad_squared) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> decay_rate) <span style="color:#f92672">*</span> dw <span style="color:#f92672">*</span> dw
</span></span><span style="display:flex;"><span>    w <span style="color:#f92672">+=</span> <span style="color:#f92672">-</span>learning_rate <span style="color:#f92672">*</span> dw <span style="color:#f92672">/</span> (grad_squared<span style="color:#f92672">.</span>sqrt() <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-7</span>)
</span></span></code></pre></div><p>The decay rate is a hyperparameter usually set to 0.9, and the typical value of the learning rate = 0.001.</p>
<p>The behavior of these algorithms can be visualized below. AdaGrad (white) and RMSProp (green) are both faster and more stable than Momentum (as it chooses a better path), but they get stuck in a local minimum (since they solve Problem 2 and not Problem 1).</p>
<figure class="align-center ">
    <img loading="lazy" src="https://yugajmera.github.io/posts/optimization/rmsprop.gif#center"
         alt="Momentum (magenta) with m = 0.99 vs. Gradient Descent (cyan) vs. AdaGrad (white) vs RMSProp (green) with decay rate = 0.9 on a surface with a global minimum (the left well) and local minimum (the right well)"/> <figcaption>
            <p>Momentum (magenta) with m = 0.99 vs. Gradient Descent (cyan) vs. AdaGrad (white) vs RMSProp (green) with decay rate = 0.9 on a surface with a global minimum (the left well) and local minimum (the right well)</p>
        </figcaption>
</figure>

<p> </p>
<h3 id="adam">Adam<a hidden class="anchor" aria-hidden="true" href="#adam">#</a></h3>
<p>Adaptive Moment Estimation (Adam) combines RMSProp and Momentum (getting the best of two worlds!). In AdaGrad and RMSProp, with a decay rate closer to one, the sum of squared gradients would be very small in the initial iterations (the moments are biased towards zero), leading to a very high learning rate at the beginning. We overcome this problem using bias correction.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Adam</span>
</span></span><span style="display:flex;"><span>m1 <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>m2 <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(num_steps):
</span></span><span style="display:flex;"><span>   m1 <span style="color:#f92672">=</span> (beta1 <span style="color:#f92672">*</span> m1) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> beta1) <span style="color:#f92672">*</span> dw
</span></span><span style="display:flex;"><span>   m2 <span style="color:#f92672">=</span> (beta2 <span style="color:#f92672">*</span> m1) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> beta2) <span style="color:#f92672">*</span> dw <span style="color:#f92672">*</span> dw
</span></span><span style="display:flex;"><span>   m1_unbias <span style="color:#f92672">=</span> m1 <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> beta1 <span style="color:#f92672">**</span> t)
</span></span><span style="display:flex;"><span>   m2_unbias <span style="color:#f92672">=</span> m2 <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> beta2 <span style="color:#f92672">**</span>t)
</span></span><span style="display:flex;"><span>   w <span style="color:#f92672">+=</span> <span style="color:#f92672">-</span>learning_rate <span style="color:#f92672">*</span> m1_unbias <span style="color:#f92672">/</span> (m2_unbias<span style="color:#f92672">.</span>sqrt() <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-7</span>)
</span></span></code></pre></div><p>Beta1 is the decay rate for the first moment, the sum of gradient (aka momentum), commonly set at 0.9. Beta 2 is the decay rate for the second moment, the sum of gradient squared, and it is commonly set at 0.999.</p>
<p>Adam has become a go-to optimizer for most of the deep learning community today. Learning rates = 1e-3, 5e-4, and 1e-4 can be a great starting point for most models.</p>
<figure class="align-center ">
    <img loading="lazy" src="https://yugajmera.github.io/posts/optimization/all.gif#center"
         alt="Momentum (magenta) with m = 0.99 vs. Gradient Descent (cyan) vs. AdaGrad (white) vs RMSProp (green) with decay rate = 0.9 vs Adam (blue) with beta1 = 0.9 and beta2 = 0.999 on a surface with a global minimum (the left well) and local minimum (the right well)"/> <figcaption>
            <p>Momentum (magenta) with m = 0.99 vs. Gradient Descent (cyan) vs. AdaGrad (white) vs RMSProp (green) with decay rate = 0.9 vs Adam (blue) with beta1 = 0.9 and beta2 = 0.999 on a surface with a global minimum (the left well) and local minimum (the right well)</p>
        </figcaption>
</figure>

<p><em>Credits</em>: I have created these visualizations using this <a href="https://github.com/lilipads/gradient_descent_viz">visualization tool</a>.<br>
You can also look at this cool visualization I came across on <a href="https://emiliendupont.github.io/2018/01/24/optimization-visualization/">Emilien Dupont&rsquo;s blog post</a>, where you can click anywhere on the loss profile to see how different methods converge from that starting point.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://yugajmera.github.io/posts/neural-networks/post/">
    <span class="title">« Prev</span>
    <br>
    <span>Neural Networks: Activation functions and Weight Initialization</span>
  </a>
  <a class="next" href="https://yugajmera.github.io/posts/loss-function/post/">
    <span class="title">Next »</span>
    <br>
    <span>Loss functions</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://yugajmera.github.io/">YA&#39;s Almanac</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
