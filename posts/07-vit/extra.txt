## Swin Transformer

Swin Transformer was an effort to expand the applicability of Transformer such that it can serve as a backbone in computer vision, as it deos for NLP and as CNNs do in vision. They do so by constructing a hierarchial feature map, and use a shifted windown approach, obtaining linear computational complexity to image size.

### Architecture

1. Input RGB image: $(\text{H} \times \text{W} \times 3)$
2. Patch Parition: Similar to ViT, split the input image into non-overlapping patches, each of size $(\text{P} \times \text{P} \times 3)$, and flatten them. They use a patch size of 4 in their implementation. 
    * $(\text{H} \times \text{W} \times 3) \rightarrow (\text{H}/4 \times \text{W}/4 \times 4 \times 4 \times 3)$.
    * Number of tokens: $\text{N} = \text{H}/4 \cdot \text{W}/4$
    * Output:  $(\text{H}/4 \times \text{W}/4 \times 48)$
3. Linear embedding: Maps the feature dimension into an arbitary dimension, denoted by $C$. 
    * $(\text{H}/4 \times \text{W}/4 \times 48) \rightarrow (\text{H}/4 \times \text{W}/4 \times C)$

This forms the input to the transformer blocks in "Stage 1". To produce hierarchial representation, the number of tokens is reduced by patch merging layers as the network gets deeper. 

Notably, the transformer's sequence length (i.e., the number of patches) is inversely proportional to the square of the patch size. This means that models with smaller patch sizes are computationally more expensive since they process longer sequences.

Additionally, for a fixed patch size, higher-resolution images result in longer sequences, further increasing computational cost. In fact, the complexity grows quadratically with image size, making ViTs inefficient as general-purpose backbones for handling arbitrary-sized images.

To address this limitation, Microsoft Research introduced the Swin Transformer [[2]](#references), which you can read more about here: [https://amaarora.github.io/posts/2022-07-04-swintransformerv1.html](https://amaarora.github.io/posts/2022-07-04-swintransformerv1.html). It has become go-to transformer-based backbone for vision tasks today.