<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Generative Image Models Part 1: VAE and GANs | YA's Almanac</title>
<meta name=keywords content><meta name=description content="Machine learning can broadly be categorized into three types:
Supervised Learning: The model is trained to learn a mapping from input data $\mathbf{x}$ to output labels $\mathbf{y}$. This includes tasks like image classification, object detection, and image captioning. However, supervised learning depends heavily on large, labeled datasets—typically created through extensive human annotation.
Unsupervised Learning: The model is trained to discover patterns or underlying structures within unlabelled data. A key objective here is to learn the data distribution $p_{data}(\mathbf{x})$, enabling us to sample from it to generate new data."><meta name=author content><link rel=canonical href=https://yugajmera.github.io/posts/04-gen1/post/><link crossorigin=anonymous href=https://yugajmera.github.io/assets/css/stylesheet.74991b51f7611c2303d0b5649703d675530589621885eb78236e099c22aa93a5.css integrity="sha256-dJkbUfdhHCMD0LVklwPWdVMFiWIYhet4I24JnCKqk6U=" rel="preload stylesheet" as=style><link rel=icon href=https://yugajmera.github.io/assets/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yugajmera.github.io/assets/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yugajmera.github.io/assets/favicon-32x32.png><link rel=apple-touch-icon href=https://yugajmera.github.io/assets/apple-touch-icon.png><link rel=mask-icon href=https://yugajmera.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yugajmera.github.io/posts/04-gen1/post/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>mjx-container[display=true]{margin:1.5em 0!important}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S759YBMKJE",{anonymize_ip:!1})}</script><meta property="og:title" content="Generative Image Models Part 1: VAE and GANs"><meta property="og:description" content="Machine learning can broadly be categorized into three types:
Supervised Learning: The model is trained to learn a mapping from input data $\mathbf{x}$ to output labels $\mathbf{y}$. This includes tasks like image classification, object detection, and image captioning. However, supervised learning depends heavily on large, labeled datasets—typically created through extensive human annotation.
Unsupervised Learning: The model is trained to discover patterns or underlying structures within unlabelled data. A key objective here is to learn the data distribution $p_{data}(\mathbf{x})$, enabling us to sample from it to generate new data."><meta property="og:type" content="article"><meta property="og:url" content="https://yugajmera.github.io/posts/04-gen1/post/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-16T00:00:00+00:00"><meta property="article:modified_time" content="2022-11-16T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Generative Image Models Part 1: VAE and GANs"><meta name=twitter:description content="Machine learning can broadly be categorized into three types:
Supervised Learning: The model is trained to learn a mapping from input data $\mathbf{x}$ to output labels $\mathbf{y}$. This includes tasks like image classification, object detection, and image captioning. However, supervised learning depends heavily on large, labeled datasets—typically created through extensive human annotation.
Unsupervised Learning: The model is trained to discover patterns or underlying structures within unlabelled data. A key objective here is to learn the data distribution $p_{data}(\mathbf{x})$, enabling us to sample from it to generate new data."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yugajmera.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Generative Image Models Part 1: VAE and GANs","item":"https://yugajmera.github.io/posts/04-gen1/post/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Generative Image Models Part 1: VAE and GANs","name":"Generative Image Models Part 1: VAE and GANs","description":"Machine learning can broadly be categorized into three types:\nSupervised Learning: The model is trained to learn a mapping from input data $\\mathbf{x}$ to output labels $\\mathbf{y}$. This includes tasks like image classification, object detection, and image captioning. However, supervised learning depends heavily on large, labeled datasets—typically created through extensive human annotation.\nUnsupervised Learning: The model is trained to discover patterns or underlying structures within unlabelled data. A key objective here is to learn the data distribution $p_{data}(\\mathbf{x})$, enabling us to sample from it to generate new data.","keywords":[],"articleBody":"Machine learning can broadly be categorized into three types:\nSupervised Learning: The model is trained to learn a mapping from input data $\\mathbf{x}$ to output labels $\\mathbf{y}$. This includes tasks like image classification, object detection, and image captioning. However, supervised learning depends heavily on large, labeled datasets—typically created through extensive human annotation.\nUnsupervised Learning: The model is trained to discover patterns or underlying structures within unlabelled data. A key objective here is to learn the data distribution $p_{data}(\\mathbf{x})$, enabling us to sample from it to generate new data. With the abundance of raw text, images, and other unlabelled data on the internet, unsupervised learning has become a significant area of machine learning.\nReinforcement Learning: The model learns to take actions in an environment to maximize cumulative rewards. Learning is driven by trial and error, guided by feedback in the form of rewards or penalties.\nMy previous blog posts have primarily focused on supervised learning for images, such as classification tasks. In this new series, we’ll shift our focus to unsupervised learning for images, specifically generative image models.\nIn Part 1, we’ll explore Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs)—two foundational approaches in generative modeling. In upcoming posts, we’ll also discuss more advanced techniques, such as Diffusion Models, which have recently gained widespread attention.\nAutoencoders An autoencoder [1] is a type of neural network with a bottleneck structure—it extracts features $\\mathbf{z}$ from an input image $\\mathbf{x}$ and uses them to reconstruct the original data $\\mathbf{x}’$, effectively learning an identity mapping. In doing so, it learns a more efficient, compressed representation of the data.\nIt consists of two main components:\nEncoder$g$ : A network that compresses the input into a latent-space representation (of a lower dimension). Decoder $f$: A network that reconstructs the original input from this latent representation. Illustration of autoencoder model architecture. [Ref: Lil’s Log]\nThe encoder network learns to perform dimensionality reduction, similar to how Principal Component Analysis (PCA) works. Through training, the autoencoder captures the most salient features in a compact latent space, obtaining $\\mathbf{z} = g_\\phi(\\mathbf{x})$.\nTo train the autoencoder, we minimize the reconstruction error between the original input and the output produced by the decoder. The loss objective is a simple L2-loss between the input and the reconstructed input represented as $\\mathbf{x}’ = f_\\theta(g_\\phi(\\mathbf{x}))$.\n\\begin{equation*} L_\\text{AE}(\\theta, \\phi) = || \\mathbf{x} - f_\\theta(g_\\phi(\\mathbf{x})) ||^2_2 \\end{equation*}\nThe parameters $(\\theta, \\phi)$ are learned jointly to reconstruct the input data, aiming for the output to be as close as possible to the original input, i.e., $\\mathbf{x} \\approx f_\\theta(g_\\phi(\\mathbf{x}))$.\nBelow is a basic autoencoder implementation using fully connected layers, designed to reconstruct MNIST digit images.\nclass Autoencoder(nn.Module): def __init__(self, input_dim=784, latent_dim=64): super(Autoencoder, self).__init__() # Encoder: compress input self.encoder = nn.Sequential( nn.Linear(input_dim, 256), nn.ReLU(), nn.Linear(256, latent_dim), ) # Decoder: reconstruct input self.decoder = nn.Sequential( nn.Linear(latent_dim, 256), nn.ReLU(), nn.Linear(256, input_dim), nn.Sigmoid(), # Since image pixels are between 0 and 1 ) def forward(self, x): z = self.encoder(x) x_recon = self.decoder(z) return x_recon After training, we typically discard the decoder and use the encoder alone to extract meaningful features for downstream tasks.\nSince autoencoders are not probabilistic models, they do not explicitly learn the underlying data distribution, which means we cannot sample new images from them. Variational Autoencoders (VAEs), a probabilistic extension of autoencoders, were introduced to address this limitation.\nVariantional Autoencoders (VAEs) A Variational Autoencoder (VAE) [2] improves upon standard autoencoders by introducing a probabilistic approach to the latent space. Instead of mapping an input image $\\mathbf{x}$ to a single fixed latent vector $\\mathbf{z}$, VAEs map it to a distribution over latent variables.\nMore specifically, a VAE defines three key components:\nPrior $p_\\theta(\\mathbf{z})$: A predefined distribution over the latent space, usually chosen to be a standard multivariate Gaussian $\\mathcal{N}(0, I)$. Likelihood $p_\\theta(\\mathbf{x}|\\mathbf{z})$: The probability of generating an image $\\mathbf{x}$ from a given latent code $\\mathbf{z}$. This is modeled by the decoder network. Posterior $p_\\theta(\\mathbf{z}|\\mathbf{x})$: The true distribution of latent codes given the input data, which we cannot compute directly, so we approximate it using another neural network — the encoder — denoted as $q_\\phi(\\mathbf{z}|\\mathbf{x})$. Illustration of VAE model architecture. [Ref: Lil’s Log]\nHere’s what a typical training step looks like:\nEncode an input image $\\mathbf{x}^{(i)}$ using the encoder to get the approximate posterior distribution $q_\\phi(\\mathbf{z}|\\mathbf{x}^{(i)}) \\approx p_\\theta(\\mathbf{z} | \\mathbf{x}^{(i)})$.\nSample a latent vector $\\mathbf{z}^{(i)} \\sim q_\\phi(\\mathbf{z}|\\mathbf{x}^{(i)})$ from this distribution.\nFeed this latent vector into the decoder to get the likelihood distribution $p_\\theta(\\mathbf{x}|\\mathbf{z}^{(i)})$.\nReconstruct the input by sampling $\\mathbf{x’}^{(i)} \\sim p_\\theta(\\mathbf{x} | \\mathbf{z}^{(i)})$ from this distribution.\nSince neural networks cannot output a full probability distribution, the encoder and decoder instead predicts the mean and diagonal covariance of a Gaussian distribution: \\begin{align} q_\\phi (\\mathbf{z}|\\mathbf{x}) \u0026= \\mathcal{N}(\\mu_{\\mathbf{z} | \\mathbf{x}}, \\Sigma_{\\mathbf{z} | \\mathbf{x}}) \\\\ p_\\theta (\\mathbf{x}|\\mathbf{z}) \u0026= \\mathcal{N}(\\mu_{\\mathbf{x} | \\mathbf{z}}, \\Sigma_{\\mathbf{x} | \\mathbf{z}}) \\end{align}\nNote: We assume the pixels in the generated image are conditionally independent given the latent vector. This assumption leads to VAEs often producing blurry images.\nSampling is a stochastic process and therefore non-differentiable, which prevents direct backpropagation of gradients. To make training possible, the reparameterization trick is used, such that:\n\\begin{align*} \\mathbf{z} \u0026\\sim q_\\phi(\\mathbf{z} | \\mathbf{x}^{(i)}) = \\mathcal{N}(\\mathbf{z}; \\boldsymbol{\\mu}^{(i)}, \\boldsymbol{\\sigma}^{2(i)} I) \\\\ \\mathbf{z} \u0026= \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon} \\text{, where } \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, I) \\end{align*}\nwhere $\\odot$ refers to an element-wise product.\nSampling new images If we somehow knew the true parameters $\\theta^*$ of the model, we could generate a new data sample $\\mathbf{x}’$ by:\nSampling a latent vector $\\mathbf{z}^{(i)} \\sim p_{\\theta^*}(\\mathbf{z})$. Remember that the prior is standard multivariate Gaussian $\\mathcal{N}(0, I)$. Feeding $\\mathbf{z}^{(i)}$ into the decoder to get $p_\\theta (\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mu_{\\mathbf{x} | \\mathbf{z}}, \\Sigma_{\\mathbf{x} | \\mathbf{z}})$ Sampling a new image $\\mathbf{x’}^{(i)}$ from this distribution. Optimization: Deriving ELBO loss To learn the optimal parameters $\\theta^*$, we use maximum likelihood estimation (MLE): maximize the log likelihood of generating real data samples:\n\\begin{align*} \\theta^* \u0026= \\arg \\max_\\theta \\mathbb{E}_{\\mathbf{x} \\sim p_{data}} \\left[ \\log p_\\theta(\\mathbf{x}) \\right] = \\arg \\max_\\theta \\sum_{i=1}^n \\log p_\\theta(\\mathbf{x}^{(i)}) \\end{align*}\nHowever, direclty computing $p_\\theta(\\mathbf{x})$ is intractable because it requires integrating over all possible latent vectors (which are infinite): \\begin{align} \\log p_\\theta(\\mathbf{x}) = \\log \\int p_\\theta(\\mathbf{x}, \\mathbf{z}) dz = \\log \\int p_\\theta(\\mathbf{x} | \\mathbf{z}) p_\\theta(\\mathbf{z}) dz \\end{align}\nTo tackle this, we introduce an approximate posterior $q_\\phi(\\mathbf{z}|\\mathbf{x})$ using an encoder network and rewrite the intractable log-likelihood by multiplying and dividing by this distribution:\n\\begin{align} \\log p_\\theta(\\mathbf{x}) \u0026= \\log \\int p_\\theta(\\mathbf{x}, \\mathbf{z}) dz \\\\ \u0026= \\log \\int p_\\theta(\\mathbf{x}, \\mathbf{z}) \\frac{q_\\phi(\\mathbf{z}| \\mathbf{x})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} dz \\\\ \u0026= \\log \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\\\ \u0026= \\log \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\frac{p_\\theta(\\mathbf{x} | \\mathbf{z}) p_\\theta{(\\mathbf{z})}}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\\\ \\end{align}\nFrom Jensen’s inequality: $f (E[X]) \\le E[ f(X)]$, when $f$ is a convex function. Since $\\log$ is a concave function, inequality flips: $$ \\log (E[X]) \\ge E[ \\log(X)] $$\nApplying this to our equation yields the Evidence Lower Bound (ELBO): \\begin{align} \\log p_\\theta(\\mathbf{x}) \u0026\\ge \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{p_\\theta(\\mathbf{x} | \\mathbf{z}) p_\\theta{(\\mathbf{z})}}{q_\\phi(\\mathbf{z}|\\mathbf{x})} \\right] \\\\ \u0026\\ge \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log p_\\theta(\\mathbf{x} | \\mathbf{z}) \\right] - \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log \\frac{q_\\phi(\\mathbf{z}|\\mathbf{x})}{p_\\theta{(\\mathbf{z})}} \\right] \\\\ \u0026\\ge \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z}|\\mathbf{x})} \\left[ \\log p_\\theta(\\mathbf{x} | \\mathbf{z}) \\right] - \\mathbf{D}_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\; || \\; p_\\theta{(\\mathbf{z})}) \\end{align}\nThe optimization objective becomes:\n\\begin{align} \\theta^* \u0026= \\arg \\max_\\theta \\sum_{i=1}^n \\log p_\\theta(\\mathbf{x}^{(i)}) \\\\ \u0026\\ge \\arg \\max_{\\theta, \\phi} \\sum_{i=1}^n \\mathbf{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x}^{(i)})} \\left[ \\log p_\\theta(\\mathbf{x}^{(i)} | \\mathbf{z}) \\right]- \\mathbf{D}_{KL}(q_\\phi(\\mathbf{z} | \\mathbf{x}^{(i)}) \\; || \\; p_\\theta(\\mathbf{z})) \\end{align}\nThis is called the Evidence Lower Bound (ELBO) because it forms a lower bound on the log-likelihood of the data, which is the term we seek to maximize. Therefore maximizing the ELBO maximizes the log probability of our data by proxy.\nThe first term is the reconstruction loss, which measures how well the decoder can reconstruct the input data from the sampled latent variables. Maximizing this term ensures that the decoder learns to accurately reconstruct input data from the latent space.\nThe second term is the KL-divergence, which measures how close the learned posterior distribution is to the prior. Minimizing this divergence encourages the encoder to produce latent variables $\\mathbf{z}$ that that stay close to the prior distribution, which is usually a standard Gaussian $\\mathcal{N}(0, I)$.\nEnsures the samples drawn from the prior $\\mathbf{z}^{(i)} \\sim p_{\\theta^*}(\\mathbf{z})$ at test time produce realistic outputs via decoder. Helps structure the latent space so that different parts correspond to valid, coherent data. Reduces overfitting — the latent space stays grounded in the assumed distribution rather than memorizing specific datapoints. The final loss function (which we minimize) is given by: \\begin{align} L_{\\text{VAE}}(\\theta, \\phi) = - \\mathbf{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})} \\left[ \\log p_\\theta(\\mathbf{x} | \\mathbf{z}) \\right] + \\mathbf{D}_{KL} (q_\\phi(\\mathbf{z} | \\mathbf{x}) \\; || \\; p_\\theta(\\mathbf{z})) \\end{align}\nWe jointly train both the encoder and decoder networks to minimize this loss.\nWhile VAEs provide a principled probabilistic framework with interpretable latent spaces, they often suffer from blurry outputs due to their likelihood-based training. In contrast, Generative Adversarial Networks (GANs) adopt a fundamentally different approach inspired from game theory - a minimax two-player game — which leads to realistic synthetic data.\nGenerative Adversarial Networks (GANs) Generative Adversarial Networks (GANs)[3] consist of two neural networks: a Generator and a Discriminator, engaged in a dynamic game of adversarial learning.\nDiscriminator $D$: Estimates the probability that a given sample comes from the real dataset. It acts as a critic and trained to distinguish fake samples from real ones.\nGenerator $G$: Takes a random noise vector $\\mathbf{z}$ and outputs synthetic data sample. It learns to capture the real data distribution ($p_G \\approx p_{data}$), with the goal of generating samples that can fool the discriminator.\nArchitecture of a generative adversarial network. [Ref: Lil’s Log]\nHere’s a typical training step:\nSample a latent variable from the prior $\\mathbf{z} \\sim p(\\mathbf{z})$. Pass it through the Generator to obtain a fake sample $\\hat{\\mathbf{x}} = G(\\mathbf{z})$. Feed both real and fake samples to the Discriminator, which performs binary classification task (real = $1$, fake = $0$). The two networks are trained simultaneously such that the generator tries to fool the discriminator, and the discriminator tries not to be fooled. This adversarial dynamic helps both networks improve over time.\nTraining Objective Discriminator’s goal: Maximize the probability of correctly classification.\nFor real samples: $D(\\mathbf{x}) = 1$. This can be achieved by maximizing the log-likelihood of the real data samples. $$ \\max_D \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\log {D}(\\mathbf{x}) \\right] $$ For fake samples generated by the Generator: $D(G(\\mathbf{z})) = 0$. In other words, minimize the log-likelihood of the fake data. \\begin{align} \u0026 \\min_D \\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z})} \\left[ \\log D(G(\\mathbf{z})) \\right] \\; \\text{or,} \\\\ \u0026 \\max_D \\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z})} \\left[ \\log(1 - D(G({\\mathbf{z}}))) \\right] \\end{align} Generator’s goal: Generate samples that the discriminator classifies as real: $D(G(\\mathbf{z})) = 1$. This can be obtained by maximizing the log-likelihood over the synthetic data. \\begin{align} \u0026 \\max_G \\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z})} \\left[ \\log(D(G({\\mathbf{z}}))) \\right] \\; \\text{or,} \\\\ \u0026 \\min_G \\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z})} \\left[ \\log(1 - D(G({\\mathbf{z}}))) \\right] \\end{align}\nPutting it all together, we get the adversarial objective: $$ \\min_G \\max_D \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\log {D}(\\mathbf{x}) \\right] + \\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z})} \\left[ \\log(1 - D(G({\\mathbf{z}}))) \\right] $$\nWe jointly train the generator and discriminator in this minimax game using alternating gradient updates: gradient ascent to optimize the discriminator $D$, and gradient descent to optimize the generator $G$\nGlobal Optimality for discriminator Let’s start by analyzing the discriminator’s objective. When the generator $G$ is fixed, the discriminator aims to maximize the following value function:\n$$ \\max_D \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\log {D}(\\mathbf{x}) \\right] + \\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z})} \\left[ \\log(1 - D(G({\\mathbf{z}}))) \\right] $$\nBy rewriting the second expectation in terms of the model distribution $p_G$, we simplify the equation to:\n\\begin{align} \u0026 \\max_D \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\log {D}(\\mathbf{x}) \\right] + \\mathbb{E}_{\\mathbf{x} \\sim p_G} \\left[ \\log(1 - D(\\mathbf{x})) \\right] \\\\ = \u0026 \\max_D \\int p_{\\text{data}}(\\mathbf{x}) \\log {D}(\\mathbf{x}) + p_G(\\mathbf{x}) \\log(1 - D(\\mathbf{x})) \\; d\\mathbf{x} \\end{align}\nTo find the optimal discriminator we take the functional derivative with respect to $D$ and set it to zero: \\begin{align} \u0026 \\frac{p_{\\text{data}}(\\mathbf{x})}{{D}(\\mathbf{x})} - \\frac{p_G(\\mathbf{x})}{1 - D(\\mathbf{x})} = 0 \\\\ \u0026 \\Rightarrow D^*(\\mathbf{x}) = \\frac{p_{\\text{data}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_G(\\mathbf{x})} \\end{align}\nGlobal Optimality for generator Now, let’s look at the generator’s goal: to minimize the same value function, but with respect to $G$, assuming $D$ is optimal, $$ \\min_G \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\log {D}(\\mathbf{x}) \\right] + \\mathbb{E}_{\\mathbf{x} \\sim p_G} \\left[ \\log(1 - D(\\mathbf{x})) \\right] $$\nPlugging in the optimal discriminator, the objective becomes:\n\\begin{align} \u0026 \\min_G \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\log \\frac{p_{\\text{data}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_G(\\mathbf{x})} \\right] + \\mathbb{E}_{\\mathbf{x} \\sim p_G} \\left[ \\log \\left( 1 - \\frac{p_{\\text{data}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_G(\\mathbf{x})} \\right) \\right] \\\\ = \u0026 \\min_G \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\log \\frac{p_{\\text{data}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_G(\\mathbf{x})} \\right] + \\mathbb{E}_{\\mathbf{x} \\sim p_G} \\left[ \\log \\frac{p_G(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_G(\\mathbf{x})} \\right] \\end{align}\nMultipy and divide by 2, \\begin{align} \u0026 \\min_G \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\log \\frac{2 p_{\\text{data}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_G(\\mathbf{x})} \\right] + \\mathbb{E}_{\\mathbf{x} \\sim p_G} \\left[ \\log \\frac{2 p_G(\\mathbf{x})}{p_{\\text{data}} + p_G} \\right] - \\log 4 \\\\ \u0026 \\min_G \\mathbf{D}_{KL} \\left( p_{\\text{data}} || \\frac{p_{\\text{data}} + p_G}{2} \\right) + \\mathbf{D}_{KL} \\left( p_G || \\frac{p_{\\text{data}} + p_G}{2} \\right) - \\log 4 \\end{align}\nThis can be rewritten using Jensen–Shannon divergence: $$ \\mathbf{D}_{JS}(p || q) = \\frac{1}{2} \\mathbf{D}_{KL}(p || \\frac{p + q}{2}) + \\frac{1}{2} \\mathbf{D}_{KL}(q || \\frac{p + q}{2}) $$\nJS divergence is is another measure of similarity between two probability distributions, bounded by $[0, 1]$. It is zero when two distributions are equal and is symmetric. Hence, the generator minimizes: $$ \\min_G 2 \\; \\mathbf{D}_{JS}(p_{\\text{data}} || p_G) - \\log 4 $$\nSo, the global minimum occurs when the JS divergence between the real and generator distributions is zero - that is, when $p_{\\text{data}} = p_{G}$. In this case, the generator perfectly replicates the real data distribution, and the total loss converges to $-\\log 4$.\nTo summarize, the global optimum of the minimax game occurs when:\n\\begin{align} \\text{Optimal Generator: } \u0026 p_{\\text{data}} = p_{G} \\\\ \\text{Optimal Discriminator: } \u0026 D^*(\\mathbf{x}) = \\frac{p_{\\text{data}}(\\mathbf{x})}{p_{\\text{data}}(\\mathbf{x}) + p_G(\\mathbf{x})} = \\frac{1}{2} \\end{align}\nAt this point, the discriminator is completely uncertain — it cannot distinguish real from fake and essentially resorts to a coin toss!\nReferences Tutorial: Deriving the Standard Variational Autoencoder (VAE) Loss Function Lil’s log: From GAN to WGAN Lil’s log: From Autoencoder to Beta-VAE [1] Hinton \u0026 Salakhutdinov, “Reducing the Dimensionality of Data with Neural Networks”, Science 2006. [2] Kingma \u0026 Welling, “Auto-Encoding Variational Bayes”, ICLR 2014. [3] Goodfellow et al., “Generative Adversarial Networks”, NeurIPS 2014. ","wordCount":"2364","inLanguage":"en","datePublished":"2022-11-16T00:00:00Z","dateModified":"2022-11-16T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://yugajmera.github.io/posts/04-gen1/post/"},"publisher":{"@type":"Organization","name":"YA's Almanac","logo":{"@type":"ImageObject","url":"https://yugajmera.github.io/assets/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yugajmera.github.io/ accesskey=h title="YA's Almanac (Alt + H)">YA's Almanac</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yugajmera.github.io/ title=Home><span>Home</span></a></li><li><a href=https://yugajmera.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://yugajmera.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Generative Image Models Part 1: VAE and GANs</h1><div class=post-meta><span title='2022-11-16 00:00:00 +0000 UTC'>November 16, 2022</span>&nbsp;·&nbsp;12 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#autoencoders aria-label=Autoencoders>Autoencoders</a><ul><li><a href=#variantional-autoencoders-vaes aria-label="Variantional Autoencoders (VAEs)">Variantional Autoencoders (VAEs)</a><ul><li><a href=#sampling-new-images aria-label="Sampling new images">Sampling new images</a></li><li><a href=#optimization-deriving-elbo-loss aria-label="Optimization: Deriving ELBO loss">Optimization: Deriving ELBO loss</a></li></ul></li></ul></li><li><a href=#generative-adversarial-networks-gans aria-label="Generative Adversarial Networks (GANs)">Generative Adversarial Networks (GANs)</a><ul><ul><li><a href=#training-objective aria-label="Training Objective">Training Objective</a></li><li><a href=#global-optimality-for-discriminator aria-label="Global Optimality for discriminator">Global Optimality for discriminator</a></li><li><a href=#global-optimality-for-generator aria-label="Global Optimality for generator">Global Optimality for generator</a></li></ul></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>Machine learning can broadly be categorized into three types:</p><ol><li><p><strong>Supervised Learning</strong>: The model is trained to learn a mapping from input data $\mathbf{x}$ to output labels $\mathbf{y}$. This includes tasks like image classification, object detection, and image captioning. However, supervised learning depends heavily on large, labeled datasets—typically created through extensive human annotation.</p></li><li><p><strong>Unsupervised Learning</strong>: The model is trained to discover patterns or underlying structures within unlabelled data. A key objective here is to learn the data distribution $p_{data}(\mathbf{x})$, enabling us to sample from it to generate new data. With the abundance of raw text, images, and other unlabelled data on the internet, unsupervised learning has become a significant area of machine learning.</p></li><li><p><strong>Reinforcement Learning</strong>: The model learns to take actions in an environment to maximize cumulative rewards. Learning is driven by trial and error, guided by feedback in the form of rewards or penalties.</p></li></ol><p>My previous blog posts have primarily focused on supervised learning for images, such as classification tasks. In this new series, we’ll shift our focus to unsupervised learning for images, specifically generative image models.</p><p>In Part 1, we’ll explore Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs)—two foundational approaches in generative modeling. In upcoming posts, we&rsquo;ll also discuss more advanced techniques, such as Diffusion Models, which have recently gained widespread attention.</p><h2 id=autoencoders>Autoencoders<a hidden class=anchor aria-hidden=true href=#autoencoders>#</a></h2><p>An autoencoder <a href=#references>[1]</a> is a type of neural network with a bottleneck structure—it extracts features $\mathbf{z}$ from an input image $\mathbf{x}$ and uses them to reconstruct the original data $\mathbf{x}&rsquo;$, effectively learning an identity mapping. In doing so, it learns a more efficient, compressed representation of the data.</p><p>It consists of two main components:</p><ul><li><strong>Encoder</strong>$g$ : A network that compresses the input into a latent-space representation (of a lower dimension).</li><li><strong>Decoder</strong> $f$: A network that reconstructs the original input from this latent representation.</li></ul><figure class=align-center><img loading=lazy src=../autoencoder.png#center alt="Illustration of autoencoder model architecture. [Ref: Lil&amp;rsquo;s Log]"><figcaption><p>Illustration of autoencoder model architecture. [Ref: <a href=https://lilianweng.github.io/posts/2018-08-12-vae/>Lil&rsquo;s Log</a>]</p></figcaption></figure><p>The encoder network learns to perform dimensionality reduction, similar to how Principal Component Analysis (PCA) works. Through training, the autoencoder captures the most salient features in a compact latent space, obtaining
$\mathbf{z} = g_\phi(\mathbf{x})$.</p><p>To train the autoencoder, we minimize the reconstruction error between the original input and the output produced by the decoder. The loss objective is a simple L2-loss between the input and the reconstructed input represented as $\mathbf{x}&rsquo; = f_\theta(g_\phi(\mathbf{x}))$.</p><p>\begin{equation*}
L_\text{AE}(\theta, \phi) = || \mathbf{x} - f_\theta(g_\phi(\mathbf{x})) ||^2_2
\end{equation*}</p><p>The parameters $(\theta, \phi)$ are learned jointly to reconstruct the input data, aiming for the output to be as close as possible to the original input, i.e., $\mathbf{x} \approx f_\theta(g_\phi(\mathbf{x}))$.</p><p>Below is a basic autoencoder implementation using fully connected layers, designed to reconstruct MNIST digit images.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Autoencoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>input_dim</span><span class=o>=</span><span class=mi>784</span><span class=p>,</span> <span class=n>latent_dim</span><span class=o>=</span><span class=mi>64</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>Autoencoder</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=c1># Encoder: compress input</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=mi>256</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=n>latent_dim</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Decoder: reconstruct input</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>latent_dim</span><span class=p>,</span> <span class=mi>256</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=n>input_dim</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Sigmoid</span><span class=p>(),</span>  <span class=c1># Since image pixels are between 0 and 1</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>z</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x_recon</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x_recon</span>
</span></span></code></pre></div><p>After training, we typically discard the decoder and use the encoder alone to extract meaningful features for downstream tasks.</p><p>Since autoencoders are not probabilistic models, they do not explicitly learn the underlying data distribution, which means we cannot sample new images from them. Variational Autoencoders (VAEs), a probabilistic extension of autoencoders, were introduced to address this limitation.</p><h3 id=variantional-autoencoders-vaes>Variantional Autoencoders (VAEs)<a hidden class=anchor aria-hidden=true href=#variantional-autoencoders-vaes>#</a></h3><p>A Variational Autoencoder (VAE) <a href=#references>[2]</a> improves upon standard autoencoders by introducing a probabilistic approach to the latent space. Instead of mapping an input image $\mathbf{x}$ to a single fixed latent vector $\mathbf{z}$, VAEs map it to a <em>distribution</em> over latent variables.</p><p>More specifically, a VAE defines three key components:</p><ul><li><strong>Prior</strong> $p_\theta(\mathbf{z})$: A predefined distribution over the latent space, usually chosen to be a standard multivariate Gaussian $\mathcal{N}(0, I)$.</li><li><strong>Likelihood</strong> $p_\theta(\mathbf{x}|\mathbf{z})$: The probability of generating an image $\mathbf{x}$ from a given latent code $\mathbf{z}$. This is modeled by the decoder network.</li><li><strong>Posterior</strong> $p_\theta(\mathbf{z}|\mathbf{x})$: The true distribution of latent codes given the input data, which we cannot compute directly, so we approximate it using another neural network — the encoder — denoted as $q_\phi(\mathbf{z}|\mathbf{x})$.</li></ul><figure class=align-center><img loading=lazy src=../vae-gaussian.png#center alt="Illustration of VAE model architecture. [Ref: Lil&amp;rsquo;s Log]"><figcaption><p>Illustration of VAE model architecture. [Ref: <a href=https://lilianweng.github.io/posts/2018-08-12-vae/>Lil&rsquo;s Log</a>]</p></figcaption></figure><p>Here’s what a typical training step looks like:</p><ol><li><p>Encode an input image $\mathbf{x}^{(i)}$ using the encoder to get the approximate posterior distribution
$q_\phi(\mathbf{z}|\mathbf{x}^{(i)}) \approx p_\theta(\mathbf{z} | \mathbf{x}^{(i)})$.</p></li><li><p>Sample a latent vector $\mathbf{z}^{(i)} \sim q_\phi(\mathbf{z}|\mathbf{x}^{(i)})$ from this distribution.</p></li><li><p>Feed this latent vector into the decoder to get the likelihood distribution $p_\theta(\mathbf{x}|\mathbf{z}^{(i)})$.</p></li><li><p>Reconstruct the input by sampling $\mathbf{x&rsquo;}^{(i)} \sim p_\theta(\mathbf{x} | \mathbf{z}^{(i)})$ from this distribution.</p></li></ol><p>Since neural networks cannot output a full probability distribution, the encoder and decoder instead predicts the mean and diagonal covariance of a Gaussian distribution:
\begin{align}
q_\phi (\mathbf{z}|\mathbf{x}) &= \mathcal{N}(\mu_{\mathbf{z} | \mathbf{x}}, \Sigma_{\mathbf{z} | \mathbf{x}})
\\
p_\theta (\mathbf{x}|\mathbf{z}) &= \mathcal{N}(\mu_{\mathbf{x} | \mathbf{z}}, \Sigma_{\mathbf{x} | \mathbf{z}})
\end{align}</p><p><em>Note</em>: We assume the pixels in the generated image are conditionally independent given the latent vector. This assumption leads to VAEs often producing blurry images.</p><p>Sampling is a stochastic process and therefore non-differentiable, which prevents direct backpropagation of gradients. To make training possible, the <strong>reparameterization trick</strong> is used, such that:</p><p>\begin{align*}
\mathbf{z} &\sim q_\phi(\mathbf{z} | \mathbf{x}^{(i)}) = \mathcal{N}(\mathbf{z}; \boldsymbol{\mu}^{(i)}, \boldsymbol{\sigma}^{2(i)} I) \\
\mathbf{z} &= \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon} \text{, where } \boldsymbol{\epsilon} \sim \mathcal{N}(0, I)
\end{align*}</p><p>where $\odot$ refers to an element-wise product.</p><h4 id=sampling-new-images>Sampling new images<a hidden class=anchor aria-hidden=true href=#sampling-new-images>#</a></h4><p>If we somehow knew the true parameters $\theta^*$ of the model, we could generate a new data sample $\mathbf{x}&rsquo;$ by:</p><ol><li>Sampling a latent vector $\mathbf{z}^{(i)} \sim p_{\theta^*}(\mathbf{z})$. Remember that the prior is standard multivariate Gaussian $\mathcal{N}(0, I)$.</li><li>Feeding $\mathbf{z}^{(i)}$ into the decoder to get $p_\theta (\mathbf{x}|\mathbf{z}) = \mathcal{N}(\mu_{\mathbf{x} | \mathbf{z}}, \Sigma_{\mathbf{x} | \mathbf{z}})$</li><li>Sampling a new image $\mathbf{x&rsquo;}^{(i)}$ from this distribution.</li></ol><h4 id=optimization-deriving-elbo-loss>Optimization: Deriving ELBO loss<a hidden class=anchor aria-hidden=true href=#optimization-deriving-elbo-loss>#</a></h4><p>To learn the optimal parameters $\theta^*$, we use maximum likelihood estimation (MLE): maximize the log likelihood of generating real data samples:</p><p>\begin{align*}
\theta^* &= \arg \max_\theta \mathbb{E}_{\mathbf{x} \sim p_{data}} \left[ \log p_\theta(\mathbf{x}) \right] = \arg \max_\theta \sum_{i=1}^n \log p_\theta(\mathbf{x}^{(i)})
\end{align*}</p><p>However, direclty computing $p_\theta(\mathbf{x})$ is intractable because it requires integrating over all possible latent vectors (which are infinite):
\begin{align}
\log p_\theta(\mathbf{x}) = \log \int p_\theta(\mathbf{x}, \mathbf{z}) dz
= \log \int p_\theta(\mathbf{x} | \mathbf{z}) p_\theta(\mathbf{z}) dz
\end{align}</p><p>To tackle this, we introduce an approximate posterior $q_\phi(\mathbf{z}|\mathbf{x})$ using an encoder network and rewrite the intractable log-likelihood by multiplying and dividing by this distribution:</p><p>\begin{align}
\log p_\theta(\mathbf{x}) &= \log \int p_\theta(\mathbf{x}, \mathbf{z}) dz \\
&= \log \int p_\theta(\mathbf{x}, \mathbf{z}) \frac{q_\phi(\mathbf{z}| \mathbf{x})}{q_\phi(\mathbf{z}|\mathbf{x})} dz \\
&= \log \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} \left[ \frac{p_\theta(\mathbf{x}, \mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})} \right] \\
&= \log \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} \left[ \frac{p_\theta(\mathbf{x} | \mathbf{z}) p_\theta{(\mathbf{z})}}{q_\phi(\mathbf{z}|\mathbf{x})} \right] \\
\end{align}</p><p>From <a href=https://en.wikipedia.org/wiki/Jensen%27s_inequality>Jensen&rsquo;s inequality</a>: $f (E[X]) \le E[ f(X)]$, when $f$ is a convex function. Since $\log$ is a concave function, inequality flips:
$$
\log (E[X]) \ge E[ \log(X)]
$$</p><p>Applying this to our equation yields the Evidence Lower Bound (ELBO):
\begin{align}
\log p_\theta(\mathbf{x}) &\ge \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{p_\theta(\mathbf{x} | \mathbf{z}) p_\theta{(\mathbf{z})}}{q_\phi(\mathbf{z}|\mathbf{x})} \right] \\
&\ge \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log p_\theta(\mathbf{x} | \mathbf{z}) \right] - \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log \frac{q_\phi(\mathbf{z}|\mathbf{x})}{p_\theta{(\mathbf{z})}} \right] \\
&\ge \mathbb{E}_{\mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} \left[ \log p_\theta(\mathbf{x} | \mathbf{z}) \right] - \mathbf{D}_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) \; || \; p_\theta{(\mathbf{z})})
\end{align}</p><p>The optimization objective becomes:</p><p>\begin{align}
\theta^* &= \arg \max_\theta \sum_{i=1}^n \log p_\theta(\mathbf{x}^{(i)}) \\
&\ge \arg \max_{\theta, \phi} \sum_{i=1}^n \mathbf{E}_{\mathbf{z} \sim q_\phi(\mathbf{z} | \mathbf{x}^{(i)})} \left[ \log p_\theta(\mathbf{x}^{(i)} | \mathbf{z}) \right]- \mathbf{D}_{KL}(q_\phi(\mathbf{z} | \mathbf{x}^{(i)}) \; || \; p_\theta(\mathbf{z}))
\end{align}</p><p>This is called the Evidence Lower Bound (ELBO) because it forms a lower bound on the log-likelihood of the data, which is the term we seek to maximize. Therefore maximizing the ELBO maximizes the log probability of our data by proxy.</p><p>The first term is the <em>reconstruction loss</em>, which measures how well the decoder can reconstruct the input data from the sampled latent variables. Maximizing this term ensures that the decoder learns to accurately reconstruct input data from the latent space.</p><p>The second term is the <em>KL-divergence</em>, which measures how close the learned posterior distribution is to the prior. Minimizing this divergence encourages the encoder to produce latent variables $\mathbf{z}$ that that stay close to the prior distribution, which is usually a standard Gaussian $\mathcal{N}(0, I)$.</p><ul><li>Ensures the samples drawn from the prior $\mathbf{z}^{(i)} \sim p_{\theta^*}(\mathbf{z})$ at test time produce realistic outputs via decoder.</li><li>Helps structure the latent space so that different parts correspond to valid, coherent data.</li><li>Reduces overfitting — the latent space stays grounded in the assumed distribution rather than memorizing specific datapoints.</li></ul><p>The final loss function (which we minimize) is given by:
\begin{align}
L_{\text{VAE}}(\theta, \phi) = - \mathbf{E}_{\mathbf{z} \sim q_\phi(\mathbf{z} | \mathbf{x})} \left[ \log p_\theta(\mathbf{x} | \mathbf{z}) \right] + \mathbf{D}_{KL} (q_\phi(\mathbf{z} | \mathbf{x}) \; || \; p_\theta(\mathbf{z}))
\end{align}</p><p>We jointly train both the encoder and decoder networks to minimize this loss.</p><p>While VAEs provide a principled probabilistic framework with interpretable latent spaces, they often suffer from blurry outputs due to their likelihood-based training. In contrast, Generative Adversarial Networks (GANs) adopt a fundamentally different approach inspired from game theory - a minimax two-player game — which leads to realistic synthetic data.</p><h2 id=generative-adversarial-networks-gans>Generative Adversarial Networks (GANs)<a hidden class=anchor aria-hidden=true href=#generative-adversarial-networks-gans>#</a></h2><p>Generative Adversarial Networks (GANs)<a href=#references>[3]</a> consist of two neural networks: a Generator and a Discriminator, engaged in a dynamic game of adversarial learning.</p><ul><li><p><strong>Discriminator</strong> $D$: Estimates the probability that a given sample comes from the real dataset. It acts as a critic and trained to distinguish fake samples from real ones.</p></li><li><p><strong>Generator</strong> $G$: Takes a random noise vector $\mathbf{z}$ and outputs synthetic data sample. It learns to capture the real data distribution ($p_G \approx p_{data}$), with the goal of generating samples that can fool the discriminator.</p></li></ul><figure class=align-center><img loading=lazy src=../GANs.png#center alt="Architecture of a generative adversarial network. [Ref: Lil&amp;rsquo;s Log]"><figcaption><p>Architecture of a generative adversarial network. [Ref: <a href=https://lilianweng.github.io/posts/2017-08-20-gan/>Lil&rsquo;s Log</a>]</p></figcaption></figure><p>Here’s a typical training step:</p><ol><li>Sample a latent variable from the prior $\mathbf{z} \sim p(\mathbf{z})$.</li><li>Pass it through the Generator to obtain a fake sample $\hat{\mathbf{x}} = G(\mathbf{z})$.</li><li>Feed both real and fake samples to the Discriminator, which performs binary classification task (real = $1$, fake = $0$).</li></ol><p>The two networks are trained simultaneously such that the generator tries to fool the discriminator, and the discriminator tries not to be fooled. This adversarial dynamic helps both networks improve over time.</p><h4 id=training-objective>Training Objective<a hidden class=anchor aria-hidden=true href=#training-objective>#</a></h4><ol><li><p>Discriminator’s goal: Maximize the probability of correctly classification.</p><ul><li>For real samples: $D(\mathbf{x}) = 1$. This can be achieved by maximizing the log-likelihood of the real data samples.
$$
\max_D \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} \left[ \log {D}(\mathbf{x}) \right]
$$</li><li>For fake samples generated by the Generator: $D(G(\mathbf{z})) = 0$. In other words, minimize the log-likelihood of the fake data.
\begin{align}
& \min_D \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} \left[ \log D(G(\mathbf{z})) \right] \; \text{or,} \\
& \max_D \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} \left[ \log(1 - D(G({\mathbf{z}}))) \right]
\end{align}</li></ul></li><li><p>Generator&rsquo;s goal: Generate samples that the discriminator classifies as real: $D(G(\mathbf{z})) = 1$. This can be obtained by maximizing the log-likelihood over the synthetic data.
\begin{align}
& \max_G \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} \left[ \log(D(G({\mathbf{z}}))) \right] \; \text{or,} \\
& \min_G \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} \left[ \log(1 - D(G({\mathbf{z}}))) \right]
\end{align}</p></li></ol><p>Putting it all together, we get the adversarial objective:
$$
\min_G \max_D \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} \left[ \log {D}(\mathbf{x}) \right] + \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} \left[ \log(1 - D(G({\mathbf{z}}))) \right]
$$</p><p>We jointly train the generator and discriminator in this minimax game using alternating gradient updates: gradient ascent to optimize the discriminator $D$, and gradient descent to optimize the generator $G$</p><h4 id=global-optimality-for-discriminator>Global Optimality for discriminator<a hidden class=anchor aria-hidden=true href=#global-optimality-for-discriminator>#</a></h4><p>Let’s start by analyzing the discriminator’s objective. When the generator $G$ is fixed, the discriminator aims to maximize the following value function:</p><p>$$
\max_D \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} \left[ \log {D}(\mathbf{x}) \right] + \mathbb{E}_{\mathbf{z} \sim p(\mathbf{z})} \left[ \log(1 - D(G({\mathbf{z}}))) \right]
$$</p><p>By rewriting the second expectation in terms of the model distribution $p_G$, we simplify the equation to:</p><p>\begin{align}
& \max_D \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} \left[ \log {D}(\mathbf{x}) \right] + \mathbb{E}_{\mathbf{x} \sim p_G} \left[ \log(1 - D(\mathbf{x})) \right] \\
= & \max_D \int p_{\text{data}}(\mathbf{x}) \log {D}(\mathbf{x}) + p_G(\mathbf{x}) \log(1 - D(\mathbf{x})) \; d\mathbf{x}
\end{align}</p><p>To find the optimal discriminator we take the functional derivative with respect to $D$ and set it to zero:
\begin{align}
& \frac{p_{\text{data}}(\mathbf{x})}{{D}(\mathbf{x})} - \frac{p_G(\mathbf{x})}{1 - D(\mathbf{x})} = 0 \\
& \Rightarrow D^*(\mathbf{x}) = \frac{p_{\text{data}}(\mathbf{x})}{p_{\text{data}}(\mathbf{x}) + p_G(\mathbf{x})}
\end{align}</p><h4 id=global-optimality-for-generator>Global Optimality for generator<a hidden class=anchor aria-hidden=true href=#global-optimality-for-generator>#</a></h4><p>Now, let’s look at the generator’s goal: to minimize the same value function, but with respect to $G$, assuming $D$ is optimal,
$$
\min_G \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} \left[ \log {D}(\mathbf{x}) \right] + \mathbb{E}_{\mathbf{x} \sim p_G} \left[ \log(1 - D(\mathbf{x})) \right]
$$</p><p>Plugging in the optimal discriminator, the objective becomes:</p><p>\begin{align}
& \min_G \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} \left[ \log \frac{p_{\text{data}}(\mathbf{x})}{p_{\text{data}}(\mathbf{x}) + p_G(\mathbf{x})} \right] + \mathbb{E}_{\mathbf{x} \sim p_G} \left[ \log \left( 1 - \frac{p_{\text{data}}(\mathbf{x})}{p_{\text{data}}(\mathbf{x}) + p_G(\mathbf{x})} \right) \right] \\
= & \min_G \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} \left[ \log \frac{p_{\text{data}}(\mathbf{x})}{p_{\text{data}}(\mathbf{x}) + p_G(\mathbf{x})} \right] + \mathbb{E}_{\mathbf{x} \sim p_G} \left[ \log \frac{p_G(\mathbf{x})}{p_{\text{data}}(\mathbf{x}) + p_G(\mathbf{x})} \right]
\end{align}</p><p>Multipy and divide by 2,
\begin{align}
& \min_G \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} \left[ \log \frac{2 p_{\text{data}}(\mathbf{x})}{p_{\text{data}}(\mathbf{x}) + p_G(\mathbf{x})} \right] + \mathbb{E}_{\mathbf{x} \sim p_G} \left[ \log \frac{2 p_G(\mathbf{x})}{p_{\text{data}} + p_G} \right] - \log 4 \\
& \min_G \mathbf{D}_{KL} \left( p_{\text{data}} || \frac{p_{\text{data}} + p_G}{2} \right) + \mathbf{D}_{KL} \left( p_G || \frac{p_{\text{data}} + p_G}{2} \right) - \log 4
\end{align}</p><p>This can be rewritten using <a href=https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence>Jensen–Shannon divergence</a>:
$$
\mathbf{D}_{JS}(p || q) = \frac{1}{2} \mathbf{D}_{KL}(p || \frac{p + q}{2}) + \frac{1}{2} \mathbf{D}_{KL}(q || \frac{p + q}{2})
$$</p><p>JS divergence is is another measure of similarity between two probability distributions, bounded by $[0, 1]$. It is zero when two distributions are equal and is symmetric. Hence, the generator minimizes:
$$
\min_G 2 \; \mathbf{D}_{JS}(p_{\text{data}} || p_G) - \log 4
$$</p><p>So, the global minimum occurs when the JS divergence between the real and generator distributions is zero - that is, when $p_{\text{data}} = p_{G}$. In this case, the generator perfectly replicates the real data distribution, and the total loss converges to $-\log 4$.</p><p>To summarize, the global optimum of the minimax game occurs when:</p><p>\begin{align}
\text{Optimal Generator: } & p_{\text{data}} = p_{G} \\
\text{Optimal Discriminator: } & D^*(\mathbf{x}) = \frac{p_{\text{data}}(\mathbf{x})}{p_{\text{data}}(\mathbf{x}) + p_G(\mathbf{x})} = \frac{1}{2}
\end{align}</p><p>At this point, the discriminator is completely uncertain — it cannot distinguish real from fake and essentially resorts to a coin toss!</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li><a href=https://arxiv.org/pdf/1907.08956>Tutorial: Deriving the Standard Variational Autoencoder (VAE) Loss Function</a></li><li><a href=https://lilianweng.github.io/posts/2017-08-20-gan/>Lil&rsquo;s log: From GAN to WGAN</a></li><li><a href=https://lilianweng.github.io/posts/2018-08-12-vae/>Lil&rsquo;s log: From Autoencoder to Beta-VAE</a></li><li>[1] Hinton & Salakhutdinov, &ldquo;<a href=https://www.science.org/doi/10.1126/science.1127647>Reducing the Dimensionality of Data with Neural Networks</a>&rdquo;, Science 2006.</li><li>[2] Kingma & Welling, &ldquo;<a href=https://arxiv.org/abs/1312.6114>Auto-Encoding Variational Bayes</a>&rdquo;, ICLR 2014.</li><li>[3] Goodfellow et al., &ldquo;<a href=https://arxiv.org/abs/1406.2661>Generative Adversarial Networks</a>&rdquo;, NeurIPS 2014.</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://yugajmera.github.io/posts/05-attention/post/><span class=title>« Prev</span><br><span>Sequence Modeling with Recurrent Neural Networks and Attention</span>
</a><a class=next href=https://yugajmera.github.io/posts/03-imagenet/post/><span class=title>Next »</span><br><span>ImageNet Challenge: The Olympics of Deep Learning</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://yugajmera.github.io/>YA's Almanac</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>