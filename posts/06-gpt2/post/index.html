<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GPT Series Part 3: Building GPT-2 & Sampling Techniques | YA's Almanac</title>
<meta name=keywords content><meta name=description content="Building on our previous exploration of GPT-1, let&rsquo;s now modify its architecture to recreate the GPT-2 [1] small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.
One major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning."><meta name=author content><link rel=canonical href=https://yugajmera.github.io/posts/06-gpt2/post/><link crossorigin=anonymous href=https://yugajmera.github.io/assets/css/stylesheet.74991b51f7611c2303d0b5649703d675530589621885eb78236e099c22aa93a5.css integrity="sha256-dJkbUfdhHCMD0LVklwPWdVMFiWIYhet4I24JnCKqk6U=" rel="preload stylesheet" as=style><link rel=icon href=https://yugajmera.github.io/assets/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yugajmera.github.io/assets/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yugajmera.github.io/assets/favicon-32x32.png><link rel=apple-touch-icon href=https://yugajmera.github.io/assets/apple-touch-icon.png><link rel=mask-icon href=https://yugajmera.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yugajmera.github.io/posts/06-gpt2/post/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>mjx-container[display=true]{margin:1.5em 0!important}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S759YBMKJE",{anonymize_ip:!1})}</script><meta property="og:title" content="GPT Series Part 3: Building GPT-2 & Sampling Techniques"><meta property="og:description" content="Building on our previous exploration of GPT-1, let&rsquo;s now modify its architecture to recreate the GPT-2 [1] small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.
One major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning."><meta property="og:type" content="article"><meta property="og:url" content="https://yugajmera.github.io/posts/06-gpt2/post/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-01T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-01T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="GPT Series Part 3: Building GPT-2 & Sampling Techniques"><meta name=twitter:description content="Building on our previous exploration of GPT-1, let&rsquo;s now modify its architecture to recreate the GPT-2 [1] small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.
One major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yugajmera.github.io/posts/"},{"@type":"ListItem","position":2,"name":"GPT Series Part 3: Building GPT-2 \u0026 Sampling Techniques","item":"https://yugajmera.github.io/posts/06-gpt2/post/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GPT Series Part 3: Building GPT-2 \u0026 Sampling Techniques","name":"GPT Series Part 3: Building GPT-2 \u0026 Sampling Techniques","description":"Building on our previous exploration of GPT-1, let\u0026rsquo;s now modify its architecture to recreate the GPT-2 [1] small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.\nOne major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning.","keywords":[],"articleBody":"Building on our previous exploration of GPT-1, let’s now modify its architecture to recreate the GPT-2 [1] small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.\nOne major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning. Additionally, GPT-2 is an excellent choice for learning how to implement LLMs, as it can run on a single GPU or even a laptop—unlike GPT-3 [2], which requires GPU clusters for training and inference.\nGPT-2 was introduced in 2019, which, by the fast-paced standards of deep learning and LLM development, is considered a long time ago. However, more recent architectures, such as Meta’s LLaMA models, are still built on the same foundational concepts with only minor modifications. Hence, understanding GPT-2 remains as relevant as ever.\nThe official model code is available here: https://github.com/openai/gpt-2/blob/master/src/model.py\nArchitecture Let’s start with our imports:\n# Import functions import torch import torch.nn as nn device = 'cuda' if torch.cuda.is_available() else 'cpu' print(\"Using\", device) Before implementing the model, let’s take a look at GPT-2’s pre-trained weights from Hugging Face’s transformers library. This will help us understand the naming conventions used in the architecture:\nfrom transformers import GPT2LMHeadModel # Load pre-trained GPT-2 (small) and retrieve its state dictionary model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device) # 124M param model model_hf_params = model_hf.state_dict() for name, param in model_hf_params.items(): print(name, param.shape) transformer.wte.weight torch.Size([50257, 768]) transformer.wpe.weight torch.Size([1024, 768]) transformer.h.0.ln_1.weight torch.Size([768]) transformer.h.0.ln_1.bias torch.Size([768]) transformer.h.0.attn.c_attn.weight torch.Size([768, 2304]) transformer.h.0.attn.c_attn.bias torch.Size([2304]) transformer.h.0.attn.c_proj.weight torch.Size([768, 768]) transformer.h.0.attn.c_proj.bias torch.Size([768]) transformer.h.0.ln_2.weight torch.Size([768]) transformer.h.0.ln_2.bias torch.Size([768]) transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072]) transformer.h.0.mlp.c_fc.bias torch.Size([3072]) transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768]) transformer.h.0.mlp.c_proj.bias torch.Size([768]) ... transformer.ln_f.weight torch.Size([768]) transformer.ln_f.bias torch.Size([768]) lm_head.weight torch.Size([50257, 768]) The output lists the layer names along with their corresponding tensor shapes, giving us insights into GPT-2’s layer structure:\nThe model is encapsulated under the namespace transformer. wte represents the word token embedding weights. wpe represents the word position embedding weights. h is a list of hidden decoder blocks. LayerNorms are denoted as ln_1 and ln_2 within each decoder block, with the final LayerNorm represented as ln_f. The scale and shift parameters are renamed to weight and bias, respectively. Our W_qkv weight matrix is renamed to c_attn (short for causal attention), while W_o is renamed to c_proj. The causal attention layer also includes bias tensors, meaning we will set qkv_bias=True. lm_head represents the final linear layer (language modeling head) and does not include a bias term. To ensure compatibility when loading OpenAI’s pre-trained weights, we will follow this naming convention while defining the subcomponents of our model. Let’s implement these next! 🚀\nclass MultiHeadCausalAttention(nn.Module): def __init__(self, d_in, d_out, context_length, attn_pdrop, num_heads, qkv_bias=False): super().__init__() self.c_attn = nn.Linear(d_in, 3 * d_out, bias=qkv_bias) self.c_proj = nn.Linear(d_out, d_out) self.attn_dropout = nn.Dropout(attn_pdrop) self.d_h = d_out // num_heads # head dimension self.num_heads = num_heads self.register_buffer(\"masked\", torch.tril(torch.ones(context_length, context_length)).view(1, 1, context_length, context_length)) def forward(self, x): # Input vectors x - (B, N, d_in) B, N, d_in = x.shape # Obtain keys, values and queries in one go - (B, N, d_out) qkv = self.c_attn(x) queries, keys, values = qkv.chunk(3, dim=-1) # Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h) k = keys.view(B, N, self.num_heads, self.d_h).transpose(1, 2) v = values.view(B, N, self.num_heads, self.d_h).transpose(1, 2) q = queries.view(B, N, self.num_heads, self.d_h).transpose(1, 2) # Apply scaled dot-product attention with causal mask on each head attn_scores = (q @ k.transpose(2, 3)) * k.shape[-1]**-0.5 attn_scores = attn_scores.masked_fill(self.masked[:, :, :N, :N] == 0, float('-inf')) attn_weights = torch.softmax(attn_scores, dim=-1) attn_weights = self.attn_dropout(attn_weights) context_vec = attn_weights @ v # Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out) out = context_vec.transpose(1, 2).contiguous().view(B, N, -1) out = self.c_proj(out) return out class LayerNorm(nn.Module): def __init__(self, emb_dim): super().__init__() self.eps = 1e-5 self.weight = nn.Parameter(torch.ones(emb_dim)) self.bias = nn.Parameter(torch.zeros(emb_dim)) def forward(self, x): mean = x.mean(dim=-1, keepdim=True) var = x.var(dim=-1, keepdim=True, unbiased=False) # Used biased variance estimation (without Bessel's correction) norm_x = (x - mean) / torch.sqrt(var + self.eps) return self.weight * norm_x + self.bias class GELU(nn.Module): def __init__(self): super().__init__() def forward(self, x): return 0.5 * x * (1 + torch.tanh( torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)) )) class MLP(nn.Module): def __init__(self, emb_dim): super().__init__() self.c_fc = nn.Linear(emb_dim, 4 * emb_dim) self.c_proj = nn.Linear(4 * emb_dim, emb_dim) self.gelu = GELU() def forward(self, x): x = self.gelu(self.c_fc(x)) x = self.c_proj(x) return x Here, I’ve taken our GPT-1 implementation and adapted it to match GPT-2’s naming conventions.\nA key detail is the masked variable, which is defined in register_buffer. This automatically adds it to the model’s state dictionary, meaning you’ll find entries like transformer.h.0.attn.masked. However, since it’s a non-trainable tensor, it does not exist in Hugging Face’s state dictionary. So, when loading pre-trained weights, we’ll simply ignore it.\nThe GPT-2 decoder block closely follows the GPT-1 model we implemented earlier, with a few key modifications. Let’s break them down.\nPre-Norm Transformer The original Transformer (as well as our GPT-1 implementation) used Post-Norm residual connections, where LayerNorm is applied after the sublayer and residual addition.\nPre-Norm vs Post-Norm Transformer Layers\nIn this setup, the raw output of each sublayer (such as the self-attention or feedforward network) is directly added to the residual before normalization. This can lead to instability as the network depth increases because the residual sum $[x_l + \\text{Sublayer}(x_l)]$ accumulates unnormalized activations over layers:\nIf $\\text{Sublayer}(x_l)$ outputs are very small values, the residual connection mostly carries $x_l$ forward without significant updates, leading to vanishing gradients.\nIf $\\text{Sublayer}(x_l)$ has large activations, they keep adding up over layers. Although LayerNorm tries to regulate this after the accumulation, the activations have already influenced the residual pathway, potentially leading to exploding gradients.\nEssentially, LayerNorm in Post-Norm setups tries to fix activations at the last moment—after instability has already propagated.\nTo address this, Xiong et al. [3] proposed Pre-Norm residual units, where LayerNorm is applied before the sublayer. This ensures each sublayer receives well-scaled inputs, improving gradient flow and training stability.\nMost modern Large Language Models, including GPT-2, have adopted this Pre-Norm approach. The key changes are:\nLayer Normalization is moved to the input of each sub-block, ensuring that each sublayer receives normalized inputs.\nAn additional Layer Normalization is added after the final self-attention block, further improving stability.\nWith these changes, the GPT-2 architecture now looks like this: GPT-2 architecture\nclass DecoderBlock(nn.Module): def __init__(self, cfg): super().__init__() self.ln_1 = LayerNorm(emb_dim=cfg['emb_dim']) self.attn = MultiHeadCausalAttention(d_in=cfg['emb_dim'], d_out=cfg['emb_dim'], context_length=cfg['context_length'], attn_pdrop=cfg['attn_pdrop'], num_heads=cfg['n_heads'], qkv_bias=cfg['qkv_bias']) self.ln_2 = LayerNorm(emb_dim=cfg['emb_dim']) self.mlp = MLP(emb_dim=cfg['emb_dim']) self.resid_dropout = nn.Dropout(cfg['resid_pdrop']) def forward(self, x): x = x + self.resid_dropout(self.attn(self.ln_1(x))) x = x + self.resid_dropout(self.mlp(self.ln_2(x))) return x Defining the GPT-2 model And our GPT-2 model will be coded as:\nclass GPT2(nn.Module): def __init__(self, cfg): super().__init__() self.transformer = nn.ModuleDict(dict( wte = nn.Embedding(cfg['vocab_size'], cfg['emb_dim']), # Token Embeddings wpe = nn.Embedding(cfg['context_length'], cfg['emb_dim']), # Position Encoding embd_dropout = nn.Dropout(cfg['embd_pdrop']), # Embedding dropout h = nn.Sequential(*[DecoderBlock(cfg) for _ in range(cfg['n_layers'])]), # Multiple Decoder blocks ln_f = LayerNorm(cfg['emb_dim']), # Final layernorm )) self.lm_head = nn.Linear(cfg['emb_dim'], cfg['vocab_size'], bias=False) # Language modelling head def forward(self, x): B, N = x.size() token_emb = self.transformer.wte(x) # (B, N, D) pos_emb = self.transformer.wpe(torch.arange(N, device=device)) # (N, D) x = token_emb + pos_emb # (B, N, D) x = self.transformer.h(x) x = self.transformer.ln_f(x) logits = self.lm_head(x) # (B, N, vocab_size) return logits The hyperparameters of our model are defined in a Python dictionary, cfg, which is passed when instantiating the model.\n# Define configuration dictionary GPT_CONFIG_124M = { \"vocab_size\": 50257, # 50,000 BPE merges + 256 byte tokens + 1 \u003c|endoftext|\u003e token \"context_length\": 1024, # Maximum sequence length \"emb_dim\": 768, # Embedding dimension \"n_heads\": 12, # Number of attention heads \"n_layers\": 12, # Number of transformer blocks \"attn_pdrop\": 0.1, # Dropout probability for attention dropout \"embd_pdrop\": 0.1, # Dropout probability for embeddings dropout \"resid_pdrop\": 0.1, # Dropout probability for residual dropout \"qkv_bias\": True # Whether to use bias in QKV layer } model = GPT2(GPT_CONFIG_124M).to(device) # Print the number of parameters in the model print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters') 163.037184 M parameters Oops! Our model has 163M parameters, even though we aimed to replicate the 124M parameter version. What’s going on?\nActually, nothing is wrong! The discrepancy arises due to the weight tying scheme used in the GPT-2 model. Let’s dive into it in more detail.\nWeight tying The first layer of our architecture is the token embedding table, which maps each token in our vocabulary to an embedding vector. The last layer is the language modeling head, which reverses this process by mapping the embeddings back to vocabulary space.\nIf we inspect our model’s state dictionary, we find that both layers have the same shape:\ngpt2_model_params = model.state_dict() print(gpt2_model_params[\"transformer.wte.weight\"].shape) print(gpt2_model_params[\"lm_head.weight\"].shape) torch.Size([50257, 768]) torch.Size([50257, 768]) This occurs because of how nn.Embedding and nn.Linear store their weights internally:\nnn.Embedding(dim_in, dim_out) stores its lookup table as torch.Size([dim_in, dim_out]). nn.Linear(features_in, features_out) stores its weight matrix as torch.Size([features_out, features_in]) to perform y = Wx operations. This observation led to the practice of weight tying, where the token embedding and language modeling head share the same set of weights. This approach was also used in the original Transformer paper.\nBy enforcing weight sharing, we inject an inductive bias into the model, indicating that:\nThe way tokens are mapped to embeddings should be similar to how embeddings are mapped back to tokens. The model should maintain consistency between input and output representations. If we visualize the parameter distribution in our model (with just one transformer block), we get: Parameter sizes of our model with one transformer block\nClearly, the majority of parameters reside in the token embeddings and the language modeling layer. By making them share weights, we reduce the overall parameter count, leading to lower memory usage and faster convergence speed since fewer parameters need to be learned.\nWe can enable weight tying by directly assigning the same weight tensor:\n# Weight tying model.transformer.wte.weight = model.lm_head.weight # Print the number of parameters in the model print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters') 124.439808 M parameters Hurray! We now have our own GPT-2 124M model.\nNext, let’s explore the weight initialization scheme used in GPT-2. This is useful if you want to train the model from scratch on your own dataset. However, if you’re only interested in loading pre-trained weights, feel free to skip this section.\nWeight initialization The GPT-2 paper doesn’t explicitly mention the weight initialization used, but we can infer it from the model.py code. Here’s the initialization strategy:\nToken embeddings: Normal distribution with standard deviation of $0.02$. Position embeddings: Normal distribution with standard deviation of $0.01$. Linear layers: Weights: Normal distribution with standard deviation of $0.02$. Bias: Initialized to zero. LayerNorm: The scale and shift parameters are initialized to ones and zeros, respectively, similar to our implementation. Additionally, the GPT-2 paper mentions using a scaling factor of $1/\\sqrt{\\text{N}}$, where $\\text{N}$ is the number of residual layers, on the weights of residual layer at initialization. This is to account for the accumulation of activations on the residual path — the standard deviation of activations grows inside the residual stream due to repeated additions. However, the GPT-2 code doesn’t seem to implement this scaling, so we’ll skip it for now.\nHere’s how we can implement the weight initialization:\n# Initialize weights as per GPT-2 def gpt2_init(m): if isinstance(m, nn.Linear): # Applies to linear layers only nn.init.normal_(m.weight, mean=0.0, std=0.02) if m.bias is not None: nn.init.zeros_(m.bias) # Bias initialized to zero elif isinstance(m, nn.Embedding): # Applied to embedding layer only nn.init.normal_(m.weight, mean=0.0, std=0.02) # Apply initialization model = model.apply(gpt2_init) Note: I’ve used a standard deviation of 0.02 for the position embeddings as well because the difference doesn’t significantly impact the model, and it keeps the code simpler.\nLoading Weights To load weights from HuggingFace, we simply copy the tensor values over to our model. For this, the tensor shapes need to match exactly between the HuggingFace model and our custom model.\nLet’s compare the state dictionaries of both models side by side to identify any mismatches in tensor shapes:\nfor name, param in model_hf_params.items(): if param.shape != gpt2_model_params[name].shape: print(f\"Shape mismatch for parameter {name}: {param.shape} vs {gpt2_model_params[name].shape}\") Shape mismatch for parameter transformer.h.0.attn.c_attn.weight: torch.Size([768, 2304]) vs torch.Size([2304, 768]) Shape mismatch for parameter transformer.h.0.mlp.c_fc.weight: torch.Size([768, 3072]) vs torch.Size([3072, 768]) Shape mismatch for parameter transformer.h.0.mlp.c_proj.weight: torch.Size([3072, 768]) vs torch.Size([768, 3072]) ... Oops, why don’t the shapes match?\nOpenAI’s GPT-2 checkpoints use a Conv1d module on each linear layer in the GPT-2 architecture. This is why the tensors are transposed and do not match directly.\nAs a result, the layers c_attn, c_proj, c_fc and c_proj need to be handled differently. We transpose the weights before copying them to ensure they match the expected shapes and are copied correctly.\nHere’s how to load the weights:\n# Loading weights in your model transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight'] with torch.no_grad(): for name, param in model_hf_params.items(): # check if the parameter name matches if name in gpt2_model_params: # if the parameter has to be transposed if name.endswith(tuple(transposed)): gpt2_model_params[name].copy_(param.t()) # Tranpose the weights and then copy # if the parameter shape matches directly elif param.shape == gpt2_model_params[name].shape: gpt2_model_params[name].copy_(param) # copy the weights over else: print(f\"Shape mismatch for parameter {name}: {param.shape} vs {gpt2_model_params[name].shape}\") else: print(f\"Parameter {name} not found in your model\") print(\"Weights are loaded successfully!\") This code will load the pre-trained HuggingFace weights into our model, handling special treatment for the linear layers to ensure that everything is correctly aligned.\nGenerating text Now that we’ve successfully loaded the weights, it’s time to test our model by generating some text. We’ll use the tiktoken tokenizer to encode the initial prompt, “Hello, I’m a language model,” and pass it into the model to generate a continuation.\nHere’s a basic text generation loop:\n# Generate text from the trained model max_new_tokens = 20 context_length = 1024 import tiktoken enc = tiktoken.get_encoding(\"gpt2\") tokens = enc.encode(\"Hello, I'm a language model,\") context = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0) # Adds the batch dimension: (B=1, N) stored_context = context for _ in range(max_new_tokens): model.eval() with torch.no_grad(): context = stored_context[:, -context_length:] # Trim the context to fit the model's context length logits = model(context) # logits: (B=1, N, vocab_size) logits = logits[:, -1, :] # take just the last time step (B, vocab_size) probs = torch.softmax(logits, dim=-1) next_token = torch.argmax(probs, dim=-1, keepdim=True) # Select the most likely next token (B=1, N) stored_context = torch.cat((stored_context, next_token), dim=1) # (B=1, N+1) print(enc.decode(stored_context[0].tolist())) Hello, I'm a language model, not a programming language. I'm a language model. I'm a language model. I'm a The model gets stuck in a loop, generating repeated phrases like “I’m a language model.” This is because we are always selecting the token with the highest probability at each step, which limits the model’s creativity and causes repetition. To resolve this, we can explore probabilistic sampling methods.\nProbabilistic Sampling To introduce more variety and creativity in the decoding process, we replace the argmax function with multinomial. This method uses the probability distribution output by the model to sample the next token proportionally to its probability score:\nlogits = model(context) # logits: (B=1, N, vocab_size) logits = logits[:, -1, :] # take just the last time step (B, vocab_size) probs = torch.softmax(logits, dim=-1) next_token = torch.multinomial(probs, num_samples=1) # Sample from the distribution (B=1, N) By using probabilistic sampling, we can explore a range of potential next tokens, leading to more diverse and interesting text.\nWhile this is a good way to sample text, there are other decoding strategies that allow us to control the distribution and selection process to generate more original text.\nTemperature Scaling Let’s understand temperature scaling through an example:\nprint(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)) print(torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])/0.001, dim=-1)) tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872]) tensor([0., 0., 0., 0., 1.]) When the magnitudes of logits are large, the softmax output saturates and converges to a one-hot encoding. Temperature scaling works similarly—by dividing the logits by a number greater than zero:\nlogits = model(context) logits = logits[:, -1, :] / temperature probs = torch.softmax(logits, dim=-1) next_token = torch.multinomial(probs, num_samples=1) Temperature scaling allows us to control the randomness of the output:\nTemperature \u003c 1: Produces more confident (sharper) distributions, picking the most likely token almost always. Temperatures \u003e 1: Results in a more uniformly distributed token probabilties, where other tokens are selected more often. This can add more variety but may also produce nonsensical text. Temperature = 1: This is equivalent to not using any temperature scaling. Top-k sampling In top-k sampling, we restrict the sampling process to the top-k most likely tokens and exclude the rest by masking their probabilities. This ensures that we avoid sampling very rare tokens while still providing some diversity in the output.\nWe achieve this by setting the logits of non-selected tokens to negative infinity, so that their softmax probabilities become zero, and the remaining probabilities sum to 1. The implementation is as follows:\ndef top_k_logits(logits, k): if k == 0: return logits # No truncation values, _ = torch.topk(logits, k=k) # Get top-k values min_value = values[:, -1] # Minimum value in top-k return torch.where(logits \u003c min_value, torch.tensor(float('-inf')), logits) torch.topk retrieves the values of top-k logits in descending order, and the where function sets the logits of tokens below the lowest logit value to negative infinity. This ensures that only the top-k logits contribute to the probability distribution.\nTop-p (Nucleus Sampling) While top-k gives us the ability to select the top-k tokens to consider in the sampling process, top-p dynamically selects the top tokens whose cumulative probability exceeds a certain threshold, denoted by p. Instead of a fixed number k, it adapts based on the distribution.\nFor example, we first sort the tokens by probability:\nToken A: 0.40 Token B: 0.30 Token C: 0.20 Token D: 0.05 Token E: 0.05 Next, we compute the cumulative probability:\nToken A: 0.40 Token A + B: 0.70 Token A + B + C: 0.90 ✅ (stop here, because we reached p=0.9) Token A + B + C + D: 0.95 Token A + B + C + D + E: 1.00 We keep only tokens A, B, and C since their cumulative probability exceeds p=0.9. The rest are discarded.\nHere’s how it is implemented in code:\ndef top_p_logits(logits, p): # Nucleus Sampling sorted_logits, _ = torch.sort(logits, dim=-1, descending=True) # Sort logits in descending order cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1) # Compute cumulative probabilities # Determine number of indices to include, keeping at least one num_to_keep = torch.clamp((cumulative_probs \u003c= p).sum(dim=-1) - 1, min=0) min_value = sorted_logits[:, num_to_keep] return torch.where(logits \u003c min_value, torch.tensor(float('-inf')), logits) Sampling code Now, let’s integrate all the sampling strategies we’ve discussed and implement a sample() function, similar to the one in GPT-2. You can find the original GPT-2 implementation here: https://github.com/openai/gpt-2/blob/master/src/sample.py\ndef sample(max_new_tokens, context_length, start_token=None, context=None, temperature=1, top_k=0, top_p=1): if start_token is None: assert context is not None, 'Specify exactly one of start_token and context!' else: assert context is None, 'Specify exactly one of start_token and context!' context = torch.full((1, 1), start_token, dtype=torch.long, device=device) stored_context = context for _ in range(max_new_tokens): model.eval() with torch.no_grad(): context = stored_context[:, -context_length:] logits = model(context) # logits: (B=1, N, vocab_size) logits = logits[:, -1, :] / temperature # Scale logits by temperature logits = top_k_logits(logits, k=top_k) # Apply top-k filtering logits = top_p_logits(logits, p=top_p) # Apply top-p (nucleus) sampling probs = torch.softmax(logits, dim=-1) # Convert logits to probabilities next_token = torch.multinomial(probs, num_samples=1) # Sample from the distribution (B=1, N) stored_context = torch.cat((stored_context, next_token), dim=1) # (B=1, N+1) return stored_context Using this function, we can start generating text either using a start token or an initial prompt.\ntokens = enc.encode(\"Hello, I'm a language model,\") prompt = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0) # Adds the batch dimension(B=1, N) next_tokens = sample(context=prompt, max_new_tokens=20, context_length=GPT_CONFIG_124M[\"context_length\"], top_p=0.9, top_k=40, temperature=1.0) print(enc.decode(next_tokens[0].tolist())) Here’s the output:\nHello, I'm a language model, and I wanted to do something more powerful than just an English translation of an English sentence.\" And with that, we’ve explored some of the most exciting text generation techniques that can bring out the true creativity of language models. Now, it’s your turn to experiment and create your own stories! Happy coding!\nSummary of GPT-2 and GPT-3 models GPT-2 model overview (2019) Below are some key implementation details of the GPT-2 model:\nTokenizer: Uses Byte Pair Encoding (BPE) with a vocabulary size of 50,257. Context length: $N = 1024$ tokens. Architecture hyperparameters: Model Parameters Layers Hidden Size Attention Heads GPT-2 Small 124 million 12 768 12 GPT-2 Medium 355 million 24 1024 16 GPT-2 Large 774 million 36 1280 20 GPT-2 XL 1.5 billion 48 1600 25 Pre-Training: Trained on the WebText dataset created by OpenAI, which contains 40GB of text from 8 million web pages scraped primarily from Reddit. Batch size: 512 Initialization: as previously discussed. While the GPT-2 paper fully describes the tokenizer and model architecture, it lacks detailed training parameters. The code that OpenAI released—and that we have been referring to so far—is also inference code, with no training specifics mentioned.\nGPT-3 model overview (2020) Architecturally, GPT-3 is identical to GPT-2, except for one key difference: it employs alternating dense and locally banded sparse attention patterns in its transformer layers. Therefore, the authors do not revisit the architectural details already specified in the GPT-2 paper but instead focus on the training details:\nContext length: $N = 2048$ tokens.\nArchitecture hyperparameters: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of GPT-3 models.\nPre-training: Trained on a dataset comprising filtered Common Crawl, WebText2 (an expanded version of WebText), Books1 and Books2 (two internet-based book corpora), and English-language Wikipedia.\nOptimizer: AdamW\n$\\beta_1 = 0.9, \\beta_2 = 0.95, \\epsilon = 10^{-8}$ Weight decay: $0.1$ Batch size: Data is sampled without replacement. Batch size increases linearly from a small value (32k tokens) to the full value over the first 4–12 billion tokens of training, depending on model size. Gradient Clipping: The global norm of the gradient is clipped at 1.0.\nLearning rate:\nIncreased linearly from zero to a peak value over the first 375M tokens. Then annealed to 10% of its peak value using a cosine schedule over 260B tokens. Training continues at this reduced learning rate. Fine-tuning: GPT-2 and GPT-3 models were not explicitly fine-tuned for specific tasks. Instead, they leveraged in-context learning, relying on zero-shot or few-shot prompting to adapt to various tasks without the need for task-specific fine-tuning.\nComparison of fine-tuning with zero-shot, one-shot, and few-shot learning using an English-to-French translation example.\nReferences Andrej Karpathy’s video: Let’s reproduce GPT-2 (124M) [1] Radford et al., “Language Models are Unsupervised Multitask Learners”, OpenAI 2019. [2] Tom Brown et al., “Language Models are Few-Shot Learners ”, NeurIPS 2020. [3] Xiong et al., “On Layer Normalization in the Transformer Architecture”, ICML 2020. ","wordCount":"3774","inLanguage":"en","datePublished":"2025-03-01T00:00:00Z","dateModified":"2025-03-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://yugajmera.github.io/posts/06-gpt2/post/"},"publisher":{"@type":"Organization","name":"YA's Almanac","logo":{"@type":"ImageObject","url":"https://yugajmera.github.io/assets/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yugajmera.github.io/ accesskey=h title="YA's Almanac (Alt + H)">YA's Almanac</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yugajmera.github.io/ title=Home><span>Home</span></a></li><li><a href=https://yugajmera.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://yugajmera.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">GPT Series Part 3: Building GPT-2 & Sampling Techniques</h1><div class=post-meta><span title='2025-03-01 00:00:00 +0000 UTC'>March 1, 2025</span>&nbsp;·&nbsp;18 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#architecture aria-label=Architecture>Architecture</a><ul><li><a href=#pre-norm-transformer aria-label="Pre-Norm Transformer">Pre-Norm Transformer</a></li><li><a href=#defining-the-gpt-2-model aria-label="Defining the GPT-2 model">Defining the GPT-2 model</a></li><li><a href=#weight-tying aria-label="Weight tying">Weight tying</a></li><li><a href=#weight-initialization aria-label="Weight initialization">Weight initialization</a></li></ul></li><li><a href=#loading-weights aria-label="Loading Weights">Loading Weights</a></li><li><a href=#generating-text aria-label="Generating text">Generating text</a><ul><li><a href=#probabilistic-sampling aria-label="Probabilistic Sampling">Probabilistic Sampling</a></li><li><a href=#temperature-scaling aria-label="Temperature Scaling">Temperature Scaling</a></li><li><a href=#top-k-sampling aria-label="Top-k sampling">Top-k sampling</a></li><li><a href=#top-p-nucleus-sampling aria-label="Top-p (Nucleus Sampling)">Top-p (Nucleus Sampling)</a></li><li><a href=#sampling-code aria-label="Sampling code">Sampling code</a></li></ul></li><li><a href=#summary-of-gpt-2-and-gpt-3-models aria-label="Summary of GPT-2 and GPT-3 models">Summary of GPT-2 and GPT-3 models</a><ul><li><a href=#gpt-2-model-overview-2019 aria-label="GPT-2 model overview (2019)">GPT-2 model overview (2019)</a></li><li><a href=#gpt-3-model-overview-2020 aria-label="GPT-3 model overview (2020)">GPT-3 model overview (2020)</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>Building on our previous exploration of GPT-1, let&rsquo;s now modify its architecture to recreate the GPT-2 <a href=#references>[1]</a> small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.</p><p>One major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning. Additionally, GPT-2 is an excellent choice for learning how to implement LLMs, as it can run on a single GPU or even a laptop—unlike GPT-3 <a href=#references>[2]</a>, which requires GPU clusters for training and inference.</p><p>GPT-2 was introduced in 2019, which, by the fast-paced standards of deep learning and LLM development, is considered a long time ago. However, more recent architectures, such as Meta&rsquo;s LLaMA models, are still built on the same foundational concepts with only minor modifications. Hence, understanding GPT-2 remains as relevant as ever.</p><p>The official model code is available here: <a href=https://github.com/openai/gpt-2/blob/master/src/model.py>https://github.com/openai/gpt-2/blob/master/src/model.py</a></p><h2 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h2><p>Let&rsquo;s start with our imports:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Import functions</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=s1>&#39;cuda&#39;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;cpu&#39;</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Using&#34;</span><span class=p>,</span> <span class=n>device</span><span class=p>)</span>
</span></span></code></pre></div><p>Before implementing the model, let&rsquo;s take a look at GPT-2&rsquo;s pre-trained weights from Hugging Face’s <code>transformers</code> library. This will help us understand the naming conventions used in the architecture:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>GPT2LMHeadModel</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Load pre-trained GPT-2 (small) and retrieve its state dictionary</span>
</span></span><span class=line><span class=cl><span class=n>model_hf</span> <span class=o>=</span> <span class=n>GPT2LMHeadModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;gpt2&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span> <span class=c1># 124M param model</span>
</span></span><span class=line><span class=cl><span class=n>model_hf_params</span> <span class=o>=</span> <span class=n>model_hf</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model_hf_params</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>name</span><span class=p>,</span> <span class=n>param</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>transformer.wte.weight torch.Size([50257, 768])
transformer.wpe.weight torch.Size([1024, 768])
transformer.h.0.ln_1.weight torch.Size([768])
transformer.h.0.ln_1.bias torch.Size([768])
transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])
transformer.h.0.attn.c_attn.bias torch.Size([2304])
transformer.h.0.attn.c_proj.weight torch.Size([768, 768])
transformer.h.0.attn.c_proj.bias torch.Size([768])
transformer.h.0.ln_2.weight torch.Size([768])
transformer.h.0.ln_2.bias torch.Size([768])
transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])
transformer.h.0.mlp.c_fc.bias torch.Size([3072])
transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])
transformer.h.0.mlp.c_proj.bias torch.Size([768])

...

transformer.ln_f.weight torch.Size([768])
transformer.ln_f.bias torch.Size([768])
lm_head.weight torch.Size([50257, 768])
</code></pre><p>The output lists the layer names along with their corresponding tensor shapes, giving us insights into GPT-2&rsquo;s layer structure:</p><ul><li>The model is encapsulated under the namespace <code>transformer</code>.</li><li><code>wte</code> represents the word token embedding weights.</li><li><code>wpe</code> represents the word position embedding weights.</li><li><code>h</code> is a list of hidden decoder blocks.</li><li>LayerNorms are denoted as <code>ln_1</code> and <code>ln_2</code> within each decoder block, with the final LayerNorm represented as <code>ln_f</code>. The <code>scale</code> and <code>shift</code> parameters are renamed to <code>weight</code> and <code>bias</code>, respectively.</li><li>Our <code>W_qkv</code> weight matrix is renamed to <code>c_attn</code> (short for causal attention), while <code>W_o</code> is renamed to <code>c_proj</code>. The causal attention layer also includes bias tensors, meaning we will set <code>qkv_bias=True</code>.</li><li><code>lm_head</code> represents the final linear layer (language modeling head) and does not include a bias term.</li></ul><p>To ensure compatibility when loading OpenAI’s pre-trained weights, we will follow this naming convention while defining the subcomponents of our model. Let’s implement these next! 🚀</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadCausalAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span> <span class=n>attn_pdrop</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>qkv_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>c_attn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=mi>3</span> <span class=o>*</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>c_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_out</span><span class=p>,</span> <span class=n>d_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attn_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>attn_pdrop</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span> <span class=o>=</span> <span class=n>d_out</span> <span class=o>//</span> <span class=n>num_heads</span>                    <span class=c1># head dimension</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s2>&#34;masked&#34;</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>context_length</span><span class=p>,</span> <span class=n>context_length</span><span class=p>))</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span> <span class=n>context_length</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Input vectors x - (B, N, d_in)</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>d_in</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=c1># Obtain keys, values and queries in one go - (B, N, d_out)</span>
</span></span><span class=line><span class=cl>        <span class=n>qkv</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_attn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>queries</span><span class=p>,</span> <span class=n>keys</span><span class=p>,</span> <span class=n>values</span> <span class=o>=</span> <span class=n>qkv</span><span class=o>.</span><span class=n>chunk</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=n>keys</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=n>values</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>queries</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Apply scaled dot-product attention with causal mask on each head</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_scores</span> <span class=o>=</span> <span class=p>(</span><span class=n>q</span> <span class=o>@</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span> <span class=o>*</span> <span class=n>k</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>**-</span><span class=mf>0.5</span>    
</span></span><span class=line><span class=cl>        <span class=n>attn_scores</span> <span class=o>=</span> <span class=n>attn_scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>masked</span><span class=p>[:,</span> <span class=p>:,</span> <span class=p>:</span><span class=n>N</span><span class=p>,</span> <span class=p>:</span><span class=n>N</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;-inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attn_scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>                             
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>attn_dropout</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>context_vec</span> <span class=o>=</span> <span class=n>attn_weights</span> <span class=o>@</span> <span class=n>v</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>context_vec</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_proj</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>LayerNorm</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>eps</span> <span class=o>=</span> <span class=mf>1e-5</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>weight</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>emb_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>bias</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>emb_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>mean</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>var</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>var</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>unbiased</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>       <span class=c1># Used biased variance estimation (without Bessel&#39;s correction)</span>
</span></span><span class=line><span class=cl>        <span class=n>norm_x</span> <span class=o>=</span> <span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>mean</span><span class=p>)</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>var</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight</span> <span class=o>*</span> <span class=n>norm_x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GELU</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>x</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>2.0</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>pi</span><span class=p>))</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>            <span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=mf>0.044715</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=p>))</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MLP</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>c_fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>emb_dim</span><span class=p>,</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>emb_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>c_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>4</span> <span class=o>*</span> <span class=n>emb_dim</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gelu</span> <span class=o>=</span> <span class=n>GELU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>gelu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>c_fc</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><p>Here, I’ve taken our GPT-1 implementation and adapted it to match GPT-2’s naming conventions.</p><p>A key detail is the <code>masked</code> variable, which is defined in <code>register_buffer</code>. This automatically adds it to the model’s state dictionary, meaning you’ll find entries like <code>transformer.h.0.attn.masked</code>. However, since it’s a non-trainable tensor, it does not exist in Hugging Face’s state dictionary. So, when loading pre-trained weights, we’ll simply ignore it.</p><p>The GPT-2 decoder block closely follows the GPT-1 model we implemented earlier, with a few key modifications. Let’s break them down.</p><h3 id=pre-norm-transformer>Pre-Norm Transformer<a hidden class=anchor aria-hidden=true href=#pre-norm-transformer>#</a></h3><p>The original Transformer (as well as our GPT-1 implementation) used Post-Norm residual connections, where LayerNorm is applied <em>after</em> the sublayer and residual addition.</p><figure class=align-center><img loading=lazy src=../pre-post-norm.png#center alt="Pre-Norm vs Post-Norm Transformer Layers"><figcaption><p>Pre-Norm vs Post-Norm Transformer Layers</p></figcaption></figure><p>In this setup, the raw output of each sublayer (such as the self-attention or feedforward network) is directly added to the residual before normalization. This can lead to instability as the network depth increases because the residual sum $[x_l + \text{Sublayer}(x_l)]$ accumulates unnormalized activations over layers:</p><ul><li><p>If $\text{Sublayer}(x_l)$ outputs are very small values, the residual connection mostly carries $x_l$ forward without significant updates, leading to vanishing gradients.</p></li><li><p>If $\text{Sublayer}(x_l)$ has large activations, they keep adding up over layers. Although LayerNorm tries to regulate this after the accumulation, the activations have already influenced the residual pathway, potentially leading to exploding gradients.</p></li></ul><p>Essentially, LayerNorm in Post-Norm setups tries to fix activations at the last moment—after instability has already propagated.</p><p>To address this, Xiong et al. <a href=#references>[3]</a> proposed Pre-Norm residual units, where LayerNorm is applied <em>before</em> the sublayer. This ensures each sublayer receives well-scaled inputs, improving gradient flow and training stability.</p><p>Most modern Large Language Models, including GPT-2, have adopted this Pre-Norm approach. The key changes are:</p><ol><li><p>Layer Normalization is moved to the input of each sub-block, ensuring that each sublayer receives normalized inputs.</p></li><li><p>An additional Layer Normalization is added after the final self-attention block, further improving stability.</p></li></ol><p>With these changes, the GPT-2 architecture now looks like this:<figure class=align-center><img loading=lazy src=../gpt-2.png#center alt="GPT-2 architecture" width=500><figcaption><p>GPT-2 architecture</p></figcaption></figure></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>DecoderBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cfg</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ln_1</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>emb_dim</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attn</span> <span class=o>=</span> <span class=n>MultiHeadCausalAttention</span><span class=p>(</span><span class=n>d_in</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>],</span> <span class=n>d_out</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>],</span> <span class=n>context_length</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;context_length&#39;</span><span class=p>],</span> <span class=n>attn_pdrop</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;attn_pdrop&#39;</span><span class=p>],</span> <span class=n>num_heads</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;n_heads&#39;</span><span class=p>],</span> <span class=n>qkv_bias</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;qkv_bias&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ln_2</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>emb_dim</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span> <span class=o>=</span> <span class=n>MLP</span><span class=p>(</span><span class=n>emb_dim</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>resid_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;resid_pdrop&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>resid_dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>attn</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ln_1</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>resid_dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>mlp</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ln_2</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><h3 id=defining-the-gpt-2-model>Defining the GPT-2 model<a hidden class=anchor aria-hidden=true href=#defining-the-gpt-2-model>#</a></h3><p>And our GPT-2 model will be coded as:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>GPT2</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cfg</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>transformer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleDict</span><span class=p>(</span><span class=nb>dict</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>wte</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;vocab_size&#39;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>]),</span>                      <span class=c1># Token Embeddings</span>
</span></span><span class=line><span class=cl>            <span class=n>wpe</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;context_length&#39;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>]),</span>                  <span class=c1># Position Encoding</span>
</span></span><span class=line><span class=cl>            <span class=n>embd_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;embd_pdrop&#39;</span><span class=p>]),</span>                               <span class=c1># Embedding dropout</span>
</span></span><span class=line><span class=cl>            <span class=n>h</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=p>[</span><span class=n>DecoderBlock</span><span class=p>(</span><span class=n>cfg</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;n_layers&#39;</span><span class=p>])]),</span>    <span class=c1># Multiple Decoder blocks</span>
</span></span><span class=line><span class=cl>            <span class=n>ln_f</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>]),</span>                                           <span class=c1># Final layernorm</span>
</span></span><span class=line><span class=cl>        <span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;vocab_size&#39;</span><span class=p>],</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>         <span class=c1># Language modelling head</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>N</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>token_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>transformer</span><span class=o>.</span><span class=n>wte</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>                                             <span class=c1># (B, N, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>pos_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>transformer</span><span class=o>.</span><span class=n>wpe</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>N</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>))</span>                  <span class=c1># (N, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>token_emb</span> <span class=o>+</span> <span class=n>pos_emb</span>                                                         <span class=c1># (B, N, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>transformer</span><span class=o>.</span><span class=n>h</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>transformer</span><span class=o>.</span><span class=n>ln_f</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>lm_head</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>                                                         <span class=c1># (B, N, vocab_size)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>logits</span>
</span></span></code></pre></div><p>The hyperparameters of our model are defined in a Python dictionary, <code>cfg</code>, which is passed when instantiating the model.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Define configuration dictionary</span>
</span></span><span class=line><span class=cl><span class=n>GPT_CONFIG_124M</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;vocab_size&#34;</span><span class=p>:</span> <span class=mi>50257</span><span class=p>,</span>    <span class=c1># 50,000 BPE merges + 256 byte tokens + 1 &lt;|endoftext|&gt; token</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;context_length&#34;</span><span class=p>:</span> <span class=mi>1024</span><span class=p>,</span> <span class=c1># Maximum sequence length</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;emb_dim&#34;</span><span class=p>:</span> <span class=mi>768</span><span class=p>,</span>         <span class=c1># Embedding dimension</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;n_heads&#34;</span><span class=p>:</span> <span class=mi>12</span><span class=p>,</span>          <span class=c1># Number of attention heads</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;n_layers&#34;</span><span class=p>:</span> <span class=mi>12</span><span class=p>,</span>         <span class=c1># Number of transformer blocks</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;attn_pdrop&#34;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>,</span>      <span class=c1># Dropout probability for attention dropout</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;embd_pdrop&#34;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>,</span>      <span class=c1># Dropout probability for embeddings dropout</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;resid_pdrop&#34;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>,</span>     <span class=c1># Dropout probability for residual dropout</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;qkv_bias&#34;</span><span class=p>:</span> <span class=kc>True</span>        <span class=c1># Whether to use bias in QKV layer</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>GPT2</span><span class=p>(</span><span class=n>GPT_CONFIG_124M</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Print the number of parameters in the model</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span> <span class=o>/</span> <span class=mf>1e6</span><span class=p>,</span> <span class=s1>&#39;M parameters&#39;</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=m>163.037184</span> <span class=n>M</span> <span class=n>parameters</span>
</span></span></code></pre></div><p>Oops! Our model has 163M parameters, even though we aimed to replicate the 124M parameter version. What&rsquo;s going on?</p><p>Actually, nothing is wrong! The discrepancy arises due to the weight tying scheme used in the GPT-2 model. Let’s dive into it in more detail.</p><h3 id=weight-tying>Weight tying<a hidden class=anchor aria-hidden=true href=#weight-tying>#</a></h3><p>The first layer of our architecture is the token embedding table, which maps each token in our vocabulary to an embedding vector. The last layer is the language modeling head, which reverses this process by mapping the embeddings back to vocabulary space.</p><p>If we inspect our model’s state dictionary, we find that both layers have the same shape:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>gpt2_model_params</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>state_dict</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>gpt2_model_params</span><span class=p>[</span><span class=s2>&#34;transformer.wte.weight&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>gpt2_model_params</span><span class=p>[</span><span class=s2>&#34;lm_head.weight&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>torch</span><span class=p>.</span><span class=n>Size</span><span class=p>([</span><span class=m>50257</span><span class=p>,</span> <span class=m>768</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=p>.</span><span class=n>Size</span><span class=p>([</span><span class=m>50257</span><span class=p>,</span> <span class=m>768</span><span class=p>])</span>
</span></span></code></pre></div><p>This occurs because of how <code>nn.Embedding</code> and <code>nn.Linear</code> store their weights internally:</p><ul><li><code>nn.Embedding(dim_in, dim_out)</code> stores its lookup table as <code>torch.Size([dim_in, dim_out])</code>.</li><li><code>nn.Linear(features_in, features_out)</code> stores its weight matrix as <code>torch.Size([features_out, features_in])</code> to perform <code>y = Wx</code> operations.</li></ul><p>This observation led to the practice of weight tying, where the token embedding and language modeling head share the same set of weights. This approach was also used in the original Transformer paper.</p><p>By enforcing weight sharing, we inject an inductive bias into the model, indicating that:</p><ul><li>The way tokens are mapped to embeddings should be similar to how embeddings are mapped back to tokens.</li><li>The model should maintain consistency between input and output representations.</li></ul><p>If we visualize the parameter distribution in our model (with just one transformer block), we get:<figure class=align-center><img loading=lazy src=../output.png#center alt="Parameter sizes of our model with one transformer block"><figcaption><p>Parameter sizes of our model with one transformer block</p></figcaption></figure></p><p>Clearly, the majority of parameters reside in the token embeddings and the language modeling layer. By making them share weights, we reduce the overall parameter count, leading to lower memory usage and faster convergence speed since fewer parameters need to be learned.</p><p>We can enable weight tying by directly assigning the same weight tensor:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Weight tying</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>transformer</span><span class=o>.</span><span class=n>wte</span><span class=o>.</span><span class=n>weight</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>lm_head</span><span class=o>.</span><span class=n>weight</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Print the number of parameters in the model</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span><span class=o>/</span><span class=mf>1e6</span><span class=p>,</span> <span class=s1>&#39;M parameters&#39;</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=m>124.439808</span> <span class=n>M</span> <span class=n>parameters</span>
</span></span></code></pre></div><p>Hurray! We now have our own GPT-2 124M model.</p><p>Next, let’s explore the weight initialization scheme used in GPT-2. This is useful if you want to train the model from scratch on your own dataset. However, if you&rsquo;re only interested in loading pre-trained weights, feel free to skip this section.</p><h3 id=weight-initialization>Weight initialization<a hidden class=anchor aria-hidden=true href=#weight-initialization>#</a></h3><p>The GPT-2 paper doesn’t explicitly mention the weight initialization used, but we can infer it from the <code>model.py</code> code. Here’s the initialization strategy:</p><ul><li>Token embeddings: Normal distribution with standard deviation of $0.02$.</li><li>Position embeddings: Normal distribution with standard deviation of $0.01$.</li><li>Linear layers:<ul><li>Weights: Normal distribution with standard deviation of $0.02$.</li><li>Bias: Initialized to zero.</li></ul></li><li>LayerNorm: The scale and shift parameters are initialized to ones and zeros, respectively, similar to our implementation.</li></ul><p>Additionally, the GPT-2 paper mentions using a scaling factor of $1/\sqrt{\text{N}}$, where $\text{N}$ is the number of residual layers, on the weights of residual layer at initialization. This is to account for the accumulation of activations on the residual path — the standard deviation of activations grows inside the residual stream due to repeated additions. However, the GPT-2 code doesn’t seem to implement this scaling, so we’ll skip it for now.</p><p>Here’s how we can implement the weight initialization:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Initialize weights as per GPT-2</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>gpt2_init</span><span class=p>(</span><span class=n>m</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>m</span><span class=p>,</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>):</span>        <span class=c1># Applies to linear layers only</span>
</span></span><span class=line><span class=cl>        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>normal_</span><span class=p>(</span><span class=n>m</span><span class=o>.</span><span class=n>weight</span><span class=p>,</span> <span class=n>mean</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span> <span class=n>std</span><span class=o>=</span><span class=mf>0.02</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>m</span><span class=o>.</span><span class=n>bias</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>zeros_</span><span class=p>(</span><span class=n>m</span><span class=o>.</span><span class=n>bias</span><span class=p>)</span>      <span class=c1># Bias initialized to zero</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>m</span><span class=p>,</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>):</span>   <span class=c1># Applied to embedding layer only</span>
</span></span><span class=line><span class=cl>        <span class=n>nn</span><span class=o>.</span><span class=n>init</span><span class=o>.</span><span class=n>normal_</span><span class=p>(</span><span class=n>m</span><span class=o>.</span><span class=n>weight</span><span class=p>,</span> <span class=n>mean</span><span class=o>=</span><span class=mf>0.0</span><span class=p>,</span> <span class=n>std</span><span class=o>=</span><span class=mf>0.02</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Apply initialization</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>apply</span><span class=p>(</span><span class=n>gpt2_init</span><span class=p>)</span>
</span></span></code></pre></div><p>Note: I’ve used a standard deviation of 0.02 for the position embeddings as well because the difference doesn’t significantly impact the model, and it keeps the code simpler.</p><h2 id=loading-weights>Loading Weights<a hidden class=anchor aria-hidden=true href=#loading-weights>#</a></h2><p>To load weights from HuggingFace, we simply copy the tensor values over to our model. For this, the tensor shapes need to match exactly between the HuggingFace model and our custom model.</p><p>Let’s compare the state dictionaries of both models side by side to identify any mismatches in tensor shapes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model_hf_params</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>param</span><span class=o>.</span><span class=n>shape</span> <span class=o>!=</span> <span class=n>gpt2_model_params</span><span class=p>[</span><span class=n>name</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Shape mismatch for parameter </span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>param</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2> vs </span><span class=si>{</span><span class=n>gpt2_model_params</span><span class=p>[</span><span class=n>name</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>Shape mismatch for parameter transformer.h.0.attn.c_attn.weight: torch.Size([768, 2304]) vs torch.Size([2304, 768])
Shape mismatch for parameter transformer.h.0.mlp.c_fc.weight: torch.Size([768, 3072]) vs torch.Size([3072, 768])
Shape mismatch for parameter transformer.h.0.mlp.c_proj.weight: torch.Size([3072, 768]) vs torch.Size([768, 3072])
...
</code></pre><p>Oops, why don&rsquo;t the shapes match?</p><p>OpenAI’s GPT-2 checkpoints use a <code>Conv1d</code> module on each linear layer in the GPT-2 architecture. This is why the tensors are transposed and do not match directly.</p><p>As a result, the layers <code>c_attn</code>, <code>c_proj</code>, <code>c_fc</code> and <code>c_proj</code> need to be handled differently. We transpose the weights before copying them to ensure they match the expected shapes and are copied correctly.</p><p>Here’s how to load the weights:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Loading weights in your model</span>
</span></span><span class=line><span class=cl><span class=n>transposed</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;attn.c_attn.weight&#39;</span><span class=p>,</span> <span class=s1>&#39;attn.c_proj.weight&#39;</span><span class=p>,</span> <span class=s1>&#39;mlp.c_fc.weight&#39;</span><span class=p>,</span> <span class=s1>&#39;mlp.c_proj.weight&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>model_hf_params</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>      <span class=c1># check if the parameter name matches</span>
</span></span><span class=line><span class=cl>      <span class=k>if</span> <span class=n>name</span> <span class=ow>in</span> <span class=n>gpt2_model_params</span><span class=p>:</span>                
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># if the parameter has to be transposed</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>name</span><span class=o>.</span><span class=n>endswith</span><span class=p>(</span><span class=nb>tuple</span><span class=p>(</span><span class=n>transposed</span><span class=p>)):</span>       
</span></span><span class=line><span class=cl>          <span class=n>gpt2_model_params</span><span class=p>[</span><span class=n>name</span><span class=p>]</span><span class=o>.</span><span class=n>copy_</span><span class=p>(</span><span class=n>param</span><span class=o>.</span><span class=n>t</span><span class=p>())</span>  <span class=c1># Tranpose the weights and then copy</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># if the parameter shape matches directly</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>param</span><span class=o>.</span><span class=n>shape</span> <span class=o>==</span> <span class=n>gpt2_model_params</span><span class=p>[</span><span class=n>name</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=p>:</span>
</span></span><span class=line><span class=cl>              <span class=n>gpt2_model_params</span><span class=p>[</span><span class=n>name</span><span class=p>]</span><span class=o>.</span><span class=n>copy_</span><span class=p>(</span><span class=n>param</span><span class=p>)</span>  <span class=c1># copy the weights over</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Shape mismatch for parameter </span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>param</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2> vs </span><span class=si>{</span><span class=n>gpt2_model_params</span><span class=p>[</span><span class=n>name</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>          <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Parameter </span><span class=si>{</span><span class=n>name</span><span class=si>}</span><span class=s2> not found in your model&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Weights are loaded successfully!&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>This code will load the pre-trained HuggingFace weights into our model, handling special treatment for the linear layers to ensure that everything is correctly aligned.</p><h2 id=generating-text>Generating text<a hidden class=anchor aria-hidden=true href=#generating-text>#</a></h2><p>Now that we’ve successfully loaded the weights, it&rsquo;s time to test our model by generating some text. We’ll use the <code>tiktoken</code> tokenizer to encode the initial prompt, &ldquo;<em>Hello, I&rsquo;m a language model,</em>&rdquo; and pass it into the model to generate a continuation.</p><p>Here’s a basic text generation loop:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Generate text from the trained model</span>
</span></span><span class=line><span class=cl><span class=n>max_new_tokens</span> <span class=o>=</span> <span class=mi>20</span>
</span></span><span class=line><span class=cl><span class=n>context_length</span> <span class=o>=</span> <span class=mi>1024</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tiktoken</span>
</span></span><span class=line><span class=cl><span class=n>enc</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>get_encoding</span><span class=p>(</span><span class=s2>&#34;gpt2&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>enc</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;Hello, I&#39;m a language model,&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>context</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>tokens</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>    <span class=c1># Adds the batch dimension: (B=1, N)</span>
</span></span><span class=line><span class=cl><span class=n>stored_context</span> <span class=o>=</span> <span class=n>context</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_new_tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>context</span> <span class=o>=</span> <span class=n>stored_context</span><span class=p>[:,</span> <span class=o>-</span><span class=n>context_length</span><span class=p>:]</span>                      <span class=c1># Trim the context to fit the model&#39;s context length</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>context</span><span class=p>)</span>                                            <span class=c1># logits: (B=1, N, vocab_size)</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>                                          <span class=c1># take just the last time step (B, vocab_size)</span>
</span></span><span class=line><span class=cl>        <span class=n>probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>next_token</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>             <span class=c1># Select the most likely next token (B=1, N)</span>
</span></span><span class=line><span class=cl>        <span class=n>stored_context</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>stored_context</span><span class=p>,</span> <span class=n>next_token</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>    <span class=c1># (B=1, N+1)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>enc</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>stored_context</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>tolist</span><span class=p>()))</span>                               
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>Hello</span><span class=p>,</span> <span class=n>I</span><span class=err>&#39;</span><span class=n>m</span> <span class=n>a</span> <span class=n>language</span> <span class=n>model</span><span class=p>,</span> <span class=n>not</span> <span class=n>a</span> <span class=n>programming</span> <span class=n>language</span><span class=p>.</span> <span class=n>I</span><span class=err>&#39;</span><span class=n>m</span> <span class=n>a</span> <span class=n>language</span> <span class=n>model</span><span class=p>.</span> <span class=n>I</span><span class=err>&#39;</span><span class=n>m</span> <span class=n>a</span> <span class=n>language</span> <span class=n>model</span><span class=p>.</span> <span class=n>I</span><span class=err>&#39;</span><span class=n>m</span> <span class=n>a</span>
</span></span></code></pre></div><p>The model gets stuck in a loop, generating repeated phrases like &ldquo;I&rsquo;m a language model.&rdquo; This is because we are always selecting the token with the highest probability at each step, which limits the model&rsquo;s creativity and causes repetition. To resolve this, we can explore probabilistic sampling methods.</p><h3 id=probabilistic-sampling>Probabilistic Sampling<a hidden class=anchor aria-hidden=true href=#probabilistic-sampling>#</a></h3><p>To introduce more variety and creativity in the decoding process, we replace the <code>argmax</code> function with <code>multinomial</code>. This method uses the probability distribution output by the model to sample the next token proportionally to its probability score:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>context</span><span class=p>)</span>                               <span class=c1># logits: (B=1, N, vocab_size)</span>
</span></span><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>                             <span class=c1># take just the last time step (B, vocab_size)</span>
</span></span><span class=line><span class=cl><span class=n>probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>next_token</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>multinomial</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># Sample from the distribution (B=1, N)</span>
</span></span></code></pre></div><p>By using probabilistic sampling, we can explore a range of potential next tokens, leading to more diverse and interesting text.</p><p>While this is a good way to sample text, there are other decoding strategies that allow us to control the distribution and selection process to generate more original text.</p><h3 id=temperature-scaling>Temperature Scaling<a hidden class=anchor aria-hidden=true href=#temperature-scaling>#</a></h3><p>Let’s understand temperature scaling through an example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>0.1</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>]),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>0.1</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>])</span><span class=o>/</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>))</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>tensor</span><span class=p>([</span><span class=m>0.1925</span><span class=p>,</span> <span class=m>0.1426</span><span class=p>,</span> <span class=m>0.2351</span><span class=p>,</span> <span class=m>0.1426</span><span class=p>,</span> <span class=m>0.2872</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>tensor</span><span class=p>([</span><span class=m>0.</span><span class=p>,</span> <span class=m>0.</span><span class=p>,</span> <span class=m>0.</span><span class=p>,</span> <span class=m>0.</span><span class=p>,</span> <span class=m>1.</span><span class=p>])</span>
</span></span></code></pre></div><p>When the magnitudes of logits are large, the softmax output saturates and converges to a one-hot encoding. Temperature scaling works similarly—by dividing the logits by a number greater than zero:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>context</span><span class=p>)</span>                               
</span></span><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span> <span class=o>/</span> <span class=n>temperature</span>                         
</span></span><span class=line><span class=cl><span class=n>probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>next_token</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>multinomial</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>  
</span></span></code></pre></div><p>Temperature scaling allows us to control the randomness of the output:</p><ul><li>Temperature &lt; 1: Produces more confident (sharper) distributions, picking the most likely token almost always.</li><li>Temperatures > 1: Results in a more uniformly distributed token probabilties, where other tokens are selected more often. This can add more variety but may also produce nonsensical text.</li><li>Temperature = 1: This is equivalent to not using any temperature scaling.</li></ul><h3 id=top-k-sampling>Top-k sampling<a hidden class=anchor aria-hidden=true href=#top-k-sampling>#</a></h3><p>In top-k sampling, we restrict the sampling process to the top-k most likely tokens and exclude the rest by masking their probabilities. This ensures that we avoid sampling very rare tokens while still providing some diversity in the output.</p><p>We achieve this by setting the logits of non-selected tokens to negative infinity, so that their softmax probabilities become zero, and the remaining probabilities sum to 1. The implementation is as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>top_k_logits</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>k</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>logits</span>                   <span class=c1># No truncation</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>values</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>topk</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=n>k</span><span class=p>)</span> <span class=c1># Get top-k values</span>
</span></span><span class=line><span class=cl>    <span class=n>min_value</span> <span class=o>=</span> <span class=n>values</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>           <span class=c1># Minimum value in top-k</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>logits</span> <span class=o>&lt;</span> <span class=n>min_value</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=nb>float</span><span class=p>(</span><span class=s1>&#39;-inf&#39;</span><span class=p>)),</span> <span class=n>logits</span><span class=p>)</span>
</span></span></code></pre></div><p><code>torch.topk</code> retrieves the values of top-k logits in descending order, and the <code>where</code> function sets the logits of tokens below the lowest logit value to negative infinity. This ensures that only the top-k logits contribute to the probability distribution.</p><h3 id=top-p-nucleus-sampling>Top-p (Nucleus Sampling)<a hidden class=anchor aria-hidden=true href=#top-p-nucleus-sampling>#</a></h3><p>While top-k gives us the ability to select the top-k tokens to consider in the sampling process, top-p dynamically selects the top tokens whose cumulative probability exceeds a certain threshold, denoted by <code>p</code>. Instead of a fixed number <code>k</code>, it adapts based on the distribution.</p><p>For example, we first sort the tokens by probability:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>Token</span> <span class=n>A</span><span class=p>:</span> <span class=m>0.40</span>  
</span></span><span class=line><span class=cl><span class=n>Token</span> <span class=n>B</span><span class=p>:</span> <span class=m>0.30</span>  
</span></span><span class=line><span class=cl><span class=n>Token</span> <span class=n>C</span><span class=p>:</span> <span class=m>0.20</span>  
</span></span><span class=line><span class=cl><span class=n>Token</span> <span class=n>D</span><span class=p>:</span> <span class=m>0.05</span>  
</span></span><span class=line><span class=cl><span class=n>Token</span> <span class=n>E</span><span class=p>:</span> <span class=m>0.05</span>  
</span></span></code></pre></div><p>Next, we compute the cumulative probability:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>Token</span> <span class=n>A</span><span class=p>:</span> <span class=m>0.40</span>  
</span></span><span class=line><span class=cl><span class=n>Token</span> <span class=n>A</span> <span class=p>+</span> <span class=n>B</span><span class=p>:</span> <span class=m>0.70</span>  
</span></span><span class=line><span class=cl><span class=n>Token</span> <span class=n>A</span> <span class=p>+</span> <span class=n>B</span> <span class=p>+</span> <span class=n>C</span><span class=p>:</span> <span class=m>0.90</span>  <span class=err>✅</span> <span class=p>(</span><span class=n>stop</span> <span class=n>here</span><span class=p>,</span> <span class=n>because</span> <span class=n>we</span> <span class=n>reached</span> <span class=n>p</span><span class=p>=</span><span class=m>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Token</span> <span class=n>A</span> <span class=p>+</span> <span class=n>B</span> <span class=p>+</span> <span class=n>C</span> <span class=p>+</span> <span class=n>D</span><span class=p>:</span> <span class=m>0.95</span>  
</span></span><span class=line><span class=cl><span class=n>Token</span> <span class=n>A</span> <span class=p>+</span> <span class=n>B</span> <span class=p>+</span> <span class=n>C</span> <span class=p>+</span> <span class=n>D</span> <span class=p>+</span> <span class=n>E</span><span class=p>:</span> <span class=m>1.00</span>  
</span></span></code></pre></div><p>We keep only tokens A, B, and C since their cumulative probability exceeds <code>p=0.9</code>. The rest are discarded.</p><p>Here’s how it is implemented in code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>top_p_logits</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>p</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Nucleus Sampling</span>
</span></span><span class=line><span class=cl>    <span class=n>sorted_logits</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>descending</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>                 <span class=c1># Sort logits in descending order</span>
</span></span><span class=line><span class=cl>    <span class=n>cumulative_probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cumsum</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>sorted_logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># Compute cumulative probabilities</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Determine number of indices to include, keeping at least one</span>
</span></span><span class=line><span class=cl>    <span class=n>num_to_keep</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>clamp</span><span class=p>((</span><span class=n>cumulative_probs</span> <span class=o>&lt;=</span> <span class=n>p</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=nb>min</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>min_value</span> <span class=o>=</span> <span class=n>sorted_logits</span><span class=p>[:,</span> <span class=n>num_to_keep</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>logits</span> <span class=o>&lt;</span> <span class=n>min_value</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=nb>float</span><span class=p>(</span><span class=s1>&#39;-inf&#39;</span><span class=p>)),</span> <span class=n>logits</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=sampling-code>Sampling code<a hidden class=anchor aria-hidden=true href=#sampling-code>#</a></h3><p>Now, let’s integrate all the sampling strategies we’ve discussed and implement a <code>sample()</code> function, similar to the one in GPT-2. You can find the original GPT-2 implementation here: <a href=https://github.com/openai/gpt-2/blob/master/src/sample.py>https://github.com/openai/gpt-2/blob/master/src/sample.py</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>sample</span><span class=p>(</span><span class=n>max_new_tokens</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span> <span class=n>start_token</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>context</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>temperature</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>top_k</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>top_p</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>start_token</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>       <span class=k>assert</span> <span class=n>context</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>,</span> <span class=s1>&#39;Specify exactly one of start_token and context!&#39;</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>context</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>,</span> <span class=s1>&#39;Specify exactly one of start_token and context!&#39;</span>
</span></span><span class=line><span class=cl>        <span class=n>context</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>full</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>start_token</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>stored_context</span> <span class=o>=</span> <span class=n>context</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_new_tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>context</span> <span class=o>=</span> <span class=n>stored_context</span><span class=p>[:,</span> <span class=o>-</span><span class=n>context_length</span><span class=p>:]</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>context</span><span class=p>)</span>                                            <span class=c1># logits: (B=1, N, vocab_size)</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span> <span class=o>/</span> <span class=n>temperature</span>                            <span class=c1># Scale logits by temperature</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span> <span class=o>=</span> <span class=n>top_k_logits</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>k</span><span class=o>=</span><span class=n>top_k</span><span class=p>)</span>                             <span class=c1># Apply top-k filtering</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span> <span class=o>=</span> <span class=n>top_p_logits</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=n>top_p</span><span class=p>)</span>                             <span class=c1># Apply top-p (nucleus) sampling</span>
</span></span><span class=line><span class=cl>            <span class=n>probs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>                              <span class=c1># Convert logits to probabilities</span>
</span></span><span class=line><span class=cl>            <span class=n>next_token</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>multinomial</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>               <span class=c1># Sample from the distribution (B=1, N)</span>
</span></span><span class=line><span class=cl>            <span class=n>stored_context</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>stored_context</span><span class=p>,</span> <span class=n>next_token</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>    <span class=c1># (B=1, N+1)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>stored_context</span>
</span></span></code></pre></div><p>Using this function, we can start generating text either using a start token or an initial prompt.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>enc</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;Hello, I&#39;m a language model,&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>tokens</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>    <span class=c1># Adds the batch dimension(B=1, N)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>next_tokens</span> <span class=o>=</span> <span class=n>sample</span><span class=p>(</span><span class=n>context</span><span class=o>=</span><span class=n>prompt</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>context_length</span><span class=o>=</span><span class=n>GPT_CONFIG_124M</span><span class=p>[</span><span class=s2>&#34;context_length&#34;</span><span class=p>],</span> <span class=n>top_p</span><span class=o>=</span><span class=mf>0.9</span><span class=p>,</span> <span class=n>top_k</span><span class=o>=</span><span class=mi>40</span><span class=p>,</span> <span class=n>temperature</span><span class=o>=</span><span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>enc</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>next_tokens</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>tolist</span><span class=p>()))</span>       
</span></span></code></pre></div><p>Here&rsquo;s the output:</p><pre tabindex=0><code>Hello, I&#39;m a language model, and I wanted to do something more powerful than just an English translation of an English sentence.&#34;
</code></pre><p>And with that, we&rsquo;ve explored some of the most exciting text generation techniques that can bring out the true creativity of language models. Now, it&rsquo;s your turn to experiment and create your own stories! Happy coding!</p><h2 id=summary-of-gpt-2-and-gpt-3-models>Summary of GPT-2 and GPT-3 models<a hidden class=anchor aria-hidden=true href=#summary-of-gpt-2-and-gpt-3-models>#</a></h2><h3 id=gpt-2-model-overview-2019>GPT-2 model overview (2019)<a hidden class=anchor aria-hidden=true href=#gpt-2-model-overview-2019>#</a></h3><p>Below are some key implementation details of the GPT-2 model:</p><ul><li><strong>Tokenizer</strong>: Uses Byte Pair Encoding (BPE) with a vocabulary size of 50,257.</li><li><strong>Context length</strong>: $N = 1024$ tokens.</li><li><strong>Architecture hyperparameters</strong>:</li></ul><table><thead><tr><th><strong>Model</strong></th><th><strong>Parameters</strong></th><th><strong>Layers</strong></th><th><strong>Hidden Size</strong></th><th><strong>Attention Heads</strong></th></tr></thead><tbody><tr><td><em>GPT-2 Small</em></td><td>124 million</td><td>12</td><td>768</td><td>12</td></tr><tr><td><em>GPT-2 Medium</em></td><td>355 million</td><td>24</td><td>1024</td><td>16</td></tr><tr><td><em>GPT-2 Large</em></td><td>774 million</td><td>36</td><td>1280</td><td>20</td></tr><tr><td><em>GPT-2 XL</em></td><td>1.5 billion</td><td>48</td><td>1600</td><td>25</td></tr></tbody></table><ul><li><strong>Pre-Training</strong>: Trained on the WebText dataset created by OpenAI, which contains 40GB of text from 8 million web pages scraped primarily from Reddit.<ul><li>Batch size: 512</li><li>Initialization: as previously discussed.</li></ul></li></ul><p>While the GPT-2 paper fully describes the tokenizer and model architecture, it lacks detailed training parameters. The code that OpenAI released—and that we have been referring to so far—is also inference code, with no training specifics mentioned.</p><h3 id=gpt-3-model-overview-2020>GPT-3 model overview (2020)<a hidden class=anchor aria-hidden=true href=#gpt-3-model-overview-2020>#</a></h3><p>Architecturally, GPT-3 is identical to GPT-2, except for one key difference: it employs alternating dense and locally banded sparse attention patterns in its transformer layers. Therefore, the authors do not revisit the architectural details already specified in the GPT-2 paper but instead focus on the training details:</p><ul><li><p><strong>Context length</strong>: $N = 2048$ tokens.</p></li><li><p><strong>Architecture hyperparameters</strong>:<figure class=align-center><img loading=lazy src=../gpt3-models.png#center alt="Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of GPT-3 models."><figcaption><p>Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of GPT-3 models.</p></figcaption></figure></p></li><li><p><strong>Pre-training</strong>: Trained on a dataset comprising filtered Common Crawl, WebText2 (an expanded version of WebText), Books1 and Books2 (two internet-based book corpora), and English-language Wikipedia.</p><ul><li><p>Optimizer: AdamW</p><ul><li>$\beta_1 = 0.9, \beta_2 = 0.95, \epsilon = 10^{-8}$</li><li>Weight decay: $0.1$</li><li>Batch size:<ul><li>Data is sampled without replacement.</li><li>Batch size increases linearly from a small value (32k tokens) to the full value over the first 4–12 billion tokens of training, depending on model size.</li></ul></li></ul></li><li><p>Gradient Clipping: The global norm of the gradient is clipped at 1.0.</p></li><li><p>Learning rate:</p><ul><li>Increased linearly from zero to a peak value over the first 375M tokens.</li><li>Then annealed to 10% of its peak value using a cosine schedule over 260B tokens.</li><li>Training continues at this reduced learning rate.</li></ul></li></ul></li><li><p><strong>Fine-tuning</strong>: GPT-2 and GPT-3 models were not explicitly fine-tuned for specific tasks. Instead, they leveraged in-context learning, relying on zero-shot or few-shot prompting to adapt to various tasks without the need for task-specific fine-tuning.</p></li></ul><figure class=align-center><img loading=lazy src=../gpt-3-eval.png#center alt="Comparison of fine-tuning with zero-shot, one-shot, and few-shot learning using an English-to-French translation example."><figcaption><p>Comparison of fine-tuning with zero-shot, one-shot, and few-shot learning using an English-to-French translation example.</p></figcaption></figure><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Andrej Karpathy&rsquo;s video: Let&rsquo;s reproduce GPT-2 (124M)</a></li><li>[1] Radford et al., &ldquo;<a href=https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>Language Models are Unsupervised Multitask Learners</a>&rdquo;, OpenAI 2019.</li><li>[2] Tom Brown et al., &ldquo;<a href=https://arxiv.org/abs/2005.14165>Language Models are Few-Shot Learners
</a>&rdquo;, NeurIPS 2020.</li><li>[3] Xiong et al., &ldquo;<a href=https://arxiv.org/abs/2002.04745>On Layer Normalization in the Transformer Architecture</a>&rdquo;, ICML 2020.</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://yugajmera.github.io/posts/07-vit/post/><span class=title>« Prev</span><br><span>Transformers for Image Classification: ViT and CLIP</span>
</a><a class=next href=https://yugajmera.github.io/posts/06-gpt-tokenizer/post/><span class=title>Next »</span><br><span>GPT Series Part 2: Implementing BPE Tokenizer</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://yugajmera.github.io/>YA's Almanac</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>