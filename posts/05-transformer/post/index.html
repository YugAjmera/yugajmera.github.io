<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Generalizing Attention with Transformers | YA's Almanac</title>
<meta name=keywords content><meta name=description content="In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to &ldquo;attend&rdquo; to different parts of the input at each time step while generating the output sequence.
Attention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption."><meta name=author content><link rel=canonical href=https://yugajmera.github.io/posts/05-transformer/post/><link crossorigin=anonymous href=https://yugajmera.github.io/assets/css/stylesheet.74991b51f7611c2303d0b5649703d675530589621885eb78236e099c22aa93a5.css integrity="sha256-dJkbUfdhHCMD0LVklwPWdVMFiWIYhet4I24JnCKqk6U=" rel="preload stylesheet" as=style><link rel=icon href=https://yugajmera.github.io/assets/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yugajmera.github.io/assets/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yugajmera.github.io/assets/favicon-32x32.png><link rel=apple-touch-icon href=https://yugajmera.github.io/assets/apple-touch-icon.png><link rel=mask-icon href=https://yugajmera.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yugajmera.github.io/posts/05-transformer/post/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>mjx-container[display=true]{margin:1.5em 0!important}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S759YBMKJE",{anonymize_ip:!1})}</script><meta property="og:title" content="Generalizing Attention with Transformers"><meta property="og:description" content="In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to &ldquo;attend&rdquo; to different parts of the input at each time step while generating the output sequence.
Attention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption."><meta property="og:type" content="article"><meta property="og:url" content="https://yugajmera.github.io/posts/05-transformer/post/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-11-18T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-18T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Generalizing Attention with Transformers"><meta name=twitter:description content="In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to &ldquo;attend&rdquo; to different parts of the input at each time step while generating the output sequence.
Attention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yugajmera.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Generalizing Attention with Transformers","item":"https://yugajmera.github.io/posts/05-transformer/post/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Generalizing Attention with Transformers","name":"Generalizing Attention with Transformers","description":"In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to \u0026ldquo;attend\u0026rdquo; to different parts of the input at each time step while generating the output sequence.\nAttention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption.","keywords":[],"articleBody":"In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to “attend” to different parts of the input at each time step while generating the output sequence.\nAttention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption.\nImage captioning task with an RNN decoder with Attention.\nGiven its usefulness, let’s abstract the attention mechanism from sequence modeling and generalize it into a layer that can be inserted into any network.\nGeneral Attention Layer (left) Attention mechanism in the image captioning task. (right) Generalized attention mechanism represented with vectors.\nRecall that in the attention mechanism:\nInput:\nFeatures: $\\mathbf{z}$ (Shape: $\\text{H} \\times \\text{W} \\times \\text{D}$) Hidden state: $\\mathbf{h}$ (Shape: $\\text{D}$) Similarity function: $f_{\\text{att}}$ Operations:\nAlignment: $e_{i, j} = f_{\\text{att}} (\\mathbf{h}, \\mathbf{z}_{i,j})$ Attention: $a = \\text{softmax} (e) $ Output:\nContext vector: $\\mathbf{c} = \\sum_{i, j} \\text{ } a_{i,j} \\text{ } \\mathbf{z}_{i,j}$ (Shape: $\\text{D}$) This mechanism can be generalized to operate on any set of vectors, making it more broadly applicable in deep learning.\nTo formalize this generalization of the attention mechanism, let’s redefine its components:\nThe input features are now represented as a set of vectors, $\\mathbf{x}$, with shape $(\\text{N} \\times \\text{D})$, where $\\text{N} = \\text{H} \\times \\text{W}$. These vectors are the elements we want to attend over.\n$\\text{N}$ represents the number of vectors $\\text{D}$ represents the dimension of each vector. The hidden state of the decoder is renamed as a query vector, $\\mathbf{q}$ (Shape: $\\text{D}$).\nThe similarity function $f_{\\text{att}}$, typically implemented as a Multi-Layer Perceptron (MLP), compares the query vector to each input vector.\nThe output context vector is denoted as $\\mathbf{y}$.\nOur general attention mechanism now looks like this:\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}$) Query vector: $\\mathbf{q}$ (Shape: $\\text{D}$) Similarity function: $f_{\\text{att}}$ Operations:\nAlignment: $e_{j} = f_{\\text{att}} (\\mathbf{q}, \\mathbf{x}_{j})$ Attention: $a = \\text{softmax} (e) $ Output:\nContext vector: $\\mathbf{y} = \\sum_j a_{j} \\text{ } \\mathbf{x}_{j}$ (Shape: $\\text{D}$) Modifications Scaled dot product for similarity function The similarity function we used earlier is called additive attention, where a feed-forward network with a single hidden layer computes the compatibility function between query vectors and input vectors.\nA more commonly used alternative is dot-product (multiplicative) attention, where the query vector and input vectors are combined using a dot product. It can also looked as a measure of similarity as a dot product quantifies how closely two vectors are aligned.\nWhile both methods have similar theoretical complexity, dot-product attention is computationally more efficient as it can be implemented using optimized matrix multiplication code.\nHowever, when the vector dimension $\\text{D}$ is large, which is typically greater than 1000 for large language models (LLMs), the resulting alignment scores can have high magnitudes. Since the softmax function normalizes these scores to compute attention weights, large magnitudes can cause softmax to saturate, leading to vanishing gradients during backpropagation.\nLet’s look at the example below to understand this better,\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1) tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872]) This looks as expected. But when the magnitudes are large, it saturates and converges to a one-hot encoding.\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*1000, dim=-1) tensor([0., 0., 0., 0., 1.]) To mitigate this, we scale the dot product by $\\frac{1}{\\sqrt{\\text{D}}}$. This adjustment reduces the impact of large vector magnitudes, similar to initialization techniques like Xavier or Kaiming. This approach, known as scaled dot-product attention, is widely used in modern models.\nMultiple Query vectors At each time step of the decoder, we use one query vector (i.e., one hidden state) to compute a probability distribution over the inputs, producing one context vector.\nWe can generalize this concept to handle multiple query vectors simultaneously, each generating a corresponding output vector. This allows us to compute multiple attention context vectors in parallel.\n(left) A general attention layer. (right) The same layer with multiple query vectors.\nWith this modification, the attention layer includes:\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}$) Query vectors: $\\mathbf{q}$ (Shape: $\\text{M} \\times \\text{D}$) Similarity function: scaled dot product Operations:\nAlignment: $e_{i, j} = (\\mathbf{q}_i \\cdot \\mathbf{x}_{j} )$ / $\\sqrt{\\text{D}}$ (Shape: $\\text{M} \\times \\text{N}$) Attention: $a = \\text{softmax} (e) $ Context vectors: $\\mathbf{y}_i = \\sum_j a_{i,j} \\text{ } \\mathbf{x}_{j}$ Output:\nOutput vectors: $\\mathbf{y}$ (Shape: $\\text{M} \\times \\text{D}$) Key and Value vectors In the attention mechanism, we use the input vectors in two different ways:\nTo generate the alignment scores that compare the input vectors with each query vector via the similarity function.\nTo compute the output context vectors by taking a weighted sum of input vectors and the attention weights.\nTo handle these roles effectively, the input vectors are separated into key vectors ($\\mathbf{k}$) and value vectors ($\\mathbf{v}$). Both are derived using learnable projection matrices applied to the input vectors:\nKeys are used to compute alignment scores with the query. Values are used to construct the context vector. The separation of keys and values enables the model to use input vectors differently for comparison and retrieval. For example:\nQuery - What am I looking for? Google search: “How tall is the Empire State Building?” Keys - What do I contain? Google compares the query with a set of webpages that may contain the answer. Values - Information in the token that will be communicated Returns the webpage saying, “At its top floor, the Empire State Building stands 1,250 feet (380 meters) tall.” Since the information used for matching (keys) is different from the information returned (values), we separate them into two distinct vectors. The key determines relevance or alignment, whereas the value is used to retrieve the actual information. This gives the model flexibility in handling different types of information.\n(left) General attention layer with multiple query vectors. (right) The same layer with separate key and value vectors.\nWith these modifications, the attention layer is described as follows:\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}$) Query vectors: $\\mathbf{q}$ (Shape: $\\text{M} \\times \\color{blue}{\\text{D}_k}$) Similarity function: scaled dot product Operations:\nKey vectors: ${\\color{blue}{\\mathbf{k}}} = \\mathbf{x} {\\color{blue}{W_\\mathbf{k}}}$ (Shape: $\\text{N} \\times \\color{blue}{\\text{D}_k}$) Value vectors: ${\\color{orange}{\\mathbf{v}}} = \\mathbf{x} {\\color{orange}{W_\\mathbf{v}}}$ (Shape: $\\text{N} \\times {\\color{orange}{\\text{D}_v}}$) Alignment: $e_{i, j} = (\\mathbf{q}_i \\cdot {\\color{blue}{\\mathbf{k}_j}} )$ / $\\sqrt{\\color{blue}{\\text{D}_k}}$ Attention: $a = \\text{softmax} (e) $ Context vectors: $\\mathbf{y}_i = \\sum_j a_{i,j} \\text{ } {\\color{orange}{\\mathbf{v}_j}}$ Output:\nOutput vectors: $\\mathbf{y}$ (Shape: $\\text{M} \\times {\\color{orange}{\\text{D}_v}}$) Matrix Representation The entire attention mechanism can be expressed compactly in matrix form:\n\\begin{align} \\text{Attention} (\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax} (\\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{\\text{D}_k}}) \\mathbf{V} \\end{align}\nWhere:\n$\\mathbf{Q}$: Matrix of query vectors (Shape: $\\text{M} \\times \\text{D}_k$) $\\mathbf{K}$: Matrix of key vectors (Shape: $\\text{N} \\times \\text{D}_k$) $\\mathbf{V}$: Matrix of value vectors (Shape: $\\text{N} \\times \\text{D}_v$) This is the most common representation of attention that we’ve derived so far.\nNotably, the attention mechanism itself has no learnable parameters, and the attention weights are not learned; instead, they are computed using a simple scaled dot product function followed by a softmax operation.\nSelf-Attention layer One special case of the attention layer is the self-attention layer, where we only have input vectors and no explicit query vectors. In this case, we use a query matrix to derive the query vectors from our input vectors.\nSince each input vector serves as its own query, we end up comparing each vector in the input set with every other vector. This design leverages the power of attention while eliminating the need for external query vectors, enabling the model to capture relationships between different parts of the input.\nSelf-attention enhances input representations by allowing each position in a sequence to interact with and weigh the importance of all other positions within the same sequence.\nA self-attention layer\nIn order to seperate the input and output dimensions, we denote $\\text{D}_k = \\text{D}_v = \\text{D}_{out}$.\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}_{in}$) Similarity function: scaled dot product Operations:\nKey vectors: ${\\color{blue}{\\mathbf{k}}} = \\mathbf{x} {\\color{blue}{W_\\mathbf{k}}}$ (Shape: $\\text{N} \\times \\text{D}_{out}$)\nValue vectors: ${\\color{orange}{\\mathbf{v}}} = \\mathbf{x} {\\color{orange}{W_\\mathbf{v}}}$ (Shape: $\\text{N} \\times \\text{D}_{out}$)\nQuery vectors: ${\\color{green}{\\mathbf{q}}} = \\mathbf{x} {\\color{green}{W_\\mathbf{q}}}$ (Shape: $\\text{N} \\times \\text{D}_{out}$)\nAlignment: $e_{i, j} = ({\\color{green}{\\mathbf{q}_i}} \\cdot {\\color{blue}{\\mathbf{k}_j}} ) / \\sqrt{\\text{D}}$\nAttention: $a = \\text{softmax} (e) $\nContext vectors: $\\mathbf{y}_i = \\sum_j a_{i,j} \\text{ } {\\color{orange}{\\mathbf{v}_j}}$\nOutput:\nOutput vectors: $\\mathbf{y}$ (Shape: $\\text{N} \\times \\text{D}_{out}$) This forms a new type of neural network layer, where we input a set of vectors and output another set of vectors, effectively allowing the model to attend to different parts of its own input.\nLet’s take a look at how this would be implemented in code.\nclass SelfAttention(nn.Module): def __init__(self, d_in, d_out, qkv_bias=False): super().__init__() self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias) def forward(self, x): # Input vectors x - (B, N, d_in) # Obtain keys, values and queries - (B, N, d_out) keys = self.W_k(x) values = self.W_v(x) queries = self.W_q(x) # Apply scaled dot-product attention attn_scores = (queries @ keys.T) * keys.shape[-1]**-0.5 attn_weights = torch.softmax(attn_scores, dim=-1) context_vec = attn_weights @ values return context_vec The bias term is omitted in the weight matrices to ensure pure matrix multiplication.\nSetting dim=-1 in the softmax function instructs it to normalize along the last dimension, which corresponds to the columns (since attn_scores has the shape [batch, row N, column N]), ensuring that the values in each row sum to 1.\nPositional encoding A key consideration with the self-attention layer is that it is permutation equivariant. This means that if we change the order of the input vectors, we would still compute the same key, value, and query vectors, but they would be permuted in the same way the input vectors were permuted. As a result, the set of output vectors would remain the same, but their order would change.\nSelf-attention layer is permutation equivariant $f(s(x)) = s(f(x))$.\nThe self-attention layer does not inherently account for the order of the input vectors; it processes them as a set, irrespective of their sequence. While this property works well for certain tasks, it poses a challenge for tasks like machine translation or text generation, where the order of tokens is crucial.\nFor example, the sentences “The dog chased the cat” and “The cat chased the dog” would appear identical to the transformer but convey different meanings.\nTo make the layer position-aware, positional encodings are added to the input vectors. These encodings capture the position of each element in the sequence. A function ${\\color{purple}{pos}}: \\text{N} \\rightarrow \\mathbb{R}^D$ transforms each position $j$ into a unique, $D$-dimensional positional vector.\nConcatenate special positional encoding $\\color{purple}{p_j}$ to each input vector $\\mathbf{x}_j$\nThere are two common ways to obtain this positional encoding function:\nLearnable Lookup Table:\nA lookup table is learned during training that assigns a unique encoding to each position. This approach learns parameters for each position $t \\in [0, N)$, where $T$ is the maximum sequence length, leading to a lookup table of size $N \\times D$. Fixed Function:\nA fixed function is designed that outputs a unique, deterministic encoding for each position. This approach doesn’t require any learnable parameters. Masked Self-Attention layer A variant of the self-attention layer, called masked self-attention or causal attention, is used for tasks like language modeling, where the goal is to predict the next word given the previous words. This is similar to a decoder RNN, where new words in the output sequence are generated one by one, with previously generated words providing context.\nWith the standard self-attention layer, the model can attend to all input tokens at once, which isn’t ideal for such tasks. To make the model “look” only at previous words while generating the next one, we mask future tokens. This prevents the model from attending to words that come after the current position in the sequence.\nTo achieve this, we set the attention weights for all future tokens to zero, ensuring that the model can only attend to current and past tokens when computing the context vector. This is particularly useful in decoders, where sequences are generated step-by-step.\nA Masked self-attention layer (Causal Attention).\nRather than zeroing out the attention weights of future tokens and renormalizing them, we can assign negative infinity to those positions and apply softmax directly. This ensures they receive zero probability while maintaining a row sum of one—all in a single step.\nThe mask will have a shape of $\\text{N} \\times \\text{N}$, where $\\text{N}$ represents the number of tokens in the sequence. We use PyTorch’s tril function to create a mask where values above the diagonal are zero. These positions are then replaced with negative infinity in the attention scores matrix.\nLet’s modify our code for causal attention.\nclass CausalAttention(nn.Module): def __init__(self, d_in, d_out, context_length, attn_pdrop, qkv_bias=False): super().__init__() self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias) self.attn_dropout = nn.Dropout(attn_pdrop) # Create a causal mask self.register_buffer(\"mask\", torch.tril(torch.ones(context_length, context_length))) def forward(self, x): # Input vectors x - (B, N, d_in) B, N, d_in = x.shape # Obtain keys, values and queries - (B, N, d_out) keys = self.W_k(x) values = self.W_v(x) queries = self.W_q(x) # Apply scaled dot-product attention with causal mask attn_scores = (queries @ keys.T) * keys.shape[-1]**-0.5 attn_scores = attn_scores.masked_fill(self.mask[:N, :N] == 0, float('-inf')) attn_weights = torch.softmax(attn_scores, dim=-1) attn_weights = self.attn_dropout(attn_weights) context_vec = attn_weights @ values return context_vec Context length refers to the maximum number of tokens allowed in any sequence (we will explore this in detail in the next post). It helps define the largest mask, which can then be applied to sequences of any length in our training data.\nWe have also used a register_buffer here, which ensures that the buffer variable masked is automatically moved to the appropriate device (CPU or GPU) along with the model. PyTorch only moves parameters, such as weights, to the selected device when we run model.to(device), which in our case is the GPU.\nSince masked is used in our forward computation, defining it as a regular variable would cause a device mismatch error because it would remain on the CPU while the model operates on the GPU. To avoid this issue, we register it as a buffer, ensuring it stays synchronized with the model’s device.\nIt is also common practice to zero out additional elements in the attention weight matrix by applying a dropout layer, also called attention dropout. As you may recall, dropout is a regularization technique that helps prevent overfitting. Since some attention weights are zeroed out based on the dropout probability, the remaining weights are re-scaled to compensate for the reduction. Note that this dropout layer is disabled during inference.\nMulti-head Self-Attention layer Instead of performing a single attention function over the entire input vector space, the multi-head self-attention mechanism splits the input into multiple representation subspaces and performs attention in parallel across these subspaces. This allows the model to attend to different aspects of the input simultaneously.\nIn this case, we divide each input vector into $H$ chunks of equal size and feed them into several parallel attention layers. The input to each head has dimension $\\text{d}_{h} = \\text{D}/H$.\nThe outputs from all attention heads are concatenated (dimension $H \\cdot \\text{d}_h$) and passed through a projection linear layer to produce the final output of dimension $\\text{D}_{out}$. While this projection layer is not strictly necessary, it is commonly used in many LLM architectures.\nA Multi-head self-attention layer\nThe above image may seem more intuitive, but it is not optimal for computation. If we first split the input vectors $\\mathbf{x}$, we would need to compute the keys, values, and queries independently for each head, increasing computation.\nInstead, a more efficient approach is to compute the keys, values, and queries for the entire input vector dimension first, and then split them into $H$ equal chunks. After computing attention independently for each head, we combine the results.\nInput:\nInput vectors: $\\mathbf{x}$ (Shape: $\\text{N} \\times \\text{D}_{in}$) Similarity function: scaled dot product Operations:\nKey vectors: ${\\color{blue}{\\mathbf{k}}} = \\mathbf{x} {\\color{blue}{W_\\mathbf{k}}}$ (Shape: $\\text{N} \\times \\text{D}_{out}$)\nValue vectors: ${\\color{orange}{\\mathbf{v}}} = \\mathbf{x} {\\color{orange}{W_\\mathbf{v}}}$ (Shape: $\\text{N} \\times \\text{D}_{out}$)\nQuery vectors: ${\\color{green}{\\mathbf{q}}} = \\mathbf{x} {\\color{green}{W_\\mathbf{q}}}$ (Shape: $\\text{N} \\times \\text{D}_{out}$)\nSplit key, value and query vectors. For each head:\nAlignment: $e_{i, j} = (\\mathbf{q}_i \\cdot \\mathbf{k}_j ) / \\sqrt{\\text{d}_{h}}$ Attention: $a = \\text{softmax} (e) $ Output vectors: ${\\mathbf{y}_i} = \\sum_j a_{i,j} \\text{ } \\mathbf{v}_j$ (Shape: $\\text{N} \\times \\text{d}_{h}$) Output:\nOutput vectors: $\\mathbf{y} = \\text{Concat} (\\text{y}^0, \\cdots, \\text{y}^{\\text{H} - 1}) W_o$ (Shape: $\\text{N} \\times \\text{D}_{out}$) Now, let’s implement a multi-head attention module that runs several causal attention heads in parallel.\nclass MultiHeadCausalAttention(nn.Module): def __init__(self, d_in, d_out, context_length, attn_pdrop, num_heads, qkv_bias=False): super().__init__() self.W_k = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_v = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_q = nn.Linear(d_in, d_out, bias=qkv_bias) self.W_o = nn.Linear(d_out, d_out) self.attn_dropout = nn.Dropout(attn_pdrop) self.d_h = d_out // num_heads # head dimension self.num_heads = num_heads self.register_buffer(\"masked\", torch.tril(torch.ones(context_length, context_length)).view(1, 1, context_length, context_length)) def forward(self, x): # Input vectors x - (B, N, d_in) B, N, d_in = x.shape # Obtain keys, values and queries - (B, N, d_out) keys = self.W_k(x) values = self.W_v(x) queries = self.W_q(x) # Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h) k = keys.view(B, N, self.num_heads, self.d_h).transpose(1, 2) v = values.view(B, N, self.num_heads, self.d_h).transpose(1, 2) q = queries.view(B, N, self.num_heads, self.d_h).transpose(1, 2) # Apply scaled dot-product attention with causal mask on each head attn_scores = (q @ k.transpose(2, 3)) * k.shape[-1]**-0.5 attn_scores = attn_scores.masked_fill(self.masked[:, :, :N, :N] == 0, float('-inf')) attn_weights = torch.softmax(attn_scores, dim=-1) attn_weights = self.attn_dropout(attn_weights) context_vec = attn_weights @ v # Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out) out = context_vec.transpose(1, 2).contiguous().view(B, N, -1) out = self.W_o(out) return out The splitting of key, value, and query tensors is achieved through tensor reshaping and transposition. First, we reshape them to introduce the num_heads dimension, then transpose to bring num_heads before the num_tokens dimension. This allows us to compute attention in parallel across all heads using batched matrix multiplication.\nAfter computing attention, the context vectors from all heads are transposed back to their original shape and flattened, effectively combining the outputs from all heads.\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, the attention mechanism would average over these subspaces, potentially losing valuable information.\nAdditionally, since each head operates on a reduced dimension, the total computational cost remains comparable to that of single-head attention with full dimensionality.\nTransformer The Transformer [1] was the first sequence model that relies solely on self-attention to compute representations of its input and output without using any recurrent units. This innovation marked a turning point for natural language processing (NLP), outperforming all state-of-the-art models of its time. It is often referred to as the “ImageNet moment” for NLP.\nLet’s take a closer look at the complete architecture of the Transformer.\nEmbeddings and Positional Encoding The input and output word tokens are converted into vectors of dimension $\\text{D} = d_{\\text{model}} = 512$ by learned embeddings.\nNext, positional encodings are added to input embedding vectors to inject information about the position of the tokens in the sequence. A fixed sinusoidal function is used to compute the positional encodings, which are of the same dimension $d_\\text{model}$ as the embeddings, so that the two can be summed together.\nEncoder block The encoder is composed of a stack of $L = 6$ identical layers, each consisting of two sub-layers:\nMulti-head Self-Attention: Each output from this layer depends on every input, allowing for interactions between all vectors in the input sequence. Since the inputs come from the previous encoder layer, each position in the encoder can attend to all positions in the previous layer.\nHyperparamters: $H = 8$, $\\text{D}_{in} = \\text{D}_{out} = d_{model} = 512$ Feed-forward network: Each input vector passes through an MLP independently, consisting of two linear layers and a ReLU activation in between. This layer internally expands the embedding dimension into a higer-dimensional space (factor of 4), which allows for exploration of a richer representation space.\nLinear 1 $(512, 2048)$ -\u003e ReLU -\u003e Linear 2 $(2048, 512)$ Additionally:\nResidual Dropout: Each of the two sublayers is followed by a dropout layer. Additionally, dropout is applied to the sum of the embeddings and position encodings. $p = 0.1$.\nResidual connection: A residual connection is used around each of the two sub-layers to improve the gradient flow through the model, and overcome the vanishing gradient problem.\nLayer Normalization: Each sub-layer is followed by layer normalization to aid optimization (similar to BatchNorm in convolutional layers).\nThe Encoder of the Transformer.\nInput: A set of vectors $\\mathbf{x}$ (Shape: $\\text{N} \\times 512$) Output: A set of vectors $\\mathbf{y}$ (Shape: $\\text{N} \\times 512$) The interaction between vectors occurs only in the self-attention layer. LayerNorm and MLP operate on each input vector independently. The uniformity in input and output dimensions enables the stacking of multiple layers, thus making the model more scalable.\nDecoder block The decoder consists of $L = 6$ identical layers, each with three sub-layers:\nMasked Multi-head Self-Attention: To prevent the decoder from attending to future tokens in the output sequence, we use masked self-attention, which ensures that each position in the decoder can only attend to past and current tokens.\nHyperparamters: $H = 8$, $\\text{D}_{in} = \\text{D}_{out} = 512$. Multi-head Cross-Attention over Encoder outputs: This layer allows each position in the decoder to attend to all positions in the input sequence, passing relevant context from the encoder. This mimics the traditional encoder-decoder attention mechanism used in seq2seq models.\nHyperparameters: $H = 8$, $\\text{D}_{in} = \\text{D}_{out} = 512$. Feed-forward network: Same structure as in the encoder:\nLinear 1 (512, 2048) -\u003e ReLU -\u003e Linear 2 (2048, 512) Similar to the encoder, each sublayer is followed by a dropout layer, with residual connections added around each one. Finally, layer normalization is applied to each vector independently.\nThe Decoder of the Transformer.\nInput: Decoder sequence: A set of vectors $\\mathbf{x}$ (Shape: $\\text{M} \\times 512$) Encoder context: A set of context vectors $\\mathbf{c}$ (Shape: $\\text{N} \\times 512$) Output: A set of vectors $\\mathbf{y}$ (Shape: $\\text{M} \\times 512$) The masked self-attention sub-layer ensures autoregressive behavior by restricting attention to past inputs. The multi-head attention over encoder outputs bridges the encoder and decoder, allowing the decoder to focus on relevant parts of the input sequence.\nThe decoder block is followed by a linear layer and softmax function to convert the decoder output into predicted next-token probabilities.\nDuring inference, the decoder sequence begins with a \u003c START \u003e token embedding (Shape: $1 \\times 512$). As the model predicts each subsequent token, we append it to this sequence.\nKey characteristics Parallel computation: Unlike RNNs, the Transformer processes entire sequences simultaneously, allowing alignment and attention scores for all inputs to be computed in parallel, significantly improving efficiency on large datasets.\nFlexibility of inputs: It can effectively handle both unordered sets and ordered sequences (with positional encodings).\nGlobal context: Self-attention enables the model to capture long-range dependencies across the entire sequence.\nScalability: The Transformer’s architecture is highly scalable, with a few key hyperparameters that can be adjusted to meet various requirements:\nNumber of Layers $L$: Applies equally to the encoder and decoder. Hidden size $d_{\\text{model}}$: Defines the dimensionality of the model. MLP size $d_{ff}$: Specifies the output size of the first layer in the feed-forward MLP. Heads $H$: Determines the number of attention heads in multi-head self-attention (encoder), masked multi-head self-attention (decoder), and multi-head cross-attention (decoder). The Transformer Architecture.\nThe Transformer’s innovative architecture has become the foundation for many advancements in NLP, inspiring models like BERT and GPT, and driving us in the new era of LLMs.\nReferences [1] Vaswani et al, “Attention is all you need”, NeurIPS 2017. Read more on Pytorch Buffers here: Understanding PyTorch Buffers ","wordCount":"3928","inLanguage":"en","datePublished":"2024-11-18T00:00:00Z","dateModified":"2024-11-18T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://yugajmera.github.io/posts/05-transformer/post/"},"publisher":{"@type":"Organization","name":"YA's Almanac","logo":{"@type":"ImageObject","url":"https://yugajmera.github.io/assets/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yugajmera.github.io/ accesskey=h title="YA's Almanac (Alt + H)">YA's Almanac</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yugajmera.github.io/ title=Home><span>Home</span></a></li><li><a href=https://yugajmera.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://yugajmera.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Generalizing Attention with Transformers</h1><div class=post-meta><span title='2024-11-18 00:00:00 +0000 UTC'>November 18, 2024</span>&nbsp;·&nbsp;19 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#general-attention-layer aria-label="General Attention Layer">General Attention Layer</a><ul><li><a href=#modifications aria-label=Modifications>Modifications</a><ul><li><a href=#scaled-dot-product-for-similarity-function aria-label="Scaled dot product for similarity function">Scaled dot product for similarity function</a></li><li><a href=#multiple-query-vectors aria-label="Multiple Query vectors">Multiple Query vectors</a></li><li><a href=#key-and-value-vectors aria-label="Key and Value vectors">Key and Value vectors</a></li></ul></li><li><a href=#matrix-representation aria-label="Matrix Representation">Matrix Representation</a></li></ul></li><li><a href=#self-attention-layer aria-label="Self-Attention layer">Self-Attention layer</a><ul><li><a href=#positional-encoding aria-label="Positional encoding">Positional encoding</a></li><li><a href=#masked-self-attention-layer aria-label="Masked Self-Attention layer">Masked Self-Attention layer</a></li><li><a href=#multi-head-self-attention-layer aria-label="Multi-head Self-Attention layer">Multi-head Self-Attention layer</a></li></ul></li><li><a href=#transformer aria-label=Transformer>Transformer</a><ul><li><a href=#embeddings-and-positional-encoding aria-label="Embeddings and Positional Encoding">Embeddings and Positional Encoding</a></li><li><a href=#encoder-block aria-label="Encoder block">Encoder block</a></li><li><a href=#decoder-block aria-label="Decoder block">Decoder block</a></li><li><a href=#key-characteristics aria-label="Key characteristics">Key characteristics</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to &ldquo;attend&rdquo; to different parts of the input at each time step while generating the output sequence.</p><p>Attention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption.</p><figure class=align-center><img loading=lazy src=../image-captioning.png#center alt="Image captioning task with an RNN decoder with Attention."><figcaption><p>Image captioning task with an RNN decoder with Attention.</p></figcaption></figure><p>Given its usefulness, let&rsquo;s abstract the attention mechanism from sequence modeling and generalize it into a layer that can be inserted into any network.</p><h2 id=general-attention-layer>General Attention Layer<a hidden class=anchor aria-hidden=true href=#general-attention-layer>#</a></h2><figure class=align-center><img loading=lazy src=../attention-layer-1.png#center alt="(left) Attention mechanism in the image captioning task. (right) Generalized attention mechanism represented with vectors."><figcaption><p>(left) Attention mechanism in the image captioning task. (right) Generalized attention mechanism represented with vectors.</p></figcaption></figure><p>Recall that in the attention mechanism:</p><ul><li><p>Input:</p><ul><li>Features: $\mathbf{z}$ (Shape: $\text{H} \times \text{W} \times \text{D}$)</li><li>Hidden state: $\mathbf{h}$ (Shape: $\text{D}$)</li><li>Similarity function: $f_{\text{att}}$</li></ul></li><li><p>Operations:</p><ul><li>Alignment: $e_{i, j} = f_{\text{att}} (\mathbf{h}, \mathbf{z}_{i,j})$</li><li>Attention: $a = \text{softmax} (e) $</li></ul></li><li><p>Output:</p><ul><li>Context vector: $\mathbf{c} = \sum_{i, j} \text{ } a_{i,j} \text{ } \mathbf{z}_{i,j}$ (Shape: $\text{D}$)</li></ul></li></ul><p>This mechanism can be generalized to operate on any set of vectors, making it more broadly applicable in deep learning.</p><p>To formalize this generalization of the attention mechanism, let’s redefine its components:</p><ol><li><p>The input features are now represented as a set of vectors, $\mathbf{x}$, with shape $(\text{N} \times \text{D})$, where $\text{N} = \text{H} \times \text{W}$. These vectors are the elements we want to attend over.</p><ul><li>$\text{N}$ represents the number of vectors</li><li>$\text{D}$ represents the dimension of each vector.</li></ul></li><li><p>The hidden state of the decoder is renamed as a query vector, $\mathbf{q}$ (Shape: $\text{D}$).</p></li><li><p>The similarity function $f_{\text{att}}$, typically implemented as a Multi-Layer Perceptron (MLP), compares the query vector to each input vector.</p></li><li><p>The output context vector is denoted as $\mathbf{y}$.</p></li></ol><p>Our general attention mechanism now looks like this:</p><ul><li><p>Input:</p><ul><li>Input vectors: $\mathbf{x}$ (Shape: $\text{N} \times \text{D}$)</li><li>Query vector: $\mathbf{q}$ (Shape: $\text{D}$)</li><li>Similarity function: $f_{\text{att}}$</li></ul></li><li><p>Operations:</p><ul><li>Alignment: $e_{j} = f_{\text{att}} (\mathbf{q}, \mathbf{x}_{j})$</li><li>Attention: $a = \text{softmax} (e) $</li></ul></li><li><p>Output:</p><ul><li>Context vector: $\mathbf{y} = \sum_j a_{j} \text{ } \mathbf{x}_{j}$ (Shape: $\text{D}$)</li></ul></li></ul><h3 id=modifications>Modifications<a hidden class=anchor aria-hidden=true href=#modifications>#</a></h3><h4 id=scaled-dot-product-for-similarity-function>Scaled dot product for similarity function<a hidden class=anchor aria-hidden=true href=#scaled-dot-product-for-similarity-function>#</a></h4><p>The similarity function we used earlier is called additive attention, where a feed-forward network with a single hidden layer computes the compatibility function between query vectors and input vectors.</p><p>A more commonly used alternative is dot-product (multiplicative) attention, where the query vector and input vectors are combined using a dot product. It can also looked as a measure of similarity as a dot product quantifies how closely two vectors are aligned.</p><p>While both methods have similar theoretical complexity, dot-product attention is computationally more efficient as it can be implemented using optimized matrix multiplication code.</p><p>However, when the vector dimension $\text{D}$ is large, which is typically greater than 1000 for large language models (LLMs), the resulting alignment scores can have high magnitudes. Since the softmax function normalizes these scores to compute attention weights, large magnitudes can cause softmax to saturate, leading to vanishing gradients during backpropagation.</p><p>Let&rsquo;s look at the example below to understand this better,</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>0.1</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>]),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])
</code></pre><p>This looks as expected. But when the magnitudes are large, it saturates and converges to a one-hot encoding.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>0.1</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.3</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.2</span><span class=p>,</span> <span class=mf>0.5</span><span class=p>])</span><span class=o>*</span><span class=mi>1000</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>tensor([0., 0., 0., 0., 1.])
</code></pre><p>To mitigate this, we scale the dot product by $\frac{1}{\sqrt{\text{D}}}$. This adjustment reduces the impact of large vector magnitudes, similar to initialization techniques like Xavier or Kaiming. This approach, known as scaled dot-product attention, is widely used in modern models.</p><h4 id=multiple-query-vectors>Multiple Query vectors<a hidden class=anchor aria-hidden=true href=#multiple-query-vectors>#</a></h4><p>At each time step of the decoder, we use one query vector (i.e., one hidden state) to compute a probability distribution over the inputs, producing one context vector.</p><p>We can generalize this concept to handle multiple query vectors simultaneously, each generating a corresponding output vector. This allows us to compute multiple attention context vectors in parallel.</p><figure class=align-center><img loading=lazy src=../attention-layer-2.png#center alt="(left) A general attention layer. (right) The same layer with multiple query vectors."><figcaption><p>(left) A general attention layer. (right) The same layer with multiple query vectors.</p></figcaption></figure><p>With this modification, the attention layer includes:</p><ul><li><p>Input:</p><ul><li>Input vectors: $\mathbf{x}$ (Shape: $\text{N} \times \text{D}$)</li><li>Query vectors: $\mathbf{q}$ (Shape: $\text{M} \times \text{D}$)</li><li>Similarity function: scaled dot product</li></ul></li><li><p>Operations:</p><ul><li>Alignment: $e_{i, j} = (\mathbf{q}_i \cdot \mathbf{x}_{j} )$ / $\sqrt{\text{D}}$ (Shape: $\text{M} \times \text{N}$)</li><li>Attention: $a = \text{softmax} (e) $</li><li>Context vectors: $\mathbf{y}_i = \sum_j a_{i,j} \text{ } \mathbf{x}_{j}$</li></ul></li><li><p>Output:</p><ul><li>Output vectors: $\mathbf{y}$ (Shape: $\text{M} \times \text{D}$)</li></ul></li></ul><h4 id=key-and-value-vectors>Key and Value vectors<a hidden class=anchor aria-hidden=true href=#key-and-value-vectors>#</a></h4><p>In the attention mechanism, we use the input vectors in two different ways:</p><ol><li><p>To generate the alignment scores that compare the input vectors with each query vector via the similarity function.</p></li><li><p>To compute the output context vectors by taking a weighted sum of input vectors and the attention weights.</p></li></ol><p>To handle these roles effectively, the input vectors are separated into key vectors ($\mathbf{k}$) and value vectors ($\mathbf{v}$). Both are derived using learnable projection matrices applied to the input vectors:</p><ul><li>Keys are used to compute alignment scores with the query.</li><li>Values are used to construct the context vector.</li></ul><p>The separation of keys and values enables the model to use input vectors differently for comparison and retrieval. For example:</p><ul><li>Query - <em>What am I looking for?</em><ul><li>Google search: &ldquo;How tall is the Empire State Building?&rdquo;</li></ul></li><li>Keys - <em>What do I contain?</em><ul><li>Google compares the query with a set of webpages that may contain the answer.</li></ul></li><li>Values - <em>Information in the token that will be communicated</em><ul><li>Returns the webpage saying, “At its top floor, the Empire State Building stands 1,250 feet (380 meters) tall.”</li></ul></li></ul><p>Since the information used for matching (keys) is different from the information returned (values), we separate them into two distinct vectors. The key determines relevance or alignment, whereas the value is used to retrieve the actual information. This gives the model flexibility in handling different types of information.</p><figure class=align-center><img loading=lazy src=../attention-layer-3.png#center alt="(left) General attention layer with multiple query vectors. (right) The same layer with separate key and value vectors."><figcaption><p>(left) General attention layer with multiple query vectors. (right) The same layer with separate key and value vectors.</p></figcaption></figure><p>With these modifications, the attention layer is described as follows:</p><ul><li><p>Input:</p><ul><li>Input vectors: $\mathbf{x}$ (Shape: $\text{N} \times \text{D}$)</li><li>Query vectors: $\mathbf{q}$ (Shape: $\text{M} \times \color{blue}{\text{D}_k}$)</li><li>Similarity function: scaled dot product</li></ul></li><li><p>Operations:</p><ul><li>Key vectors: ${\color{blue}{\mathbf{k}}} = \mathbf{x} {\color{blue}{W_\mathbf{k}}}$ (Shape: $\text{N} \times \color{blue}{\text{D}_k}$)</li><li>Value vectors: ${\color{orange}{\mathbf{v}}} = \mathbf{x} {\color{orange}{W_\mathbf{v}}}$ (Shape: $\text{N} \times {\color{orange}{\text{D}_v}}$)</li><li>Alignment: $e_{i, j} = (\mathbf{q}_i \cdot {\color{blue}{\mathbf{k}_j}} )$ / $\sqrt{\color{blue}{\text{D}_k}}$</li><li>Attention: $a = \text{softmax} (e) $</li><li>Context vectors: $\mathbf{y}_i = \sum_j a_{i,j} \text{ } {\color{orange}{\mathbf{v}_j}}$</li></ul></li><li><p>Output:</p><ul><li>Output vectors: $\mathbf{y}$ (Shape: $\text{M} \times {\color{orange}{\text{D}_v}}$)</li></ul></li></ul><h3 id=matrix-representation>Matrix Representation<a hidden class=anchor aria-hidden=true href=#matrix-representation>#</a></h3><p>The entire attention mechanism can be expressed compactly in matrix form:</p><p>\begin{align}
\text{Attention} (\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax} (\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{\text{D}_k}}) \mathbf{V}
\end{align}</p><p>Where:</p><ul><li>$\mathbf{Q}$: Matrix of query vectors (Shape: $\text{M} \times \text{D}_k$)</li><li>$\mathbf{K}$: Matrix of key vectors (Shape: $\text{N} \times \text{D}_k$)</li><li>$\mathbf{V}$: Matrix of value vectors (Shape: $\text{N} \times \text{D}_v$)</li></ul><p>This is the most common representation of attention that we’ve derived so far.</p><p>Notably, the attention mechanism itself has no learnable parameters, and the attention weights are not learned; instead, they are computed using a simple scaled dot product function followed by a softmax operation.</p><h2 id=self-attention-layer>Self-Attention layer<a hidden class=anchor aria-hidden=true href=#self-attention-layer>#</a></h2><p>One special case of the attention layer is the self-attention layer, where we only have input vectors and no explicit query vectors. In this case, we use a query matrix to derive the query vectors from our input vectors.</p><p>Since each input vector serves as its own query, we end up comparing each vector in the input set with every other vector. This design leverages the power of attention while eliminating the need for external query vectors, enabling the model to capture relationships between different parts of the input.</p><p>Self-attention enhances input representations by allowing each position in a sequence to interact with and weigh the importance of all other positions within the same sequence.</p><figure class=align-center><img loading=lazy src=../self-attention-layer.png#center alt="A self-attention layer" width=350><figcaption><p>A self-attention layer</p></figcaption></figure><p>In order to seperate the input and output dimensions, we denote $\text{D}_k = \text{D}_v = \text{D}_{out}$.</p><ul><li><p>Input:</p><ul><li>Input vectors: $\mathbf{x}$ (Shape: $\text{N} \times \text{D}_{in}$)</li><li>Similarity function: scaled dot product</li></ul></li><li><p>Operations:</p><ul><li><p>Key vectors: ${\color{blue}{\mathbf{k}}} = \mathbf{x} {\color{blue}{W_\mathbf{k}}}$ (Shape: $\text{N} \times \text{D}_{out}$)</p></li><li><p>Value vectors: ${\color{orange}{\mathbf{v}}} = \mathbf{x} {\color{orange}{W_\mathbf{v}}}$ (Shape: $\text{N} \times \text{D}_{out}$)</p></li><li><p>Query vectors: ${\color{green}{\mathbf{q}}} = \mathbf{x} {\color{green}{W_\mathbf{q}}}$ (Shape: $\text{N} \times \text{D}_{out}$)</p></li><li><p>Alignment: $e_{i, j} = ({\color{green}{\mathbf{q}_i}} \cdot {\color{blue}{\mathbf{k}_j}} ) / \sqrt{\text{D}}$</p></li><li><p>Attention: $a = \text{softmax} (e) $</p></li><li><p>Context vectors: $\mathbf{y}_i = \sum_j a_{i,j} \text{ } {\color{orange}{\mathbf{v}_j}}$</p></li></ul></li><li><p>Output:</p><ul><li>Output vectors: $\mathbf{y}$ (Shape: $\text{N} \times \text{D}_{out}$)</li></ul></li></ul><p>This forms a new type of neural network layer, where we input a set of vectors and output another set of vectors, effectively allowing the model to attend to different parts of its own input.</p><p>Let’s take a look at how this would be implemented in code.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>SelfAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>qkv_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Input vectors x - (B, N, d_in)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Obtain keys, values and queries - (B, N, d_out)</span>
</span></span><span class=line><span class=cl>        <span class=n>keys</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>values</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>queries</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Apply scaled dot-product attention</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_scores</span> <span class=o>=</span> <span class=p>(</span><span class=n>queries</span> <span class=o>@</span> <span class=n>keys</span><span class=o>.</span><span class=n>T</span><span class=p>)</span> <span class=o>*</span> <span class=n>keys</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>**-</span><span class=mf>0.5</span>    
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attn_scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>context_vec</span> <span class=o>=</span> <span class=n>attn_weights</span> <span class=o>@</span> <span class=n>values</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>context_vec</span>
</span></span></code></pre></div><p>The bias term is omitted in the weight matrices to ensure pure matrix multiplication.</p><p>Setting <code>dim=-1</code> in the <code>softmax</code> function instructs it to normalize along the last dimension, which corresponds to the columns (since <code>attn_scores</code> has the shape [batch, row N, column N]), ensuring that the values in each row sum to 1.</p><h3 id=positional-encoding>Positional encoding<a hidden class=anchor aria-hidden=true href=#positional-encoding>#</a></h3><p>A key consideration with the self-attention layer is that it is permutation equivariant. This means that if we change the order of the input vectors, we would still compute the same key, value, and query vectors, but they would be permuted in the same way the input vectors were permuted. As a result, the set of output vectors would remain the same, but their order would change.</p><figure class=align-center><img loading=lazy src=../position-permutation.png#center alt="Self-attention layer is permutation equivariant $f(s(x)) = s(f(x))$." width=600><figcaption><p>Self-attention layer is permutation equivariant $f(s(x)) = s(f(x))$.</p></figcaption></figure><p>The self-attention layer does not inherently account for the order of the input vectors; it processes them as a set, irrespective of their sequence. While this property works well for certain tasks, it poses a challenge for tasks like machine translation or text generation, where the order of tokens is crucial.</p><p>For example, the sentences &ldquo;The dog chased the cat&rdquo; and &ldquo;The cat chased the dog&rdquo; would appear identical to the transformer but convey different meanings.</p><p>To make the layer position-aware, positional encodings are added to the input vectors. These encodings capture the position of each element in the sequence. A function ${\color{purple}{pos}}: \text{N} \rightarrow \mathbb{R}^D$ transforms each position $j$ into a unique, $D$-dimensional positional vector.</p><figure class=align-center><img loading=lazy src=../positional-encoding.png#center alt="Concatenate special positional encoding $\color{purple}{p_j}$ to each input vector $\mathbf{x}_j$" width=250><figcaption><p>Concatenate special positional encoding $\color{purple}{p_j}$ to each input vector $\mathbf{x}_j$</p></figcaption></figure><p>There are two common ways to obtain this positional encoding function:</p><ol><li><p><strong>Learnable Lookup Table</strong>:</p><ul><li>A lookup table is learned during training that assigns a unique encoding to each position.</li><li>This approach learns parameters for each position $t \in [0, N)$, where $T$ is the maximum sequence length, leading to a lookup table of size $N \times D$.</li></ul></li><li><p><strong>Fixed Function</strong>:</p><ul><li>A fixed function is designed that outputs a unique, deterministic encoding for each position.</li><li>This approach doesn’t require any learnable parameters.</li></ul></li></ol><h3 id=masked-self-attention-layer>Masked Self-Attention layer<a hidden class=anchor aria-hidden=true href=#masked-self-attention-layer>#</a></h3><p>A variant of the self-attention layer, called masked self-attention or causal attention, is used for tasks like language modeling, where the goal is to predict the next word given the previous words. This is similar to a decoder RNN, where new words in the output sequence are generated one by one, with previously generated words providing context.</p><p>With the standard self-attention layer, the model can attend to all input tokens at once, which isn&rsquo;t ideal for such tasks. To make the model &ldquo;look&rdquo; only at previous words while generating the next one, we mask future tokens. This prevents the model from attending to words that come after the current position in the sequence.</p><p>To achieve this, we set the attention weights for all future tokens to zero, ensuring that the model can only attend to current and past tokens when computing the context vector. This is particularly useful in decoders, where sequences are generated step-by-step.</p><figure class=align-center><img loading=lazy src=../masked-self-attention-mask.png#center alt="A Masked self-attention layer (Causal Attention)." width=500><figcaption><p>A Masked self-attention layer (Causal Attention).</p></figcaption></figure><p>Rather than zeroing out the attention weights of future tokens and renormalizing them, we can assign negative infinity to those positions and apply softmax directly. This ensures they receive zero probability while maintaining a row sum of one—all in a single step.</p><p>The mask will have a shape of $\text{N} \times \text{N}$, where $\text{N}$ represents the number of tokens in the sequence. We use PyTorch&rsquo;s <code>tril</code> function to create a mask where values above the diagonal are zero. These positions are then replaced with negative infinity in the attention scores matrix.</p><p>Let’s modify our code for causal attention.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>CausalAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span> <span class=n>attn_pdrop</span><span class=p>,</span> <span class=n>qkv_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attn_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>attn_pdrop</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Create a causal mask</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s2>&#34;mask&#34;</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>context_length</span><span class=p>,</span> <span class=n>context_length</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Input vectors x - (B, N, d_in)</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>d_in</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=c1># Obtain keys, values and queries - (B, N, d_out)</span>
</span></span><span class=line><span class=cl>        <span class=n>keys</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>values</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>queries</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Apply scaled dot-product attention with causal mask</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_scores</span> <span class=o>=</span> <span class=p>(</span><span class=n>queries</span> <span class=o>@</span> <span class=n>keys</span><span class=o>.</span><span class=n>T</span><span class=p>)</span> <span class=o>*</span> <span class=n>keys</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>**-</span><span class=mf>0.5</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_scores</span> <span class=o>=</span> <span class=n>attn_scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>mask</span><span class=p>[:</span><span class=n>N</span><span class=p>,</span> <span class=p>:</span><span class=n>N</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;-inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attn_scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>attn_dropout</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>context_vec</span> <span class=o>=</span> <span class=n>attn_weights</span> <span class=o>@</span> <span class=n>values</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>context_vec</span>
</span></span></code></pre></div><p>Context length refers to the maximum number of tokens allowed in any sequence (we will explore this in detail in the next post). It helps define the largest mask, which can then be applied to sequences of any length in our training data.</p><p>We have also used a <code>register_buffer</code> here, which ensures that the buffer variable <code>masked</code> is automatically moved to the appropriate device (CPU or GPU) along with the model. PyTorch only moves parameters, such as weights, to the selected device when we run <code>model.to(device)</code>, which in our case is the GPU.</p><p>Since <code>masked</code> is used in our forward computation, defining it as a regular variable would cause a device mismatch error because it would remain on the CPU while the model operates on the GPU. To avoid this issue, we register it as a buffer, ensuring it stays synchronized with the model&rsquo;s device.</p><p>It is also common practice to zero out additional elements in the attention weight matrix by applying a dropout layer, also called <em>attention dropout</em>. As you may recall, dropout is a regularization technique that helps prevent overfitting. Since some attention weights are zeroed out based on the dropout probability, the remaining weights are re-scaled to compensate for the reduction. Note that this dropout layer is disabled during inference.</p><h3 id=multi-head-self-attention-layer>Multi-head Self-Attention layer<a hidden class=anchor aria-hidden=true href=#multi-head-self-attention-layer>#</a></h3><p>Instead of performing a single attention function over the entire input vector space, the multi-head self-attention mechanism splits the input into multiple representation subspaces and performs attention in parallel across these subspaces. This allows the model to attend to different aspects of the input simultaneously.</p><p>In this case, we divide each input vector into $H$ chunks of equal size and feed them into several parallel attention layers. The input to each head has dimension $\text{d}_{h} = \text{D}/H$.</p><p>The outputs from all attention heads are concatenated (dimension $H \cdot \text{d}_h$) and passed through a projection linear layer to produce the final output of dimension $\text{D}_{out}$. While this projection layer is not strictly necessary, it is commonly used in many LLM architectures.</p><figure class=align-center><img loading=lazy src=../multi-head-self-attention-layer.png#center alt="A Multi-head self-attention layer" width=600><figcaption><p>A Multi-head self-attention layer</p></figcaption></figure><p>The above image may seem more intuitive, but it is not optimal for computation. If we first split the input vectors $\mathbf{x}$, we would need to compute the keys, values, and queries independently for each head, increasing computation.</p><p>Instead, a more efficient approach is to compute the keys, values, and queries for the entire input vector dimension first, and then split them into $H$ equal chunks. After computing attention independently for each head, we combine the results.</p><ul><li><p>Input:</p><ul><li>Input vectors: $\mathbf{x}$ (Shape: $\text{N} \times \text{D}_{in}$)</li><li>Similarity function: scaled dot product</li></ul></li><li><p>Operations:</p><ul><li><p>Key vectors: ${\color{blue}{\mathbf{k}}} = \mathbf{x} {\color{blue}{W_\mathbf{k}}}$ (Shape: $\text{N} \times \text{D}_{out}$)</p></li><li><p>Value vectors: ${\color{orange}{\mathbf{v}}} = \mathbf{x} {\color{orange}{W_\mathbf{v}}}$ (Shape: $\text{N} \times \text{D}_{out}$)</p></li><li><p>Query vectors: ${\color{green}{\mathbf{q}}} = \mathbf{x} {\color{green}{W_\mathbf{q}}}$ (Shape: $\text{N} \times \text{D}_{out}$)</p></li><li><p>Split key, value and query vectors. For each head:</p><ul><li>Alignment: $e_{i, j} = (\mathbf{q}_i \cdot \mathbf{k}_j ) / \sqrt{\text{d}_{h}}$</li><li>Attention: $a = \text{softmax} (e) $</li><li>Output vectors: ${\mathbf{y}_i} = \sum_j a_{i,j} \text{ } \mathbf{v}_j$ (Shape: $\text{N} \times \text{d}_{h}$)</li></ul></li></ul></li><li><p>Output:</p><ul><li>Output vectors: $\mathbf{y} = \text{Concat} (\text{y}^0, \cdots, \text{y}^{\text{H} - 1}) W_o$ (Shape: $\text{N} \times \text{D}_{out}$)</li></ul></li></ul><p>Now, let&rsquo;s implement a multi-head attention module that runs several causal attention heads in parallel.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadCausalAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span> <span class=n>attn_pdrop</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>qkv_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_o</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_out</span><span class=p>,</span> <span class=n>d_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attn_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>attn_pdrop</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span> <span class=o>=</span> <span class=n>d_out</span> <span class=o>//</span> <span class=n>num_heads</span>                    <span class=c1># head dimension</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s2>&#34;masked&#34;</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>context_length</span><span class=p>,</span> <span class=n>context_length</span><span class=p>))</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>context_length</span><span class=p>,</span> <span class=n>context_length</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Input vectors x - (B, N, d_in)</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>d_in</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=c1># Obtain keys, values and queries - (B, N, d_out)</span>
</span></span><span class=line><span class=cl>        <span class=n>keys</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_k</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>values</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_v</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>queries</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_q</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=n>keys</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=n>values</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>queries</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Apply scaled dot-product attention with causal mask on each head</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_scores</span> <span class=o>=</span> <span class=p>(</span><span class=n>q</span> <span class=o>@</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span> <span class=o>*</span> <span class=n>k</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>**-</span><span class=mf>0.5</span>    
</span></span><span class=line><span class=cl>        <span class=n>attn_scores</span> <span class=o>=</span> <span class=n>attn_scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>masked</span><span class=p>[:,</span> <span class=p>:,</span> <span class=p>:</span><span class=n>N</span><span class=p>,</span> <span class=p>:</span><span class=n>N</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;-inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attn_scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>                             
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>attn_dropout</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>context_vec</span> <span class=o>=</span> <span class=n>attn_weights</span> <span class=o>@</span> <span class=n>v</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>context_vec</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_o</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span></code></pre></div><p>The splitting of key, value, and query tensors is achieved through tensor reshaping and transposition. First, we reshape them to introduce the <code>num_heads</code> dimension, then transpose to bring <code>num_heads</code> before the <code>num_tokens</code> dimension. This allows us to compute attention in parallel across all heads using batched matrix multiplication.</p><p>After computing attention, the context vectors from all heads are transposed back to their original shape and flattened, effectively combining the outputs from all heads.</p><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, the attention mechanism would average over these subspaces, potentially losing valuable information.</p><p>Additionally, since each head operates on a reduced dimension, the total computational cost remains comparable to that of single-head attention with full dimensionality.</p><h2 id=transformer>Transformer<a hidden class=anchor aria-hidden=true href=#transformer>#</a></h2><p>The Transformer <a href=#references>[1]</a> was the first sequence model that relies solely on self-attention to compute representations of its input and output without using any recurrent units. This innovation marked a turning point for natural language processing (NLP), outperforming all state-of-the-art models of its time. It is often referred to as the &ldquo;ImageNet moment&rdquo; for NLP.</p><p>Let&rsquo;s take a closer look at the complete architecture of the Transformer.</p><h3 id=embeddings-and-positional-encoding>Embeddings and Positional Encoding<a hidden class=anchor aria-hidden=true href=#embeddings-and-positional-encoding>#</a></h3><p>The input and output word tokens are converted into vectors of dimension $\text{D} = d_{\text{model}} = 512$ by learned embeddings.</p><p>Next, positional encodings are added to input embedding vectors to inject information about the position of the tokens in the sequence. A fixed sinusoidal function is used to compute the positional encodings, which are of the same dimension $d_\text{model}$ as the embeddings, so that the two can be summed together.</p><figure class=align-center><img loading=lazy src=../sin-cos-encoding.png#center width=400></figure><h3 id=encoder-block>Encoder block<a hidden class=anchor aria-hidden=true href=#encoder-block>#</a></h3><p>The encoder is composed of a stack of $L = 6$ identical layers, each consisting of two sub-layers:</p><ol><li><p><strong>Multi-head Self-Attention</strong>: Each output from this layer depends on every input, allowing for interactions between all vectors in the input sequence. Since the inputs come from the previous encoder layer, each position in the encoder can attend to all positions in the previous layer.</p><ul><li>Hyperparamters: $H = 8$, $\text{D}_{in} = \text{D}_{out} = d_{model} = 512$</li></ul></li><li><p><strong>Feed-forward network</strong>: Each input vector passes through an MLP independently, consisting of two linear layers and a ReLU activation in between. This layer internally expands the embedding dimension into a higer-dimensional space (factor of 4), which allows for exploration of a richer representation space.</p><ul><li>Linear 1 $(512, 2048)$ -> ReLU -> Linear 2 $(2048, 512)$</li></ul></li></ol><p>Additionally:</p><ul><li><p><strong>Residual Dropout</strong>: Each of the two sublayers is followed by a dropout layer. Additionally, dropout is applied to the sum of the embeddings and position encodings. $p = 0.1$.</p></li><li><p><strong>Residual connection</strong>: A residual connection is used around each of the two sub-layers to improve the gradient flow through the model, and overcome the vanishing gradient problem.</p></li><li><p><strong>Layer Normalization</strong>: Each sub-layer is followed by layer normalization to aid optimization (similar to BatchNorm in convolutional layers).</p></li></ul><figure class=align-center><img loading=lazy src=../transformer-encoder.png#center alt="The Encoder of the Transformer." width=600><figcaption><p>The Encoder of the Transformer.</p></figcaption></figure><ul><li>Input: A set of vectors $\mathbf{x}$ (Shape: $\text{N} \times 512$)</li><li>Output: A set of vectors $\mathbf{y}$ (Shape: $\text{N} \times 512$)</li></ul><p>The interaction between vectors occurs only in the self-attention layer. LayerNorm and MLP operate on each input vector independently. The uniformity in input and output dimensions enables the stacking of multiple layers, thus making the model more scalable.</p><h3 id=decoder-block>Decoder block<a hidden class=anchor aria-hidden=true href=#decoder-block>#</a></h3><p>The decoder consists of $L = 6$ identical layers, each with three sub-layers:</p><ol><li><p><strong>Masked Multi-head Self-Attention</strong>: To prevent the decoder from attending to future tokens in the output sequence, we use masked self-attention, which ensures that each position in the decoder can only attend to past and current tokens.</p><ul><li>Hyperparamters: $H = 8$, $\text{D}_{in} = \text{D}_{out} = 512$.</li></ul></li><li><p><strong>Multi-head Cross-Attention over Encoder outputs</strong>: This layer allows each position in the decoder to attend to all positions in the input sequence, passing relevant context from the encoder. This mimics the traditional encoder-decoder attention mechanism used in seq2seq models.</p><ul><li>Hyperparameters: $H = 8$, $\text{D}_{in} = \text{D}_{out} = 512$.</li></ul></li><li><p><strong>Feed-forward network</strong>: Same structure as in the encoder:</p><ul><li>Linear 1 (512, 2048) -> ReLU -> Linear 2 (2048, 512)</li></ul></li></ol><p>Similar to the encoder, each sublayer is followed by a dropout layer, with residual connections added around each one. Finally, layer normalization is applied to each vector independently.</p><figure class=align-center><img loading=lazy src=../transformer-decoder.png#center alt="The Decoder of the Transformer." width=700><figcaption><p>The Decoder of the Transformer.</p></figcaption></figure><ul><li>Input:<ul><li>Decoder sequence: A set of vectors $\mathbf{x}$ (Shape: $\text{M} \times 512$)</li><li>Encoder context: A set of context vectors $\mathbf{c}$ (Shape: $\text{N} \times 512$)</li></ul></li><li>Output: A set of vectors $\mathbf{y}$ (Shape: $\text{M} \times 512$)</li></ul><p>The masked self-attention sub-layer ensures autoregressive behavior by restricting attention to past inputs. The multi-head attention over encoder outputs bridges the encoder and decoder, allowing the decoder to focus on relevant parts of the input sequence.</p><p>The decoder block is followed by a linear layer and softmax function to convert the decoder output into predicted next-token probabilities.</p><p>During inference, the decoder sequence begins with a &lt; START > token embedding (Shape: $1 \times 512$). As the model predicts each subsequent token, we append it to this sequence.</p><h3 id=key-characteristics>Key characteristics<a hidden class=anchor aria-hidden=true href=#key-characteristics>#</a></h3><ul><li><p><strong>Parallel computation</strong>: Unlike RNNs, the Transformer processes entire sequences simultaneously, allowing alignment and attention scores for all inputs to be computed in parallel, significantly improving efficiency on large datasets.</p></li><li><p><strong>Flexibility of inputs</strong>: It can effectively handle both unordered sets and ordered sequences (with positional encodings).</p></li><li><p><strong>Global context</strong>: Self-attention enables the model to capture long-range dependencies across the entire sequence.</p></li><li><p><strong>Scalability</strong>: The Transformer&rsquo;s architecture is highly scalable, with a few key hyperparameters that can be adjusted to meet various requirements:</p><ol><li><strong>Number of Layers $L$</strong>: Applies equally to the encoder and decoder.</li><li><strong>Hidden size $d_{\text{model}}$</strong>: Defines the dimensionality of the model.</li><li><strong>MLP size</strong> $d_{ff}$: Specifies the output size of the first layer in the feed-forward MLP.</li><li><strong>Heads $H$</strong>: Determines the number of attention heads in multi-head self-attention (encoder), masked multi-head self-attention (decoder), and multi-head cross-attention (decoder).</li></ol><figure class=align-center><img loading=lazy src=../transformer-full.png#center alt="The Transformer Architecture." width=400><figcaption><p>The Transformer Architecture.</p></figcaption></figure></li></ul><p>The Transformer&rsquo;s innovative architecture has become the foundation for many advancements in NLP, inspiring models like BERT and GPT, and driving us in the new era of LLMs.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li>[1] Vaswani et al, &ldquo;<a href=https://arxiv.org/abs/1706.03762>Attention is all you need</a>&rdquo;, NeurIPS 2017.</li><li>Read more on Pytorch Buffers here: <a href=https://github.com/rasbt/LLMs-from-scratch/blob/main/ch03/03_understanding-buffers/understanding-buffers.ipynb>Understanding PyTorch Buffers</a></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://yugajmera.github.io/posts/06-gpt/post/><span class=title>« Prev</span><br><span>GPT Series Part 1: Understanding LLMs & Coding GPT-1 from scratch</span>
</a><a class=next href=https://yugajmera.github.io/posts/05-attention/post/><span class=title>Next »</span><br><span>Sequence Modeling with Recurrent Neural Networks and Attention</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://yugajmera.github.io/>YA's Almanac</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>