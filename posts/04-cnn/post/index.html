<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Convolutional Neural Networks: Deep Learning for Image Recognition | YA&#39;s Almanac</title>
<meta name="keywords" content="">
<meta name="description" content="Linear classifiers or MLPs that we have discussed so far don&rsquo;t respect the 2D spatial structure of input images. These images are flattened into a 1D vector before passing them through the network which destroys the spatial structure of the image.
This creates a need for a new computational model that can operate on images while preserving spatial relationships — Convolutional Neural Networks (CNNs). Let&rsquo;s understand the components of this CNN model.">
<meta name="author" content="">
<link rel="canonical" href="https://yugajmera.github.io/posts/04-cnn/post/">
<link crossorigin="anonymous" href="https://yugajmera.github.io/assets/css/stylesheet.334cb677465a1d1e9767dd05b097c98de3a5b4085e0d43c708104df17bc49585.css" integrity="sha256-M0y2d0ZaHR6XZ90FsJfJjeOltAheDUPHCBBN8XvElYU=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://yugajmera.github.io/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yugajmera.github.io/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yugajmera.github.io/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yugajmera.github.io/assets/apple-touch-icon.png">
<link rel="mask-icon" href="https://yugajmera.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://yugajmera.github.io/posts/04-cnn/post/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
<style>
   mjx-container[display="true"] {
       margin: 1.5em 0 ! important
   }
</style>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-S759YBMKJE', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Convolutional Neural Networks: Deep Learning for Image Recognition" />
<meta property="og:description" content="Linear classifiers or MLPs that we have discussed so far don&rsquo;t respect the 2D spatial structure of input images. These images are flattened into a 1D vector before passing them through the network which destroys the spatial structure of the image.
This creates a need for a new computational model that can operate on images while preserving spatial relationships — Convolutional Neural Networks (CNNs). Let&rsquo;s understand the components of this CNN model." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yugajmera.github.io/posts/04-cnn/post/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-09-18T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-09-18T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Convolutional Neural Networks: Deep Learning for Image Recognition"/>
<meta name="twitter:description" content="Linear classifiers or MLPs that we have discussed so far don&rsquo;t respect the 2D spatial structure of input images. These images are flattened into a 1D vector before passing them through the network which destroys the spatial structure of the image.
This creates a need for a new computational model that can operate on images while preserving spatial relationships — Convolutional Neural Networks (CNNs). Let&rsquo;s understand the components of this CNN model."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yugajmera.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Convolutional Neural Networks: Deep Learning for Image Recognition",
      "item": "https://yugajmera.github.io/posts/04-cnn/post/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Convolutional Neural Networks: Deep Learning for Image Recognition",
  "name": "Convolutional Neural Networks: Deep Learning for Image Recognition",
  "description": "Linear classifiers or MLPs that we have discussed so far don\u0026rsquo;t respect the 2D spatial structure of input images. These images are flattened into a 1D vector before passing them through the network which destroys the spatial structure of the image.\nThis creates a need for a new computational model that can operate on images while preserving spatial relationships — Convolutional Neural Networks (CNNs). Let\u0026rsquo;s understand the components of this CNN model.",
  "keywords": [
    
  ],
  "articleBody": "Linear classifiers or MLPs that we have discussed so far don’t respect the 2D spatial structure of input images. These images are flattened into a 1D vector before passing them through the network which destroys the spatial structure of the image.\nThis creates a need for a new computational model that can operate on images while preserving spatial relationships — Convolutional Neural Networks (CNNs). Let’s understand the components of this CNN model.\nConvolutional Layer To respect the 2D structure of the input image, we use a 2D learnable weight matrix of shape $(K_w, K_h)$, called a kernel, which convolves over the input image. That is, the kernel slides spatially across the image channel, computing dot products at each position.\nRecall that the input image is a 3D tensor of shape $(C, H, W)$, where $C = 3$ for an RGB image. For the example below, consider a single-channel input of shape $(5,5)$ convolving with a $3 \\times 3$ kernel.\nThe values in this kernel matrix are weights learned during training. The same kernel is applied across all positions of the input channel, and each position computes a dot product, outputting a single number.\nThe intuition behind convolving kernels with images is rooted in classical computer vision. Kernels help extract feature information from an image, such as edges, textures, and patterns. These are useful for tasks like blurring, sharpening, edge detection, and more.\nInstead of manually designing these kernels for feature extraction, we let the model learn them from the training data. This allows the network to automatically discover “good” kernels that extract the most relevant features for correctly classifying the input image.\nDifferent channels of the image encode distinct features, so it’s essential to learn a different kernel matrix for each input channel and combine the results to form a unique feature representation of the image.\nFilter Since we have a unique kernel matrix for each input channel, the learnable matrix is a 3D tensor of weights with shape $(C, K_w, K_h)$, known as a filter. The depth of the filter (i.e., the number of kernels) always matches the number of input channels.\nThe figure below shows an example of a convolution operation on an RGB image with $3 \\times 3$ kernels.\nThese outputs of convolutions are summed across all channels, along with a bias term, to produce a single value. This value forms one entry in the 2D output called an activation map, or feature map.\nSimilar to a linear classifier where we have: $$ f(\\mathbf{x}, \\mathbf{w}) = \\mathbf{w}_1 \\mathbf{x}_1 + \\mathbf{w}_2 \\mathbf{x}_2 + \\mathbf{w}_3 \\mathbf{x}_3 + \\mathbf{b} $$ In a convolution, we have: $$ A(\\mathbf{x}, \\mathbf{k}_{1:3}) = \\mathbf{k}_1 * \\mathbf{x}_{\\text{channel 1}} + \\mathbf{k}_2 * \\mathbf{x}_{\\text{channel 2}} + \\mathbf{k}_3 * \\mathbf{x}_{\\text{channel 3}} + \\mathbf{b} $$\nEach 3D filter produces one 2D feature map, with each map corresponding to a specific feature or pattern detected in the input. Since we want multiple features to be detected, we use multiple 3D filters. The output is formed by stacking the activation maps computed from each filter, resulting in a 3D tensor, where the depth corresponds to the number of filters used.\nIn the example below, a $(3, 32, 32)$ RGB image is convolved with six filters of $5 \\times 5$ kernels.\nTo formulate this, let $N$ be the number of images in a mini-batch, $C_{in}$ the number of input channels, and $C_{out}$ the number of output channels (or filters) in the convolution layer:\n\\begin{align} \\underbrace{N \\times C_{in} \\times H \\times W}_{\\text{Input size}} + \\underbrace{C_{out} \\times C_{in} \\times K_w \\times K_h}_{\\text{Filters size}} \\Rightarrow \\underbrace{N \\times C_{out} \\times H’ \\times W’}_{\\text{Output size}} \\end{align}\nAssuming $H = W$ (square image) and $K_w = K_h$ (square kernel), the output size is given by: $$ W’ = \\frac{(W - K_w + 2P)}{S} + 1 $$ where $P$ is the padding size and $S$ is the stride. Let’s understand the significance of these terms.\nPadding When we convolve a filter with an image, the spatial size of the output reduces, i.e., $W’ \u003c W$. The feature map shrinks, as shown below.\nTo preserve the spatial size, we pad the input image with zeros around the borders. For instance, $P = 1$ refers to adding a single border of zeros around the input.\nPadding also helps retain edge information, as without it, edge pixels contribute less to the output because the filter doesn’t fully cover them.\nThe two most common padding terms are:\nValid padding: No padding is applied, i.e., $P = 0$. Same padding: The output size equals the input size, i.e., $W’ = W$. This is achieved with $P = (K_w - 1)/2 $. Stride Earlier, I mentioned that we convolve our filter with every pixel of the input channel. However, we can choose to convolve with every alternate pixel by setting the stride $S = 2$.\nStride refers to the number of pixels by which the filter moves across the input image during the convolution operation. When using a stride greater than 1, the filter skips certain pixels, effectively reducing the spatial size of the input.\nThe image below shows a convolution operation with padding = 1 and stride = 2.\nUsing larger strides can effectively downsample the input, reducing computational costs and speeding up training. However, it may result in skipping over fine-grained details in the input image.\nNext, let’s examine how the spatial size of feature maps affects computation costs in a convolutional layer.\nComputational Cost Consider a convolutional layer with 10 filters of size $5 \\times 5$, stride 1, and pad 2 (same padding), applied to an RGB image of size $(3, 32, 32)$.\nInput: $[C_{in} \\times H \\times W] = 3 \\times 32 \\times 32$ Convolution: $[C_{out} \\times C_{in} \\times K_w \\times K_h]$ = 10 filters of size $3 \\times 5 \\times 5$ Output: $[C_{out} \\times H’ \\times W’] = 10 \\times 32 \\times 32$ For a batch size of $B=1$, let’s calculate:\n1. Memory: To store the output as floats: $(C_{out} \\times H’ \\times W’) * 4/1024$ $(10 \\times 32 \\times 32) * 4/1024$ = 40 KB.\n2. Number of learnable parameters: $\\underbrace{(C_{out} * C_{in} \\times K_w \\times K_h)}_{\\text{filters}} + \\underbrace{C_{out}}_{\\text{bias}}$ $(10 * 3 \\times 5 \\times 5) + 10$ = 760.\n3. Number of Floating-point operations (FLOPs): Total multiply-add operations $\\underbrace{(C_{out} \\times H’ \\times W’)}_{\\text{output size}} * \\underbrace{(C_{in} \\times K_w \\times K_h)}_{\\text{filter size}}$\n$10 \\times 32 \\times 32 = 10240$ outputs, each of which is the inner product of input with a $3 \\times 5 \\times 5 = 75$ tensor. Total = $75 * 10240$ = 768k FLOPs.\nMemory calculations help estimate the maximum batch size that can be used without exceeding GPU memory limits, learnable parameters define the model’s capacity to learn from data, and FLOPs influence the computational time during training and inference.\nReceptive Fields Receptive fields refer to the specific regions of the input data that a feature map responds to. In the convolution shown below, each element of the output layer depends on a $3 \\times 3$ receptive field in the input.\nTo cover the entire input of size $(7, 7)$ with $3 \\times 3$ kernels, at least 3 convolution layers are needed to ensure that a single output can “see” the whole input image.\nStacking multiple convolution layers increases the size of the receptive fields, allowing the network to capture more global information about the input. However, this stacking would result in one large convolution, so we typically apply activation functions (non-linearities) after each convolution. ReLU is a commonly used activation function in this context.\nFor large images, many layers may be required to capture global information, which can become computationally expensive. A solution is to downsample the feature map within the network, effectively increasing the receptive fields of subsequent layers while reducing computation. This is where pooling layers come into play.\nPooling Layer Unlike convolutional layers, which apply a filter to extract features, pooling layers summarize information within a localized region of the input. One common pooling method is max pooling, where we take the maximum value from the elements within a defined kernel.\nPooling layers have two hyperparameters: kernel size and stride. For example, consider a max pooling layer with a kernel size of 2 and a stride of 2:\nThis operation effectively reduces the spatial dimensions of the input by half, while retaining the most prominent features (the strongest responses) and discarding less relevant details.\nThe max operation introduces translational invariance: whether a key feature, like a cat’s ear, is located in the top-left corner of the filter or the bottom-right, the output remains the same since we are taking the maximum over the kernel. This property allows the model to be more robust to small spatial shifts in the input.\nHere are some key features of pooling layers:\nPooling layers do not contain learnable parameters, which simplifies the backpropagation process. During backpropagation, they simply pass gradients back to the locations of the maximum values from the previous layer. As pooling is a non-linear operation, it adds non-linearity to the model without requiring an additional activation function. Another pooling method, average pooling, calculates the average of the elements within the kernel. This approach tends to smooth out the feature map by considering all values within the window, making it useful for reducing noise. However, max pooling is generally preferred for its ability to highlight the most significant features in the input.\nNetwork Architecture To maintain the spatial dimension of the input, same padding with stride 1 is frequently used. Below are some common choices for hyperparameters in convolutional layers:\n$C_{in}$, $C_{out}$ = 32, 64, 128, 256 (in powers of 2) Kernel = $5 \\times 5$, Padding = 2, Stride = 1 Kernel = $3 \\times 3$, Padding = 1, Stride = 1 Kernel = $3 \\times 3$, Padding = 1, Stride = 2 [downsample by 2] Kernel = $1 \\times 1$, Padding = 0, Stride = 1 [Pointwise convolution] Using channel dimensions that are powers of 2 allows for more efficient memory allocation on GPUs.\nThe $1 \\times 1$ convolution layer is also called pointwise convolution because the filter operates on each pixel individually across the depth (channels) of the input. This allows changing the number of channels while keeping the spatial dimensions intact, as the depth of the resulting feature map is determined by the number of filters used.\nA classical architecture for a convolutional neural network follows this structure: $$ [\\text{Conv, ReLU, Pool}]_{\\times N} \\rightarrow Flatten \\rightarrow [\\text{FC, ReLU}]_{\\times M} \\rightarrow FC $$\nThe input image is first processed through multiple layers of convolution, followed by ReLU activations, and then pooling to downsample the feature maps.\nThe initial layers of a CNN learn to detect basic patterns such as edges, corners, and simple textures. These low-level features are often universal across different images, making them not specific to any particular object class.\nAs the network deepens, subsequent layers begin to detect more complex patterns by combining low-level features to recognize higher-level representations, such as shapes, textures, or parts of objects (e.g., eyes, wheels, or fur textures).\nAfter the convolutional and pooling layers, the output is a multi-dimensional tensor representing the learned features. This tensor is then flattened into a 1D vector and fed into fully connected (FC) layers with ReLU activations. These layers combine the learned features to make predictions.\nThe final fully-connected layer produces the output, typically class scores for each category in a classification task.\nVisualization of features in a fully trained model. The left image shows the kernels learned by the first convolutional layer of AlexNet [1], while the right image displays the features learned in Layers 3-5 of ZFNet [2].\nCoding Let’s modify our model from here to a convolutional neural network for classifying handwritten digits in the MNIST dataset.\n# Define the model class CNN(torch.nn.Module): def __init__(self): super(CNN, self).__init__() self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1) # Output: 32 x 28 x 28 self.max_pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2) # Output: 32 x 14 x 14 self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1) # Output: 64 x 14 x 14 self.max_pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) # Output: 64 x 7 x 7 self.fc1 = torch.nn.Linear(7 * 7 * 64, 128) self.fc2 = torch.nn.Linear(128, 10) self.relu = torch.nn.ReLU() def forward(self, x): x = self.relu(self.conv1(x)) x = self.max_pool1(x) x = self.relu(self.conv2(x)) x = self.max_pool2(x) x = x.view(x.shape[0], -1) # Flatten x = self.relu(self.fc1(x)) x = self.fc2(x) return x # Initialize the model and move it to the selected device model = CNN().to(device) I used the Adam optimizer with a learning rate of 0.001, achieving a test accuracy of 99%.\nTo view the computational statistics of our model, we can use torchsummary:\nfrom torchsummary import summary summary(model, (1, 28, 28)) We can draw the following conclusions from this:\nThe memory used to store the output as floats is primarily consumed by the convolutional layers. Almost all learnable parameters are found in the fully connected layers. While it’s possible to create deeper convolutional neural networks, training them can be quite challenging. Like linear networks, they face overfitting issues, and convergence becomes increasingly difficult as depth increases. A common solution to this problem is Batch Normalization [3].\nBatch Normalization The idea behind batch normalization is to normalize the activations of a layer so that they have zero mean and unit variance. But why is this necessary?\nAs a neural network trains, the distribution of inputs to its layers can shift due to weight updates. This phenomenon, known as internal covariate shift, can cause the learning algorithm to struggle, effectively chasing a moving target. Batch normalization helps mitigate this shift, stabilizing the training process and improving optimization.\nIn addition, normalizing the outputs of a layer ensures that the distribution is well-behaved before passing through the non-linear activation function, which helps prevent issues like saturation.\nIn practice, batch normalization is implemented as a layer that processes inputs before they are passed to the next layer. The normalization can be mathematically described as:\n\\begin{align} \u0026 \\hat{x}_{ij} = \\frac{x_{ij} - \\mu_j}{\\sqrt{\\sigma_j^2 + \\epsilon}} \\\\ \u0026 y_{ij} = \\gamma_j \\hat{x}_{ij} + \\beta_j \\end{align}\nHere, $\\mu_j$ and $\\sigma_j$ are the mean and standard deviation calculated across the mini-batch of inputs for each channel.\nSince maintaining zero mean and unit variance can be too restrictive, we introduce scale ($\\gamma$) and shift ($\\beta$) parameters that can be learned during training. This ensures that the activations of the layer remain Gaussian throughout the training process.\nA beneficial side effect of this normalization process is that it introduces some randomness, which can enhance regularization and improve the model’s generalization ability.\nUnused Bias The bias added to $x_{ij}$ in the previous layer is effectively canceled out when the mean of the outputs is subtracted during batch normalization. As a result, the bias in the layer preceding the batch normalization layer becomes redundant and can be omitted.\nTraining vs Testing Since the $\\mu_j$ and $\\sigma_j$ are computed from the mini-batch, their values depend on the specific batch. For example, if one test batch contains [cat, dog, frog] and another contains [cat, car, horse], the output for the common image of the cat will differ due to varying means and standard deviations.\nTo address this, batch normalization behaves differently during training and testing. During testing, we do not compute the mean and standard deviation from the batch; instead, we fix these values and use the running averages collected during training.\nThe running averages are updated during each training step as:\n\\begin{align} \\mu_{\\text{running}} = (1 - \\beta) * \\mu_{\\text{running}} + \\beta * \\mu_j \\end{align}\nwhere $\\beta$, called the momentum, is typically set to $0.1$.\nAs a result, during inference, batch normalization becomes a linear operation, allowing it to be easily fused with preceding linear or convolutional layers. Typically, the batch norm layer is inserted after fully connected or convolutional layers and before the activation function.\nBenefits of Batch Normalization In summary, batch normalization offers several advantages:\nMakes deep networks much easier to train by mitigating internal covariate shift. Allows for higher learning rates, which can lead to faster convergence during training. Normalization process makes networks more robust to weight initialization. Acts as a form of regularization during training. Zero computational overhead at test time (uses fixed parameters so can be fused with the previous layer) Variants While batch normalization is powerful, it behaves differently during training and testing. To address this, several variants have been developed.\nLayer Normalization Instead of averaging over batch dimensions, it averages over the feature dimension, resulting in per-channel mean and standard deviation. This makes it independent of the batch size, and it behaves the same during both training and testing. Layer normalization is commonly used in RNNs and Transformers, where batch sizes can vary significantly.\nInstance Normalization Here, the normalization is done over the spatial dimensions of each image, resulting in per-image mean and standard deviation. It also behaves consistently during training and testing, and is often used in style transfer and image generation tasks.\nGroup Normalization Instead of normalizing across the entire channel dimension like layer normalization, group normalization splits the channels into groups and normalizes within each group. This method is particularly effective for tasks like object detection and is commonly used in group convolutions.\nThe figure below provides an intuitive understanding of the four types of normalizations.\nCoding Let’s add Batch Normalization to our model. Since Batch Normalization behaves differently during training and testing, we must ensure to use model.train() and model.eval() to switch between these modes. This guarantees that we compute the mean and variance in training mode and use the running averages in evaluation mode.\nAdditionally, since most of the learnable parameters are in the fully connected layers, there’s a higher risk of overfitting. To address this, I’ll include a dropout layer for regularization.\n# Creating the architecture class DigitClassification(torch.nn.Module): def __init__(self): super(DigitClassification, self).__init__() # First convolutional block: Conv -\u003e BatchNorm -\u003e ReLU -\u003e MaxPool self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1, bias=False) self.bn1 = torch.nn.BatchNorm2d(32) self.max_pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2) # Second convolutional block: Conv -\u003e BatchNorm -\u003e ReLU -\u003e MaxPool self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False) self.bn2 = torch.nn.BatchNorm2d(64) self.max_pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2) # Fully connected layers: FC -\u003e ReLU -\u003e Dropout -\u003e FC self.fc1 = torch.nn.Linear(7 * 7 * 64, 128) self.dropout = torch.nn.Dropout(p=0.5) self.fc2 = torch.nn.Linear(128, 10) self.relu = torch.nn.ReLU() pass def forward(self, x): x = self.relu(self.bn1(self.conv1(x))) x = self.max_pool1(x) x = self.relu(self.bn2(self.conv2(x))) x = self.max_pool2(x) x = x.view(x.shape[0], -1) # Flatten x = self.relu(self.fc1(x)) x = self.dropout(x) x = self.fc2(x) return x With this implementation, I achieved a test accuracy of 99.3%, giving us the best model we’ve had until now.\nIt can be challenging to make numerous decisions regarding the architecture of a CNN model and its various hyperparameters in pursuit of the best possible accuracy. Therefore, it’s always beneficial to review state-of-the-art CNN models that have been successful in the past for inspiration. Let’s explore that in our next post.\nReferences [1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, “ImageNet Classification with Deep Convolutional Neural Networks”, NeurIPS 2012.\n[2] Matthew D. Zeiler and Rob Fergus. “Visualizing and Understanding Convolutional Networks”, ECCV 2014.\n[3] Ioffe and Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift”, ICML 2015.\n",
  "wordCount" : "3190",
  "inLanguage": "en",
  "datePublished": "2022-09-18T00:00:00Z",
  "dateModified": "2022-09-18T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yugajmera.github.io/posts/04-cnn/post/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "YA's Almanac",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yugajmera.github.io/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yugajmera.github.io/" accesskey="h" title="YA&#39;s Almanac (Alt + H)">YA&#39;s Almanac</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yugajmera.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Convolutional Neural Networks: Deep Learning for Image Recognition
    </h1>
    <div class="post-meta"><span title='2022-09-18 00:00:00 +0000 UTC'>September 18, 2022</span>&nbsp;·&nbsp;15 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#convolutional-layer" aria-label="Convolutional Layer">Convolutional Layer</a><ul>
                        
                <li>
                    <a href="#filter" aria-label="Filter">Filter</a></li>
                <li>
                    <a href="#padding" aria-label="Padding">Padding</a></li>
                <li>
                    <a href="#stride" aria-label="Stride">Stride</a></li>
                <li>
                    <a href="#computational-cost" aria-label="Computational Cost">Computational Cost</a></li>
                <li>
                    <a href="#receptive-fields" aria-label="Receptive Fields">Receptive Fields</a></li></ul>
                </li>
                <li>
                    <a href="#pooling-layer" aria-label="Pooling Layer">Pooling Layer</a></li>
                <li>
                    <a href="#network-architecture" aria-label="Network Architecture">Network Architecture</a><ul>
                        
                <li>
                    <a href="#coding" aria-label="Coding">Coding</a></li></ul>
                </li>
                <li>
                    <a href="#batch-normalization" aria-label="Batch Normalization">Batch Normalization</a><ul>
                        
                <li>
                    <a href="#unused-bias" aria-label="Unused Bias">Unused Bias</a></li>
                <li>
                    <a href="#training-vs-testing" aria-label="Training vs Testing">Training vs Testing</a></li>
                <li>
                    <a href="#benefits-of-batch-normalization" aria-label="Benefits of Batch Normalization">Benefits of Batch Normalization</a></li>
                <li>
                    <a href="#variants" aria-label="Variants">Variants</a><ul>
                        
                <li>
                    <a href="#layer-normalization" aria-label="Layer Normalization">Layer Normalization</a></li>
                <li>
                    <a href="#instance-normalization" aria-label="Instance Normalization">Instance Normalization</a></li>
                <li>
                    <a href="#group-normalization" aria-label="Group Normalization">Group Normalization</a></li></ul>
                </li>
                <li>
                    <a href="#coding-1" aria-label="Coding">Coding</a></li></ul>
                </li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Linear classifiers or MLPs that we have discussed so far don&rsquo;t respect the 2D spatial structure of input images. These images are flattened into a 1D vector before passing them through the network which destroys the spatial structure of the image.</p>
<figure class="align-center ">
    <img loading="lazy" src="../flatten-image.png#center"/> 
</figure>

<p>This creates a need for a new computational model that can operate on images while preserving spatial relationships — Convolutional Neural Networks (CNNs). Let&rsquo;s understand the components of this CNN model.</p>
<h2 id="convolutional-layer">Convolutional Layer<a hidden class="anchor" aria-hidden="true" href="#convolutional-layer">#</a></h2>
<p>To respect the 2D structure of the input image, we use a 2D learnable weight matrix of shape $(K_w, K_h)$, called a kernel, which convolves over the input image. That is, the kernel slides spatially across the image channel, computing dot products at each position.</p>
<p>Recall that the input image is a 3D tensor of shape $(C, H, W)$, where $C = 3$ for an RGB image. For the example below, consider a single-channel input of shape $(5,5)$ convolving with a $3 \times 3$ kernel.</p>
<figure class="align-center ">
    <img loading="lazy" src="../2d-cnn.gif#center" width="600"/> 
</figure>

<p>The values in this kernel matrix are weights learned during training. The same kernel is applied across all positions of the input channel, and each position computes a dot product, outputting a single number.</p>
<p>The intuition behind convolving kernels with images is rooted in classical computer vision. Kernels help extract feature information from an image, such as edges, textures, and patterns. These are useful for tasks like blurring, sharpening, edge detection, and more.</p>
<figure class="align-center ">
    <img loading="lazy" src="../kernels.png#center"/> 
</figure>

<p>Instead of manually designing these kernels for feature extraction, we let the model learn them from the training data. This allows the network to automatically discover &ldquo;good&rdquo; kernels that extract the most relevant features for correctly classifying the input image.</p>
<p>Different channels of the image encode distinct features, so it&rsquo;s essential to learn a different kernel matrix for each input channel and combine the results to form a unique feature representation of the image.</p>
<figure class="align-center ">
    <img loading="lazy" src="../color-channels.png#center"/> 
</figure>

<h3 id="filter">Filter<a hidden class="anchor" aria-hidden="true" href="#filter">#</a></h3>
<p>Since we have a unique kernel matrix for each input channel, the learnable matrix is a 3D tensor of weights with shape $(C, K_w, K_h)$, known as a filter. The depth of the filter (i.e., the number of kernels) always matches the number of input channels.</p>
<p>The figure below shows an example of a convolution operation on an RGB image with $3 \times 3$ kernels.</p>
<figure class="align-center ">
    <img loading="lazy" src="../convolution2.gif#center"/> 
</figure>

<figure class="align-center ">
    <img loading="lazy" src="../convolution.gif#center"/> 
</figure>

<p>These outputs of convolutions are summed across all channels, along with a bias term, to produce a single value. This value forms one entry in the 2D output called an activation map, or feature map.</p>
<p>Similar to a linear classifier where we have:
$$
f(\mathbf{x}, \mathbf{w}) = \mathbf{w}_1 \mathbf{x}_1 + \mathbf{w}_2 \mathbf{x}_2 + \mathbf{w}_3 \mathbf{x}_3 + \mathbf{b}
$$
In a convolution, we have:
$$
A(\mathbf{x}, \mathbf{k}_{1:3}) = \mathbf{k}_1 * \mathbf{x}_{\text{channel 1}} + \mathbf{k}_2 * \mathbf{x}_{\text{channel 2}} + \mathbf{k}_3 * \mathbf{x}_{\text{channel 3}} + \mathbf{b}
$$</p>
<p>Each 3D filter produces one 2D feature map, with each map corresponding to a specific feature or pattern detected in the input. Since we want multiple features to be detected, we use multiple 3D filters. The output is formed by stacking the activation maps computed from each filter, resulting in a 3D tensor, where the depth corresponds to the number of filters used.</p>
<p>In the example below, a $(3, 32, 32)$ RGB image is convolved with six filters of $5 \times 5$ kernels.</p>
<figure class="align-center ">
    <img loading="lazy" src="../convolution3.png#center"/> 
</figure>

<p>To formulate this, let $N$ be the number of images in a mini-batch, $C_{in}$ the number of input channels, and $C_{out}$ the number of output channels (or filters) in the convolution layer:</p>
<p>\begin{align}
\underbrace{N \times C_{in} \times H \times W}_{\text{Input size}} + \underbrace{C_{out} \times C_{in} \times K_w \times K_h}_{\text{Filters size}} \Rightarrow \underbrace{N \times C_{out} \times H&rsquo; \times W&rsquo;}_{\text{Output size}}
\end{align}</p>
<p>Assuming $H = W$ (square image) and $K_w = K_h$ (square kernel), the output size is given by:
$$
W&rsquo; = \frac{(W - K_w + 2P)}{S} + 1
$$
where $P$ is the padding size and $S$ is the stride. Let&rsquo;s understand the significance of these terms.</p>
<h3 id="padding">Padding<a hidden class="anchor" aria-hidden="true" href="#padding">#</a></h3>
<p>When we convolve a filter with an image, the spatial size of the output reduces, i.e., $W&rsquo; &lt; W$. The feature map shrinks, as shown below.</p>
<figure class="align-center ">
    <img loading="lazy" src="../conv-1.gif#center"/> 
</figure>

<p>To preserve the spatial size, we pad the input image with zeros around the borders. For instance, $P = 1$ refers to adding a single border of zeros around the input.</p>
<p>Padding also helps retain edge information, as without it, edge pixels contribute less to the output because the filter doesn&rsquo;t fully cover them.</p>
<figure class="align-center ">
    <img loading="lazy" src="../conv-2.gif#center"/> 
</figure>

<p>The two most common padding terms are:</p>
<ul>
<li>Valid padding: No padding is applied, i.e., $P = 0$.</li>
<li>Same padding: The output size equals the input size, i.e., $W&rsquo; = W$. This is achieved with $P = (K_w - 1)/2 $.</li>
</ul>
<h3 id="stride">Stride<a hidden class="anchor" aria-hidden="true" href="#stride">#</a></h3>
<p>Earlier, I mentioned that we convolve our filter with every pixel of the input channel. However, we can choose to convolve with every alternate pixel by setting the stride $S = 2$.</p>
<p>Stride refers to the number of pixels by which the filter moves across the input image during the convolution operation. When using a stride greater than 1, the filter skips certain pixels, effectively reducing the spatial size of the input.</p>
<p>The image below shows a convolution operation with padding = 1 and stride = 2.</p>
<figure class="align-center ">
    <img loading="lazy" src="../conv-3.gif#center"/> 
</figure>

<p>Using larger strides can effectively downsample the input, reducing computational costs and speeding up training. However, it may result in skipping over fine-grained details in the input image.</p>
<p>Next, let&rsquo;s examine how the spatial size of feature maps affects computation costs in a convolutional layer.</p>
<h3 id="computational-cost">Computational Cost<a hidden class="anchor" aria-hidden="true" href="#computational-cost">#</a></h3>
<p>Consider a convolutional layer with 10 filters of size $5 \times 5$, stride 1, and pad 2 (same padding), applied to an RGB image of size $(3, 32, 32)$.</p>
<ul>
<li>Input: $[C_{in} \times H \times W] = 3 \times 32 \times 32$</li>
<li>Convolution: $[C_{out} \times C_{in} \times K_w \times K_h]$ = 10 filters of size $3 \times 5 \times 5$</li>
<li>Output: $[C_{out} \times H&rsquo; \times W&rsquo;] = 10 \times 32  \times 32$</li>
</ul>
<p>For a batch size of $B=1$, let&rsquo;s calculate:</p>
<p><strong>1. Memory</strong>: To store the output as floats: $(C_{out} \times H&rsquo; \times W&rsquo;) * 4/1024$
$(10 \times 32  \times 32) * 4/1024$ = 40 KB.</p>
<p><strong>2. Number of learnable parameters</strong>: $\underbrace{(C_{out} * C_{in} \times K_w \times K_h)}_{\text{filters}} + \underbrace{C_{out}}_{\text{bias}}$
$(10 * 3 \times 5 \times 5) + 10$ = 760.</p>
<p><strong>3. Number of Floating-point operations</strong> (FLOPs): Total multiply-add operations
$\underbrace{(C_{out} \times H&rsquo; \times W&rsquo;)}_{\text{output size}} * \underbrace{(C_{in} \times K_w \times K_h)}_{\text{filter size}}$</p>
<p>$10 \times 32 \times 32 = 10240$ outputs, each of which is the inner product of input with a $3 \times 5 \times 5 = 75$ tensor. Total = $75 * 10240$ = 768k FLOPs.</p>
<p>Memory calculations help estimate the maximum batch size that can be used without exceeding GPU memory limits, learnable parameters define the model’s capacity to learn from data, and FLOPs influence the computational time during training and inference.</p>
<h3 id="receptive-fields">Receptive Fields<a hidden class="anchor" aria-hidden="true" href="#receptive-fields">#</a></h3>
<p>Receptive fields refer to the specific regions of the input data that a feature map responds to. In the convolution shown below, each element of the output layer depends on a $3 \times 3$ receptive field in the input.</p>
<figure class="align-center ">
    <img loading="lazy" src="../receptive-field.png#center"/> 
</figure>

<p>To cover the entire input of size $(7, 7)$ with $3 \times 3$ kernels, at least 3 convolution layers are needed to ensure that a single output can &ldquo;see&rdquo; the whole input image.</p>
<p>Stacking multiple convolution layers increases the size of the receptive fields, allowing the network to capture more global information about the input. However, this stacking would result in one large convolution, so we typically apply activation functions (non-linearities) after each convolution. ReLU is a commonly used activation function in this context.</p>
<p>For large images, many layers may be required to capture global information, which can become computationally expensive. A solution is to downsample the feature map within the network, effectively increasing the receptive fields of subsequent layers while reducing computation. This is where pooling layers come into play.</p>
<h2 id="pooling-layer">Pooling Layer<a hidden class="anchor" aria-hidden="true" href="#pooling-layer">#</a></h2>
<p>Unlike convolutional layers, which apply a filter to extract features, pooling layers summarize information within a localized region of the input. One common pooling method is max pooling, where we take the maximum value from the elements within a defined kernel.</p>
<p>Pooling layers have two hyperparameters: kernel size and stride. For example, consider a max pooling layer with a kernel size of 2 and a stride of 2:</p>
<figure class="align-center ">
    <img loading="lazy" src="../maxpool.gif#center" width="400"/> 
</figure>

<p>This operation effectively reduces the spatial dimensions of the input by half, while retaining the most prominent features (the strongest responses) and discarding less relevant details.</p>
<p>The max operation introduces <strong>translational invariance</strong>: whether a key feature, like a cat&rsquo;s ear, is located in the top-left corner of the filter or the bottom-right, the output remains the same since we are taking the maximum over the kernel. This property allows the model to be more robust to small spatial shifts in the input.</p>
<p>Here are some key features of pooling layers:</p>
<ul>
<li>Pooling layers do not contain learnable parameters, which simplifies the backpropagation process. During backpropagation, they simply pass gradients back to the locations of the maximum values from the previous layer.</li>
<li>As pooling is a non-linear operation, it adds non-linearity to the model without requiring an additional activation function.</li>
</ul>
<p>Another pooling method, average pooling, calculates the average of the elements within the kernel. This approach tends to smooth out the feature map by considering all values within the window, making it useful for reducing noise. However, max pooling is generally preferred for its ability to highlight the most significant features in the input.</p>
<h2 id="network-architecture">Network Architecture<a hidden class="anchor" aria-hidden="true" href="#network-architecture">#</a></h2>
<p>To maintain the spatial dimension of the input, same padding with stride 1 is frequently used. Below are some common choices for hyperparameters in convolutional layers:</p>
<ul>
<li>$C_{in}$, $C_{out}$ = 32, 64, 128, 256 (in powers of 2)</li>
<li>Kernel = $5 \times 5$, Padding = 2, Stride = 1</li>
<li>Kernel = $3 \times 3$, Padding = 1, Stride = 1</li>
<li>Kernel = $3 \times 3$, Padding = 1, Stride = 2  [downsample by 2]</li>
<li>Kernel = $1 \times 1$, Padding = 0, Stride = 1  [Pointwise convolution]</li>
</ul>
<p>Using channel dimensions that are powers of 2 allows for more efficient memory allocation on GPUs.</p>
<p>The $1 \times 1$ convolution layer is also called pointwise convolution because the filter operates on each pixel individually across the depth (channels) of the input. This allows changing the number of channels while keeping the spatial dimensions intact, as the depth of the resulting feature map is determined by the number of filters used.</p>
<figure class="align-center ">
    <img loading="lazy" src="../architecture.png#center"/> 
</figure>

<p>A classical architecture for a convolutional neural network follows this structure:
$$
[\text{Conv, ReLU, Pool}]_{\times N}  \rightarrow Flatten \rightarrow [\text{FC, ReLU}]_{\times M} \rightarrow FC
$$</p>
<p>The input image is first processed through multiple layers of convolution, followed by ReLU activations, and then pooling to downsample the feature maps.</p>
<p>The initial layers of a CNN learn to detect basic patterns such as edges, corners, and simple textures. These low-level features are often universal across different images, making them not specific to any particular object class.</p>
<p>As the network deepens, subsequent layers begin to detect more complex patterns by combining low-level features to recognize higher-level representations, such as shapes, textures, or parts of objects (e.g., eyes, wheels, or fur textures).</p>
<p>After the convolutional and pooling layers, the output is a multi-dimensional tensor representing the learned features. This tensor is then flattened into a 1D vector and fed into fully connected (FC) layers with ReLU activations. These layers combine the learned features to make predictions.</p>
<p>The final fully-connected layer produces the output, typically class scores for each category in a classification task.</p>
<figure class="align-center ">
    <img loading="lazy" src="../feature-map.png#center"
         alt="Visualization of features in a fully trained model. The left image shows the kernels learned by the first convolutional layer of AlexNet [1], while the right image displays the features learned in Layers 3-5 of ZFNet [2]."/> <figcaption>
            <p>Visualization of features in a fully trained model. The left image shows the kernels learned by the first convolutional layer of AlexNet <a href="#references">[1]</a>, while the right image displays the features learned in Layers 3-5 of ZFNet <a href="#references">[2]</a>.</p>
        </figcaption>
</figure>

<h3 id="coding">Coding<a hidden class="anchor" aria-hidden="true" href="#coding">#</a></h3>
<p>Let&rsquo;s modify our model from <a href="https://yugajmera.github.io/posts/02-dl2/post/#mnist">here</a> to a convolutional neural network for classifying handwritten digits in the MNIST dataset.</p>
<figure class="align-center ">
    <img loading="lazy" src="../coding-arch.png#center"/> 
</figure>

<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Define the model</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CNN</span>(torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(CNN, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Output: 32 x 28 x 28</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>max_pool1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)                                      
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Output: 32 x 14 x 14</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Output: 64 x 14 x 14</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>max_pool2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)                                       
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Output: 64 x 7 x 7</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">7</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">7</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>ReLU()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>max_pool1(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv2(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>max_pool2(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># Flatten</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc2(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize the model and move it to the selected device</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> CNN()<span style="color:#f92672">.</span>to(device)
</span></span></code></pre></div><p>I used the Adam optimizer with a learning rate of 0.001, achieving a test accuracy of 99%.</p>
<p>To view the computational statistics of our model, we can use <code>torchsummary</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> torchsummary <span style="color:#f92672">import</span> summary
</span></span><span style="display:flex;"><span>summary(model, (<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>))
</span></span></code></pre></div><figure class="align-center ">
    <img loading="lazy" src="../model-summary.png#center"/> 
</figure>

<p>We can draw the following conclusions from this:</p>
<ul>
<li>The memory used to store the output as floats is primarily consumed by the convolutional layers.</li>
<li>Almost all learnable parameters are found in the fully connected layers.</li>
</ul>
<p>While it&rsquo;s possible to create deeper convolutional neural networks, training them can be quite challenging. Like linear networks, they face overfitting issues, and convergence becomes increasingly difficult as depth increases. A common solution to this problem is Batch Normalization <a href="#references">[3]</a>.</p>
<h2 id="batch-normalization">Batch Normalization<a hidden class="anchor" aria-hidden="true" href="#batch-normalization">#</a></h2>
<p>The idea behind batch normalization is to normalize the activations of a layer so that they have zero mean and unit variance. But why is this necessary?</p>
<p>As a neural network trains, the distribution of inputs to its layers can shift due to weight updates. This phenomenon, known as internal covariate shift, can cause the learning algorithm to struggle, effectively chasing a moving target. Batch normalization helps mitigate this shift, stabilizing the training process and improving optimization.</p>
<p>In addition, normalizing the outputs of a layer ensures that the distribution is well-behaved before passing through the non-linear activation function, which helps prevent issues like saturation.</p>
<p>In practice, batch normalization is implemented as a layer that processes inputs before they are passed to the next layer. The normalization can be mathematically described as:</p>
<p>\begin{align}
&amp; \hat{x}_{ij} = \frac{x_{ij} - \mu_j}{\sqrt{\sigma_j^2 + \epsilon}} \\
&amp; y_{ij} = \gamma_j \hat{x}_{ij} + \beta_j
\end{align}</p>
<p>Here, $\mu_j$ and $\sigma_j$ are the mean and standard deviation calculated across the mini-batch of inputs for each channel.</p>
<p>Since maintaining zero mean and unit variance can be too restrictive, we introduce scale ($\gamma$) and shift ($\beta$) parameters that can be learned during training. This ensures that the activations of the layer remain Gaussian throughout the training process.</p>
<p>A beneficial side effect of this normalization process is that it introduces some randomness, which can enhance regularization and improve the model’s generalization ability.</p>
<h3 id="unused-bias">Unused Bias<a hidden class="anchor" aria-hidden="true" href="#unused-bias">#</a></h3>
<p>The bias added to $x_{ij}$ in the previous layer is effectively canceled out when the mean of the outputs is subtracted during batch normalization. As a result, the bias in the layer preceding the batch normalization layer becomes redundant and can be omitted.</p>
<h3 id="training-vs-testing">Training vs Testing<a hidden class="anchor" aria-hidden="true" href="#training-vs-testing">#</a></h3>
<p>Since the $\mu_j$ and $\sigma_j$ are computed from the mini-batch, their values depend on the specific batch. For example, if one test batch contains [cat, dog, frog] and another contains [cat, car, horse], the output for the common image of the cat will differ due to varying means and standard deviations.</p>
<p>To address this, batch normalization behaves differently during training and testing. During testing, we do not compute the mean and standard deviation from the batch; instead, we fix these values and use the running averages collected during training.</p>
<p>The running averages are updated during each training step as:</p>
<p>\begin{align}
\mu_{\text{running}} = (1 - \beta) * \mu_{\text{running}} + \beta * \mu_j
\end{align}</p>
<p>where $\beta$, called the momentum, is typically set to $0.1$.</p>
<p>As a result, during inference, batch normalization becomes a linear operation, allowing it to be easily fused with preceding linear or convolutional layers. Typically, the batch norm layer is inserted after fully connected or convolutional layers and before the activation function.</p>
<h3 id="benefits-of-batch-normalization">Benefits of Batch Normalization<a hidden class="anchor" aria-hidden="true" href="#benefits-of-batch-normalization">#</a></h3>
<p>In summary, batch normalization offers several advantages:</p>
<ul>
<li>Makes deep networks much easier to train by mitigating internal covariate shift.</li>
<li>Allows for higher learning rates, which can lead to faster convergence during training.</li>
<li>Normalization process makes networks more robust to weight initialization.</li>
<li>Acts as a form of regularization during training.</li>
<li>Zero computational overhead at test time (uses fixed parameters so can be fused with the previous layer)</li>
</ul>
<h3 id="variants">Variants<a hidden class="anchor" aria-hidden="true" href="#variants">#</a></h3>
<p>While batch normalization is powerful, it behaves differently during training and testing. To address this, several variants have been developed.</p>
<h4 id="layer-normalization">Layer Normalization<a hidden class="anchor" aria-hidden="true" href="#layer-normalization">#</a></h4>
<p>Instead of averaging over batch dimensions, it averages over the feature dimension, resulting in per-channel mean and standard deviation. This makes it independent of the batch size, and it behaves the same during both training and testing. Layer normalization is commonly used in RNNs and Transformers, where batch sizes can vary significantly.</p>
<h4 id="instance-normalization">Instance Normalization<a hidden class="anchor" aria-hidden="true" href="#instance-normalization">#</a></h4>
<p>Here, the normalization is done over the spatial dimensions of each image, resulting in per-image mean and standard deviation. It also behaves consistently during training and testing, and is often used in style transfer and image generation tasks.</p>
<h4 id="group-normalization">Group Normalization<a hidden class="anchor" aria-hidden="true" href="#group-normalization">#</a></h4>
<p>Instead of normalizing across the entire channel dimension like layer normalization, group normalization splits the channels into groups and normalizes within each group. This method is particularly effective for tasks like object detection and is commonly used in group convolutions.</p>
<p>The figure below provides an intuitive understanding of the four types of normalizations.</p>
<figure class="align-center ">
    <img loading="lazy" src="../batchnorm.png#center"/> 
</figure>

<h3 id="coding-1">Coding<a hidden class="anchor" aria-hidden="true" href="#coding-1">#</a></h3>
<p>Let&rsquo;s add Batch Normalization to our model. Since Batch Normalization behaves differently during training and testing, we must ensure to use <code>model.train()</code> and <code>model.eval()</code> to switch between these modes. This guarantees that we compute the mean and variance in training mode and use the running averages in evaluation mode.</p>
<p>Additionally, since most of the learnable parameters are in the fully connected layers, there’s a higher risk of overfitting. To address this, I’ll include a dropout layer for regularization.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Creating the architecture</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DigitClassification</span>(torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(DigitClassification, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># First convolutional block: Conv -&gt; BatchNorm -&gt; ReLU -&gt; MaxPool</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>BatchNorm2d(<span style="color:#ae81ff">32</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>max_pool1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Second convolutional block: Conv -&gt; BatchNorm -&gt; ReLU -&gt; MaxPool</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>BatchNorm2d(<span style="color:#ae81ff">64</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>max_pool2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>MaxPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Fully connected layers: FC -&gt; ReLU -&gt; Dropout -&gt; FC</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">7</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">7</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Dropout(p<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>ReLU()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>conv1(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>max_pool1(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>conv2(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>max_pool2(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># Flatten</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(x)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc2(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>With this implementation, I achieved a test accuracy of 99.3%, giving us the best model we’ve had until now.</p>
<p>It can be challenging to make numerous decisions regarding the architecture of a CNN model and its various hyperparameters in pursuit of the best possible accuracy. Therefore, it’s always beneficial to review state-of-the-art CNN models that have been successful in the past for inspiration. Let’s explore that in our next post.</p>
<p> </p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, &ldquo;<a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a>&rdquo;, NeurIPS 2012.</p>
<p>[2] Matthew D. Zeiler and Rob Fergus. “<a href="https://arxiv.org/abs/1311.2901">Visualizing and Understanding Convolutional Networks</a>”, ECCV 2014.</p>
<p>[3] Ioffe and Szegedy, “<a href="https://arxiv.org/abs/1502.03167">Batch normalization: Accelerating deep network training by reducing internal covariate shift</a>”, ICML 2015.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://yugajmera.github.io/posts/05-imagenet/post/">
    <span class="title">« Prev</span>
    <br>
    <span>ImageNet Challenge: The Olympics of Deep Learning</span>
  </a>
  <a class="next" href="https://yugajmera.github.io/posts/03-dl3/post/">
    <span class="title">Next »</span>
    <br>
    <span>Deep Learning Basics Part 3: The Cherry on Top</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://yugajmera.github.io/">YA&#39;s Almanac</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
