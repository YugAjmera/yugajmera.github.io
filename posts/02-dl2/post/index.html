<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Deep Learning Basics Part 2: The Icing | YA&#39;s Almanac</title>
<meta name="keywords" content="">
<meta name="description" content="In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.
However, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable.">
<meta name="author" content="">
<link rel="canonical" href="https://yugajmera.github.io/posts/02-dl2/post/">
<link crossorigin="anonymous" href="https://yugajmera.github.io/assets/css/stylesheet.334cb677465a1d1e9767dd05b097c98de3a5b4085e0d43c708104df17bc49585.css" integrity="sha256-M0y2d0ZaHR6XZ90FsJfJjeOltAheDUPHCBBN8XvElYU=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://yugajmera.github.io/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://yugajmera.github.io/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://yugajmera.github.io/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://yugajmera.github.io/assets/apple-touch-icon.png">
<link rel="mask-icon" href="https://yugajmera.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://yugajmera.github.io/posts/02-dl2/post/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[', '\\]']],
      processEscapes: true,
      processEnvironments: true
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  };

  window.addEventListener('load', (event) => {
      document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
<style>
   mjx-container[display="true"] {
       margin: 1.5em 0 ! important
   }
</style>



<script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-S759YBMKJE', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Deep Learning Basics Part 2: The Icing" />
<meta property="og:description" content="In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.
However, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://yugajmera.github.io/posts/02-dl2/post/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-08-05T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-08-05T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deep Learning Basics Part 2: The Icing"/>
<meta name="twitter:description" content="In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.
However, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://yugajmera.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Deep Learning Basics Part 2: The Icing",
      "item": "https://yugajmera.github.io/posts/02-dl2/post/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Deep Learning Basics Part 2: The Icing",
  "name": "Deep Learning Basics Part 2: The Icing",
  "description": "In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.\nHowever, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable.",
  "keywords": [
    
  ],
  "articleBody": "In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.\nHowever, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable. We could then apply a linear classifier on top of these features to classify each image. This would result in a non-linear decision boundary when projected back to the input space.\nWhile possible, this approach is cumbersome since it requires manually defining feature extraction methods for each specific task. Instead, we want our model to jointly learn both the feature representation and the linear classifier. This is where neural networks come in. Neural networks are built using chunks of linear classifiers, which is why they are also called Multi-Layer Perceptrons (MLPs).\nNeural Networks A neural network is composed of interconnected layers, where every neuron in one layer connects to every neuron in the next layer. A typical deep learning model has multiple layers, where the ‚Äúdepth‚Äù refers to the number of layers or the number of learnable weight matrices.\n\\begin{align} \\text{Linear function: } \u0026 f = W x \\\\ \\text{2-Layer Neural Network: } \u0026 f = W_2 \\hspace{2mm} z(W_1 x) \\\\ \\text{3-Layer Neural Network: } \u0026 f = W_3 \\hspace{2mm} z_2(W_2 \\hspace{2mm} z_1(W_1 x)) \\\\ \\end{align}\nEach layer is followed by a non-linear function called an activation function $z_i$. Activation functions are critical because, without them, stacking multiple linear layers would simply result in a linear classifier ($ y = W_2 W_1 x = W x $).\nThe non-linearity introduced by activation functions allows neural networks to create complex decision boundaries. As the number of layers (and activation functions) increases, the decision boundary becomes more intricate in the input space, enabling the model to fit the data more effectively.\nActivation functions There are several popular activation functions to choose from, each with its pros and cons. Let‚Äôs briefly explore them:\nSigmoid This function transforms input data into the range $[0, 1]$, which can be interpreted as a probability (i.e., the likelihood of a particular feature being present). But, it has significant drawbacks:\nFor large positive or negative inputs, the output saturates (the curve becomes parallel to the x-axis), causing the gradient to approach zero ($\\mathbf{dw} \\approx 0$). This is known as the vanishing gradient problem, which effectively kills the learning process. Computing the exponential function is expensive, slowing down training. Tanh The tanh function is a scaled and shifted version of sigmoid that outputs in the range $[-1, 1]$, making it zero-centered. This allows for both positive and negative outputs, improving gradient flow compared to sigmoid. However:\nIt still suffers from the vanishing gradient problem. Like sigmoid, it relies on exponentials, making it computationally expensive. ReLU ReLU is the most commonly used activation function due to its computational efficiency, which leads to faster training. But,\nIt has a dying ReLU problem‚Äîwhen the inputs are negative, the gradient becomes zero, and those neurons stop learning. These become inactive (as they always ouput zero), reducing the capacity of the model. Leaky ReLU Leaky ReLU is a variation that addresses the dying ReLU problem by adding a small, non-zero gradient for negative input. This prevents neurons from dying in the negative region.\nThe slope in the negative region (usually 0.1) is a hyperparameter, introducing another variable to tune üòî. There are advanced variations like Exponential Linear Unit (ELU), Scaled ELU (SELU), and Gaussian Error Linear Unit (GELU), each offering specific benefits. A good rule of thumb is to start with ReLU as your default choice. For finer improvements, you can experiment with these advanced variants to push your model‚Äôs accuracy by a small margin.\nWeight Initialization Once the architecture of the network is fixed, the next step is to initialize the weights of each layer. Weight initialization sets the starting point on the loss landscape for gradient descent. Each new set of initial weights kicks off the optimization process from a different spot, potentially leading to different final solutions. Therefore, the choice of weight initialization is crucial, as it significantly influences the optimization path and the model‚Äôs convergence.\nInitalizing with zeros - If the weights are initialized to a constant value, such as zero, the activations (outputs of neurons: linear layer + activation function) will also be zero, and the gradients will be zero. This effectively kills learning, preventing the network from even starting to train!\nW_l = 0 # very bad idea Initializing with small random numbers - Using a Gaussian distribution with zero mean and a standard deviation of 0.01 can work for shallow networks. However, for deeper networks, repeatedly multiplying small weights (less than 1) through many layers will shrink the activations as they propagate, causing them to approach zero. This leads to the vanishing gradient problem with tanh and ReLU non-linearities.\nW_l = 0.01 * np.random.randn(Din, Dout) Initializing with larger standard deviation - If weights are initialized with a larger standard deviation, the activations can grow exponentially as they propagate, leading to very large outputs and gradients. This is known as the exploding gradient problem.\nThus, weight initialization must strike a balance to avoid both extremes‚Äîactivations that neither vanish nor explode. This concept is applied in Xavier Initialization [1].\nXavier Initialization If we can ensure that the variance of the output of a layer is the same as the variance of the input, the scale of activations won‚Äôt change as they propagate allowing us to avoid vanishing or exploding gradients. To derive this, let‚Äôs consider a linear layer:\n\\begin{align} y \u0026= \\sum_{i=1}^{D_{in}} x_i w_i \\\\ Var(y) \u0026= D_{in} * Var(x_i w_i) \\\\ \u0026= D_{in} * (E[x_i^2]E[w_i^2] - E[x_i]^2E[w_i]^2) \u0026\\text{[Assume x,w independent]} \\\\ \u0026= D_{in} * (E[x_i^2]E[w_i^2]) \u0026\\text{[Assume x,w are zero-mean]} \\\\ \u0026= D_{in} * Var(x_i) * Var(w_i) \\end{align} To maintain the variance between input and output, $Var(y) = Var(x_i)$, $$ \\Rightarrow Var(w_i) = 1/D_{in} $$\nTherefore, we initialize the weights of each layer with zero mean and a standard deviation equal to the inverse square root of the input dimension of that particular layer.\nW_l = np.random.randn(Din, Dout) * np.sqrt(1/Din) Note the input $x_i$ refers to the activation (output) from the previous layer. Our derivation assumes that this activation should be zero-centered (or have zero-mean), which holds true for the activation functions like tanh that are centered around zero. However, this initialization does not work well for networks with ReLU non-linearities and needs to be adjusted. This modification is incorporated in Kaiming Initialization [2].\nKaiming/ He Initialization Assuming that the inputs are zero-centered, and ReLU kills all non-negative inputs, we effectively obtaining only half of our expected outputs, resulting in the the variance being halved, $Var(y) = Var(x_i) / 2$. To maintain the scale of activations across layers, $$ \\Rightarrow Var(w_i) = 2/D_{in} $$\nW_l = np.random.randn(Din, Dout) * np.sqrt(2/Din) Backpropagation Most of our choices so far have revolved around gradients, the essential flour of our deep learning cake. To compute these derivatives of the loss function with respect to the weights of our neural network, we rely on a computation graph‚Äîour recipe for success!\nA computation graph is a directed graph that represents the computations inside our model. Let‚Äôs take a simple example of a computation graph that represents the following equation: \\begin{align} q = x + y \\ \\ \\text{and} \\ \\ f = z * q \\end{align}\nFirst, we perform a forward pass through the graph (denoted in green), where we compute the outputs of each node sequentially, given the inputs: $x = -2, y = 5, z = -4$. \\begin{align} q \u0026= x + y = -2 + 5 = 3\\\\ f \u0026 = z * q = -4 * 3 = -12 \\end{align}\nBackpropagation is the algorithm we use to compute gradients in this computation graph. In our backward pass (denoted in red), we apply the chain rule to compute the derivatives of each node in reverse order with respect to the output $f$. \\begin{align} \\frac{df}{df} \u0026= 1 \\\\ \\frac{df}{dq} \u0026= z = -4 \\\\ \\frac{df}{dz} \u0026= q = 3 \\\\ \\frac{df}{dx} \u0026= \\frac{df}{dq} \\frac{dq}{dx} = -4 * 1 = -4 \\\\ \\frac{df}{dy} \u0026= \\frac{df}{dq} \\frac{dq}{dy} = -4 * 1 = -4 \\end{align}\nIn this way, we compute the derivative of the output with respect to the inputs. In deep learning, the output $f$ is typically the loss function, and we use this method to compute the gradients of the loss with respect to the weights. With more complex neural networks, we follow the same procedure‚Äîbut don‚Äôt worry, you don‚Äôt have to compute these gradients manually! Libraries like PyTorch handle it for us.\nCoding from scratch Okay, enough theory‚Äîlet‚Äôs dive into coding a neural network from scratch using the PyTorch library. Here are a couple of key PyTorch terminologies to understand:\nTensor: Similar to a NumPy array but with the ability to run on GPUs. Autograd: A package that builds computational graphs from Tensors and automatically computes gradients. For our example, we will build a simple neural network for image classification. The input is an image of size (32 x 32 x 3 = 3072), and our model will have two layers with ReLU activation, outputting scores for 10 classes: [Input -\u003e Linear -\u003e ReLU -\u003e Linear -\u003e Output]\nWe‚Äôll follow this pipeline for training:\nInitialize the model and weights. # Initialize the weights w1 = torch.randn(3072, 100, requires_grad=True) * torch.sqrt(2/3072) w2 = torch.randn(100, 10, requires_grad=True) * torch.sqrt(2/100) Since we want to compute the gradients with respect to weights, we initialize the weights as tensors with the requires_grad = True flag. This tells Pytorch to enable autograd on these tensors and build a computational graph as the tensors are used in operations later in our code. We initialize the weights using Kaiming initialization because we use the ReLU activation function.\nPerform a forward pass of the model to get predictions. # Forward Pass y_pred = X_batch.mm(w1).clamp(min=0).mm(w2) X_batch is the mini-batch of inputs, y_batch is the corresponding labels, and y_pred is the model‚Äôs output.\nCompute the loss value (between true labels and predictions) via the loss function. # Compute the loss loss = loss_fn(y_pred, y_batch) The loss function returns the average loss over a mini-batch, which we use to compute gradients.\nUse backpropagation to compute the gradients. # Compute gradients of loss wrt weights loss.backward() Once we compute the loss, the gradient of the loss with respect to the weights can automatically be computed by running loss.backward(). This function implements backpropagation on all the inputs that have the requires_grad flag set to true and accumulates the gradients their .grad() attribute.\nUpdate the weights using gradient descent. # Gradient descent step with torch.no_grad(): # Tells Pytorch not to build a graph w1 -= learning_rate * w1.grad() w2 -= learning_rate * w2.grad() # Set gradients to zero w1.zero_grad() w2.zero_grad() The torch.no_grad() function ensures that no computation graph is built during the weight update step. It‚Äôs crucial to run .zero_grad() function to clear the current gradients after each update step and accumulate fresh gradients-‚Äîotherwise, gradients would keep accumulating.\nThe entire code put together looks something like this:\n# Initialize the weights w1 = torch.randn(3072, 100, requires_grad=True) * torch.sqrt(2/3072) w2 = torch.randn(100, 10, requires_grad=True) * torch.sqrt(2/100) # Split data into mini-batches mini_batches = split(data, batch_size) for t in range(num_steps): for X_batch, y_batch in mini_batches: # Forward Pass y_pred = X_batch.mm(w1).clamp(min=0).mm(w2) # Compute the loss loss = loss_fn(y_pred, y_batch) # Compute gradients of loss wrt weights loss.backward() # Gradient descent step with torch.no_grad(): # Tells Pytorch not to build a graph w1 -= learning_rate * w1.grad() w2 -= learning_rate * w2.grad() # Set gradients to zero w1.zero_grad() w2.zero_grad() Pretty simple, right? Kudos! You can now code a neural network!\nNext, let‚Äôs leverage PyTorch‚Äôs high-level APIs to implement the same model in a more scalable manner:\nDataLoader: Helps in creating mini-batches efficiently. Module: A layer or model class where weights are automatically set to requires_grad=True and initialized using Kaiming initialization. optim: PyTorch‚Äôs package for optimization algorithms like Stochastic Gradient Descent (SGD) and Adam. Loss functions: PyTorch provides a variety of loss functions to choose from, depending on the task at hand. class TwoLayerNet(torch.nn.Module): def __init__(self): super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(3072, 100) self.linear2 = torch.nn.Linear(100, 10) def forward(self, X): output1 = torch.nn.functional.relu(self.linear1(X)) y_pred = self.linear2(output1) return y_pred # Define the model model = TwoLayerNet() # Define the loss function criterion = nn.CrossEntropyLoss() # Define the optimizer optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Create mini-batches mini_batches = torch.utils.data.DataLoader(data, batch_size=batch_size) # Train for t in range(num_steps): for X_batch, y_batch in mini_batches: y_pred = model(X_batch) loss = criterion(y_pred, y_batch) loss.backward() optimizer.step() optimizer.zero_grad() With these powerful APIs, PyTorch takes care of most of the grunt work, allowing us to focus on the higher-level logic. This approach to writing neural networks has become the standard due to its clarity and efficiency, and you‚Äôll notice that nearly all deep learning code follows a similar structure.\nOur deep learning cake is almost ready‚Äînow, it‚Äôs time to add the toppings!\nReferences [1] Glorot and Bengio, ‚ÄúUnderstanding the difficulty of training deep feedforward neural networks‚Äù, JMLR 2010.\n[2] He et al, ‚ÄúDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification‚Äù, ICCV 2015.\n",
  "wordCount" : "2227",
  "inLanguage": "en",
  "datePublished": "2022-08-05T00:00:00Z",
  "dateModified": "2022-08-05T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yugajmera.github.io/posts/02-dl2/post/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "YA's Almanac",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yugajmera.github.io/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://yugajmera.github.io/" accesskey="h" title="YA&#39;s Almanac (Alt + H)">YA&#39;s Almanac</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://yugajmera.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://yugajmera.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Deep Learning Basics Part 2: The Icing
    </h1>
    <div class="post-meta"><span title='2022-08-05 00:00:00 +0000 UTC'>August 5, 2022</span>&nbsp;¬∑&nbsp;11 min

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#neural-networks" aria-label="Neural Networks">Neural Networks</a><ul>
                        
                <li>
                    <a href="#activation-functions" aria-label="Activation functions">Activation functions</a><ul>
                        
                <li>
                    <a href="#sigmoid" aria-label="Sigmoid">Sigmoid</a></li>
                <li>
                    <a href="#tanh" aria-label="Tanh">Tanh</a></li>
                <li>
                    <a href="#relu" aria-label="ReLU">ReLU</a></li>
                <li>
                    <a href="#leaky-relu" aria-label="Leaky ReLU">Leaky ReLU</a></li></ul>
                </li>
                <li>
                    <a href="#weight-initialization" aria-label="Weight Initialization">Weight Initialization</a><ul>
                        
                <li>
                    <a href="#xavier-initialization" aria-label="Xavier Initialization">Xavier Initialization</a></li>
                <li>
                    <a href="#kaiming-he-initialization" aria-label="Kaiming/ He Initialization">Kaiming/ He Initialization</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#backpropagation" aria-label="Backpropagation">Backpropagation</a></li>
                <li>
                    <a href="#coding-from-scratch" aria-label="Coding from scratch">Coding from scratch</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.</p>
<p>However, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable. We could then apply a linear classifier on top of these features to classify each image. This would result in a non-linear decision boundary when projected back to the input space.</p>
<figure class="align-center ">
    <img loading="lazy" src="https://yugajmera.github.io/posts/02-DL2/transform.png#center"/> 
</figure>

<p>While possible, this approach is cumbersome since it requires manually defining feature extraction methods for each specific task. Instead, we want our model to jointly learn both the feature representation and the linear classifier. This is where neural networks come in. Neural networks are built using chunks of linear classifiers, which is why they are also called Multi-Layer Perceptrons (MLPs).</p>
<h2 id="neural-networks">Neural Networks<a hidden class="anchor" aria-hidden="true" href="#neural-networks">#</a></h2>
<figure class="align-center ">
    <img loading="lazy" src="https://yugajmera.github.io/posts/02-DL2/neural-network.png#center"/> 
</figure>

<p>A neural network is composed of interconnected layers, where every neuron in one layer connects to every neuron in the next layer. A typical deep learning model has multiple layers, where the &ldquo;depth&rdquo; refers to the number of layers or the number of learnable weight matrices.</p>
<p>\begin{align}
\text{Linear function: } &amp; f = W x \\
\text{2-Layer Neural Network: } &amp; f = W_2 \hspace{2mm} z(W_1 x) \\
\text{3-Layer Neural Network: } &amp; f = W_3 \hspace{2mm} z_2(W_2 \hspace{2mm} z_1(W_1 x)) \\
\end{align}</p>
<p>Each layer is followed by a non-linear function called an activation function $z_i$. Activation functions are critical because, without them, stacking multiple linear layers would simply result in a linear classifier ($ y = W_2 W_1 x = W x $).</p>
<p>The non-linearity introduced by activation functions allows neural networks to create complex decision boundaries. As the number of layers (and activation functions) increases, the decision boundary becomes more intricate in the input space, enabling the model to fit the data more effectively.</p>
<h3 id="activation-functions">Activation functions<a hidden class="anchor" aria-hidden="true" href="#activation-functions">#</a></h3>
<p>There are several popular activation functions to choose from, each with its pros and cons. Let&rsquo;s briefly explore them:</p>
<figure class="align-center ">
    <img loading="lazy" src="https://yugajmera.github.io/posts/02-DL2/activations.png#center"/> 
</figure>

<h4 id="sigmoid">Sigmoid<a hidden class="anchor" aria-hidden="true" href="#sigmoid">#</a></h4>
<p>This function transforms input data into the range $[0, 1]$, which can be interpreted as a probability (i.e., the likelihood of a particular feature being present). But, it has significant drawbacks:</p>
<ul>
<li>For large positive or negative inputs, the output saturates (the curve becomes parallel to the x-axis), causing the gradient to approach zero ($\mathbf{dw} \approx 0$). This is known as the vanishing gradient problem, which effectively kills the learning process.</li>
<li>Computing the exponential function is expensive, slowing down training.</li>
</ul>
<h4 id="tanh">Tanh<a hidden class="anchor" aria-hidden="true" href="#tanh">#</a></h4>
<p>The tanh function is a scaled and shifted version of sigmoid that outputs in the range $[-1, 1]$, making it zero-centered. This allows for both positive and negative outputs, improving gradient flow compared to sigmoid. However:</p>
<ul>
<li>It still suffers from the vanishing gradient problem.</li>
<li>Like sigmoid, it relies on exponentials, making it computationally expensive.</li>
</ul>
<h4 id="relu">ReLU<a hidden class="anchor" aria-hidden="true" href="#relu">#</a></h4>
<p>ReLU is the most commonly used activation function due to its computational efficiency, which leads to faster training. But,</p>
<ul>
<li>It has a dying ReLU problem‚Äîwhen the inputs are negative, the gradient becomes zero, and those neurons stop learning. These become inactive (as they always ouput zero), reducing the capacity of the model.</li>
</ul>
<h4 id="leaky-relu">Leaky ReLU<a hidden class="anchor" aria-hidden="true" href="#leaky-relu">#</a></h4>
<p>Leaky ReLU is a variation that addresses the dying ReLU problem by adding a small, non-zero gradient for negative input. This prevents neurons from dying in the negative region.</p>
<ul>
<li>The slope in the negative region (usually 0.1) is a hyperparameter, introducing another variable to tune üòî.</li>
</ul>
<p>There are advanced variations like Exponential Linear Unit (ELU), Scaled ELU (SELU), and Gaussian Error Linear Unit (GELU), each offering specific benefits. A good rule of thumb is to start with ReLU as your default choice. For finer improvements, you can experiment with these advanced variants to push your model‚Äôs accuracy by a small margin.</p>
<h3 id="weight-initialization">Weight Initialization<a hidden class="anchor" aria-hidden="true" href="#weight-initialization">#</a></h3>
<p>Once the architecture of the network is fixed, the next step is to initialize the weights of each layer. Weight initialization sets the starting point on the loss landscape for gradient descent.
Each new set of initial weights kicks off the optimization process from a different spot, potentially leading to different final solutions. Therefore, the choice of weight initialization is crucial, as it significantly influences the optimization path and the model&rsquo;s convergence.</p>
<p>Initalizing with zeros - If the weights are initialized to a constant value, such as zero, the activations (outputs of neurons: linear layer + activation function) will also be zero, and the gradients will be zero. This effectively kills learning, preventing the network from even starting to train!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>W_l <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span> <span style="color:#75715e"># very bad idea</span>
</span></span></code></pre></div><p>Initializing with small random numbers - Using a Gaussian distribution with zero mean and a standard deviation of 0.01 can work for shallow networks. However, for deeper networks, repeatedly multiplying small weights (less than 1) through many layers will shrink the activations as they propagate, causing them to approach zero. This leads to the vanishing gradient problem with tanh and ReLU non-linearities.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>W_l <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span> <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(Din, Dout)
</span></span></code></pre></div><p>Initializing with larger standard deviation - If weights are initialized with a larger standard deviation, the activations can grow exponentially as they propagate, leading to very large outputs and gradients. This is known as the exploding gradient problem.</p>
<p>Thus, weight initialization must strike a balance to avoid both extremes‚Äîactivations that neither vanish nor explode. This concept is applied in Xavier Initialization [<a href="#references">1</a>].</p>
<h4 id="xavier-initialization">Xavier Initialization<a hidden class="anchor" aria-hidden="true" href="#xavier-initialization">#</a></h4>
<p>If we can ensure that the variance of the output of a layer is the same as the variance of the input, the scale of activations won&rsquo;t change as they propagate allowing us to avoid vanishing or exploding gradients. To derive this, let&rsquo;s consider a linear layer:</p>
<p>\begin{align}
y &amp;= \sum_{i=1}^{D_{in}} x_i w_i \\
Var(y) &amp;= D_{in} * Var(x_i w_i)  \\
&amp;= D_{in} * (E[x_i^2]E[w_i^2] - E[x_i]^2E[w_i]^2) &amp;\text{[Assume x,w independent]} \\
&amp;= D_{in} * (E[x_i^2]E[w_i^2]) &amp;\text{[Assume x,w are zero-mean]} \\
&amp;= D_{in} * Var(x_i) * Var(w_i)
\end{align}
To maintain the variance between input and output, $Var(y) = Var(x_i)$,
$$
\Rightarrow Var(w_i) = 1/D_{in}
$$</p>
<p>Therefore, we initialize the weights of each layer with zero mean and a standard deviation equal to the inverse square root of the input dimension of that particular layer.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>W_l <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(Din, Dout) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>Din)
</span></span></code></pre></div><p>Note the input $x_i$ refers to the activation (output) from the previous layer. Our derivation assumes that this activation should be zero-centered (or have zero-mean), which holds true for the activation functions like tanh that are centered around zero. However, this initialization does not work well for networks with ReLU non-linearities and needs to be adjusted. This modification is incorporated in Kaiming Initialization [<a href="#references">2</a>].</p>
<h4 id="kaiming-he-initialization">Kaiming/ He Initialization<a hidden class="anchor" aria-hidden="true" href="#kaiming-he-initialization">#</a></h4>
<p>Assuming that the inputs are zero-centered, and ReLU kills all   non-negative inputs, we effectively obtaining only half of our expected outputs, resulting in the the variance being halved, $Var(y) = Var(x_i) / 2$. To maintain the scale of activations across layers,
$$
\Rightarrow Var(w_i) = 2/D_{in}
$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>W_l <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(Din, Dout) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">2</span><span style="color:#f92672">/</span>Din)
</span></span></code></pre></div><h2 id="backpropagation">Backpropagation<a hidden class="anchor" aria-hidden="true" href="#backpropagation">#</a></h2>
<p>Most of our choices so far have revolved around gradients, the essential flour of our deep learning cake. To compute these derivatives of the loss function with respect to the weights of our neural network, we rely on a computation graph‚Äîour recipe for success!</p>
<p>A computation graph is a directed graph that represents the computations inside our model. Let&rsquo;s take a simple example of a computation graph that represents the following equation:
\begin{align}
q = x + y \ \ \text{and} \ \ f = z * q
\end{align}</p>
<figure class="align-center ">
    <img loading="lazy" src="https://yugajmera.github.io/posts/02-DL2/backprop.png#center"/> 
</figure>

<p>First, we perform a forward pass through the graph (denoted in green), where we compute the outputs of each node sequentially, given the inputs: $x = -2, y = 5, z = -4$.
\begin{align}
q &amp;= x + y = -2 + 5 = 3\\
f &amp; = z * q = -4 * 3 = -12
\end{align}</p>
<p>Backpropagation is the algorithm we use to compute gradients in this computation graph. In our backward pass (denoted in red), we apply the chain rule to compute the derivatives of each node in reverse order with respect to the output $f$.
\begin{align}
\frac{df}{df} &amp;= 1 \\
\frac{df}{dq} &amp;= z = -4 \\
\frac{df}{dz} &amp;= q = 3 \\
\frac{df}{dx} &amp;= \frac{df}{dq} \frac{dq}{dx} = -4 * 1 = -4 \\
\frac{df}{dy} &amp;= \frac{df}{dq} \frac{dq}{dy} = -4 * 1 = -4
\end{align}</p>
<p>In this way, we compute the derivative of the output with respect to the inputs. In deep learning, the output $f$ is typically the loss function, and we use this method to compute the gradients of the loss with respect to the weights. With more complex neural networks, we follow the same procedure‚Äîbut don&rsquo;t worry, you don&rsquo;t have to compute these gradients manually! Libraries like PyTorch handle it for us.</p>
<h2 id="coding-from-scratch">Coding from scratch<a hidden class="anchor" aria-hidden="true" href="#coding-from-scratch">#</a></h2>
<p>Okay, enough theory‚Äîlet‚Äôs dive into coding a neural network from scratch using the PyTorch library.  Here are a couple of key PyTorch terminologies to understand:</p>
<ul>
<li>Tensor: Similar to a NumPy array but with the ability to run on GPUs.</li>
<li>Autograd: A package that builds computational graphs from Tensors and automatically computes gradients.</li>
</ul>
<p>For our example, we will build a simple neural network for image classification. The input is an image of size (32 x 32 x 3 = 3072), and our model will have two layers with ReLU activation,  outputting scores for 10 classes: [Input -&gt; Linear -&gt; ReLU -&gt; Linear -&gt; Output]</p>
<figure class="align-center ">
    <img loading="lazy" src="https://yugajmera.github.io/posts/02-DL2/model.png#center"/> 
</figure>

<p>We&rsquo;ll follow this pipeline for training:</p>
<ol>
<li>Initialize the model and weights.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Initialize the weights</span>
</span></span><span style="display:flex;"><span>w1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">3072</span>, <span style="color:#ae81ff">100</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">2</span><span style="color:#f92672">/</span><span style="color:#ae81ff">3072</span>)
</span></span><span style="display:flex;"><span>w2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">10</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">2</span><span style="color:#f92672">/</span><span style="color:#ae81ff">100</span>)
</span></span></code></pre></div><p>Since we want to compute the gradients with respect to weights, we initialize the weights as tensors with the <code>requires_grad = True</code> flag. This tells Pytorch to enable autograd on these tensors and build a computational graph as the tensors are used in operations later in our code. We initialize the weights using Kaiming initialization because we use the ReLU activation function.</p>
<ol start="2">
<li>Perform a forward pass of the model to get predictions.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Forward Pass</span>
</span></span><span style="display:flex;"><span>y_pred <span style="color:#f92672">=</span> X_batch<span style="color:#f92672">.</span>mm(w1)<span style="color:#f92672">.</span>clamp(min<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>mm(w2)  
</span></span></code></pre></div><p><code>X_batch</code> is the mini-batch of inputs, <code>y_batch</code> is the corresponding labels, and <code>y_pred</code> is the model&rsquo;s output.</p>
<ol start="3">
<li>Compute the loss value (between true labels and predictions) via the loss function.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Compute the loss</span>
</span></span><span style="display:flex;"><span>loss <span style="color:#f92672">=</span> loss_fn(y_pred, y_batch)
</span></span></code></pre></div><p>The loss function returns the average loss over a mini-batch, which we use to compute gradients.</p>
<ol start="4">
<li>Use backpropagation to compute the gradients.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Compute gradients of loss wrt weights</span>
</span></span><span style="display:flex;"><span>loss<span style="color:#f92672">.</span>backward()     
</span></span></code></pre></div><p>Once we compute the loss, the gradient of the loss with respect to the weights can automatically be computed by running <code>loss.backward()</code>. This function implements backpropagation on all the inputs that have the <code>requires_grad</code> flag set to true and accumulates the gradients their <code>.grad()</code> attribute.</p>
<ol start="5">
<li>Update the weights using gradient descent.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Gradient descent step</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():       <span style="color:#75715e"># Tells Pytorch not to build a graph</span>
</span></span><span style="display:flex;"><span>    w1 <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> w1<span style="color:#f92672">.</span>grad()
</span></span><span style="display:flex;"><span>    w2 <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> w2<span style="color:#f92672">.</span>grad()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Set gradients to zero</span>
</span></span><span style="display:flex;"><span>w1<span style="color:#f92672">.</span>zero_grad()      
</span></span><span style="display:flex;"><span>w2<span style="color:#f92672">.</span>zero_grad()
</span></span></code></pre></div><p>The <code>torch.no_grad()</code> function ensures that no computation graph is built during the weight update step. It&rsquo;s crucial to run <code>.zero_grad()</code> function to clear the current gradients after each update step and accumulate fresh gradients-‚Äîotherwise, gradients would keep accumulating.</p>
<p>The entire code put together looks something like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Initialize the weights</span>
</span></span><span style="display:flex;"><span>w1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">3072</span>, <span style="color:#ae81ff">100</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">2</span><span style="color:#f92672">/</span><span style="color:#ae81ff">3072</span>)
</span></span><span style="display:flex;"><span>w2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">10</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">2</span><span style="color:#f92672">/</span><span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Split data into mini-batches</span>
</span></span><span style="display:flex;"><span>mini_batches <span style="color:#f92672">=</span> split(data, batch_size)        
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(num_steps):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> X_batch, y_batch <span style="color:#f92672">in</span> mini_batches:
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Forward Pass</span>
</span></span><span style="display:flex;"><span>        y_pred <span style="color:#f92672">=</span> X_batch<span style="color:#f92672">.</span>mm(w1)<span style="color:#f92672">.</span>clamp(min<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>mm(w2)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Compute the loss </span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> loss_fn(y_pred, y_batch)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Compute gradients of loss wrt weights</span>
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()          
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Gradient descent step</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():       <span style="color:#75715e"># Tells Pytorch not to build a graph</span>
</span></span><span style="display:flex;"><span>            w1 <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> w1<span style="color:#f92672">.</span>grad()
</span></span><span style="display:flex;"><span>            w2 <span style="color:#f92672">-=</span> learning_rate <span style="color:#f92672">*</span> w2<span style="color:#f92672">.</span>grad()
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Set gradients to zero</span>
</span></span><span style="display:flex;"><span>        w1<span style="color:#f92672">.</span>zero_grad()      
</span></span><span style="display:flex;"><span>        w2<span style="color:#f92672">.</span>zero_grad()
</span></span></code></pre></div><p>Pretty simple, right? Kudos! You can now code a neural network!</p>
<p>Next, let‚Äôs leverage PyTorch&rsquo;s high-level APIs to implement the same model in a more scalable manner:</p>
<ul>
<li>DataLoader: Helps in creating mini-batches efficiently.</li>
<li>Module: A layer or model class where weights are automatically set to <code>requires_grad=True</code> and initialized using Kaiming initialization.</li>
<li>optim: PyTorch&rsquo;s package for optimization algorithms like Stochastic Gradient Descent (SGD) and Adam.</li>
<li>Loss functions: PyTorch provides a variety of loss functions to choose from, depending on the task at hand.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TwoLayerNet</span>(torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(TwoLayerNet, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">3072</span>, <span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear2 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, X):
</span></span><span style="display:flex;"><span>        output1 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>linear1(X))
</span></span><span style="display:flex;"><span>        y_pred <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>linear2(output1)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> y_pred
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define the model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> TwoLayerNet()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define the loss function</span>
</span></span><span style="display:flex;"><span>criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>CrossEntropyLoss()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define the optimizer</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>SGD(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span>learning_rate)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create mini-batches</span>
</span></span><span style="display:flex;"><span>mini_batches <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>DataLoader(data, batch_size<span style="color:#f92672">=</span>batch_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Train</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(num_steps):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> X_batch, y_batch <span style="color:#f92672">in</span> mini_batches:
</span></span><span style="display:flex;"><span>        y_pred <span style="color:#f92672">=</span> model(X_batch)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> criterion(y_pred, y_batch)
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span></code></pre></div><p>With these powerful APIs, PyTorch takes care of most of the grunt work, allowing us to focus on the higher-level logic. This approach to writing neural networks has become the standard due to its clarity and efficiency, and you‚Äôll notice that nearly all deep learning code follows a similar structure.</p>
<p>Our deep learning cake is almost ready‚Äînow, it&rsquo;s time to add the toppings!</p>
<p>¬†</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1] Glorot and Bengio, ‚Äú<a href="https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a>‚Äù, JMLR 2010.</p>
<p>[2] He et al, ‚Äú<a href="https://arxiv.org/pdf/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>‚Äù, ICCV 2015.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://yugajmera.github.io/posts/03-dl3/post/">
    <span class="title">¬´ Prev</span>
    <br>
    <span>Deep Learning Basics Part 3: The Cherry on Top</span>
  </a>
  <a class="next" href="https://yugajmera.github.io/posts/01-dl1/post/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>Deep Learning Part 1: The Base  of the Cake</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://yugajmera.github.io/">YA&#39;s Almanac</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
