<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Deep Learning Basics Part 2: The Icing | YA's Almanac</title>
<meta name=keywords content><meta name=description content="In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.
However, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable."><meta name=author content><link rel=canonical href=https://yugajmera.github.io/posts/01-dl2/post/><link crossorigin=anonymous href=https://yugajmera.github.io/assets/css/stylesheet.74991b51f7611c2303d0b5649703d675530589621885eb78236e099c22aa93a5.css integrity="sha256-dJkbUfdhHCMD0LVklwPWdVMFiWIYhet4I24JnCKqk6U=" rel="preload stylesheet" as=style><link rel=icon href=https://yugajmera.github.io/assets/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yugajmera.github.io/assets/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yugajmera.github.io/assets/favicon-32x32.png><link rel=apple-touch-icon href=https://yugajmera.github.io/assets/apple-touch-icon.png><link rel=mask-icon href=https://yugajmera.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yugajmera.github.io/posts/01-dl2/post/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>mjx-container[display=true]{margin:1.5em 0!important}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S759YBMKJE",{anonymize_ip:!1})}</script><meta property="og:title" content="Deep Learning Basics Part 2: The Icing"><meta property="og:description" content="In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.
However, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable."><meta property="og:type" content="article"><meta property="og:url" content="https://yugajmera.github.io/posts/01-dl2/post/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-05T00:00:00+00:00"><meta property="article:modified_time" content="2022-08-05T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Deep Learning Basics Part 2: The Icing"><meta name=twitter:description content="In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.
However, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yugajmera.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Deep Learning Basics Part 2: The Icing","item":"https://yugajmera.github.io/posts/01-dl2/post/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Deep Learning Basics Part 2: The Icing","name":"Deep Learning Basics Part 2: The Icing","description":"In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.\nHowever, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable.","keywords":[],"articleBody":"In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.\nHowever, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable. We could then apply a linear classifier on top of these features to classify each image. This would result in a non-linear decision boundary when projected back to the input space.\nWhile possible, this approach is cumbersome since it requires manually defining feature extraction methods for each specific task. Instead, we want our model to jointly learn both the feature representation and the linear classifier. This is where neural networks come in. Neural networks are built using chunks of linear classifiers, which is why they are also called Multi-Layer Perceptrons (MLPs).\nNeural Networks A neural network is composed of interconnected layers, where every neuron in one layer connects to every neuron in the next layer. A typical deep learning model has multiple layers, where the “depth” refers to the number of layers or the number of learnable weight matrices.\n\\begin{align} \\text{Linear function: } \u0026 f = W x \\\\ \\text{2-Layer Neural Network: } \u0026 f = W_2 \\hspace{2mm} z(W_1 x) \\\\ \\text{3-Layer Neural Network: } \u0026 f = W_3 \\hspace{2mm} z_2(W_2 \\hspace{2mm} z_1(W_1 x)) \\\\ \\end{align}\nEach layer is followed by a non-linear function called an activation function $z_i$. Activation functions are critical because, without them, stacking multiple linear layers would simply result in a linear classifier ($ y = W_2 W_1 x = W x $).\nThe non-linearity introduced by activation functions allows neural networks to create complex decision boundaries. As the number of layers (and activation functions) increases, the decision boundary becomes more intricate in the input space, enabling the model to fit the data more effectively.\nActivation functions There are several popular activation functions to choose from, each with its pros and cons. Let’s briefly explore them:\nSigmoid This function transforms input data into the range $[0, 1]$, which can be interpreted as a probability (i.e., the likelihood of a particular feature being present). But, it has significant drawbacks:\nFor large positive or negative inputs, the output saturates (the curve becomes parallel to the x-axis), causing the gradient to approach zero ($\\mathbf{dw} \\approx 0$). This is known as the vanishing gradient problem, which effectively kills the learning process. Computing the exponential function is expensive, slowing down training. Tanh The tanh function is a scaled and shifted version of sigmoid that outputs in the range $[-1, 1]$, making it zero-centered. This allows for both positive and negative outputs, improving gradient flow compared to sigmoid. However:\nIt still suffers from the vanishing gradient problem. Like sigmoid, it relies on exponentials, making it computationally expensive. ReLU ReLU is the most commonly used activation function due to its computational efficiency, which leads to faster training. But,\nIt has a dying ReLU problem—when the inputs are negative, the gradient becomes zero, and those neurons stop learning. These become inactive (as they always ouput zero), reducing the capacity of the model. Leaky ReLU Leaky ReLU is a variation that addresses the dying ReLU problem by adding a small, non-zero gradient for negative input. This prevents neurons from dying in the negative region.\nThe slope in the negative region (usually 0.1) is a hyperparameter, introducing another variable to tune 😔. There are advanced variations like Exponential Linear Unit (ELU), Scaled ELU (SELU), and Gaussian Error Linear Unit (GELU), each offering specific benefits. A good rule of thumb is to start with ReLU as your default choice. For finer improvements, you can experiment with these advanced variants to push your model’s accuracy by a small margin.\nWeight Initialization Once the architecture of the network is fixed, the next step is to initialize the weights of each layer. Weight initialization sets the starting point on the loss landscape for gradient descent. Each new set of initial weights kicks off the optimization process from a different spot, potentially leading to different final solutions. Therefore, the choice of weight initialization is crucial, as it significantly influences the optimization path and the model’s convergence.\nInitalizing with zeros - If the weights are initialized to a constant value, such as zero, the activations (outputs of neurons: linear layer + activation function) will also be zero, and the gradients will be zero. This effectively kills learning, preventing the network from even starting to train!\nW_l = 0 # very bad idea Initializing with small random numbers - Using a Gaussian distribution with zero mean and a standard deviation of 0.01 can work for shallow networks. However, for deeper networks, repeatedly multiplying small weights (less than 1) through many layers will shrink the activations as they propagate, causing them to approach zero. This leads to the vanishing gradient problem with tanh and ReLU non-linearities.\nW_l = 0.01 * np.random.randn(Din, Dout) Initializing with larger standard deviation - If weights are initialized with a larger standard deviation, the activations can grow exponentially as they propagate, leading to very large outputs and gradients. This is known as the exploding gradient problem.\nThus, weight initialization must strike a balance to avoid both extremes—activations that neither vanish nor explode. This concept is applied in Xavier Initialization [1].\nXavier Initialization If we can ensure that the variance of the output of a layer is the same as the variance of the input, the scale of activations won’t change as they propagate allowing us to avoid vanishing or exploding gradients. To derive this, let’s consider a linear layer:\n\\begin{align} y \u0026= \\sum_{i=1}^{D_{in}} x_i w_i \\\\ Var(y) \u0026= D_{in} * Var(x_i w_i) \\\\ \u0026= D_{in} * (E[x_i^2]E[w_i^2] - E[x_i]^2E[w_i]^2) \u0026\\text{[Assume x,w independent]} \\\\ \u0026= D_{in} * (E[x_i^2]E[w_i^2]) \u0026\\text{[Assume x,w are zero-mean]} \\\\ \u0026= D_{in} * Var(x_i) * Var(w_i) \\end{align} To maintain the variance between input and output, $Var(y) = Var(x_i)$, $$ \\Rightarrow Var(w_i) = 1/D_{in} $$\nTherefore, we initialize the weights of each layer with zero mean and a standard deviation equal to the inverse square root of the input dimension of that particular layer.\nW_l = np.random.randn(Din, Dout) * np.sqrt(1 / Din) Note the input $x_i$ refers to the activation (output) from the previous layer. Our derivation assumes that this activation should be zero-centered (or have zero-mean), which holds true for the activation functions like tanh that are centered around zero. However, this initialization does not work well for networks with ReLU non-linearities and needs to be adjusted. This modification is incorporated in Kaiming Initialization [2].\nKaiming/ He Initialization Assuming that the inputs are zero-centered, and ReLU kills all non-negative inputs, we effectively obtaining only half of our expected outputs, resulting in the the variance being halved, $Var(y) = Var(x_i) / 2$. To maintain the scale of activations across layers, $$ \\Rightarrow Var(w_i) = 2/D_{in} $$\nW_l = np.random.randn(Din, Dout) * np.sqrt(2 / Din) Backpropagation Most of our choices so far have revolved around gradients, the essential flour of our deep learning cake. To compute these derivatives of the loss function with respect to the weights of our neural network, we rely on a computation graph—our recipe for success!\nA computation graph is a directed graph that represents the computations inside our model. Let’s take a simple example of a computation graph that represents the following equation: \\begin{align} q = x + y \\ \\ \\text{and} \\ \\ f = z * q \\end{align}\nFirst, we perform a forward pass through the graph (denoted in green), where we compute the outputs of each node sequentially, given the inputs: $x = -2, y = 5, z = -4$. \\begin{align} q \u0026= x + y = -2 + 5 = 3\\\\ f \u0026 = z * q = -4 * 3 = -12 \\end{align}\nBackpropagation is the algorithm we use to compute gradients in this computation graph. In our backward pass (denoted in red), we apply the chain rule to compute the derivatives of each node in reverse order with respect to the output $f$. \\begin{align} \\frac{df}{df} \u0026= 1 \\\\ \\frac{df}{dq} \u0026= z = -4 \\\\ \\frac{df}{dz} \u0026= q = 3 \\\\ \\frac{df}{dx} \u0026= \\frac{df}{dq} \\frac{dq}{dx} = -4 * 1 = -4 \\\\ \\frac{df}{dy} \u0026= \\frac{df}{dq} \\frac{dq}{dy} = -4 * 1 = -4 \\end{align}\nIn this way, we compute the derivative of the output with respect to the inputs. In deep learning, the output $f$ is typically the loss function, and we use this method to compute the gradients of the loss with respect to the weights. With more complex neural networks, we follow the same procedure—but don’t worry, you don’t have to compute these gradients manually! Libraries like PyTorch handle it for us.\nCoding from scratch Okay, enough theory—let’s dive into coding a neural network from scratch using the PyTorch library. Here are a couple of key PyTorch terminologies to understand:\nTensor: Similar to a NumPy array but with the ability to run on GPUs. Autograd: A package that builds computational graphs from Tensors and automatically computes gradients, allowing for easy backpropagation. For our example, we’ll build a simple neural network for image classification. The input is a grayscale image of size (28, 28), flattened to a 784-dimensional vector, and our model will have two layers with ReLU activation, outputting scores for 10 classes:\n[Input -\u003e Linear -\u003e ReLU -\u003e Linear -\u003e Output]\nTraining Pipeline We’ll follow this pipeline for training:\n1. Initialize the model and weights # Initialize the weights w1 = torch.randn(784, 512, requires_grad=True) * torch.sqrt(2 / 784) w2 = torch.randn(512, 10, requires_grad=True) * torch.sqrt(2 / 512) Since we want to compute the gradients with respect to weights, we initialize them as tensors with the requires_grad = True flag. This tells Pytorch to enable autograd on these tensors and build a computational graph that tracks how these tensors are used in subsequent operations.\nWe initialize the weights using Kaiming initialization because we have the ReLU activation function in our network. Additionally, we prefer powers of 2 for hidden dimensions, such as 512, 256, 128, and 64, due to their computational efficiency on GPUs.\n2. Perform a forward pass of the model # Forward Pass y_pred = X_batch.mm(w1).clamp(min=0).mm(w2) Here, X_batch represents the mini-batch of inputs, while y_batch contains the corresponding labels. The output of the model, y_pred, is the score vector, which contains scores for each category. The clamp(min=0) operation implements the ReLU activation.\n3. Compute the loss # Compute the loss loss = loss_fn(y_pred, y_batch) The loss function calculates the average loss between predicted scores (y_pred) and the true labels(y_batch) across the mini-batch. This loss value is then used to compute gradients for updating the model parameters during training.\n4. Backpropagate the gradients # Compute gradients of loss wrt weights loss.backward() After computing the loss, the gradient of the loss with respect to the weights is automatically computed by calling loss.backward(). This function performs backpropagation on all the inputs that have the requires_grad flag set to true, accumulating the gradients in their .grad() attributes.\n5. Update the weights # Gradient descent step with torch.no_grad(): # Tells Pytorch not to build a graph w1 -= learning_rate * w1.grad() w2 -= learning_rate * w2.grad() # Set gradients to zero w1.zero_grad() w2.zero_grad() The torch.no_grad() context manager ensures that no computation graph is built during the weight update step.\nIt’s crucial to run .zero_grad() function to clear the current gradients after each update step and obtain fresh gradients in the next pass—otherwise, gradients would keep accumulating.\nThe entire code put together looks something like the following. We train the model for 10 steps, also known as epochs in machine learning terminology.\n# Initialize the weights w1 = torch.randn(784, 512, requires_grad=True) * torch.sqrt(2 / 784) w2 = torch.randn(512, 10, requires_grad=True) * torch.sqrt(2 / 512) # Split data into mini-batches mini_batches = split(data, batch_size) num_epochs = 10 # Train for epoch in range(num_epochs): for (X_batch, y_batch) in mini_batches: # Forward Pass y_pred = X_batch.mm(w1).clamp(min=0).mm(w2) # Compute the loss loss = loss_fn(y_pred, y_batch) # Backpropagate loss.backward() # Gradient descent step with torch.no_grad(): # Tells PyTorch not to build a graph w1 -= learning_rate * w1.grad w2 -= learning_rate * w2.grad # Set gradients to zero w1.zero_grad() w2.zero_grad() Pretty simple, right? Kudos! You can now code a neural network!\nWhile this was a naive way to code a deep learning model, let’s leverage PyTorch’s high-level APIs to rewrite the same model in a more scalable manner.\nModule: A layer or model class where weights are automatically set to requires_grad=True and initialized using Kaiming initialization. Loss functions: PyTorch provides a variety of loss functions to choose from, depending on the task at hand. We use Cross-Entropy loss for our example below. optim: PyTorch’s package for optimization algorithms including SGD, SGD with Momentum, and Adam. DataLoader: Helps in creating mini-batches efficiently. class TwoLayerNet(torch.nn.Module): def __init__(self): super(TwoLayerNet, self).__init__() self.linear1 = torch.nn.Linear(784, 512) self.linear2 = torch.nn.Linear(512, 10) self.relu = torch.nn.ReLU() def forward(self, x): x = self.relu(self.linear1(x)) x = self.linear2(x) return x # Initialize the model model = TwoLayerNet() # Define the loss function criterion = torch.nn.CrossEntropyLoss() # Define the optimizer optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # Create mini-batches mini_batches = torch.utils.data.DataLoader(data, batch_size=batch_size) num_epochs = 10 # Train for epoch in range(num_epochs): for (X_batch, y_batch) in mini_batches: # Forward Pass y_pred = model(X_batch) # Compute the loss loss = criterion(y_pred, y_batch) # Backpropagate loss.backward() # Gradient descent step optimizer.step() # Set gradients to zero optimizer.zero_grad() With these powerful APIs, PyTorch takes care of most of the grunt work, allowing us to focus on the higher-level logic.\nThis approach to writing neural networks has become the standard due to its clarity and efficiency, and you’ll notice that nearly all deep learning code follows a similar structure. Next, let’s apply this to create a model that can classify handwritten digits (MNIST Dataset).\nMNIST The MNIST dataset consists of 70,000 grayscale images of size (28, 28) pixels, featuring handwritten digits from 0 to 9, making up 10 classes.\nThe first step is to import the relevant libraries that we will be using throughout our code.\nimport torch import torchvision import matplotlib.pyplot as plt Downloading \u0026 Pre-processing the dataset Let’s download the dataset in two subsets: the training set containing 60,000 images and the test set containing 10,000 images. PyTorch provides transforms to convert these datasets into tensors. Each pixel value is originally in the range [0, 255], gets divided by 255 during the conversion to tensors, resulting in a range [0, 1].\n# Convert images to tensors transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()]) # Download the dataset mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform) mnist_testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform) Loading the dataset We randomly split the training data into a training set containing 80% of the images and a validation set containing the rest 20%. We then create data loaders for each set, choosing a batch size of 64 for this example.\nRemember, the test set is reserved for evaluation at the very end of our pipeline, and we use a batch size of 1 here since no processing is performed on it.\n# Randomly split the dataset into train (80%) and validation (20%) len_train = int(0.8 * len(mnist_trainset)) len_val = len(mnist_trainset) - len_train train_dataset, val_dataset = torch.utils.data.random_split(mnist_trainset, [len_train, len_val]) # Data loaders for training, validation, and testing train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True) val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True) test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=1) Define the Architecture Let’s use the same two-layer neural network that we’ve been discussing.\n# select the device on which the model will train use_cuda = torch.cuda.is_available() device = torch.device(\"cuda:0\" if use_cuda else \"cpu\") print(\"Device: \", device) # Define the model class TwoLayerNet(torch.nn.Module): def __init__(self): super(TwoLayerNet, self).__init__() self.fc1 = torch.nn.Linear(28 * 28 * 1, 512) self.fc2 = torch.nn.Linear(512, 10) self.relu = torch.nn.ReLU() def forward(self, x): x = self.relu(self.fc1(x)) x = self.fc2(x) return x # Initialize the model and move it to the selected device model = TwoLayerNet().to(device) If you are using Google Colab, don’t forget to change the runtime to GPU to take advantage of hardware acceleration.\nTrain the model The training loop looks exactly like before.\n# Define the loss function criterion = torch.nn.CrossEntropyLoss() # Define the optimizer optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) num_epochs = 10 train_loss, train_acc, val_acc = [], [], [] for epoch in range(num_epochs): # Training loop running_loss = 0 correct, total = 0, 0 for (image, label) in train_loader: # Send the batch of images and labels to the GPU image, label = image.to(device), label.to(device) # Flatten the image image = image.view(image.shape[0], -1) # Forward pass and optimization output = model(image) loss = criterion(output, label) loss.backward() optimizer.step() optimizer.zero_grad() # Track training loss running_loss += loss.item() # Get model predictions (category with max score) _, predicted = torch.max(output, dim=1) # Track total labels and correct predictions total += label.shape[0] correct += (predicted == label).sum().item() train_loss.append(running_loss / len(train_loader)) train_acc.append(correct / total) # Validation loop correct, total = 0, 0 for (image, label) in val_loader: # Send the batch of images and labels to the GPU image, label = image.to(device), label.to(device) # Flatten the image image = image.view(image.shape[0], -1) # Get model predictions output = model(image) _, predicted = torch.max(output, dim=1) # Track total labels and correct predictions total += label.shape[0] correct += (predicted == label).sum().item() val_acc.append(correct/total) print('\\nEpoch: {}/{}, Train Loss: {:.4f}, Train Accuracy: {:.4f}, Val Accuracy: {:.4f}'.format(epoch + 1, num_epochs, train_loss[-1], train_acc[-1], val_acc[-1])) # Plot training statistics plt.figure(1) plt.plot(range(1, num_epochs + 1), train_loss, label='Training Loss') plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.title('Loss vs Number of Epochs') plt.legend() plt.figure(2) plt.plot(range(1, num_epochs + 1), train_acc, label='Training Accuracy') plt.plot(range(1, num_epochs + 1), val_acc, label='Validation Accuracy') plt.xlabel(\"Epochs\") plt.ylabel(\"Accuracy\") plt.title('Accuracy vs Number of Epochs') plt.legend() plt.show() Additionally, we plot training statistics to assess the model’s training performance. Typically, we visualize three curves:\nTraining loss vs Number of Epochs Training accuracy vs Number of Epochs Validation accuracy vs Number of Epochs We calculate the training loss for the entire epoch by summing the average losses across batches and dividing by the number of batches in the training loader. To compute accuracy, we compare the model’s predictions with the actual labels. A similar approach is taken for validation, except we do not backpropagate the loss for weight updates.\nThe training loss should ideally decrease with each epoch, while both training and validation accuracy should increase. Below are the plots obtained after running the code above.\nTest the model We evaluate the model’s accuracy on unseen data using the same approach as in the validation loop.\n# Get test accuracy correct, total = 0, 0 for (image, label) in test_loader: # Send image and label to the GPU image, label = image.to(device), label.to(device) # Flatten the image image = image.view(image.shape[0], -1) # Get model predictions output = model(image) _, predicted = torch.max(output, dim=1) # Track total labels and correct predictions total += label.shape[0] correct += (predicted == label).sum().item() print(\"Test Accuracy: \", correct/total) With this model, I achieved 97.5% accuracy on the test set after training for just 10 epochs!\nThis simple yet effective two-layer neural network demonstrates how accessible deep learning can be, enabling us to achieve impressive results with this tiny dataset.\nOur deep learning cake is almost ready—now, it’s time to add the toppings!\nReferences [1] Glorot and Bengio, “Understanding the difficulty of training deep feedforward neural networks”, JMLR 2010.\n[2] He et al, “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification”, ICCV 2015.\n","wordCount":"3239","inLanguage":"en","datePublished":"2022-08-05T00:00:00Z","dateModified":"2022-08-05T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://yugajmera.github.io/posts/01-dl2/post/"},"publisher":{"@type":"Organization","name":"YA's Almanac","logo":{"@type":"ImageObject","url":"https://yugajmera.github.io/assets/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yugajmera.github.io/ accesskey=h title="YA's Almanac (Alt + H)">YA's Almanac</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yugajmera.github.io/ title=Home><span>Home</span></a></li><li><a href=https://yugajmera.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://yugajmera.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Deep Learning Basics Part 2: The Icing</h1><div class=post-meta><span title='2022-08-05 00:00:00 +0000 UTC'>August 5, 2022</span>&nbsp;·&nbsp;16 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#neural-networks aria-label="Neural Networks">Neural Networks</a><ul><li><a href=#activation-functions aria-label="Activation functions">Activation functions</a><ul><li><a href=#sigmoid aria-label=Sigmoid>Sigmoid</a></li><li><a href=#tanh aria-label=Tanh>Tanh</a></li><li><a href=#relu aria-label=ReLU>ReLU</a></li><li><a href=#leaky-relu aria-label="Leaky ReLU">Leaky ReLU</a></li></ul></li><li><a href=#weight-initialization aria-label="Weight Initialization">Weight Initialization</a><ul><li><a href=#xavier-initialization aria-label="Xavier Initialization">Xavier Initialization</a></li><li><a href=#kaiming-he-initialization aria-label="Kaiming/ He Initialization">Kaiming/ He Initialization</a></li></ul></li></ul></li><li><a href=#backpropagation aria-label=Backpropagation>Backpropagation</a></li><li><a href=#coding-from-scratch aria-label="Coding from scratch">Coding from scratch</a><ul><li><a href=#training-pipeline aria-label="Training Pipeline">Training Pipeline</a><ul><li><a href=#1-initialize-the-model-and-weights aria-label="1. Initialize the model and weights">1. Initialize the model and weights</a></li><li><a href=#2-perform-a-forward-pass-of-the-model aria-label="2. Perform a forward pass of the model">2. Perform a forward pass of the model</a></li><li><a href=#3-compute-the-loss aria-label="3. Compute the loss">3. Compute the loss</a></li><li><a href=#4-backpropagate-the-gradients aria-label="4. Backpropagate the gradients">4. Backpropagate the gradients</a></li><li><a href=#5-update-the-weights aria-label="5. Update the weights">5. Update the weights</a></li></ul></li><li><a href=#mnist aria-label=MNIST>MNIST</a><ul><li><a href=#downloading--pre-processing-the-dataset aria-label="Downloading & Pre-processing the dataset">Downloading & Pre-processing the dataset</a></li><li><a href=#loading-the-dataset aria-label="Loading the dataset">Loading the dataset</a></li><li><a href=#define-the-architecture aria-label="Define the Architecture">Define the Architecture</a></li><li><a href=#train-the-model aria-label="Train the model">Train the model</a></li><li><a href=#test-the-model aria-label="Test the model">Test the model</a></li></ul></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>In the last post, we introduced Linear Classifiers as the simplest model in deep learning for image classification problems. We discussed how Loss Functions express preferences over different choices of weights, and how Optimization minimizes these loss functions to train the model.</p><p>However, linear classifiers have limitations: their decision boundaries are linear, which makes them inadequate for classifying complex data. One option is to manually extract features from the input image and transform them into a feature space, hoping that it makes the data linearly separable. We could then apply a linear classifier on top of these features to classify each image. This would result in a non-linear decision boundary when projected back to the input space.</p><figure class=align-center><img loading=lazy src=../transform.png#center></figure><p>While possible, this approach is cumbersome since it requires manually defining feature extraction methods for each specific task. Instead, we want our model to jointly learn both the feature representation and the linear classifier. This is where neural networks come in. Neural networks are built using chunks of linear classifiers, which is why they are also called Multi-Layer Perceptrons (MLPs).</p><h2 id=neural-networks>Neural Networks<a hidden class=anchor aria-hidden=true href=#neural-networks>#</a></h2><figure class=align-center><img loading=lazy src=../neural-network.png#center></figure><p>A neural network is composed of interconnected layers, where every neuron in one layer connects to every neuron in the next layer. A typical deep learning model has multiple layers, where the &ldquo;depth&rdquo; refers to the number of layers or the number of learnable weight matrices.</p><p>\begin{align}
\text{Linear function: } & f = W x \\
\text{2-Layer Neural Network: } & f = W_2 \hspace{2mm} z(W_1 x) \\
\text{3-Layer Neural Network: } & f = W_3 \hspace{2mm} z_2(W_2 \hspace{2mm} z_1(W_1 x)) \\
\end{align}</p><p>Each layer is followed by a non-linear function called an activation function $z_i$. Activation functions are critical because, without them, stacking multiple linear layers would simply result in a linear classifier ($ y = W_2 W_1 x = W x $).</p><p>The non-linearity introduced by activation functions allows neural networks to create complex decision boundaries. As the number of layers (and activation functions) increases, the decision boundary becomes more intricate in the input space, enabling the model to fit the data more effectively.</p><h3 id=activation-functions>Activation functions<a hidden class=anchor aria-hidden=true href=#activation-functions>#</a></h3><p>There are several popular activation functions to choose from, each with its pros and cons. Let&rsquo;s briefly explore them:</p><figure class=align-center><img loading=lazy src=../activations.png#center></figure><h4 id=sigmoid>Sigmoid<a hidden class=anchor aria-hidden=true href=#sigmoid>#</a></h4><p>This function transforms input data into the range $[0, 1]$, which can be interpreted as a probability (i.e., the likelihood of a particular feature being present). But, it has significant drawbacks:</p><ul><li>For large positive or negative inputs, the output saturates (the curve becomes parallel to the x-axis), causing the gradient to approach zero ($\mathbf{dw} \approx 0$). This is known as the vanishing gradient problem, which effectively kills the learning process.</li><li>Computing the exponential function is expensive, slowing down training.</li></ul><h4 id=tanh>Tanh<a hidden class=anchor aria-hidden=true href=#tanh>#</a></h4><p>The tanh function is a scaled and shifted version of sigmoid that outputs in the range $[-1, 1]$, making it zero-centered. This allows for both positive and negative outputs, improving gradient flow compared to sigmoid. However:</p><ul><li>It still suffers from the vanishing gradient problem.</li><li>Like sigmoid, it relies on exponentials, making it computationally expensive.</li></ul><h4 id=relu>ReLU<a hidden class=anchor aria-hidden=true href=#relu>#</a></h4><p>ReLU is the most commonly used activation function due to its computational efficiency, which leads to faster training. But,</p><ul><li>It has a dying ReLU problem—when the inputs are negative, the gradient becomes zero, and those neurons stop learning. These become inactive (as they always ouput zero), reducing the capacity of the model.</li></ul><h4 id=leaky-relu>Leaky ReLU<a hidden class=anchor aria-hidden=true href=#leaky-relu>#</a></h4><p>Leaky ReLU is a variation that addresses the dying ReLU problem by adding a small, non-zero gradient for negative input. This prevents neurons from dying in the negative region.</p><ul><li>The slope in the negative region (usually 0.1) is a hyperparameter, introducing another variable to tune 😔.</li></ul><p>There are advanced variations like Exponential Linear Unit (ELU), Scaled ELU (SELU), and Gaussian Error Linear Unit (GELU), each offering specific benefits. A good rule of thumb is to start with ReLU as your default choice. For finer improvements, you can experiment with these advanced variants to push your model’s accuracy by a small margin.</p><h3 id=weight-initialization>Weight Initialization<a hidden class=anchor aria-hidden=true href=#weight-initialization>#</a></h3><p>Once the architecture of the network is fixed, the next step is to initialize the weights of each layer. Weight initialization sets the starting point on the loss landscape for gradient descent.
Each new set of initial weights kicks off the optimization process from a different spot, potentially leading to different final solutions. Therefore, the choice of weight initialization is crucial, as it significantly influences the optimization path and the model&rsquo;s convergence.</p><p>Initalizing with zeros - If the weights are initialized to a constant value, such as zero, the activations (outputs of neurons: linear layer + activation function) will also be zero, and the gradients will be zero. This effectively kills learning, preventing the network from even starting to train!</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>W_l</span> <span class=o>=</span> <span class=mi>0</span> <span class=c1># very bad idea</span>
</span></span></code></pre></div><p>Initializing with small random numbers - Using a Gaussian distribution with zero mean and a standard deviation of 0.01 can work for shallow networks. However, for deeper networks, repeatedly multiplying small weights (less than 1) through many layers will shrink the activations as they propagate, causing them to approach zero. This leads to the vanishing gradient problem with tanh and ReLU non-linearities.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>W_l</span> <span class=o>=</span> <span class=mf>0.01</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>Din</span><span class=p>,</span> <span class=n>Dout</span><span class=p>)</span>
</span></span></code></pre></div><p>Initializing with larger standard deviation - If weights are initialized with a larger standard deviation, the activations can grow exponentially as they propagate, leading to very large outputs and gradients. This is known as the exploding gradient problem.</p><p>Thus, weight initialization must strike a balance to avoid both extremes—activations that neither vanish nor explode. This concept is applied in Xavier Initialization <a href=#references>[1]</a>.</p><h4 id=xavier-initialization>Xavier Initialization<a hidden class=anchor aria-hidden=true href=#xavier-initialization>#</a></h4><p>If we can ensure that the variance of the output of a layer is the same as the variance of the input, the scale of activations won&rsquo;t change as they propagate allowing us to avoid vanishing or exploding gradients. To derive this, let&rsquo;s consider a linear layer:</p><p>\begin{align}
y &= \sum_{i=1}^{D_{in}} x_i w_i \\
Var(y) &= D_{in} * Var(x_i w_i) \\
&= D_{in} * (E[x_i^2]E[w_i^2] - E[x_i]^2E[w_i]^2) &\text{[Assume x,w independent]} \\
&= D_{in} * (E[x_i^2]E[w_i^2]) &\text{[Assume x,w are zero-mean]} \\
&= D_{in} * Var(x_i) * Var(w_i)
\end{align}
To maintain the variance between input and output, $Var(y) = Var(x_i)$,
$$
\Rightarrow Var(w_i) = 1/D_{in}
$$</p><p>Therefore, we initialize the weights of each layer with zero mean and a standard deviation equal to the inverse square root of the input dimension of that particular layer.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>W_l</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>Din</span><span class=p>,</span> <span class=n>Dout</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>1</span> <span class=o>/</span> <span class=n>Din</span><span class=p>)</span>
</span></span></code></pre></div><p>Note the input $x_i$ refers to the activation (output) from the previous layer. Our derivation assumes that this activation should be zero-centered (or have zero-mean), which holds true for the activation functions like tanh that are centered around zero. However, this initialization does not work well for networks with ReLU non-linearities and needs to be adjusted. This modification is incorporated in Kaiming Initialization <a href=#references>[2]</a>.</p><h4 id=kaiming-he-initialization>Kaiming/ He Initialization<a hidden class=anchor aria-hidden=true href=#kaiming-he-initialization>#</a></h4><p>Assuming that the inputs are zero-centered, and ReLU kills all non-negative inputs, we effectively obtaining only half of our expected outputs, resulting in the the variance being halved, $Var(y) = Var(x_i) / 2$. To maintain the scale of activations across layers,
$$
\Rightarrow Var(w_i) = 2/D_{in}
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>W_l</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>Din</span><span class=p>,</span> <span class=n>Dout</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>2</span> <span class=o>/</span> <span class=n>Din</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=backpropagation>Backpropagation<a hidden class=anchor aria-hidden=true href=#backpropagation>#</a></h2><p>Most of our choices so far have revolved around gradients, the essential flour of our deep learning cake. To compute these derivatives of the loss function with respect to the weights of our neural network, we rely on a computation graph—our recipe for success!</p><p>A computation graph is a directed graph that represents the computations inside our model. Let&rsquo;s take a simple example of a computation graph that represents the following equation:
\begin{align}
q = x + y \ \ \text{and} \ \ f = z * q
\end{align}</p><figure class=align-center><img loading=lazy src=../backprop.png#center></figure><p>First, we perform a forward pass through the graph (denoted in green), where we compute the outputs of each node sequentially, given the inputs: $x = -2, y = 5, z = -4$.
\begin{align}
q &= x + y = -2 + 5 = 3\\
f & = z * q = -4 * 3 = -12
\end{align}</p><p>Backpropagation is the algorithm we use to compute gradients in this computation graph. In our backward pass (denoted in red), we apply the chain rule to compute the derivatives of each node in reverse order with respect to the output $f$.
\begin{align}
\frac{df}{df} &= 1 \\
\frac{df}{dq} &= z = -4 \\
\frac{df}{dz} &= q = 3 \\
\frac{df}{dx} &= \frac{df}{dq} \frac{dq}{dx} = -4 * 1 = -4 \\
\frac{df}{dy} &= \frac{df}{dq} \frac{dq}{dy} = -4 * 1 = -4
\end{align}</p><p>In this way, we compute the derivative of the output with respect to the inputs. In deep learning, the output $f$ is typically the loss function, and we use this method to compute the gradients of the loss with respect to the weights. With more complex neural networks, we follow the same procedure—but don&rsquo;t worry, you don&rsquo;t have to compute these gradients manually! Libraries like PyTorch handle it for us.</p><h2 id=coding-from-scratch>Coding from scratch<a hidden class=anchor aria-hidden=true href=#coding-from-scratch>#</a></h2><p>Okay, enough theory—let’s dive into coding a neural network from scratch using the PyTorch library. Here are a couple of key PyTorch terminologies to understand:</p><ul><li>Tensor: Similar to a NumPy array but with the ability to run on GPUs.</li><li>Autograd: A package that builds computational graphs from Tensors and automatically computes gradients, allowing for easy backpropagation.</li></ul><p>For our example, we&rsquo;ll build a simple neural network for image classification. The input is a grayscale image of size (28, 28), flattened to a 784-dimensional vector, and our model will have two layers with ReLU activation, outputting scores for 10 classes:</p><p>[Input -> Linear -> ReLU -> Linear -> Output]</p><figure class=align-center><img loading=lazy src=../eg-model.png#center width=500></figure><h3 id=training-pipeline>Training Pipeline<a hidden class=anchor aria-hidden=true href=#training-pipeline>#</a></h3><p>We&rsquo;ll follow this pipeline for training:</p><h4 id=1-initialize-the-model-and-weights>1. Initialize the model and weights<a hidden class=anchor aria-hidden=true href=#1-initialize-the-model-and-weights>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Initialize the weights</span>
</span></span><span class=line><span class=cl><span class=n>w1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>784</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>2</span> <span class=o>/</span> <span class=mi>784</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>w2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>2</span> <span class=o>/</span> <span class=mi>512</span><span class=p>)</span>
</span></span></code></pre></div><p>Since we want to compute the gradients with respect to weights, we initialize them as tensors with the <code>requires_grad = True</code> flag. This tells Pytorch to enable autograd on these tensors and build a computational graph that tracks how these tensors are used in subsequent operations.</p><p>We initialize the weights using Kaiming initialization because we have the ReLU activation function in our network. Additionally, we prefer powers of 2 for hidden dimensions, such as 512, 256, 128, and 64, due to their computational efficiency on GPUs.</p><h4 id=2-perform-a-forward-pass-of-the-model>2. Perform a forward pass of the model<a hidden class=anchor aria-hidden=true href=#2-perform-a-forward-pass-of-the-model>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Forward Pass</span>
</span></span><span class=line><span class=cl><span class=n>y_pred</span> <span class=o>=</span> <span class=n>X_batch</span><span class=o>.</span><span class=n>mm</span><span class=p>(</span><span class=n>w1</span><span class=p>)</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span><span class=nb>min</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>mm</span><span class=p>(</span><span class=n>w2</span><span class=p>)</span>  
</span></span></code></pre></div><p>Here, <code>X_batch</code> represents the mini-batch of inputs, while <code>y_batch</code> contains the corresponding labels. The output of the model, <code>y_pred</code>, is the score vector, which contains scores for each category. The <code>clamp(min=0)</code> operation implements the ReLU activation.</p><h4 id=3-compute-the-loss>3. Compute the loss<a hidden class=anchor aria-hidden=true href=#3-compute-the-loss>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Compute the loss</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fn</span><span class=p>(</span><span class=n>y_pred</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
</span></span></code></pre></div><p>The loss function calculates the average loss between predicted scores (<code>y_pred</code>) and the true labels(<code>y_batch</code>) across the mini-batch. This loss value is then used to compute gradients for updating the model parameters during training.</p><h4 id=4-backpropagate-the-gradients>4. Backpropagate the gradients<a hidden class=anchor aria-hidden=true href=#4-backpropagate-the-gradients>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Compute gradients of loss wrt weights</span>
</span></span><span class=line><span class=cl><span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>     
</span></span></code></pre></div><p>After computing the loss, the gradient of the loss with respect to the weights is automatically computed by calling <code>loss.backward()</code>. This function performs backpropagation on all the inputs that have the <code>requires_grad</code> flag set to true, accumulating the gradients in their <code>.grad()</code> attributes.</p><h4 id=5-update-the-weights>5. Update the weights<a hidden class=anchor aria-hidden=true href=#5-update-the-weights>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Gradient descent step</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>       <span class=c1># Tells Pytorch not to build a graph</span>
</span></span><span class=line><span class=cl>    <span class=n>w1</span> <span class=o>-=</span> <span class=n>learning_rate</span> <span class=o>*</span> <span class=n>w1</span><span class=o>.</span><span class=n>grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>w2</span> <span class=o>-=</span> <span class=n>learning_rate</span> <span class=o>*</span> <span class=n>w2</span><span class=o>.</span><span class=n>grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Set gradients to zero</span>
</span></span><span class=line><span class=cl><span class=n>w1</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>      
</span></span><span class=line><span class=cl><span class=n>w2</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span></code></pre></div><p>The <code>torch.no_grad()</code> context manager ensures that no computation graph is built during the weight update step.</p><p>It&rsquo;s crucial to run <code>.zero_grad()</code> function to clear the current gradients after each update step and obtain fresh gradients in the next pass—otherwise, gradients would keep accumulating.</p><p>The entire code put together looks something like the following. We train the model for 10 steps, also known as epochs in machine learning terminology.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Initialize the weights</span>
</span></span><span class=line><span class=cl><span class=n>w1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>784</span><span class=p>,</span> <span class=mi>512</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>2</span> <span class=o>/</span> <span class=mi>784</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>w2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>2</span> <span class=o>/</span> <span class=mi>512</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Split data into mini-batches</span>
</span></span><span class=line><span class=cl><span class=n>mini_batches</span> <span class=o>=</span> <span class=n>split</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>)</span>        
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>num_epochs</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Train</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span> <span class=ow>in</span> <span class=n>mini_batches</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Forward Pass</span>
</span></span><span class=line><span class=cl>        <span class=n>y_pred</span> <span class=o>=</span> <span class=n>X_batch</span><span class=o>.</span><span class=n>mm</span><span class=p>(</span><span class=n>w1</span><span class=p>)</span><span class=o>.</span><span class=n>clamp</span><span class=p>(</span><span class=nb>min</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>mm</span><span class=p>(</span><span class=n>w2</span><span class=p>)</span>  
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Compute the loss </span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>loss_fn</span><span class=p>(</span><span class=n>y_pred</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Backpropagate</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>          
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Gradient descent step</span>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>       <span class=c1># Tells PyTorch not to build a graph</span>
</span></span><span class=line><span class=cl>            <span class=n>w1</span> <span class=o>-=</span> <span class=n>learning_rate</span> <span class=o>*</span> <span class=n>w1</span><span class=o>.</span><span class=n>grad</span>
</span></span><span class=line><span class=cl>            <span class=n>w2</span> <span class=o>-=</span> <span class=n>learning_rate</span> <span class=o>*</span> <span class=n>w2</span><span class=o>.</span><span class=n>grad</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Set gradients to zero</span>
</span></span><span class=line><span class=cl>        <span class=n>w1</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>      
</span></span><span class=line><span class=cl>        <span class=n>w2</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span></code></pre></div><p>Pretty simple, right? Kudos! You can now code a neural network!</p><p>While this was a naive way to code a deep learning model, let’s leverage PyTorch&rsquo;s high-level APIs to rewrite the same model in a more scalable manner.</p><ul><li>Module: A layer or model class where weights are automatically set to <code>requires_grad=True</code> and initialized using Kaiming initialization.</li><li>Loss functions: PyTorch provides a variety of loss functions to choose from, depending on the task at hand. We use Cross-Entropy loss for our example below.</li><li>optim: PyTorch&rsquo;s package for optimization algorithms including SGD, SGD with Momentum, and Adam.</li><li>DataLoader: Helps in creating mini-batches efficiently.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>TwoLayerNet</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>TwoLayerNet</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>784</span><span class=p>,</span> <span class=mi>512</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>linear1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialize the model</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>TwoLayerNet</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define the loss function</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define the optimizer</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create mini-batches</span>
</span></span><span class=line><span class=cl><span class=n>mini_batches</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=n>batch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>num_epochs</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Train</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>X_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span> <span class=ow>in</span> <span class=n>mini_batches</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Forward Pass</span>
</span></span><span class=line><span class=cl>        <span class=n>y_pred</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X_batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Compute the loss </span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>y_pred</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Backpropagate</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Gradient descent step</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Set gradients to zero</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span></code></pre></div><p>With these powerful APIs, PyTorch takes care of most of the grunt work, allowing us to focus on the higher-level logic.</p><p>This approach to writing neural networks has become the standard due to its clarity and efficiency, and you’ll notice that nearly all deep learning code follows a similar structure. Next, let&rsquo;s apply this to create a model that can classify handwritten digits (MNIST Dataset).</p><h3 id=mnist>MNIST<a hidden class=anchor aria-hidden=true href=#mnist>#</a></h3><p>The MNIST dataset consists of 70,000 grayscale images of size (28, 28) pixels, featuring handwritten digits from 0 to 9, making up 10 classes.</p><figure class=align-center><img loading=lazy src=../mnist.png#center></figure><p>The first step is to import the relevant libraries that we will be using throughout our code.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torchvision</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span></code></pre></div><h4 id=downloading--pre-processing-the-dataset>Downloading & Pre-processing the dataset<a hidden class=anchor aria-hidden=true href=#downloading--pre-processing-the-dataset>#</a></h4><p>Let&rsquo;s download the dataset in two subsets: the training set containing 60,000 images and the test set containing 10,000 images. PyTorch provides transforms to convert these datasets into tensors. Each pixel value is originally in the range [0, 255], gets divided by 255 during the conversion to tensors, resulting in a range [0, 1].</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Convert images to tensors</span>
</span></span><span class=line><span class=cl><span class=n>transform</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span><span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>()])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Download the dataset</span>
</span></span><span class=line><span class=cl><span class=n>mnist_trainset</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>datasets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=n>transform</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mnist_testset</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>datasets</span><span class=o>.</span><span class=n>MNIST</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> <span class=n>train</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=n>transform</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=loading-the-dataset>Loading the dataset<a hidden class=anchor aria-hidden=true href=#loading-the-dataset>#</a></h4><p>We randomly split the training data into a training set containing 80% of the images and a validation set containing the rest 20%. We then create data loaders for each set, choosing a batch size of 64 for this example.</p><p>Remember, the test set is reserved for evaluation at the very end of our pipeline, and we use a batch size of 1 here since no processing is performed on it.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Randomly split the dataset into train (80%) and validation (20%)</span>
</span></span><span class=line><span class=cl><span class=n>len_train</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=mf>0.8</span> <span class=o>*</span> <span class=nb>len</span><span class=p>(</span><span class=n>mnist_trainset</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>len_val</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>mnist_trainset</span><span class=p>)</span> <span class=o>-</span> <span class=n>len_train</span>
</span></span><span class=line><span class=cl><span class=n>train_dataset</span><span class=p>,</span> <span class=n>val_dataset</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>random_split</span><span class=p>(</span><span class=n>mnist_trainset</span><span class=p>,</span> <span class=p>[</span><span class=n>len_train</span><span class=p>,</span> <span class=n>len_val</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Data loaders for training, validation, and testing</span>
</span></span><span class=line><span class=cl><span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>train_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>val_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>val_dataset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>test_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>mnist_testset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=define-the-architecture>Define the Architecture<a hidden class=anchor aria-hidden=true href=#define-the-architecture>#</a></h4><p>Let’s use the same two-layer neural network that we’ve been discussing.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># select the device on which the model will train</span>
</span></span><span class=line><span class=cl><span class=n>use_cuda</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=s2>&#34;cuda:0&#34;</span> <span class=k>if</span> <span class=n>use_cuda</span> <span class=k>else</span> <span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Device: &#34;</span><span class=p>,</span> <span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define the model</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>TwoLayerNet</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>TwoLayerNet</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>28</span> <span class=o>*</span> <span class=mi>28</span> <span class=o>*</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>512</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>512</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>fc1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>fc2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialize the model and move it to the selected device</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>TwoLayerNet</span><span class=p>()</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span></code></pre></div><p>If you are using Google Colab, don’t forget to change the runtime to GPU to take advantage of hardware acceleration.</p><h4 id=train-the-model>Train the model<a hidden class=anchor aria-hidden=true href=#train-the-model>#</a></h4><p>The training loop looks exactly like before.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Define the loss function</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define the optimizer</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span>  <span class=n>momentum</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>num_epochs</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl><span class=n>train_loss</span><span class=p>,</span> <span class=n>train_acc</span><span class=p>,</span> <span class=n>val_acc</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Training loop</span>
</span></span><span class=line><span class=cl>    <span class=n>running_loss</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>correct</span><span class=p>,</span> <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>image</span><span class=p>,</span> <span class=n>label</span><span class=p>)</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Send the batch of images and labels to the GPU</span>
</span></span><span class=line><span class=cl>        <span class=n>image</span><span class=p>,</span> <span class=n>label</span> <span class=o>=</span> <span class=n>image</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>label</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Flatten the image</span>
</span></span><span class=line><span class=cl>        <span class=n>image</span> <span class=o>=</span> <span class=n>image</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>image</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Forward pass and optimization</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>image</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>label</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Track training loss</span>
</span></span><span class=line><span class=cl>        <span class=n>running_loss</span> <span class=o>+=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Get model predictions (category with max score)</span>
</span></span><span class=line><span class=cl>        <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Track total labels and correct predictions</span>
</span></span><span class=line><span class=cl>        <span class=n>total</span> <span class=o>+=</span> <span class=n>label</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span> <span class=o>==</span> <span class=n>label</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>train_loss</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>running_loss</span> <span class=o>/</span> <span class=nb>len</span><span class=p>(</span><span class=n>train_loader</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>train_acc</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>correct</span> <span class=o>/</span> <span class=n>total</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Validation loop</span>
</span></span><span class=line><span class=cl>    <span class=n>correct</span><span class=p>,</span> <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>image</span><span class=p>,</span> <span class=n>label</span><span class=p>)</span> <span class=ow>in</span> <span class=n>val_loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Send the batch of images and labels to the GPU</span>
</span></span><span class=line><span class=cl>        <span class=n>image</span><span class=p>,</span> <span class=n>label</span> <span class=o>=</span> <span class=n>image</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>label</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Flatten the image</span>
</span></span><span class=line><span class=cl>        <span class=n>image</span> <span class=o>=</span> <span class=n>image</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>image</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Get model predictions</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>image</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Track total labels and correct predictions</span>
</span></span><span class=line><span class=cl>        <span class=n>total</span> <span class=o>+=</span> <span class=n>label</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span> <span class=o>==</span> <span class=n>label</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>val_acc</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>correct</span><span class=o>/</span><span class=n>total</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;</span><span class=se>\n</span><span class=s1>Epoch: </span><span class=si>{}</span><span class=s1>/</span><span class=si>{}</span><span class=s1>, Train Loss: </span><span class=si>{:.4f}</span><span class=s1>, Train Accuracy: </span><span class=si>{:.4f}</span><span class=s1>, Val Accuracy: </span><span class=si>{:.4f}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>epoch</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>num_epochs</span><span class=p>,</span> <span class=n>train_loss</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>train_acc</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>val_acc</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot training statistics</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>num_epochs</span> <span class=o>+</span> <span class=mi>1</span><span class=p>),</span> <span class=n>train_loss</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Training Loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Epochs&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Loss&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Loss vs Number of Epochs&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>num_epochs</span> <span class=o>+</span> <span class=mi>1</span><span class=p>),</span> <span class=n>train_acc</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Training Accuracy&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>num_epochs</span> <span class=o>+</span> <span class=mi>1</span><span class=p>),</span> <span class=n>val_acc</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Validation Accuracy&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Epochs&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Accuracy&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Accuracy vs Number of Epochs&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p>Additionally, we plot training statistics to assess the model&rsquo;s training performance. Typically, we visualize three curves:</p><ul><li>Training loss vs Number of Epochs</li><li>Training accuracy vs Number of Epochs</li><li>Validation accuracy vs Number of Epochs</li></ul><p>We calculate the training loss for the entire epoch by summing the average losses across batches and dividing by the number of batches in the training loader. To compute accuracy, we compare the model&rsquo;s predictions with the actual labels. A similar approach is taken for validation, except we do not backpropagate the loss for weight updates.</p><p>The training loss should ideally decrease with each epoch, while both training and validation accuracy should increase. Below are the plots obtained after running the code above.</p><figure class=align-center><img loading=lazy src=../train-curves.png#center></figure><h4 id=test-the-model>Test the model<a hidden class=anchor aria-hidden=true href=#test-the-model>#</a></h4><p>We evaluate the model&rsquo;s accuracy on unseen data using the same approach as in the validation loop.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Get test accuracy</span>
</span></span><span class=line><span class=cl><span class=n>correct</span><span class=p>,</span> <span class=n>total</span> <span class=o>=</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=n>image</span><span class=p>,</span> <span class=n>label</span><span class=p>)</span> <span class=ow>in</span> <span class=n>test_loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Send image and label to the GPU</span>
</span></span><span class=line><span class=cl>    <span class=n>image</span><span class=p>,</span> <span class=n>label</span> <span class=o>=</span> <span class=n>image</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>label</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Flatten the image</span>
</span></span><span class=line><span class=cl>    <span class=n>image</span> <span class=o>=</span> <span class=n>image</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>image</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Get model predictions</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>image</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>_</span><span class=p>,</span> <span class=n>predicted</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>output</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Track total labels and correct predictions</span>
</span></span><span class=line><span class=cl>    <span class=n>total</span> <span class=o>+=</span> <span class=n>label</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>correct</span> <span class=o>+=</span> <span class=p>(</span><span class=n>predicted</span> <span class=o>==</span> <span class=n>label</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Test Accuracy: &#34;</span><span class=p>,</span> <span class=n>correct</span><span class=o>/</span><span class=n>total</span><span class=p>)</span>
</span></span></code></pre></div><p>With this model, I achieved 97.5% accuracy on the test set after training for just 10 epochs!</p><p>This simple yet effective two-layer neural network demonstrates how accessible deep learning can be, enabling us to achieve impressive results with this tiny dataset.</p><p>Our deep learning cake is almost ready—now, it&rsquo;s time to add the toppings!</p><p> </p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Glorot and Bengio, “<a href=https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf>Understanding the difficulty of training deep feedforward neural networks</a>”, JMLR 2010.</p><p>[2] He et al, “<a href=https://arxiv.org/abs/1502.01852>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>”, ICCV 2015.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://yugajmera.github.io/posts/01-dl3/post/><span class=title>« Prev</span><br><span>Deep Learning Basics Part 3: The Cherry on Top</span>
</a><a class=next href=https://yugajmera.github.io/posts/01-dl1/post/><span class=title>Next »</span><br><span>Deep Learning Basics Part 1: The Base of the Cake</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://yugajmera.github.io/>YA's Almanac</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>