<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Transformers for Image Classification: ViT and CLIP | YA's Almanac</title>
<meta name=keywords content><meta name=description content="Modelling in computer vision has long been dominated by convolutional neural networks (CNNs). We’ve already discussed famous architectures like VGGNet and ResNet in previous posts, which have served as the primary backbones for a variety of vision tasks.
In contrast, network architectures in natural language processing (NLP) have evolved along a different trajectory. The dominant architecture in NLP today is the Transformer, designed for sequence modeling. Models like GPT-3 have achieved remarkable success, scaling to over 100 billion parameters thanks to their computational efficiency and scalability."><meta name=author content><link rel=canonical href=http://localhost:1313/posts/11-vit/post/><link crossorigin=anonymous href=http://localhost:1313/assets/css/stylesheet.74991b51f7611c2303d0b5649703d675530589621885eb78236e099c22aa93a5.css integrity="sha256-dJkbUfdhHCMD0LVklwPWdVMFiWIYhet4I24JnCKqk6U=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/assets/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/assets/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/assets/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/assets/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/11-vit/post/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>mjx-container[display=true]{margin:1.5em 0!important}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S759YBMKJE",{anonymize_ip:!1})}</script><meta property="og:title" content="Transformers for Image Classification: ViT and CLIP"><meta property="og:description" content="Modelling in computer vision has long been dominated by convolutional neural networks (CNNs). We’ve already discussed famous architectures like VGGNet and ResNet in previous posts, which have served as the primary backbones for a variety of vision tasks.
In contrast, network architectures in natural language processing (NLP) have evolved along a different trajectory. The dominant architecture in NLP today is the Transformer, designed for sequence modeling. Models like GPT-3 have achieved remarkable success, scaling to over 100 billion parameters thanks to their computational efficiency and scalability."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/11-vit/post/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-17T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-17T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Transformers for Image Classification: ViT and CLIP"><meta name=twitter:description content="Modelling in computer vision has long been dominated by convolutional neural networks (CNNs). We’ve already discussed famous architectures like VGGNet and ResNet in previous posts, which have served as the primary backbones for a variety of vision tasks.
In contrast, network architectures in natural language processing (NLP) have evolved along a different trajectory. The dominant architecture in NLP today is the Transformer, designed for sequence modeling. Models like GPT-3 have achieved remarkable success, scaling to over 100 billion parameters thanks to their computational efficiency and scalability."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Transformers for Image Classification: ViT and CLIP","item":"http://localhost:1313/posts/11-vit/post/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Transformers for Image Classification: ViT and CLIP","name":"Transformers for Image Classification: ViT and CLIP","description":"Modelling in computer vision has long been dominated by convolutional neural networks (CNNs). We’ve already discussed famous architectures like VGGNet and ResNet in previous posts, which have served as the primary backbones for a variety of vision tasks.\nIn contrast, network architectures in natural language processing (NLP) have evolved along a different trajectory. The dominant architecture in NLP today is the Transformer, designed for sequence modeling. Models like GPT-3 have achieved remarkable success, scaling to over 100 billion parameters thanks to their computational efficiency and scalability.","keywords":[],"articleBody":"Modelling in computer vision has long been dominated by convolutional neural networks (CNNs). We’ve already discussed famous architectures like VGGNet and ResNet in previous posts, which have served as the primary backbones for a variety of vision tasks.\nIn contrast, network architectures in natural language processing (NLP) have evolved along a different trajectory. The dominant architecture in NLP today is the Transformer, designed for sequence modeling. Models like GPT-3 have achieved remarkable success, scaling to over 100 billion parameters thanks to their computational efficiency and scalability.\nInspired by the success of Transformers in NLP, researchers sought to apply them directly to images with minimal modifications, with the aim to completely replace CNNs. This led to the introduction of Vision Transformers (ViTs) in 2021, marking a paradigm shift in deep learning for vision.\nIn this post, we’ll explore how to adapt the Transformer architecture for images by coding a Vision Transformer model from scratch. Then, we’ll dive into the famous CLIP model, which connects text and images to learn a wide range of visual concepts directly from natural language.\nVision Transformer (ViT) The paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, introduced the Vision Transformer (ViT) [1] model, which was the first Transformer-based approach to achieve impressive results on ImageNet dataset.\nAs the title suggests, ViT represents an input image (of size $224 \\times 224$ in ImageNet) as a sequence of image patches (each of size $16 \\times 16$), similar to the sequence of word embeddings in NLP models, and directly predicts class labels for the image.\nTo better understand the ViT architecture, we’ll take a top-down approach: first examining the overall framework, then coding each module, most of which we’ve encountered before.\nChoosing the Right Transformer Model Recall the two broad categories of large language models we’ve discussed before:\nEncoder-only models – Primarily used for classification and regression tasks (e.g., sentiment analysis, spam detection, stock price prediction). A prime example is BERT, where a special [CLS] token is prepended to the input sequence, and its final hidden state is used for classification.\nDecoder-only models – Designed for language generation tasks like translation, summarization, and question answering. We covered these extensively in our GPT series.\nSince the goal is image classification, an encoder-based approach is the most sensible choice. Similar to BERT, we add an extra learnable “classification” token to the input sequence and pass it through the Transformer Encoder model. The output corresponding to this token can then be used to predict the image class label. Note that we train this model in a supervised manner.\nTokenizing Images Traditionally, Transformers process text by tokenizing it into discrete units (words or subwords), converting them into token IDs, and mapping them into continuous embeddings. The challenge is: how do we apply this to images, which are fundamentally different from text?\nThe trick is to “tokenize” an image by splitting it into fixed-size patches, treating each patch as a token. Each patch is then flattened and linearly projected into an embedding vector, just like word embeddings in NLP. This results in a sequence of patch embeddings that can be processed by the Transformer model.\nWe also add learnable positional encodings to each patch to retain its position information in the image. This helps the model differentiate between patches and understand their arrangement in the image, thereby learning the underlying 2D structure of the image.\nThis transformations allows us to feed an input image into a standard Transformer encoder and generate a class label, completely eliminating the need for convolutions. Simple, right?\nLet’s visualize the overall architecture:\nThe Vision Transformer model.\nOverview of the ViT Pipeline Split an image into fixed-size patches. Flatten the patches. Project the flattened patches into the embedding dimension. Prepend a [class] token embedding to the sequence. Add positional embeddings. Feed the sequence as an input to a standard transformer encoder. Pretrain the model in a fully supervised setting on a large dataset. Replace the classification head and fine-tune on a smaller dataset. Similar to other transformer models, Vision Transformers (ViTs) require pre-training on large, diverse datasets to generalize well and outperform standard CNNs on smaller or mid-sized datasets (e.g., ImageNet with 1M images).\nUnlike Convolutional Neural Networks (CNNs), which have strong inductive biases (such as spatial locality and translation invariance), ViTs rely solely on self-attention, lacking these built-in properties. As a result, ViTs do not generalize as effectively when trained on limited datasets and often underperform compared to CNNs.\nHowever, when pre-trained on much larger datasets (e.g., ImageNet-21k with 14M images and JFT with 300M images) and then transferred to downstream tasks, the situation changes. Large-scale training helps compensate for the lack of inductive biases, enabling pre-trained ViTs to match or even surpass state-of-the-art performance on several image recognition benchmarks.\nThis is why I haven’t included ViT training in this post. It’s generally a good idea to adopt pretrained ViTs and fine-tune them on small datasets, rather than training them from scratch.\nTraining a ViT from scratch vs. fine-tuning a pre-trained ViT. Ref: Sebastian Raschka’s blog\nSpelled out in Code Let’s implement the Vision Transformer (ViT) architecture step by step. Here’s the basic set of imports to get started:\nimport torch import torch.nn as nn import matplotlib.pyplot as plt import torchvision Patch Embeddings (naive implementation) Ignoring the batch dimension for now, a standard Transformer processes a 1D sequence of token embeddings with shape $(\\text{N} \\times \\text{D})$, where:\n$\\text{N}$ is the number of tokens. $\\text{D}$ is the embedding dimension. Since images are inherently 2D, we convert them into a sequence of patch embeddings through the following steps:\nInput image: $(C \\times H \\times W)$, where $H$ is height, $W$ is width, and $C$ is the number of channels.\nSplitting into Patches: We divide the image into $\\text{N}$ non-overlapping patches, each of size $(C \\times \\text{P} \\times \\text{P})$.\n$(C \\times H \\times W) \\rightarrow (C \\times H/\\text{P} \\times \\text{P} \\times W/\\text{P} \\times \\text{P})$. Reshape: Rearrange the dimensions to group patches separately.\n$(C \\times H/\\text{P} \\times \\text{P} \\times W/\\text{P} \\times \\text{P}) \\rightarrow (H/\\text{P} \\times W/\\text{P} \\times C \\times \\text{P} \\times \\text{P})$. Number of Patches: The number of patches is computed as $\\text{N} = (H \\cdot W)/\\text{P}^2$.\n$(H/\\text{P} \\times W/\\text{P} \\times C \\times \\text{P} \\times \\text{P}) \\rightarrow (\\text{N} \\times C \\times \\text{P} \\times \\text{P})$. This is analogous to the number of tokens in NLP. Flatten the Patches: Each patch is flattened into a vector of size $\\text{P}^2 C$.\n$(\\text{N} \\times C \\times \\text{P} \\times \\text{P}) \\rightarrow (\\text{N} \\times \\text{P}^2 C)$ Obtain Patch Embeddings: Map the patch dimension to the embedding dimension $\\text{D}$ using a linear layer.\n$(\\text{N} \\times \\text{P}^2 C) \\rightarrow (\\text{N} \\times \\text{D})$ Visually, this process is illustrated as follows:\nWe split the input image into 4 patches, each represented with a different color. We obtain patch embeddings using the naive implementation.\nImplementing these steps in code:\nclass PatchEmbedding(nn.Module): def __init__(self, patch_size, n_channels, emb_dim): super().__init__() self.patch_size = patch_size self.projection = nn.Linear(patch_size * patch_size * n_channels, emb_dim) def forward(self, x, visualize=False): B, C, H, W = x.size() x = x.view(B, C, H // self.patch_size, self.patch_size, W // self.patch_size, self.patch_size) # Split it into (B, C, H', p_H, W', p_W) x = x.permute(0, 2, 4, 1, 3, 5) # Reshape into (B, H', W', C, p_H, p_W) x = x.flatten(1, 2) # Obtain N (B, N, C, p_H, p_W) if visualize: return x # Return unflattened patches for visualization x = x.flatten(2, 4) # Flatten the last dim (B, N, C*p_H*p_W) x = self.projection(x) # Project into the embedding dimension (B, N, D) return x It takes a batch of images, splits each image into fixed-size patches, flattens them and linearly project them into the embedding dimension to obtain patch embeddings.\nIf visualize=True, it returns the patches before flattening them, allowing us to plot and inspect their structure. Let’s patchify a sample image from CIFAR-10 and verify the output:\n# Download CIFAR-10 dataset transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()]) cifar10_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=64, shuffle=True) # Extract a batch of images for image, label in train_loader: break fig, axes = plt.subplots(1, 2, figsize=(10, 5)) # Plot the original first image in the batch axes[0].imshow(image[0].permute(1, 2, 0)) axes[0].set_title('Original Image') # Plot the patches of first image in the batch patchify = PatchEmbedding(patch_size=8, n_channels=3, emb_dim=768) patches = patchify(image, visualize=True)[0] img_grid = torchvision.utils.make_grid(patches, nrow=4, pad_value=0.9) axes[1].imshow(img_grid.permute(1, 2, 0)) axes[1].set_title('Patches') plt.show() Each CIFAR-10 image has a shape of $(32, 32, 3)$. Using patch_size=8 we get:\n$\\text{N} = (32 \\cdot 32) / (8)^2 = 16$ patches in total. Each patch of shape $(8, 8, 3)$. We arrange the image patches into a grid and visualize them as shown below:\nA 32x32x3 image split into 16 patches of size 8x8x3.\nThis output verifies that our function is accurately splitting the image into the desired patches.\nPatch Embeddings (convolution) Another common approach to obtaining patch embeddings directly from images is to use a single convolution operation, where the kernel size and stride are set to the patch size, and the number of kernels matches the embedding dimension. This operation functions as a sliding window, capturing each patch without overlap.\nInput image: $(C \\times H \\times W)$\nApply convolution: We apply a convolution with kernel size and stride = patch size $P$, and number of kernels = embedding dimension $\\text{D}$.\nThis results in feature maps of size $(H’ \\times W’)$, where: $H’ = (H - P)/P + 1 = H/P$ $W’ = (W - P)/P + 1 = W/P$ $(C \\times H \\times W) \\rightarrow (\\text{D} \\times H/\\text{P} \\times W/\\text{P})$ Number of Patches: The number of patches is computed as $\\text{N} = (H \\cdot W)/\\text{P}^2$.\n$(\\text{D} \\times H/\\text{P} \\times W/\\text{P}) \\rightarrow (\\text{D} \\times \\text{N})$ Transpose:\n$(\\text{D} \\times \\text{N}) \\rightarrow (\\text{N} \\times \\text{D})$ These steps yield the same result as the standard approach, as shown below:\nWe split the input image into 4 patches, each represented with a different color. We obtain patch embeddings using convolution.\nImplementing in code:\nclass PatchEmbedding(nn.Module): def __init__(self, patch_size, n_channels, emb_dim): super().__init__() self.patch_size = patch_size self.projection = nn.Conv2d(n_channels, emb_dim, kernel_size=patch_size, stride=patch_size) def forward(self, x): B, C, H, W = x.size() x = self.projection(x) # Output from Conv (B, D, H', W') x = x.flatten(2, 3) # Obtain N (B, D, H'*W') x = x.transpose(1, 2) # Transpose to (B, N, D) return x Both methods produce the same results, but the convolution-based approach is more concise in code and computationally efficient, particularly on a GPU.\nArchitecture Similar to GPT models, in ViT:\nLayer normalization is applied before each block, and residual connections are applied after each block. GeLU activation is used in the MLP layers. A key distinction is that, since we are using the encoder part of the Transformer, self-attention is used instead of causal attention. Everything else remains the same.\nclass MultiHeadSelfAttention(nn.Module): def __init__(self, d_in, d_out, attn_pdrop, num_heads, qkv_bias=False): super().__init__() self.c_attn = nn.Linear(d_in, 3 * d_out, bias=qkv_bias) self.c_proj = nn.Linear(d_out, d_out) self.attn_dropout = nn.Dropout(attn_pdrop) self.d_h = d_out // num_heads # head dimension self.num_heads = num_heads def forward(self, x): # Input vectors x - (B, N, d_in) B, N, d_in = x.shape # Obtain keys, values and queries in one go - (B, N, d_out) qkv = self.c_attn(x) queries, keys, values = qkv.chunk(3, dim=-1) # Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h) k = keys.view(B, N, self.num_heads, self.d_h).transpose(1, 2) v = values.view(B, N, self.num_heads, self.d_h).transpose(1, 2) q = queries.view(B, N, self.num_heads, self.d_h).transpose(1, 2) # Apply scaled dot-product attention on each head attn_scores = (q @ k.transpose(2, 3)) * k.shape[-1]**-0.5 attn_weights = torch.softmax(attn_scores, dim=-1) attn_weights = self.attn_dropout(attn_weights) context_vec = attn_weights @ v # Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out) out = context_vec.transpose(1, 2).contiguous().view(B, N, -1) out = self.c_proj(out) return out class LayerNorm(nn.Module): def __init__(self, emb_dim): super().__init__() self.eps = 1e-5 self.weight = nn.Parameter(torch.ones(emb_dim)) self.bias = nn.Parameter(torch.zeros(emb_dim)) def forward(self, x): mean = x.mean(dim=-1, keepdim=True) var = x.var(dim=-1, keepdim=True, unbiased=False) # Used biased variance estimation (without Bessel's correction) norm_x = (x - mean) / torch.sqrt(var + self.eps) return self.weight * norm_x + self.bias class GELU(nn.Module): def __init__(self): super().__init__() def forward(self, x): return 0.5 * x * (1 + torch.tanh( torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)) )) class MLP(nn.Module): def __init__(self, emb_dim): super().__init__() self.c_fc = nn.Linear(emb_dim, 4 * emb_dim) self.c_proj = nn.Linear(4 * emb_dim, emb_dim) self.gelu = GELU() def forward(self, x): x = self.gelu(self.c_fc(x)) x = self.c_proj(x) return x class EncoderBlock(nn.Module): def __init__(self, cfg): super().__init__() self.ln_1 = LayerNorm(emb_dim=cfg['emb_dim']) self.attn = MultiHeadSelfAttention(d_in=cfg['emb_dim'], d_out=cfg['emb_dim'], attn_pdrop=cfg['attn_pdrop'], num_heads=cfg['n_heads'], qkv_bias=cfg['qkv_bias']) self.ln_2 = LayerNorm(emb_dim=cfg['emb_dim']) self.mlp = MLP(emb_dim=cfg['emb_dim']) self.resid_dropout = nn.Dropout(cfg['resid_pdrop']) def forward(self, x): x = x + self.resid_dropout(self.attn(self.ln_1(x))) x = x + self.resid_dropout(self.mlp(self.ln_2(x))) return x Defining the ViT model Let’s implement the Vision Transformer model in code as follows:\nclass ViT(nn.Module): def __init__(self, cfg): super().__init__() self.patch_size = cfg['patch_size'] self.patch_embedding = PatchEmbedding(cfg['patch_size'], cfg['n_channels'], cfg['emb_dim']) # Define class token and position embeddings as learnable parameters self.cls_token_embedding = nn.Parameter(torch.randn(1, 1, cfg['emb_dim'])) self.position_embedding = nn.Parameter(torch.randn(1, 1 + cfg['n_patches'], cfg['emb_dim'])) self.embd_dropout = nn.Dropout(cfg['embd_pdrop']) self.blocks = nn.Sequential(*[EncoderBlock(cfg) for _ in range(cfg['n_layers'])]) self.ln_f = LayerNorm(cfg['emb_dim']) self.cls_head = nn.Linear(cfg['emb_dim'], cfg['n_classes']) def forward(self, img): # Obtain patch embeddings patch_emb = self.patch_embeddings(img) # (B, N, D) B, N, _ = patch_emb.size() # Prepend CLS token to the sequence of patch embeddings cls_token_emb = self.cls_token_embedding.expand(B, -1, -1) # (B, 1, D) patch_emb = torch.cat((cls_token_emb, patch_emb), dim=1) # (B, N+1, D) # Add positional embeddings pos_emb = self.position_embedding[:, :N+1] # (1, N+1, D) x = patch_emb + pos_emb # (B, N+1, D) x = self.embd_dropout(x) # Pass through Transformer encoder blocks x = self.blocks(x) x = self.ln_f(x) # Perform classification x = x[:, 0, :] # (B, D) logits = self.cls_head(x) # (B, num_classes) return logits We use nn.Parameter for the class token and position embeddings because these are learnable parameters that need to be optimized during training, just like the weights in a neural network.\nAfter passing the input through the Transformer blocks, we extract the learned representations of the first token (the classification token) and feed it into the classification head, which outputs logits for the class labels.\nNow, let’s define the configuration dictionary for the base ViT model and perform a single forward pass using a batch of CIFAR-10 images as a sanity check:\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu' print(\"Using\", device) # Define configuration dictionary ViT_B8_CONFIG = { \"patch_size\": 8, # Patch size \"n_patches\": 16, # Maximum number of patches \"n_channels\": 3, # Number of channels in the image \"n_classes\": 10, # Number of classes in CIFAR-10 dataset \"emb_dim\": 768, # Embedding dimension \"n_heads\": 12, # Number of attention heads \"n_layers\": 12, # Number of transformer blocks \"attn_pdrop\": 0.0, # Dropout probability for attention dropout \"embd_pdrop\": 0.0, # Dropout probability for embeddings dropout \"resid_pdrop\": 0.0, # Dropout probability for residual dropout \"qkv_bias\": True # Whether to use bias in QKV layer } model = ViT(ViT_B8_CONFIG).to(device) # Print the number of parameters in the model print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters') # Forward pass with one example image, label = image.to(device), label.to(device) logits = model(image) print(\"Model output shape:\", logits.size()) loss = nn.functional.cross_entropy(logits, label) print(\"Initial Loss:\", loss.item()) 85.223434 M parameters Model output shape: torch.Size([64, 10]) Initial Loss: 2.3756895065307617 As expected the model outputs a probability distribution over all the class labels for each example in the batch. At initialization, the model assigns equal probability to each class (since it’s untrained), which results in a loss close to $ -\\ln(1/10) = 2.302$, confirming that everything is working as expected.\nArchitecture variants The authors present three variants of the ViT architecture: Base, Large, and Huge, with their respective hyperparameters defined below.\nDetails of Vision Transformer model variants.\nTo denote model size and input patch size, models are named using the format ViT-[Size]/[Patch Size]. For example, ViT-B/16 refers to the Base variant with a $16 \\times 16$ input patch size.\nThe transformer’s sequence length (i.e., the number of patches) is inversely proportional to the square of the patch size and directly proportional to the image size. This means that models using smaller patches or higher-resolution images are more computationally expensive, as they must process longer sequences.\nTo address this limitation, Microsoft Research introduced the Swin Transformer [2], which you can read more about here: Aman Arora’s blog on Swin Transfomer. It has become the go-to transformer-based backbone for vision tasks today.\nContrastive Language-Image Pre-Training (CLIP) With the success of GPT-3 in “zero-shot” transfer to downstream datasets, OpenAI aimed to extend this capability to image models for tasks like image classification. Pre-training on vast amounts of web-sourced data has enabled such task-agnostic models to be competitive across various tasks while requiring little to no dataset-specific training.\nHowever, vision models have traditionally been pre-trained on crowd-labeled datasets like ImageNet. This form of supervision limits models to predict a fixed set of predetermined object categories and typically requires fine-tuning on new labeled datasets to adapt to different tasks.\nCLIP [3] (Contrastive Language-Image Pre-training) bridges this gap by learning visual concepts through natural language supervision on a large scale. The primary motivation behind natural language supervision is the vast amount of publicly available data on the internet. CLIP can be applied to any visual classification benchmark by simply providing the names of the visual categories to recognize, much like the “zero-shot” capabilities of GPT-2 and GPT-3.\nDataset The training data leverages an abundant source of supervision: text paired with images found across the internet. Specifically, the authors constructed a new dataset of 400 million (image, text) pairs, naming it WIT (WebImageText).\nContrastive Pre-training The initial approach was to jointly train an image CNN and a text transformer to predict the caption of an image (i.e., an image captioning model). However, predicting the exact words of a caption proved challenging due to the vast variability in descriptions, comments, and related text accompanying images, making training slow.\nPrior research has shown that contrastive objectives can learn better representations than their equivalent predictive objective. Thus, CLIP was trained using a more efficient proxy task—predicting which text as a whole is paired with which image rather than generating exact words of that text.\nCLIP jointly trains an image encoder and a text encoder to learn a shared multi-modal embedding space, mapping both text and images to the same representation. The contrastive learning approach optimizes this space such that similar pairs (image and its corresponding text) stay close, while dissimilar ones are pushed apart.\nFor example, an image of a dog and the sentence “an image of a dog” will have similar embeddings and will be close to each other in the vector space, whereas a sentence like “an image of a bird” will have dissimilar embeddings and will be farther from that image.\nCLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples.\nGiven a batch of $N$ (image, text) pairs, CLIP is trained to predict the correct pairings among the $N \\times N$ possible combinations:\nIt maximizes the cosine similariy between the image and text embeddings for the $N$ correct pairs (i.e., the diagonal elements of the similarity matrix). It minimizes the cosine similarity for the $N^2 - N$ incorrect pairings. These scores are optimized using symmetric cross-entropy loss. Let’s break this down with pseudocode:\nInput: Batch of images and texts\n# Image batch: [N, C, H, W] # Text batch: [N, L] An example of a batch from the dataset.\nExtract feature representations: Images are passed through an image encoder and texts through a text encoder to obtain features of dimensions $\\text{D}\\text{i}$ and $\\text{D}\\text{t}$, respectively. (These features come from different models, so they will have different dimensions.)\n# Extract feature representation I_f = image_encoder(images) # Image features [N, D_i] T_f = text_encoder(texts) # Text features [N, D_t] Obtain embeddings: A linear layer projects both feature sets into a common embedding space of dimension $\\text{D}_\\text{e}$.\n# Use linear projection to map to the multi-modal embedding space. W_i = nn.Linear(D_i, D_e) W_t = nn.Linear(D_t, D_e) I_e = W_i(I_f) # Image in embedding space [N, D_e] T_e = W_t(T_f) # Text in embedding space [N, D_e] Normalize: The embeddings are normalized (L2) to unit vectors, preventing any scale differences.\n# Obtain normalized features I_e = nn.functional.normalize(I_e, p=2, dim=1) T_e = nn.functional.normalize(T_e, p=2, dim=1) Compute cosine similarity: A dot product operation between image and text embeddings yields similarity scores.\n# Find cosine similarity logits = I_e @ T_e.T # [N, N] Cosine similarity matrix of our batch.\nCalculate Contrastive loss: The cross entropy loss is computed for each row and column of our similarity matrix and divide by 2, since each pair is computed twice.\n# symmetric loss function labels = np.arange(N) loss_i = cross_entropy_loss(logits, labels, axis=0) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2 The correct labels correspond to the diagonal elements of the similarity matrix. The variable loss_i and loss_t push the logits for image similarities and text similarities to be high along the diagonal and low elsewhere. This is referred to as the contrastive loss.\nUsing this objective, CLIP learns to recognize a wide range of visual concepts in images directly from natural language, making it applicable to diverse visual classification tasks.\nTemperature scaling Cross-entropy loss includes a softmax function that converts logits into probability scores. CLIP uses temperature scaling (as discussed in our previous post) to adjust the range of logits before applying the softmax, effectively controlling the strength of separation between positive and negative pairs.\nTo avoid manually tuning this as a hyperparameter, temperature scaling is optimized during training as a log-parameterized multiplicative scalar, called logit_scale.\n# Find scaled cosine similarity logit_scale = np.exp(temperature) logits = (I_e @ T_e.T) * logit_scale # [N, N] Architecture CLIP consists of two main components: an image encoder and a text encoder.\nImage Encoder: The authors experimented with two different models:\nResNets: ResNet-50, ResNet-101, RN50x4, RN50x16, and RN50x64. Vision Transformers (ViTs): ViT-B/32, a ViT-B/16, and a ViT-L/14 (without the classification head). Modification: An additional layer normalization was added to the combined patch and position embeddings before inputting into the transformer. ViTs offered 3x compute efficiency over ResNets and trained faster, making them the preferred choice. Text Encoder: A 63M-parameter decoder-only Transformer, similar to GPT-2:\nNumber of layers: $L = 12$. Number of heads: $H = 8$. Embedding dimension: $\\text{d}_{model} = 512$. Tokenizer: Lowercased BPE with a vocabulary size of 49,152. Context length: $N = 76$. Special tokens: Sequence is bracketed with [SOS] (start) and [EOS] (end) token. CLIP is trained from scratch without initializing the image encoder with ImageNet weights or the text encoder with pre-trained weights.\nA batch size of 32,768 is used-meaning given an image, CLIP predicts which one of 32,768 text snippets was actually paired with it in our dataset.\nZero-shot transfer CLIP achieves zero-shot performance on various benchmarks, such as ImageNet, meaning the model was not explicitly trained on any of the 1.28M training examples in ImageNet but still matched the accuracy of the original ResNet-50, which was trained on the dataset.\nCLIP is pre-trained to predict whether an image and a text snippet (rather than a single class label) are paired together in its dataset. To bridge the distribution gap between a text snippet and a class label, we use the prompt template: “$\\text{A photo of a {label}}$”.\nLet’s take ImageNet as an example and see how we can perform zero-shot transfer with CLIP:\nCreate text descriptions from labels – If we are working with ImageNet, which has 1000 possible classes, we create text descriptions for each label using the prompt template (e.g., “A photo of a dog.”).\nObtain text embeddings – We feed these text descriptions into CLIP’s text encoder to obtain 1000 different embeddings corresponding to all possible classes.\nObtain image embeddings – Next, we take the input image we want to classify (e.g., an image of a dog) and embed it using CLIP.\nCompute similarity scores – We compute cosine similarity scores between the image embedding and all text embeddings.\nObtain class probabilities – Finally, we pass these similarity scores through a softmax function to obtain a probability distribution over all classes, just as in a typical vision model trained for image classification.\nAt test time, we convert all of a dataset’s classes into captions such as ‘A photo of a dog’ and predict the class whose caption CLIP estimates best pairs with the given image.\nLike the GPT family, CLIP learns a wide variety of tasks during pre-training, enabling powerful zero-shot transfer for image classification and beyond.\nReferences Google’s blog on ViT https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/11-vision-transformer.html [1] Alexey Dosovitskiy, et al. “An image is worth 16x16 words: Transformers for image recognition at scale.”, ICLR 2021. [2] Ze Liu, et al. “Swin transformer: Hierarchical vision transformer using shifted windows”, ICCV 2021. [3] Alec Radford, et al. “Learning transferable visual models from natural language supervision”, ICML 2021. Medium: Understanding OpenAI’s CLIP model Open AI’s blog on CLIP Aman Arora’s blog on Annotated CLIP ","wordCount":"4145","inLanguage":"en","datePublished":"2025-03-17T00:00:00Z","dateModified":"2025-03-17T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/11-vit/post/"},"publisher":{"@type":"Organization","name":"YA's Almanac","logo":{"@type":"ImageObject","url":"http://localhost:1313/assets/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="YA's Almanac (Alt + H)">YA's Almanac</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Home><span>Home</span></a></li><li><a href=http://localhost:1313/archives/ title=Archives><span>Archives</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Transformers for Image Classification: ViT and CLIP</h1><div class=post-meta><span title='2025-03-17 00:00:00 +0000 UTC'>March 17, 2025</span>&nbsp;·&nbsp;20 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#vision-transformer-vit aria-label="Vision Transformer (ViT)">Vision Transformer (ViT)</a><ul><li><a href=#choosing-the-right-transformer-model aria-label="Choosing the Right Transformer Model">Choosing the Right Transformer Model</a></li><li><a href=#tokenizing-images aria-label="Tokenizing Images">Tokenizing Images</a></li><li><a href=#overview-of-the-vit-pipeline aria-label="Overview of the ViT Pipeline">Overview of the ViT Pipeline</a></li><li><a href=#spelled-out-in-code aria-label="Spelled out in Code">Spelled out in Code</a><ul><li><a href=#patch-embeddings-naive-implementation aria-label="Patch Embeddings (naive implementation)">Patch Embeddings (naive implementation)</a></li><li><a href=#patch-embeddings-convolution aria-label="Patch Embeddings (convolution)">Patch Embeddings (convolution)</a></li><li><a href=#architecture aria-label=Architecture>Architecture</a></li><li><a href=#defining-the-vit-model aria-label="Defining the ViT model">Defining the ViT model</a></li></ul></li><li><a href=#architecture-variants aria-label="Architecture variants">Architecture variants</a></li></ul></li><li><a href=#contrastive-language-image-pre-training-clip aria-label="Contrastive Language-Image Pre-Training (CLIP)">Contrastive Language-Image Pre-Training (CLIP)</a><ul><li><a href=#dataset aria-label=Dataset>Dataset</a></li><li><a href=#contrastive-pre-training aria-label="Contrastive Pre-training">Contrastive Pre-training</a><ul><li><a href=#temperature-scaling aria-label="Temperature scaling">Temperature scaling</a></li></ul></li><li><a href=#architecture-1 aria-label=Architecture>Architecture</a></li><li><a href=#zero-shot-transfer aria-label="Zero-shot transfer">Zero-shot transfer</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>Modelling in computer vision has long been dominated by convolutional neural networks (CNNs). We’ve already discussed famous architectures like VGGNet and ResNet in previous posts, which have served as the primary backbones for a variety of vision tasks.</p><p>In contrast, network architectures in natural language processing (NLP) have evolved along a different trajectory. The dominant architecture in NLP today is the Transformer, designed for sequence modeling. Models like GPT-3 have achieved remarkable success, scaling to over 100 billion parameters thanks to their computational efficiency and scalability.</p><p>Inspired by the success of Transformers in NLP, researchers sought to apply them directly to images with minimal modifications, with the aim to completely replace CNNs. This led to the introduction of Vision Transformers (ViTs) in 2021, marking a paradigm shift in deep learning for vision.</p><p>In this post, we’ll explore how to adapt the Transformer architecture for images by coding a Vision Transformer model from scratch. Then, we’ll dive into the famous CLIP model, which connects text and images to learn a wide range of visual concepts directly from natural language.</p><h2 id=vision-transformer-vit>Vision Transformer (ViT)<a hidden class=anchor aria-hidden=true href=#vision-transformer-vit>#</a></h2><p>The paper <em>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em>, introduced the Vision Transformer (ViT) <a href=#references>[1]</a> model, which was the first Transformer-based approach to achieve impressive results on ImageNet dataset.</p><p>As the title suggests, ViT represents an input image (of size $224 \times 224$ in ImageNet) as a sequence of image patches (each of size $16 \times 16$), similar to the sequence of word embeddings in NLP models, and directly predicts class labels for the image.</p><p>To better understand the ViT architecture, we’ll take a top-down approach: first examining the overall framework, then coding each module, most of which we&rsquo;ve encountered before.</p><h3 id=choosing-the-right-transformer-model>Choosing the Right Transformer Model<a hidden class=anchor aria-hidden=true href=#choosing-the-right-transformer-model>#</a></h3><p>Recall the two broad categories of large language models we’ve discussed before:</p><ol><li><p>Encoder-only models – Primarily used for classification and regression tasks (e.g., sentiment analysis, spam detection, stock price prediction). A prime example is BERT, where a special [CLS] token is prepended to the input sequence, and its final hidden state is used for classification.</p></li><li><p>Decoder-only models – Designed for language generation tasks like translation, summarization, and question answering. We covered these extensively in our GPT series.</p></li></ol><p>Since the goal is image classification, an encoder-based approach is the most sensible choice. Similar to BERT, we add an extra learnable &ldquo;classification&rdquo; token to the input sequence and pass it through the Transformer Encoder model. The output corresponding to this token can then be used to predict the image class label. Note that we train this model in a supervised manner.</p><h3 id=tokenizing-images>Tokenizing Images<a hidden class=anchor aria-hidden=true href=#tokenizing-images>#</a></h3><p>Traditionally, Transformers process text by tokenizing it into discrete units (words or subwords), converting them into token IDs, and mapping them into continuous embeddings. The challenge is: how do we apply this to images, which are fundamentally different from text?</p><p>The trick is to &ldquo;tokenize&rdquo; an image by splitting it into fixed-size patches, treating each patch as a token. Each patch is then flattened and linearly projected into an embedding vector, just like word embeddings in NLP. This results in a sequence of patch embeddings that can be processed by the Transformer model.</p><p>We also add learnable positional encodings to each patch to retain its position information in the image. This helps the model differentiate between patches and understand their arrangement in the image, thereby learning the underlying 2D structure of the image.</p><p>This transformations allows us to feed an input image into a standard Transformer encoder and generate a class label, completely eliminating the need for convolutions. Simple, right?</p><p>Let’s visualize the overall architecture:</p><figure class=align-center><img loading=lazy src=../vit-model.png#center alt="The Vision Transformer model."><figcaption><p>The Vision Transformer model.</p></figcaption></figure><h3 id=overview-of-the-vit-pipeline>Overview of the ViT Pipeline<a hidden class=anchor aria-hidden=true href=#overview-of-the-vit-pipeline>#</a></h3><ol><li>Split an image into fixed-size patches.</li><li>Flatten the patches.</li><li>Project the flattened patches into the embedding dimension.</li><li>Prepend a [class] token embedding to the sequence.</li><li>Add positional embeddings.</li><li>Feed the sequence as an input to a standard transformer encoder.</li><li>Pretrain the model in a fully supervised setting on a large dataset.</li><li>Replace the classification head and fine-tune on a smaller dataset.</li></ol><p>Similar to other transformer models, Vision Transformers (ViTs) require pre-training on large, diverse datasets to generalize well and outperform standard CNNs on smaller or mid-sized datasets (e.g., ImageNet with 1M images).</p><p>Unlike Convolutional Neural Networks (CNNs), which have strong inductive biases (such as spatial locality and translation invariance), ViTs rely solely on self-attention, lacking these built-in properties. As a result, ViTs do not generalize as effectively when trained on limited datasets and often underperform compared to CNNs.</p><p>However, when pre-trained on much larger datasets (e.g., ImageNet-21k with 14M images and JFT with 300M images) and then transferred to downstream tasks, the situation changes. Large-scale training helps compensate for the lack of inductive biases, enabling pre-trained ViTs to match or even surpass state-of-the-art performance on several image recognition benchmarks.</p><p>This is why I haven’t included ViT training in this post. It’s generally a good idea to adopt pretrained ViTs and fine-tune them on small datasets, rather than training them from scratch.</p><figure class=align-center><img loading=lazy src=../finetune.png#center alt="Training a ViT from scratch vs. fine-tuning a pre-trained ViT. Ref: Sebastian Raschka&amp;rsquo;s blog" width=600><figcaption><p>Training a ViT from scratch vs. fine-tuning a pre-trained ViT. Ref: <a href="https://magazine.sebastianraschka.com/p/ahead-of-ai-10-state-of-computer?utm_source=publication-search">Sebastian Raschka&rsquo;s blog</a></p></figcaption></figure><h3 id=spelled-out-in-code>Spelled out in Code<a hidden class=anchor aria-hidden=true href=#spelled-out-in-code>#</a></h3><p>Let&rsquo;s implement the Vision Transformer (ViT) architecture step by step. Here&rsquo;s the basic set of imports to get started:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torchvision</span>
</span></span></code></pre></div><h4 id=patch-embeddings-naive-implementation>Patch Embeddings (naive implementation)<a hidden class=anchor aria-hidden=true href=#patch-embeddings-naive-implementation>#</a></h4><p>Ignoring the batch dimension for now, a standard Transformer processes a 1D sequence of token embeddings with shape $(\text{N} \times \text{D})$, where:</p><ul><li>$\text{N}$ is the number of tokens.</li><li>$\text{D}$ is the embedding dimension.</li></ul><p>Since images are inherently 2D, we convert them into a sequence of patch embeddings through the following steps:</p><ol><li><p>Input image: $(C \times H \times W)$, where $H$ is height, $W$ is width, and $C$ is the number of channels.</p></li><li><p>Splitting into Patches: We divide the image into $\text{N}$ non-overlapping patches, each of size $(C \times \text{P} \times \text{P})$.</p><ul><li>$(C \times H \times W) \rightarrow (C \times H/\text{P} \times \text{P} \times W/\text{P} \times \text{P})$.</li></ul></li><li><p>Reshape: Rearrange the dimensions to group patches separately.</p><ul><li>$(C \times H/\text{P} \times \text{P} \times W/\text{P} \times \text{P}) \rightarrow (H/\text{P} \times W/\text{P} \times C \times \text{P} \times \text{P})$.</li></ul></li><li><p>Number of Patches: The number of patches is computed as $\text{N} = (H \cdot W)/\text{P}^2$.</p><ul><li>$(H/\text{P} \times W/\text{P} \times C \times \text{P} \times \text{P}) \rightarrow (\text{N} \times C \times \text{P} \times \text{P})$.</li><li>This is analogous to the number of tokens in NLP.</li></ul></li><li><p>Flatten the Patches: Each patch is flattened into a vector of size $\text{P}^2 C$.</p><ul><li>$(\text{N} \times C \times \text{P} \times \text{P}) \rightarrow (\text{N} \times \text{P}^2 C)$</li></ul></li><li><p>Obtain Patch Embeddings: Map the patch dimension to the embedding dimension $\text{D}$ using a linear layer.</p><ul><li>$(\text{N} \times \text{P}^2 C) \rightarrow (\text{N} \times \text{D})$</li></ul></li></ol><p>Visually, this process is illustrated as follows:</p><figure class=align-center><img loading=lazy src=../patchify_1.png#center alt="We split the input image into 4 patches, each represented with a different color. We obtain patch embeddings using the naive implementation."><figcaption><p>We split the input image into 4 patches, each represented with a different color. We obtain patch embeddings using the naive implementation.</p></figcaption></figure><p>Implementing these steps in code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>PatchEmbedding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>patch_size</span><span class=p>,</span> <span class=n>n_channels</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>patch_size</span> <span class=o>=</span> <span class=n>patch_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>projection</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>patch_size</span> <span class=o>*</span> <span class=n>patch_size</span> <span class=o>*</span> <span class=n>n_channels</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>visualize</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>C</span><span class=p>,</span> <span class=n>H</span><span class=p>,</span> <span class=n>W</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>C</span><span class=p>,</span> <span class=n>H</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>patch_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>patch_size</span><span class=p>,</span> <span class=n>W</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>patch_size</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>patch_size</span><span class=p>)</span>  <span class=c1># Split it into (B, C, H&#39;, p_H, W&#39;, p_W)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>                                                                 <span class=c1># Reshape into (B, H&#39;, W&#39;, C, p_H, p_W)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>                                                                             <span class=c1># Obtain N (B, N, C, p_H, p_W)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>visualize</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>x</span>                                                                                    <span class=c1># Return unflattened patches for visualization</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>                                                                             <span class=c1># Flatten the last dim (B, N, C*p_H*p_W)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>projection</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>                                                                          <span class=c1># Project into the embedding dimension (B, N, D)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><p>It takes a batch of images, splits each image into fixed-size patches, flattens them and linearly project them into the embedding dimension to obtain patch embeddings.</p><p>If <code>visualize=True</code>, it returns the patches before flattening them, allowing us to plot and inspect their structure. Let&rsquo;s patchify a sample image from CIFAR-10 and verify the output:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Download CIFAR-10 dataset</span>
</span></span><span class=line><span class=cl><span class=n>transform</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span><span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>()])</span>
</span></span><span class=line><span class=cl><span class=n>cifar10_trainset</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>datasets</span><span class=o>.</span><span class=n>CIFAR10</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=n>transform</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>cifar10_trainset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Extract a batch of images</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>image</span><span class=p>,</span> <span class=n>label</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>break</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the original first image in the batch</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>image</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Original Image&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the patches of first image in the batch</span>
</span></span><span class=line><span class=cl><span class=n>patchify</span> <span class=o>=</span> <span class=n>PatchEmbedding</span><span class=p>(</span><span class=n>patch_size</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span> <span class=n>n_channels</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>emb_dim</span><span class=o>=</span><span class=mi>768</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>patches</span> <span class=o>=</span> <span class=n>patchify</span><span class=p>(</span><span class=n>image</span><span class=p>,</span> <span class=n>visualize</span><span class=o>=</span><span class=kc>True</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>img_grid</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>make_grid</span><span class=p>(</span><span class=n>patches</span><span class=p>,</span> <span class=n>nrow</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>pad_value</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>img_grid</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Patches&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p>Each CIFAR-10 image has a shape of $(32, 32, 3)$. Using <code>patch_size=8</code> we get:</p><ul><li>$\text{N} = (32 \cdot 32) / (8)^2 = 16$ patches in total.</li><li>Each patch of shape $(8, 8, 3)$.</li></ul><p>We arrange the image patches into a grid and visualize them as shown below:</p><figure class=align-center><img loading=lazy src=../patchify_plot.png#center alt="A 32x32x3 image split into 16 patches of size 8x8x3."><figcaption><p>A 32x32x3 image split into 16 patches of size 8x8x3.</p></figcaption></figure><p>This output verifies that our function is accurately splitting the image into the desired patches.</p><h4 id=patch-embeddings-convolution>Patch Embeddings (convolution)<a hidden class=anchor aria-hidden=true href=#patch-embeddings-convolution>#</a></h4><p>Another common approach to obtaining patch embeddings directly from images is to use a single convolution operation, where the kernel size and stride are set to the patch size, and the number of kernels matches the embedding dimension. This operation functions as a sliding window, capturing each patch without overlap.</p><ol><li><p>Input image: $(C \times H \times W)$</p></li><li><p>Apply convolution: We apply a convolution with kernel size and stride = patch size $P$, and number of kernels = embedding dimension $\text{D}$.</p><ul><li>This results in feature maps of size $(H&rsquo; \times W&rsquo;)$, where:<ul><li>$H&rsquo; = (H - P)/P + 1 = H/P$</li><li>$W&rsquo; = (W - P)/P + 1 = W/P$</li></ul></li><li>$(C \times H \times W) \rightarrow (\text{D} \times H/\text{P} \times W/\text{P})$</li></ul></li><li><p>Number of Patches: The number of patches is computed as $\text{N} = (H \cdot W)/\text{P}^2$.</p><ul><li>$(\text{D} \times H/\text{P} \times W/\text{P}) \rightarrow (\text{D} \times \text{N})$</li></ul></li><li><p>Transpose:</p><ul><li>$(\text{D} \times \text{N}) \rightarrow (\text{N} \times \text{D})$</li></ul></li></ol><p>These steps yield the same result as the standard approach, as shown below:</p><figure class=align-center><img loading=lazy src=../patchify_2.png#center alt="We split the input image into 4 patches, each represented with a different color. We obtain patch embeddings using convolution."><figcaption><p>We split the input image into 4 patches, each represented with a different color. We obtain patch embeddings using convolution.</p></figcaption></figure><p>Implementing in code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>PatchEmbedding</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>patch_size</span><span class=p>,</span> <span class=n>n_channels</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>patch_size</span> <span class=o>=</span> <span class=n>patch_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>projection</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Conv2d</span><span class=p>(</span><span class=n>n_channels</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>,</span> <span class=n>kernel_size</span><span class=o>=</span><span class=n>patch_size</span><span class=p>,</span> <span class=n>stride</span><span class=o>=</span><span class=n>patch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>C</span><span class=p>,</span> <span class=n>H</span><span class=p>,</span> <span class=n>W</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>projection</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>                                                                          <span class=c1># Output from Conv (B, D, H&#39;, W&#39;)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>                                                                             <span class=c1># Obtain N (B, D, H&#39;*W&#39;)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>                                                                           <span class=c1># Transpose to (B, N, D)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><p>Both methods produce the same results, but the convolution-based approach is more concise in code and computationally efficient, particularly on a GPU.</p><h4 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h4><p>Similar to GPT models, in ViT:</p><ul><li>Layer normalization is applied before each block, and residual connections are applied after each block.</li><li>GeLU activation is used in the MLP layers.</li></ul><p>A key distinction is that, since we are using the encoder part of the Transformer, self-attention is used instead of causal attention. Everything else remains the same.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadSelfAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>attn_pdrop</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>qkv_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>c_attn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=mi>3</span> <span class=o>*</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>c_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_out</span><span class=p>,</span> <span class=n>d_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attn_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>attn_pdrop</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span> <span class=o>=</span> <span class=n>d_out</span> <span class=o>//</span> <span class=n>num_heads</span>                    <span class=c1># head dimension</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Input vectors x - (B, N, d_in)</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>d_in</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=c1># Obtain keys, values and queries in one go - (B, N, d_out)</span>
</span></span><span class=line><span class=cl>        <span class=n>qkv</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_attn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>queries</span><span class=p>,</span> <span class=n>keys</span><span class=p>,</span> <span class=n>values</span> <span class=o>=</span> <span class=n>qkv</span><span class=o>.</span><span class=n>chunk</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=n>keys</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=n>values</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>queries</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Apply scaled dot-product attention on each head</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_scores</span> <span class=o>=</span> <span class=p>(</span><span class=n>q</span> <span class=o>@</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span> <span class=o>*</span> <span class=n>k</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>**-</span><span class=mf>0.5</span>    
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attn_scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>                             
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>attn_dropout</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>context_vec</span> <span class=o>=</span> <span class=n>attn_weights</span> <span class=o>@</span> <span class=n>v</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>context_vec</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_proj</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>LayerNorm</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>eps</span> <span class=o>=</span> <span class=mf>1e-5</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>weight</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>emb_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>bias</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>emb_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>mean</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>var</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>var</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>unbiased</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>       <span class=c1># Used biased variance estimation (without Bessel&#39;s correction)</span>
</span></span><span class=line><span class=cl>        <span class=n>norm_x</span> <span class=o>=</span> <span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>mean</span><span class=p>)</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>var</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight</span> <span class=o>*</span> <span class=n>norm_x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GELU</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>x</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>2.0</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>pi</span><span class=p>))</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>            <span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=mf>0.044715</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=p>))</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MLP</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>c_fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>emb_dim</span><span class=p>,</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>emb_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>c_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>4</span> <span class=o>*</span> <span class=n>emb_dim</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gelu</span> <span class=o>=</span> <span class=n>GELU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>gelu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>c_fc</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>EncoderBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cfg</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ln_1</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>emb_dim</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attn</span> <span class=o>=</span> <span class=n>MultiHeadSelfAttention</span><span class=p>(</span><span class=n>d_in</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>],</span> <span class=n>d_out</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>],</span> <span class=n>attn_pdrop</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;attn_pdrop&#39;</span><span class=p>],</span> <span class=n>num_heads</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;n_heads&#39;</span><span class=p>],</span> <span class=n>qkv_bias</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;qkv_bias&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ln_2</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>emb_dim</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span> <span class=o>=</span> <span class=n>MLP</span><span class=p>(</span><span class=n>emb_dim</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>resid_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;resid_pdrop&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>resid_dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>attn</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ln_1</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>resid_dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>mlp</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ln_2</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><h4 id=defining-the-vit-model>Defining the ViT model<a hidden class=anchor aria-hidden=true href=#defining-the-vit-model>#</a></h4><p>Let&rsquo;s implement the Vision Transformer model in code as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>ViT</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cfg</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>patch_size</span> <span class=o>=</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;patch_size&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>patch_embedding</span> <span class=o>=</span> <span class=n>PatchEmbedding</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;patch_size&#39;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;n_channels&#39;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>        <span class=c1># Define class token and position embeddings as learnable parameters</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cls_token_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>position_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span> <span class=o>+</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;n_patches&#39;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>]))</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embd_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;embd_pdrop&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>blocks</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=p>[</span><span class=n>EncoderBlock</span><span class=p>(</span><span class=n>cfg</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;n_layers&#39;</span><span class=p>])])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ln_f</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cls_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;n_classes&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>img</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Obtain patch embeddings</span>
</span></span><span class=line><span class=cl>        <span class=n>patch_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>patch_embeddings</span><span class=p>(</span><span class=n>img</span><span class=p>)</span>                                   <span class=c1># (B, N, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>patch_emb</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>        <span class=c1># Prepend CLS token to the sequence of patch embeddings</span>
</span></span><span class=line><span class=cl>        <span class=n>cls_token_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cls_token_embedding</span><span class=o>.</span><span class=n>expand</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>                <span class=c1># (B, 1, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>patch_emb</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>cls_token_emb</span><span class=p>,</span> <span class=n>patch_emb</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>                <span class=c1># (B, N+1, D)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>        <span class=c1># Add positional embeddings</span>
</span></span><span class=line><span class=cl>        <span class=n>pos_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>position_embedding</span><span class=p>[:,</span> <span class=p>:</span><span class=n>N</span><span class=o>+</span><span class=mi>1</span><span class=p>]</span>                              <span class=c1># (1, N+1, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>patch_emb</span> <span class=o>+</span> <span class=n>pos_emb</span>                                                 <span class=c1># (B, N+1, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embd_dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Pass through Transformer encoder blocks</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>blocks</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ln_f</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>        <span class=c1># Perform classification</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>,</span> <span class=p>:]</span>                                                          <span class=c1># (B, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cls_head</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>                                               <span class=c1># (B, num_classes)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>logits</span>
</span></span></code></pre></div><p>We use <code>nn.Parameter</code> for the class token and position embeddings because these are learnable parameters that need to be optimized during training, just like the weights in a neural network.</p><p>After passing the input through the Transformer blocks, we extract the learned representations of the first token (the classification token) and feed it into the classification head, which outputs logits for the class labels.</p><p>Now, let&rsquo;s define the configuration dictionary for the base ViT model and perform a single forward pass using a batch of CIFAR-10 images as a sanity check:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=s1>&#39;cuda&#39;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;cpu&#39;</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Using&#34;</span><span class=p>,</span> <span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1># Define configuration dictionary</span>
</span></span><span class=line><span class=cl><span class=n>ViT_B8_CONFIG</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;patch_size&#34;</span><span class=p>:</span> <span class=mi>8</span><span class=p>,</span>        <span class=c1># Patch size</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;n_patches&#34;</span><span class=p>:</span> <span class=mi>16</span><span class=p>,</span>        <span class=c1># Maximum number of patches</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;n_channels&#34;</span><span class=p>:</span> <span class=mi>3</span><span class=p>,</span>        <span class=c1># Number of channels in the image</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;n_classes&#34;</span><span class=p>:</span> <span class=mi>10</span><span class=p>,</span>        <span class=c1># Number of classes in CIFAR-10 dataset</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;emb_dim&#34;</span><span class=p>:</span> <span class=mi>768</span><span class=p>,</span>         <span class=c1># Embedding dimension</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;n_heads&#34;</span><span class=p>:</span> <span class=mi>12</span><span class=p>,</span>          <span class=c1># Number of attention heads</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;n_layers&#34;</span><span class=p>:</span> <span class=mi>12</span><span class=p>,</span>         <span class=c1># Number of transformer blocks</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;attn_pdrop&#34;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>,</span>      <span class=c1># Dropout probability for attention dropout</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;embd_pdrop&#34;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>,</span>      <span class=c1># Dropout probability for embeddings dropout</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;resid_pdrop&#34;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>,</span>     <span class=c1># Dropout probability for residual dropout</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;qkv_bias&#34;</span><span class=p>:</span> <span class=kc>True</span>        <span class=c1># Whether to use bias in QKV layer</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>ViT</span><span class=p>(</span><span class=n>ViT_B8_CONFIG</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1># Print the number of parameters in the model</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span> <span class=o>/</span> <span class=mf>1e6</span><span class=p>,</span> <span class=s1>&#39;M parameters&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1># Forward pass with one example</span>
</span></span><span class=line><span class=cl><span class=n>image</span><span class=p>,</span> <span class=n>label</span> <span class=o>=</span> <span class=n>image</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>label</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>image</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Model output shape:&#34;</span><span class=p>,</span> <span class=n>logits</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>label</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Initial Loss:&#34;</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=m>85.223434</span> <span class=n>M</span> <span class=n>parameters</span>
</span></span><span class=line><span class=cl><span class=n>Model</span> <span class=n>output</span> <span class=n>shape</span><span class=p>:</span> <span class=n>torch</span><span class=p>.</span><span class=n>Size</span><span class=p>([</span><span class=m>64</span><span class=p>,</span> <span class=m>10</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>Initial</span> <span class=n>Loss</span><span class=p>:</span> <span class=m>2.3756895065307617</span>
</span></span></code></pre></div><p>As expected the model outputs a probability distribution over all the class labels for each example in the batch. At initialization, the model assigns equal probability to each class (since it&rsquo;s untrained), which results in a loss close to $ -\ln(1/10) = 2.302$, confirming that everything is working as expected.</p><h3 id=architecture-variants>Architecture variants<a hidden class=anchor aria-hidden=true href=#architecture-variants>#</a></h3><p>The authors present three variants of the ViT architecture: Base, Large, and Huge, with their respective hyperparameters defined below.</p><figure class=align-center><img loading=lazy src=../vit-hyper.png#center alt="Details of Vision Transformer model variants." width=600><figcaption><p>Details of Vision Transformer model variants.</p></figcaption></figure><p>To denote model size and input patch size, models are named using the format ViT-[Size]/[Patch Size]. For example, ViT-B/16 refers to the Base variant with a $16 \times 16$ input patch size.</p><p>The transformer&rsquo;s sequence length (i.e., the number of patches) is inversely proportional to the square of the patch size and directly proportional to the image size. This means that models using smaller patches or higher-resolution images are more computationally expensive, as they must process longer sequences.</p><p>To address this limitation, Microsoft Research introduced the Swin Transformer [2], which you can read more about here: <a href=https://amaarora.github.io/posts/2022-07-04-swintransformerv1.html>Aman Arora&rsquo;s blog on Swin Transfomer</a>. It has become the go-to transformer-based backbone for vision tasks today.</p><h2 id=contrastive-language-image-pre-training-clip>Contrastive Language-Image Pre-Training (CLIP)<a hidden class=anchor aria-hidden=true href=#contrastive-language-image-pre-training-clip>#</a></h2><p>With the success of GPT-3 in &ldquo;zero-shot&rdquo; transfer to downstream datasets, OpenAI aimed to extend this capability to image models for tasks like image classification. Pre-training on vast amounts of web-sourced data has enabled such task-agnostic models to be competitive across various tasks while requiring little to no dataset-specific training.</p><p>However, vision models have traditionally been pre-trained on crowd-labeled datasets like ImageNet. This form of supervision limits models to predict a fixed set of predetermined object categories and typically requires fine-tuning on new labeled datasets to adapt to different tasks.</p><p>CLIP <a href=#references>[3]</a> (Contrastive Language-Image Pre-training) bridges this gap by learning visual concepts through natural language supervision on a large scale. The primary motivation behind natural language supervision is the vast amount of publicly available data on the internet. CLIP can be applied to any visual classification benchmark by simply providing the names of the visual categories to recognize, much like the &ldquo;zero-shot&rdquo; capabilities of GPT-2 and GPT-3.</p><h3 id=dataset>Dataset<a hidden class=anchor aria-hidden=true href=#dataset>#</a></h3><p>The training data leverages an abundant source of supervision: text paired with images found across the internet. Specifically, the authors constructed a new dataset of 400 million (image, text) pairs, naming it WIT (WebImageText).</p><h3 id=contrastive-pre-training>Contrastive Pre-training<a hidden class=anchor aria-hidden=true href=#contrastive-pre-training>#</a></h3><p>The initial approach was to jointly train an image CNN and a text transformer to predict the caption of an image (i.e., an image captioning model). However, predicting the <em>exact</em> words of a caption proved challenging due to the vast variability in descriptions, comments, and related text accompanying images, making training slow.</p><p>Prior research has shown that contrastive objectives can learn better representations than their equivalent predictive objective. Thus, CLIP was trained using a more efficient proxy task—predicting which text <em>as a whole</em> is paired with which image rather than generating exact words of that text.</p><p>CLIP jointly trains an image encoder and a text encoder to learn a shared multi-modal embedding space, mapping both text and images to the same representation. The contrastive learning approach optimizes this space such that similar pairs (image and its corresponding text) stay close, while dissimilar ones are pushed apart.</p><p>For example, an image of a dog and the sentence “an image of a dog” will have similar embeddings and will be close to each other in the vector space, whereas a sentence like “an image of a bird” will have dissimilar embeddings and will be farther from that image.</p><figure class=align-center><img loading=lazy src=../clip-train.png#center alt="CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples."><figcaption><p>CLIP jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples.</p></figcaption></figure><p>Given a batch of $N$ (image, text) pairs, CLIP is trained to predict the correct pairings among the $N \times N$ possible combinations:</p><ul><li>It maximizes the cosine similariy between the image and text embeddings for the $N$ correct pairs (i.e., the diagonal elements of the similarity matrix).</li><li>It minimizes the cosine similarity for the $N^2 - N$ incorrect pairings.</li></ul><p>These scores are optimized using symmetric cross-entropy loss. Let&rsquo;s break this down with pseudocode:</p><ol><li><p><strong>Input</strong>: Batch of images and texts</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Image batch: [N, C, H, W]</span>
</span></span><span class=line><span class=cl><span class=c1># Text batch:  [N, L]</span>
</span></span></code></pre></div><figure class=align-center><img loading=lazy src=../batch_illustration.png#center alt="An example of a batch from the dataset."><figcaption><p>An example of a batch from the dataset.</p></figcaption></figure></li><li><p><strong>Extract feature representations</strong>: Images are passed through an image encoder and texts through a text encoder to obtain features of dimensions $\text{D}\text{i}$ and $\text{D}\text{t}$, respectively.
(These features come from different models, so they will have different dimensions.)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Extract feature representation</span>
</span></span><span class=line><span class=cl><span class=n>I_f</span> <span class=o>=</span> <span class=n>image_encoder</span><span class=p>(</span><span class=n>images</span><span class=p>)</span>         <span class=c1># Image features [N, D_i]</span>
</span></span><span class=line><span class=cl><span class=n>T_f</span> <span class=o>=</span> <span class=n>text_encoder</span><span class=p>(</span><span class=n>texts</span><span class=p>)</span>           <span class=c1># Text features  [N, D_t]</span>
</span></span></code></pre></div></li><li><p><strong>Obtain embeddings</strong>: A linear layer projects both feature sets into a common embedding space of dimension $\text{D}_\text{e}$.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Use linear projection to map to the multi-modal embedding space.</span>
</span></span><span class=line><span class=cl><span class=n>W_i</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>D_i</span><span class=p>,</span> <span class=n>D_e</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>W_t</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>D_t</span><span class=p>,</span> <span class=n>D_e</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>I_e</span> <span class=o>=</span> <span class=n>W_i</span><span class=p>(</span><span class=n>I_f</span><span class=p>)</span>                     <span class=c1># Image in embedding space [N, D_e]</span>
</span></span><span class=line><span class=cl><span class=n>T_e</span> <span class=o>=</span> <span class=n>W_t</span><span class=p>(</span><span class=n>T_f</span><span class=p>)</span>                     <span class=c1># Text in embedding space  [N, D_e]</span>
</span></span></code></pre></div></li><li><p><strong>Normalize</strong>: The embeddings are normalized (L2) to unit vectors, preventing any scale differences.</p></li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Obtain normalized features</span>
</span></span><span class=line><span class=cl><span class=n>I_e</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>normalize</span><span class=p>(</span><span class=n>I_e</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>T_e</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>normalize</span><span class=p>(</span><span class=n>T_e</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><ol start=5><li><p><strong>Compute cosine similarity</strong>: A dot product operation between image and text embeddings yields similarity scores.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Find cosine similarity</span>
</span></span><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>I_e</span> <span class=o>@</span> <span class=n>T_e</span><span class=o>.</span><span class=n>T</span>               <span class=c1># [N, N]</span>
</span></span></code></pre></div><figure class=align-center><img loading=lazy src=../cosine_similarity.png#center alt="Cosine similarity matrix of our batch."><figcaption><p>Cosine similarity matrix of our batch.</p></figcaption></figure></li><li><p><strong>Calculate Contrastive loss</strong>: The cross entropy loss is computed for each row and column of our similarity matrix and divide by 2, since each pair is computed twice.</p></li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># symmetric loss function</span>
</span></span><span class=line><span class=cl><span class=n>labels</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>N</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=n>loss_i</span> <span class=o>=</span> <span class=n>cross_entropy_loss</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=n>loss_t</span> <span class=o>=</span> <span class=n>cross_entropy_loss</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>labels</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=p>(</span><span class=n>loss_i</span> <span class=o>+</span> <span class=n>loss_t</span><span class=p>)</span><span class=o>/</span><span class=mi>2</span>
</span></span></code></pre></div><p>The correct labels correspond to the diagonal elements of the similarity matrix. The variable <code>loss_i</code> and <code>loss_t</code> push the logits for image similarities and text similarities to be high along the diagonal and low elsewhere. This is referred to as the contrastive loss.</p><p>Using this objective, CLIP learns to recognize a wide range of visual concepts in images directly from natural language, making it applicable to diverse visual classification tasks.</p><h4 id=temperature-scaling>Temperature scaling<a hidden class=anchor aria-hidden=true href=#temperature-scaling>#</a></h4><p>Cross-entropy loss includes a softmax function that converts logits into probability scores. CLIP uses temperature scaling (as discussed in our previous post) to adjust the range of logits before applying the softmax, effectively controlling the strength of separation between positive and negative pairs.</p><p>To avoid manually tuning this as a hyperparameter, temperature scaling is optimized during training as a log-parameterized multiplicative scalar, called <code>logit_scale</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Find scaled cosine similarity</span>
</span></span><span class=line><span class=cl><span class=n>logit_scale</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>temperature</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=p>(</span><span class=n>I_e</span> <span class=o>@</span> <span class=n>T_e</span><span class=o>.</span><span class=n>T</span><span class=p>)</span> <span class=o>*</span> <span class=n>logit_scale</span>               <span class=c1># [N, N]</span>
</span></span></code></pre></div><h3 id=architecture-1>Architecture<a hidden class=anchor aria-hidden=true href=#architecture-1>#</a></h3><p>CLIP consists of two main components: an image encoder and a text encoder.</p><ul><li><p><strong>Image Encoder</strong>: The authors experimented with two different models:</p><ul><li>ResNets: ResNet-50, ResNet-101, RN50x4, RN50x16, and RN50x64.</li><li>Vision Transformers (ViTs): ViT-B/32, a ViT-B/16, and a ViT-L/14 (without the classification head).<ul><li>Modification: An additional layer normalization was added to the combined patch and position embeddings before inputting into the transformer.</li><li>ViTs offered 3x compute efficiency over ResNets and trained faster, making them the preferred choice.</li></ul></li></ul></li><li><p><strong>Text Encoder</strong>: A 63M-parameter decoder-only Transformer, similar to GPT-2:</p><ul><li>Number of layers: $L = 12$.</li><li>Number of heads: $H = 8$.</li><li>Embedding dimension: $\text{d}_{model} = 512$.</li><li>Tokenizer: Lowercased BPE with a vocabulary size of 49,152.</li><li>Context length: $N = 76$.</li><li>Special tokens: Sequence is bracketed with [SOS] (start) and [EOS] (end) token.</li></ul></li><li><p>CLIP is trained from scratch without initializing the image encoder with ImageNet weights or the text encoder with pre-trained weights.</p></li><li><p>A batch size of 32,768 is used-meaning given an image, CLIP predicts which one of 32,768 text snippets was actually paired with it in our dataset.</p></li></ul><h3 id=zero-shot-transfer>Zero-shot transfer<a hidden class=anchor aria-hidden=true href=#zero-shot-transfer>#</a></h3><p>CLIP achieves zero-shot performance on various benchmarks, such as ImageNet, meaning the model was not explicitly trained on any of the 1.28M training examples in ImageNet but still matched the accuracy of the original ResNet-50, which was trained on the dataset.</p><p>CLIP is pre-trained to predict whether an image and a text snippet (rather than a single class label) are paired together in its dataset. To bridge the distribution gap between a text snippet and a class label, we use the prompt template: &ldquo;$\text{A photo of a {label}}$&rdquo;.</p><p>Let’s take ImageNet as an example and see how we can perform zero-shot transfer with CLIP:</p><ol><li><p><strong>Create text descriptions from labels</strong> – If we are working with ImageNet, which has 1000 possible classes, we create text descriptions for each label using the prompt template (e.g., &ldquo;A photo of a dog.&rdquo;).</p></li><li><p><strong>Obtain text embeddings</strong> – We feed these text descriptions into CLIP’s text encoder to obtain 1000 different embeddings corresponding to all possible classes.</p></li><li><p><strong>Obtain image embeddings</strong> – Next, we take the input image we want to classify (e.g., an image of a dog) and embed it using CLIP.</p></li><li><p><strong>Compute similarity scores</strong> – We compute cosine similarity scores between the image embedding and all text embeddings.</p></li><li><p><strong>Obtain class probabilities</strong> – Finally, we pass these similarity scores through a softmax function to obtain a probability distribution over all classes, just as in a typical vision model trained for image classification.</p></li></ol><figure class=align-center><img loading=lazy src=../clip-test.png#center alt="At test time, we convert all of a dataset’s classes into captions such as &amp;lsquo;A photo of a dog&amp;rsquo; and predict the class whose caption CLIP estimates best pairs with the given image."><figcaption><p>At test time, we convert all of a dataset’s classes into captions such as &lsquo;A photo of a dog&rsquo; and predict the class whose caption CLIP estimates best pairs with the given image.</p></figcaption></figure><p>Like the GPT family, CLIP learns a wide variety of tasks during pre-training, enabling powerful zero-shot transfer for image classification and beyond.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li><a href="https://research.google/blog/transformers-for-image-recognition-at-scale/?m=1">Google&rsquo;s blog on ViT</a></li><li><a href=https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/11-vision-transformer.html>https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/11-vision-transformer.html</a></li><li>[1] Alexey Dosovitskiy, et al. &ldquo;<a href=https://arxiv.org/abs/2010.11929>An image is worth 16x16 words: Transformers for image recognition at scale.</a>&rdquo;, ICLR 2021.</li><li>[2] Ze Liu, et al. &ldquo;<a href=https://arxiv.org/abs/2103.14030>Swin transformer: Hierarchical vision transformer using shifted windows</a>&rdquo;, ICCV 2021.</li><li>[3] Alec Radford, et al. &ldquo;<a href=https://arxiv.org/abs/2103.00020>Learning transferable visual models from natural language supervision</a>&rdquo;, ICML 2021.</li><li><a href=https://medium.com/@paluchasz/understanding-openais-clip-model-6b52bade3fa3>Medium: Understanding OpenAI’s CLIP model</a></li><li><a href=https://openai.com/index/clip/>Open AI&rsquo;s blog on CLIP</a></li><li><a href=https://amaarora.github.io/posts/2023-03-06_Understanding_CLIP.html#fig-cosine-similarity>Aman Arora&rsquo;s blog on Annotated CLIP</a></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/10-gpt2/post/><span class=title>Next »</span><br><span>GPT Series Part 3: Building GPT-2 & Sampling Techniques</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>YA's Almanac</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>