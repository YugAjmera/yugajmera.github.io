<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Vision Transformers: Adapting Transformers for Images | YA's Almanac</title>
<meta name=keywords content><meta name=description content="Modelling in computer vision has long been dominated by convolutional neural networks (CNNs). We’ve already discussed famous architectures like VGGNet and ResNet in previous posts, which have served as the primary backbones for a variety of vision tasks.
In contrast, network architectures in natural language processing (NLP) have evolved along a different trajectory. The dominant architecture in NLP today is the Transformer, designed for sequence modeling. Models like GPT-3 have achieved remarkable success, scaling to over 100 billion parameters thanks to their computational efficiency and scalability."><meta name=author content><link rel=canonical href=https://yugajmera.github.io/posts/11-vit/post/><link crossorigin=anonymous href=https://yugajmera.github.io/assets/css/stylesheet.74991b51f7611c2303d0b5649703d675530589621885eb78236e099c22aa93a5.css integrity="sha256-dJkbUfdhHCMD0LVklwPWdVMFiWIYhet4I24JnCKqk6U=" rel="preload stylesheet" as=style><link rel=icon href=https://yugajmera.github.io/assets/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yugajmera.github.io/assets/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yugajmera.github.io/assets/favicon-32x32.png><link rel=apple-touch-icon href=https://yugajmera.github.io/assets/apple-touch-icon.png><link rel=mask-icon href=https://yugajmera.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yugajmera.github.io/posts/11-vit/post/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>mjx-container[display=true]{margin:1.5em 0!important}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S759YBMKJE",{anonymize_ip:!1})}</script><meta property="og:title" content="Vision Transformers: Adapting Transformers for Images"><meta property="og:description" content="Modelling in computer vision has long been dominated by convolutional neural networks (CNNs). We’ve already discussed famous architectures like VGGNet and ResNet in previous posts, which have served as the primary backbones for a variety of vision tasks.
In contrast, network architectures in natural language processing (NLP) have evolved along a different trajectory. The dominant architecture in NLP today is the Transformer, designed for sequence modeling. Models like GPT-3 have achieved remarkable success, scaling to over 100 billion parameters thanks to their computational efficiency and scalability."><meta property="og:type" content="article"><meta property="og:url" content="https://yugajmera.github.io/posts/11-vit/post/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-17T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-17T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Vision Transformers: Adapting Transformers for Images"><meta name=twitter:description content="Modelling in computer vision has long been dominated by convolutional neural networks (CNNs). We’ve already discussed famous architectures like VGGNet and ResNet in previous posts, which have served as the primary backbones for a variety of vision tasks.
In contrast, network architectures in natural language processing (NLP) have evolved along a different trajectory. The dominant architecture in NLP today is the Transformer, designed for sequence modeling. Models like GPT-3 have achieved remarkable success, scaling to over 100 billion parameters thanks to their computational efficiency and scalability."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yugajmera.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Vision Transformers: Adapting Transformers for Images","item":"https://yugajmera.github.io/posts/11-vit/post/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Vision Transformers: Adapting Transformers for Images","name":"Vision Transformers: Adapting Transformers for Images","description":"Modelling in computer vision has long been dominated by convolutional neural networks (CNNs). We’ve already discussed famous architectures like VGGNet and ResNet in previous posts, which have served as the primary backbones for a variety of vision tasks.\nIn contrast, network architectures in natural language processing (NLP) have evolved along a different trajectory. The dominant architecture in NLP today is the Transformer, designed for sequence modeling. Models like GPT-3 have achieved remarkable success, scaling to over 100 billion parameters thanks to their computational efficiency and scalability.","keywords":[],"articleBody":"Modelling in computer vision has long been dominated by convolutional neural networks (CNNs). We’ve already discussed famous architectures like VGGNet and ResNet in previous posts, which have served as the primary backbones for a variety of vision tasks.\nIn contrast, network architectures in natural language processing (NLP) have evolved along a different trajectory. The dominant architecture in NLP today is the Transformer, designed for sequence modeling. Models like GPT-3 have achieved remarkable success, scaling to over 100 billion parameters thanks to their computational efficiency and scalability.\nInspired by the success of Transformers in NLP, researchers sought to apply them directly to images with minimal modifications. This led to the introduction of Vision Transformers (ViTs) [1] in 2021, marking a paradigm shift in deep learning for vision.\nIn this post, we’ll explore how to adapt the Transformer architecture for images, converting our existing code architecture into a Vision Transformer model.\nVision Transformers for Image Classification The paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, introduced the Vision Transformer (ViT) model, which achieved impressive results on ImageNet by applying a Transformer-based approach to image classification.\nAs the title suggests, the authors divide the input image into $16 \\times 16$ patches and treat each patch as a token, similar to how words are treated in NLP models. The sequence of linear embeddings of these patches is then fed into a Transformer and trained in a supervised manner for image classification.\nTo better understand the ViT architecture, we’ll take a top-down approach: first examining the overall framework, then coding each module, most of which we’ve encountered before.\nChoosing the Right Transformer Model Recall the two broad categories of large language models we’ve discussed before:\nEncoder-only models – Primarily used for classification and regression tasks (e.g., sentiment analysis, spam detection, stock price prediction). A prime example is BERT, where a special [CLS] token is prepended to the input sequence, and its final hidden state is used for classification.\nDecoder-only models – Designed for language generation tasks like translation, summarization, and question answering. We covered these extensively in our GPT series.\nSince the goal is image classification, an encoder-based approach makes the most sense. Similar to BERT, we prepend a classification token to the input sequence and pass it through the Transformer Encoder model. The hidden state output from the model can then be utilized to predict the image class label.\nTokenizing Images Traditionally, Transformers process text by tokenizing it into discrete units (words or subwords), converting them into token IDs, and mapping them into continuous embeddings. The challenge is: how do we apply this to images, which are fundamentally different from text?\nThe trick is to “tokenize” an image by splitting it into fixed-size patches, treating each patch as a token. Each patch is then flattened and linearly projected into an embedding vector, just like word embeddings in NLP. This results in a sequence of patch embeddings that can be processed by the Transformer model.\nWe also add learnable positional encodings to retain some notion of spatial structure of the image. They provide a unique encoding for each patch’s position, ensuring that the model can differentiate between patches and their arrangement in the image.\nThis transformations allows us to feed an input image into a standard Transformer encoder and generate a class label, completely eliminating the need for convolutions. Simple, right?\nLet’s visualize the overall architecture:\nThe Vision Transformer model.\nOverview of the ViT Pipeline Split an image into fixed-size patches. Flatten the patches. Project the flattened patches into the embedding dimension. Prepend a [cls] token embedding to the sequence. Add positional embeddings. Feed the sequence as an input to a standard transformer encoder. Pretrain the model in a fully supervised setting on a large dataset. Replace the classification head and fine-tune on a smaller dataset. Similar to other transformer models, Vision Transformers (ViTs) require pre-training on large and diverse datasets to generalize well and outperform standard CNNs on smaller or mid-sized datasets.\nUnlike Convolutional Neural Networks (CNNs), which have strong inductive biases (such as spatial locality and translation invariance), ViTs rely solely on self-attention, lacking these built-in properties. As a result, ViTs do not generalize as effectively when trained on limited datasets and tend to underperform compared to CNNs.\n(This is why I haven’t included ViT training in this post.)\nHowever, when pre-trained on much larger datasets (14M–300M images) and then transferred to downstream tasks, the situation changes. Large-scale training helps compensate for the lack of inductive biases, enabling pre-trained ViTs to match or even surpass state-of-the-art performance on several image recognition benchmarks.\nSpelled out in Code Let’s implement the Vision Transformer (ViT) architecture step by step.\nPatchifying the image Ignoring the batch dimension for now, a standard Transformer processes a 1D sequence of token embeddings with shape $(\\text{N} \\times \\text{D})$, where:\n$\\text{N}$ is the number of tokens. $\\text{D}$ is the embedding dimension. Since images are 2D, we convert them into a sequence of patches through the following steps:\nInput image: $(\\text{H} \\times \\text{W} \\times \\text{C})$, where $\\text{H}$ is height, $\\text{W}$ is width, and $\\text{C}$ is the number of channels.\nSplitting into Patches: We divide the image into $\\text{N}$ non-overlapping patches, each of size $(\\text{P} \\times \\text{P} \\times \\text{C})$.\n$(\\text{H} \\times \\text{W} \\times \\text{C}) \\rightarrow (\\text{H}/\\text{P} \\times \\text{P} \\times \\text{W}/\\text{P} \\times \\text{P} \\times \\text{C})$. Number of Patches: The number of patches is computed as $\\text{N} = (\\text{H} \\cdot \\text{W})/\\text{P}^2$.\n$(\\text{H}/\\text{P} \\times \\text{W}/\\text{P} \\times \\text{P} \\times \\text{P} \\times \\text{C}) \\rightarrow (\\text{N} \\times \\text{P} \\times \\text{P} \\times \\text{C})$. This is analogous to the number of tokens in NLP. Flattening Patches: Each patch is flattened into a vector of size $\\text{P}^2 C$ and mapped to the embedding dimension $\\text{D}$ using a linear layer:\n$(\\text{N} \\times \\text{P}^2 \\text{C}) \\rightarrow (\\text{N} \\times \\text{D})$ Tokenization of an image by splitting it into patches. The 28x28x1 image is split into 49 patches, each of size 16 (4x4x1).\nImplementing this in code:\ndef patchify_image(x, patch_size, visualize=False): B, C, H, W = x.size() x = x.view(B, C, H // patch_size, patch_size, W // patch_size, patch_size) # Split it into (B, C, H', p_H, W', p_W) x = x.permute(0, 2, 4, 1, 3, 5) # Reshape into (B, H', W', C, p_H, p_W) x = x.flatten(1, 2) # Obtain N (B, N, C, p_H, p_W) if visualize: return x # Return unflattened patches for visualization x = x.flatten(2, 4) # Flatten the last dim (B, N, C*p_H*p_W] return x This function takes a batch of images and splits each one into fixed-size patches defined by the variable patch_size. If visualize=True, it returns the before flattening them so we can plot them and inspect their structure.\nWe will then use a linear layer in our model class that takes these flattened patches as input and maps the patch dimension $\\text{P}^2 \\text{C}$ to the embedding dimension $\\text{D}$.\nLet’s apply this function to a sample image from CIFAR-10 and verify the output:\nimport torch import torch.nn as nn import matplotlib.pyplot as plt import torchvision # Download CIFAR-10 dataset transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()]) cifar10_trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(cifar10_trainset, batch_size=64, shuffle=False) # Extract a batch of images for image, label in train_loader: break fig, axes = plt.subplots(1, 2, figsize=(10, 5)) # Plot the original first image in the batch axes[0].imshow(image[0].permute(1, 2, 0)) axes[0].set_title('Original Image') # Plot the patches of first image in the batch patches = patchify_image(image, 8, visualize=True)[0] img_grid = torchvision.utils.make_grid(patches, nrow=4, pad_value=0.9) axes[1].imshow(img_grid.permute(1, 2, 0)) axes[1].set_title('Patches') plt.show() Each CIFAR-10 image has a shape of $(32, 32, 3)$. Using patch_size=8 we get:\n$\\text{N} = (32 \\cdot 32) / (8)^2 = 16$ patches in total. Each patch of shape $(8, 8, 3)$. We arrange the image patches into a grid and visualize them as shown below:\nA 32x32x3 image split into 16 patches of size 8x8x3.\nThis output verifies that our function is accurately splitting the image into the desired patches.\nArchitecture Similar to GPT models, ViT applies layer normalization before every block and residual connections after every block. GeLU activation is used in the MLP layers. A key distinction is that, since we are using the encoder part of the Transformer, self-attention is employed instead of causal attention. Everything else remains the same.\nclass MultiHeadSelfAttention(nn.Module): def __init__(self, d_in, d_out, attn_pdrop, num_heads, qkv_bias=False): super().__init__() self.c_attn = nn.Linear(d_in, 3 * d_out, bias=qkv_bias) self.c_proj = nn.Linear(d_out, d_out) self.attn_dropout = nn.Dropout(attn_pdrop) self.d_h = d_out // num_heads # head dimension self.num_heads = num_heads def forward(self, x): # Input vectors x - (B, N, d_in) B, N, d_in = x.shape # Obtain keys, values and queries in one go - (B, N, d_out) qkv = self.c_attn(x) queries, keys, values = qkv.chunk(3, dim=-1) # Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h) k = keys.view(B, N, self.num_heads, self.d_h).transpose(1, 2) v = values.view(B, N, self.num_heads, self.d_h).transpose(1, 2) q = queries.view(B, N, self.num_heads, self.d_h).transpose(1, 2) # Apply scaled dot-product attention on each head attn_scores = (q @ k.transpose(2, 3)) * k.shape[-1]**-0.5 attn_weights = torch.softmax(attn_scores, dim=-1) attn_weights = self.attn_dropout(attn_weights) context_vec = attn_weights @ v # Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out) out = context_vec.transpose(1, 2).contiguous().view(B, N, -1) out = self.c_proj(out) return out class LayerNorm(nn.Module): def __init__(self, emb_dim): super().__init__() self.eps = 1e-5 self.weight = nn.Parameter(torch.ones(emb_dim)) self.bias = nn.Parameter(torch.zeros(emb_dim)) def forward(self, x): mean = x.mean(dim=-1, keepdim=True) var = x.var(dim=-1, keepdim=True, unbiased=False) # Used biased variance estimation (without Bessel's correction) norm_x = (x - mean) / torch.sqrt(var + self.eps) return self.weight * norm_x + self.bias class GELU(nn.Module): def __init__(self): super().__init__() def forward(self, x): return 0.5 * x * (1 + torch.tanh( torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)) )) class MLP(nn.Module): def __init__(self, emb_dim): super().__init__() self.c_fc = nn.Linear(emb_dim, 4 * emb_dim) self.c_proj = nn.Linear(4 * emb_dim, emb_dim) self.gelu = GELU() def forward(self, x): x = self.gelu(self.c_fc(x)) x = self.c_proj(x) return x We define the Transformer Encoder Block, which follows the standard architecture: layer normalization before each block, multi-head self-attention, and a feedforward MLP with residual connections.\nclass EncoderBlock(nn.Module): def __init__(self, cfg): super().__init__() self.ln_1 = LayerNorm(emb_dim=cfg['emb_dim']) self.attn = MultiHeadSelfAttention(d_in=cfg['emb_dim'], d_out=cfg['emb_dim'], attn_pdrop=cfg['attn_pdrop'], num_heads=cfg['n_heads'], qkv_bias=cfg['qkv_bias']) self.ln_2 = LayerNorm(emb_dim=cfg['emb_dim']) self.mlp = MLP(emb_dim=cfg['emb_dim']) self.resid_dropout = nn.Dropout(cfg['resid_pdrop']) def forward(self, x): x = x + self.resid_dropout(self.attn(self.ln_1(x))) x = x + self.resid_dropout(self.mlp(self.ln_2(x))) return x Defining the ViT model Let’s implement the Vision Transformer model in code as follows:\nclass ViT(nn.Module): def __init__(self, cfg): super().__init__() self.patch_size = cfg['patch_size'] self.patch_embedding = nn.Linear(cfg['patch_size'] * cfg['patch_size'] * cfg['n_channels'], cfg['emb_dim'], bias=False) # Define class token and position embeddings as learnable parameters self.cls_token_embedding = nn.Parameter(torch.randn(1, 1, cfg['emb_dim'])) self.position_embedding = nn.Parameter(torch.randn(1, 1 + cfg['n_patches'], cfg['emb_dim'])) self.embd_dropout = nn.Dropout(cfg['embd_pdrop']) self.blocks = nn.Sequential(*[EncoderBlock(cfg) for _ in range(cfg['n_layers'])]) self.cls_head = nn.Linear(cfg['emb_dim'], cfg['n_classes']) def forward(self, img): patches = patchify_image(img, self.patch_size) # (B, N, P*P*C) B, N, _ = patches.size() # Project the flattened patches into the embedding dimension patch_emb = self.patch_embedding(patches) # (B, N, D) # Prepend CLS token to the sequence of patch embeddings cls_token_emb = self.cls_token_embedding.repeat(B, 1, 1) # (B, 1, D) patch_emb = torch.cat((cls_token_emb, patch_emb), dim=1) # (B, N+1, D) # Add positional embeddings pos_emb = self.position_embedding[:, :N+1] # (1, N+1, D) x = patch_emb + pos_emb # (B, N+1, D) x = self.embd_dropout(x) # Pass through Transformer encoder blocks x = self.blocks(x) # Perform classification x = x[:, 0, :] # (B, D) logits = self.cls_head(x) # (B, N, num_classes) return logits We use nn.Parameter for the class token and position embeddings because these are learnable parameters that need to be optimized during training, just like the weights in a neural network.\nAfter passing the input through the Transformer blocks, we extract the learned representations of the first token (the classification token) and feed it into the classification head, which outputs logits for the class labels.\nNow, let’s define the configuration dictionary for the base ViT model and perform a single forward pass using a batch of CIFAR-10 images as a sanity check:\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu' print(\"Using\", device) # Define configuration dictionary ViT_B8_CONFIG = { \"patch_size\": 8, # Patch size \"n_patches\": 16, # Maximum number of patches \"n_channels\": 3, # Number of channels in the image \"n_classes\": 10, # Number of classes in CIFAR-10 dataset \"emb_dim\": 768, # Embedding dimension \"n_heads\": 12, # Number of attention heads \"n_layers\": 12, # Number of transformer blocks \"attn_pdrop\": 0.0, # Dropout probability for attention dropout \"embd_pdrop\": 0.0, # Dropout probability for embeddings dropout \"resid_pdrop\": 0.0, # Dropout probability for residual dropout \"qkv_bias\": True # Whether to use bias in QKV layer } model = ViT(ViT_B8_CONFIG).to(device) # Print the number of parameters in the model print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters') # Forward pass with one example image, label = image.to(device), label.to(device) logits = model(image) print(\"Model output shape:\", logits.size()) loss = nn.functional.cross_entropy(logits, label) print(\"Initial Loss:\", loss.item()) 85.149706 M parameters Model output shape: torch.Size([64, 10]) Initial Loss: 2.3756895065307617 As expected the model outputs a probability distribution over all the class labels for each example in the batch. At initialization, the model assigns equal probability to each class (since it’s untrained), which results in a loss close to $ -\\ln(1/10) = 2.302$, confirming that everything is working as expected.\nArchitecture variants The authors present three variants of the ViT architecture: Base, Large, and Huge, with their respective hyperparameters defined below.\nDetails of Vision Transformer model variants.\nTo denote model size and input patch size, models are named using the format ViT-[Size]/[Patch Size]. For example, ViT-B/16 refers to the Base variant with a $16 \\times 16$ input patch size.\nNotably, the transformer’s sequence length (i.e., the number of patches) is inversely proportional to the square of the patch size. This means that models with smaller patch sizes are computationally more expensive since they process longer sequences.\nAdditionally, for a fixed patch size, higher-resolution images result in longer sequences, further increasing computational cost. In fact, the complexity grows quadratically with image size, making ViTs inefficient as general-purpose backbones for handling arbitrary-sized images.\nTo address this limitation, Microsoft Research introduced the Swin Transformer [2], which you can read more about here: https://amaarora.github.io/posts/2022-07-04-swintransformerv1.html. It has become go-to transformer-based backbone for vision tasks today.\nReferences https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/11-vision-transformer.html\n[1] Alexey Dosovitskiy, et al. “An image is worth 16x16 words: Transformers for image recognition at scale.”, ICLR 2021.\n[2] Ze Liu, et al. “Swin transformer: Hierarchical vision transformer using shifted windows”, ICCV 2021.\n","wordCount":"2378","inLanguage":"en","datePublished":"2025-03-17T00:00:00Z","dateModified":"2025-03-17T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://yugajmera.github.io/posts/11-vit/post/"},"publisher":{"@type":"Organization","name":"YA's Almanac","logo":{"@type":"ImageObject","url":"https://yugajmera.github.io/assets/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yugajmera.github.io/ accesskey=h title="YA's Almanac (Alt + H)">YA's Almanac</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yugajmera.github.io/ title=Home><span>Home</span></a></li><li><a href=https://yugajmera.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://yugajmera.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Vision Transformers: Adapting Transformers for Images</h1><div class=post-meta><span title='2025-03-17 00:00:00 +0000 UTC'>March 17, 2025</span>&nbsp;·&nbsp;12 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#vision-transformers-for-image-classification aria-label="Vision Transformers for Image Classification">Vision Transformers for Image Classification</a><ul><li><a href=#choosing-the-right-transformer-model aria-label="Choosing the Right Transformer Model">Choosing the Right Transformer Model</a></li><li><a href=#tokenizing-images aria-label="Tokenizing Images">Tokenizing Images</a></li><li><a href=#overview-of-the-vit-pipeline aria-label="Overview of the ViT Pipeline">Overview of the ViT Pipeline</a></li><li><a href=#spelled-out-in-code aria-label="Spelled out in Code">Spelled out in Code</a><ul><li><a href=#patchifying-the-image aria-label="Patchifying the image">Patchifying the image</a></li><li><a href=#architecture aria-label=Architecture>Architecture</a></li><li><a href=#defining-the-vit-model aria-label="Defining the ViT model">Defining the ViT model</a></li></ul></li><li><a href=#architecture-variants aria-label="Architecture variants">Architecture variants</a></li><li><a href=#references aria-label=References>References</a></li></ul></li></ul></div></details></div><div class=post-content><p>Modelling in computer vision has long been dominated by convolutional neural networks (CNNs). We’ve already discussed famous architectures like VGGNet and ResNet in previous posts, which have served as the primary backbones for a variety of vision tasks.</p><p>In contrast, network architectures in natural language processing (NLP) have evolved along a different trajectory. The dominant architecture in NLP today is the Transformer, designed for sequence modeling. Models like GPT-3 have achieved remarkable success, scaling to over 100 billion parameters thanks to their computational efficiency and scalability.</p><p>Inspired by the success of Transformers in NLP, researchers sought to apply them directly to images with minimal modifications. This led to the introduction of Vision Transformers (ViTs) <a href=#references>[1]</a> in 2021, marking a paradigm shift in deep learning for vision.</p><p>In this post, we’ll explore how to adapt the Transformer architecture for images, converting our existing code architecture into a Vision Transformer model.</p><h2 id=vision-transformers-for-image-classification>Vision Transformers for Image Classification<a hidden class=anchor aria-hidden=true href=#vision-transformers-for-image-classification>#</a></h2><p>The paper <em>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em>, introduced the Vision Transformer (ViT) model, which achieved impressive results on ImageNet by applying a Transformer-based approach to image classification.</p><p>As the title suggests, the authors divide the input image into $16 \times 16$ patches and treat each patch as a token, similar to how words are treated in NLP models. The sequence of linear embeddings of these patches is then fed into a Transformer and trained in a supervised manner for image classification.</p><p>To better understand the ViT architecture, we’ll take a top-down approach: first examining the overall framework, then coding each module, most of which we&rsquo;ve encountered before.</p><h3 id=choosing-the-right-transformer-model>Choosing the Right Transformer Model<a hidden class=anchor aria-hidden=true href=#choosing-the-right-transformer-model>#</a></h3><p>Recall the two broad categories of large language models we’ve discussed before:</p><ol><li><p>Encoder-only models – Primarily used for classification and regression tasks (e.g., sentiment analysis, spam detection, stock price prediction). A prime example is BERT, where a special [CLS] token is prepended to the input sequence, and its final hidden state is used for classification.</p></li><li><p>Decoder-only models – Designed for language generation tasks like translation, summarization, and question answering. We covered these extensively in our GPT series.</p></li></ol><p>Since the goal is image classification, an encoder-based approach makes the most sense. Similar to BERT, we prepend a classification token to the input sequence and pass it through the Transformer Encoder model. The hidden state output from the model can then be utilized to predict the image class label.</p><h3 id=tokenizing-images>Tokenizing Images<a hidden class=anchor aria-hidden=true href=#tokenizing-images>#</a></h3><p>Traditionally, Transformers process text by tokenizing it into discrete units (words or subwords), converting them into token IDs, and mapping them into continuous embeddings. The challenge is: how do we apply this to images, which are fundamentally different from text?</p><p>The trick is to &ldquo;tokenize&rdquo; an image by splitting it into fixed-size patches, treating each patch as a token. Each patch is then flattened and linearly projected into an embedding vector, just like word embeddings in NLP. This results in a sequence of patch embeddings that can be processed by the Transformer model.</p><p>We also add learnable positional encodings to retain some notion of spatial structure of the image. They provide a unique encoding for each patch’s position, ensuring that the model can differentiate between patches and their arrangement in the image.</p><p>This transformations allows us to feed an input image into a standard Transformer encoder and generate a class label, completely eliminating the need for convolutions. Simple, right?</p><p>Let’s visualize the overall architecture:</p><figure class=align-center><img loading=lazy src=../vit-model.png#center alt="The Vision Transformer model."><figcaption><p>The Vision Transformer model.</p></figcaption></figure><h3 id=overview-of-the-vit-pipeline>Overview of the ViT Pipeline<a hidden class=anchor aria-hidden=true href=#overview-of-the-vit-pipeline>#</a></h3><ol><li>Split an image into fixed-size patches.</li><li>Flatten the patches.</li><li>Project the flattened patches into the embedding dimension.</li><li>Prepend a [cls] token embedding to the sequence.</li><li>Add positional embeddings.</li><li>Feed the sequence as an input to a standard transformer encoder.</li><li>Pretrain the model in a fully supervised setting on a large dataset.</li><li>Replace the classification head and fine-tune on a smaller dataset.</li></ol><p>Similar to other transformer models, Vision Transformers (ViTs) require pre-training on large and diverse datasets to generalize well and outperform standard CNNs on smaller or mid-sized datasets.</p><p>Unlike Convolutional Neural Networks (CNNs), which have strong inductive biases (such as spatial locality and translation invariance), ViTs rely solely on self-attention, lacking these built-in properties. As a result, ViTs do not generalize as effectively when trained on limited datasets and tend to underperform compared to CNNs.</p><p>(This is why I haven’t included ViT training in this post.)</p><p>However, when pre-trained on much larger datasets (14M–300M images) and then transferred to downstream tasks, the situation changes. Large-scale training helps compensate for the lack of inductive biases, enabling pre-trained ViTs to match or even surpass state-of-the-art performance on several image recognition benchmarks.</p><h3 id=spelled-out-in-code>Spelled out in Code<a hidden class=anchor aria-hidden=true href=#spelled-out-in-code>#</a></h3><p>Let&rsquo;s implement the Vision Transformer (ViT) architecture step by step.</p><h4 id=patchifying-the-image>Patchifying the image<a hidden class=anchor aria-hidden=true href=#patchifying-the-image>#</a></h4><p>Ignoring the batch dimension for now, a standard Transformer processes a 1D sequence of token embeddings with shape $(\text{N} \times \text{D})$, where:</p><ul><li>$\text{N}$ is the number of tokens.</li><li>$\text{D}$ is the embedding dimension.</li></ul><p>Since images are 2D, we convert them into a sequence of patches through the following steps:</p><ol><li><p>Input image: $(\text{H} \times \text{W} \times \text{C})$, where $\text{H}$ is height, $\text{W}$ is width, and $\text{C}$ is the number of channels.</p></li><li><p>Splitting into Patches: We divide the image into $\text{N}$ non-overlapping patches, each of size $(\text{P} \times \text{P} \times \text{C})$.</p><ul><li>$(\text{H} \times \text{W} \times \text{C}) \rightarrow (\text{H}/\text{P} \times \text{P} \times \text{W}/\text{P} \times \text{P} \times \text{C})$.</li></ul></li><li><p>Number of Patches: The number of patches is computed as $\text{N} = (\text{H} \cdot \text{W})/\text{P}^2$.</p><ul><li>$(\text{H}/\text{P} \times \text{W}/\text{P} \times \text{P} \times \text{P} \times \text{C}) \rightarrow (\text{N} \times \text{P} \times \text{P} \times \text{C})$.</li><li>This is analogous to the number of tokens in NLP.</li></ul></li><li><p>Flattening Patches: Each patch is flattened into a vector of size $\text{P}^2 C$ and mapped to the embedding dimension $\text{D}$ using a linear layer:</p><ul><li>$(\text{N} \times \text{P}^2 \text{C}) \rightarrow (\text{N} \times \text{D})$</li></ul></li></ol><figure class=align-center><img loading=lazy src=../patchify.png#center alt="Tokenization of an image by splitting it into patches. The 28x28x1 image is split into 49 patches, each of size 16 (4x4x1)."><figcaption><p>Tokenization of an image by splitting it into patches. The 28x28x1 image is split into 49 patches, each of size 16 (4x4x1).</p></figcaption></figure><p>Implementing this in code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>patchify_image</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>patch_size</span><span class=p>,</span> <span class=n>visualize</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>B</span><span class=p>,</span> <span class=n>C</span><span class=p>,</span> <span class=n>H</span><span class=p>,</span> <span class=n>W</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>C</span><span class=p>,</span> <span class=n>H</span> <span class=o>//</span> <span class=n>patch_size</span><span class=p>,</span> <span class=n>patch_size</span><span class=p>,</span> <span class=n>W</span> <span class=o>//</span> <span class=n>patch_size</span><span class=p>,</span> <span class=n>patch_size</span><span class=p>)</span>  <span class=c1># Split it into (B, C, H&#39;, p_H, W&#39;, p_W)</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>                                             <span class=c1># Reshape into (B, H&#39;, W&#39;, C, p_H, p_W)</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>                                                         <span class=c1># Obtain N (B, N, C, p_H, p_W)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>visualize</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>                                                                <span class=c1># Return unflattened patches for visualization</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>                                                         <span class=c1># Flatten the last dim (B, N, C*p_H*p_W]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><p>This function takes a batch of images and splits each one into fixed-size patches defined by the variable <code>patch_size</code>. If <code>visualize=True</code>, it returns the before flattening them so we can plot them and inspect their structure.</p><p>We will then use a linear layer in our model class that takes these flattened patches as input and
maps the patch dimension $\text{P}^2 \text{C}$ to the embedding dimension $\text{D}$.</p><p>Let&rsquo;s apply this function to a sample image from CIFAR-10 and verify the output:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torchvision</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1># Download CIFAR-10 dataset</span>
</span></span><span class=line><span class=cl><span class=n>transform</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>Compose</span><span class=p>([</span><span class=n>torchvision</span><span class=o>.</span><span class=n>transforms</span><span class=o>.</span><span class=n>ToTensor</span><span class=p>()])</span>
</span></span><span class=line><span class=cl><span class=n>cifar10_trainset</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>datasets</span><span class=o>.</span><span class=n>CIFAR10</span><span class=p>(</span><span class=n>root</span><span class=o>=</span><span class=s1>&#39;./data&#39;</span><span class=p>,</span> <span class=n>train</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>download</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>transform</span><span class=o>=</span><span class=n>transform</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>train_loader</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>DataLoader</span><span class=p>(</span><span class=n>cifar10_trainset</span><span class=p>,</span> <span class=n>batch_size</span><span class=o>=</span><span class=mi>64</span><span class=p>,</span> <span class=n>shuffle</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1># Extract a batch of images</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>image</span><span class=p>,</span> <span class=n>label</span> <span class=ow>in</span> <span class=n>train_loader</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>break</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1># Plot the original first image in the batch</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>image</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Original Image&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1># Plot the patches of first image in the batch</span>
</span></span><span class=line><span class=cl><span class=n>patches</span> <span class=o>=</span> <span class=n>patchify_image</span><span class=p>(</span><span class=n>image</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=n>visualize</span><span class=o>=</span><span class=kc>True</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>img_grid</span> <span class=o>=</span> <span class=n>torchvision</span><span class=o>.</span><span class=n>utils</span><span class=o>.</span><span class=n>make_grid</span><span class=p>(</span><span class=n>patches</span><span class=p>,</span> <span class=n>nrow</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>pad_value</span><span class=o>=</span><span class=mf>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>img_grid</span><span class=o>.</span><span class=n>permute</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Patches&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p>Each CIFAR-10 image has a shape of $(32, 32, 3)$. Using <code>patch_size=8</code> we get:</p><ul><li>$\text{N} = (32 \cdot 32) / (8)^2 = 16$ patches in total.</li><li>Each patch of shape $(8, 8, 3)$.</li></ul><p>We arrange the image patches into a grid and visualize them as shown below:</p><figure class=align-center><img loading=lazy src=../patchify2.png#center alt="A 32x32x3 image split into 16 patches of size 8x8x3."><figcaption><p>A 32x32x3 image split into 16 patches of size 8x8x3.</p></figcaption></figure><p>This output verifies that our function is accurately splitting the image into the desired patches.</p><h4 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h4><p>Similar to GPT models, ViT applies layer normalization before every block and residual connections after every block. GeLU activation is used in the MLP layers. A key distinction is that, since we are using the encoder part of the Transformer, self-attention is employed instead of causal attention. Everything else remains the same.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadSelfAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_in</span><span class=p>,</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>attn_pdrop</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>qkv_bias</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>c_attn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_in</span><span class=p>,</span> <span class=mi>3</span> <span class=o>*</span> <span class=n>d_out</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=n>qkv_bias</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>c_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_out</span><span class=p>,</span> <span class=n>d_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attn_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>attn_pdrop</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span> <span class=o>=</span> <span class=n>d_out</span> <span class=o>//</span> <span class=n>num_heads</span>                    <span class=c1># head dimension</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Input vectors x - (B, N, d_in)</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>d_in</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=c1># Obtain keys, values and queries in one go - (B, N, d_out)</span>
</span></span><span class=line><span class=cl>        <span class=n>qkv</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_attn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>queries</span><span class=p>,</span> <span class=n>keys</span><span class=p>,</span> <span class=n>values</span> <span class=o>=</span> <span class=n>qkv</span><span class=o>.</span><span class=n>chunk</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Split into H heads - (B, N, H, d_h) and then transpose to (B, H, N, d_h)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=n>keys</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=n>values</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=n>queries</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_h</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Apply scaled dot-product attention on each head</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_scores</span> <span class=o>=</span> <span class=p>(</span><span class=n>q</span> <span class=o>@</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span> <span class=o>*</span> <span class=n>k</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span><span class=o>**-</span><span class=mf>0.5</span>    
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attn_scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>                             
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>attn_dropout</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>context_vec</span> <span class=o>=</span> <span class=n>attn_weights</span> <span class=o>@</span> <span class=n>v</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Concatenate: transpose back to (B, N, H, d_h), then combine heads (B, N, d_out)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>context_vec</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_proj</span><span class=p>(</span><span class=n>out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>LayerNorm</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>eps</span> <span class=o>=</span> <span class=mf>1e-5</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>weight</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>emb_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>bias</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>emb_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>mean</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>var</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>var</span><span class=p>(</span><span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>unbiased</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>       <span class=c1># Used biased variance estimation (without Bessel&#39;s correction)</span>
</span></span><span class=line><span class=cl>        <span class=n>norm_x</span> <span class=o>=</span> <span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>mean</span><span class=p>)</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>var</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>weight</span> <span class=o>*</span> <span class=n>norm_x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>bias</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GELU</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>x</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>torch</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>2.0</span> <span class=o>/</span> <span class=n>torch</span><span class=o>.</span><span class=n>pi</span><span class=p>))</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>            <span class=p>(</span><span class=n>x</span> <span class=o>+</span> <span class=mf>0.044715</span> <span class=o>*</span> <span class=n>torch</span><span class=o>.</span><span class=n>pow</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=p>))</span> 
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MLP</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>c_fc</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>emb_dim</span><span class=p>,</span> <span class=mi>4</span> <span class=o>*</span> <span class=n>emb_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>c_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=mi>4</span> <span class=o>*</span> <span class=n>emb_dim</span><span class=p>,</span> <span class=n>emb_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gelu</span> <span class=o>=</span> <span class=n>GELU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>gelu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>c_fc</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><p>We define the Transformer Encoder Block, which follows the standard architecture: layer normalization before each block, multi-head self-attention, and a feedforward MLP with residual connections.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>EncoderBlock</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cfg</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ln_1</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>emb_dim</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attn</span> <span class=o>=</span> <span class=n>MultiHeadSelfAttention</span><span class=p>(</span><span class=n>d_in</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>],</span> <span class=n>d_out</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>],</span> <span class=n>attn_pdrop</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;attn_pdrop&#39;</span><span class=p>],</span> <span class=n>num_heads</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;n_heads&#39;</span><span class=p>],</span> <span class=n>qkv_bias</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;qkv_bias&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ln_2</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>emb_dim</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span> <span class=o>=</span> <span class=n>MLP</span><span class=p>(</span><span class=n>emb_dim</span><span class=o>=</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>resid_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;resid_pdrop&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>resid_dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>attn</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ln_1</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>resid_dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>mlp</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ln_2</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><h4 id=defining-the-vit-model>Defining the ViT model<a hidden class=anchor aria-hidden=true href=#defining-the-vit-model>#</a></h4><p>Let&rsquo;s implement the Vision Transformer model in code as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>ViT</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cfg</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>patch_size</span> <span class=o>=</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;patch_size&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>patch_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;patch_size&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;patch_size&#39;</span><span class=p>]</span> <span class=o>*</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;n_channels&#39;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>],</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>        <span class=c1># Define class token and position embeddings as learnable parameters</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cls_token_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>position_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span> <span class=o>+</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;n_patches&#39;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>]))</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embd_dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;embd_pdrop&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>blocks</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=p>[</span><span class=n>EncoderBlock</span><span class=p>(</span><span class=n>cfg</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;n_layers&#39;</span><span class=p>])])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cls_head</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;emb_dim&#39;</span><span class=p>],</span> <span class=n>cfg</span><span class=p>[</span><span class=s1>&#39;n_classes&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>img</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>patches</span> <span class=o>=</span> <span class=n>patchify_image</span><span class=p>(</span><span class=n>img</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>patch_size</span><span class=p>)</span>                          <span class=c1># (B, N, P*P*C)</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>N</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>patches</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Project the flattened patches into the embedding dimension</span>
</span></span><span class=line><span class=cl>        <span class=n>patch_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>patch_embedding</span><span class=p>(</span><span class=n>patches</span><span class=p>)</span>                               <span class=c1># (B, N, D)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>        <span class=c1># Prepend CLS token to the sequence of patch embeddings</span>
</span></span><span class=line><span class=cl>        <span class=n>cls_token_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cls_token_embedding</span><span class=o>.</span><span class=n>repeat</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>                <span class=c1># (B, 1, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>patch_emb</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>cls_token_emb</span><span class=p>,</span> <span class=n>patch_emb</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>                <span class=c1># (B, N+1, D)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>        <span class=c1># Add positional embeddings</span>
</span></span><span class=line><span class=cl>        <span class=n>pos_emb</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>position_embedding</span><span class=p>[:,</span> <span class=p>:</span><span class=n>N</span><span class=o>+</span><span class=mi>1</span><span class=p>]</span>                              <span class=c1># (1, N+1, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>patch_emb</span> <span class=o>+</span> <span class=n>pos_emb</span>                                                 <span class=c1># (B, N+1, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>embd_dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Pass through Transformer encoder blocks</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>blocks</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>        <span class=c1># Perform classification</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>,</span> <span class=p>:]</span>                                                          <span class=c1># (B, D)</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>cls_head</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>                                               <span class=c1># (B, N, num_classes)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>logits</span>
</span></span></code></pre></div><p>We use <code>nn.Parameter</code> for the class token and position embeddings because these are learnable parameters that need to be optimized during training, just like the weights in a neural network.</p><p>After passing the input through the Transformer blocks, we extract the learned representations of the first token (the classification token) and feed it into the classification head, which outputs logits for the class labels.</p><p>Now, let&rsquo;s define the configuration dictionary for the base ViT model and perform a single forward pass using a batch of CIFAR-10 images as a sanity check:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=s1>&#39;cuda&#39;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;cpu&#39;</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Using&#34;</span><span class=p>,</span> <span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1># Define configuration dictionary</span>
</span></span><span class=line><span class=cl><span class=n>ViT_B8_CONFIG</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;patch_size&#34;</span><span class=p>:</span> <span class=mi>8</span><span class=p>,</span>        <span class=c1># Patch size</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;n_patches&#34;</span><span class=p>:</span> <span class=mi>16</span><span class=p>,</span>        <span class=c1># Maximum number of patches</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;n_channels&#34;</span><span class=p>:</span> <span class=mi>3</span><span class=p>,</span>        <span class=c1># Number of channels in the image</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;n_classes&#34;</span><span class=p>:</span> <span class=mi>10</span><span class=p>,</span>        <span class=c1># Number of classes in CIFAR-10 dataset</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;emb_dim&#34;</span><span class=p>:</span> <span class=mi>768</span><span class=p>,</span>         <span class=c1># Embedding dimension</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;n_heads&#34;</span><span class=p>:</span> <span class=mi>12</span><span class=p>,</span>          <span class=c1># Number of attention heads</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;n_layers&#34;</span><span class=p>:</span> <span class=mi>12</span><span class=p>,</span>         <span class=c1># Number of transformer blocks</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;attn_pdrop&#34;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>,</span>      <span class=c1># Dropout probability for attention dropout</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;embd_pdrop&#34;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>,</span>      <span class=c1># Dropout probability for embeddings dropout</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;resid_pdrop&#34;</span><span class=p>:</span> <span class=mf>0.0</span><span class=p>,</span>     <span class=c1># Dropout probability for residual dropout</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;qkv_bias&#34;</span><span class=p>:</span> <span class=kc>True</span>        <span class=c1># Whether to use bias in QKV layer</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>ViT</span><span class=p>(</span><span class=n>ViT_B8_CONFIG</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1># Print the number of parameters in the model</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span> <span class=o>/</span> <span class=mf>1e6</span><span class=p>,</span> <span class=s1>&#39;M parameters&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=c1># Forward pass with one example</span>
</span></span><span class=line><span class=cl><span class=n>image</span><span class=p>,</span> <span class=n>label</span> <span class=o>=</span> <span class=n>image</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>label</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>image</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Model output shape:&#34;</span><span class=p>,</span> <span class=n>logits</span><span class=o>.</span><span class=n>size</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>functional</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>label</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Initial Loss:&#34;</span><span class=p>,</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=m>85.149706</span> <span class=n>M</span> <span class=n>parameters</span>
</span></span><span class=line><span class=cl><span class=n>Model</span> <span class=n>output</span> <span class=n>shape</span><span class=p>:</span> <span class=n>torch</span><span class=p>.</span><span class=n>Size</span><span class=p>([</span><span class=m>64</span><span class=p>,</span> <span class=m>10</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>Initial</span> <span class=n>Loss</span><span class=p>:</span> <span class=m>2.3756895065307617</span>
</span></span></code></pre></div><p>As expected the model outputs a probability distribution over all the class labels for each example in the batch. At initialization, the model assigns equal probability to each class (since it&rsquo;s untrained), which results in a loss close to $ -\ln(1/10) = 2.302$, confirming that everything is working as expected.</p><h3 id=architecture-variants>Architecture variants<a hidden class=anchor aria-hidden=true href=#architecture-variants>#</a></h3><p>The authors present three variants of the ViT architecture: Base, Large, and Huge, with their respective hyperparameters defined below.</p><figure class=align-center><img loading=lazy src=../vit-hyper.png#center alt="Details of Vision Transformer model variants." width=600><figcaption><p>Details of Vision Transformer model variants.</p></figcaption></figure><p>To denote model size and input patch size, models are named using the format ViT-[Size]/[Patch Size]. For example, ViT-B/16 refers to the Base variant with a $16 \times 16$ input patch size.</p><p>Notably, the transformer&rsquo;s sequence length (i.e., the number of patches) is inversely proportional to the square of the patch size. This means that models with smaller patch sizes are computationally more expensive since they process longer sequences.</p><p>Additionally, for a fixed patch size, higher-resolution images result in longer sequences, further increasing computational cost. In fact, the complexity grows quadratically with image size, making ViTs inefficient as general-purpose backbones for handling arbitrary-sized images.</p><p>To address this limitation, Microsoft Research introduced the Swin Transformer <a href=#references>[2]</a>, which you can read more about here: <a href=https://amaarora.github.io/posts/2022-07-04-swintransformerv1.html>https://amaarora.github.io/posts/2022-07-04-swintransformerv1.html</a>. It has become go-to transformer-based backbone for vision tasks today.</p><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><ul><li><p><a href=https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/11-vision-transformer.html>https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/11-vision-transformer.html</a></p></li><li><p>[1] Alexey Dosovitskiy, et al. &ldquo;<a href=https://arxiv.org/abs/2010.11929>An image is worth 16x16 words: Transformers for image recognition at scale.</a>&rdquo;, ICLR 2021.</p></li><li><p>[2] Ze Liu, et al. &ldquo;<a href=https://arxiv.org/abs/2103.14030>Swin transformer: Hierarchical vision transformer using shifted windows</a>&rdquo;, ICCV 2021.</p></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://yugajmera.github.io/posts/10-gpt2/post/><span class=title>Next »</span><br><span>GPT Series Part 3: Building GPT-2 & Sampling Techniques</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://yugajmera.github.io/>YA's Almanac</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>