<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GPT Series Part 2: Implementing BPE Tokenizer | YA's Almanac</title>
<meta name=keywords content><meta name=description content="In this post, we will look at tokenization, one of the most crucial preprocessing steps in LLMs. Tokenization breaks raw text into smaller units called tokens, which can be words, subwords, or special characters. These unique tokens form a vocabulary, each mapped to a unique token ID. The model then uses an embedding table to convert token IDs into dense vector representations, which are fed into the neural network.
Unlike other GPT models, OpenAI has released the code for GPT-2 [1], giving us the opportunity to explore the model&rsquo;s inner workings, including the tokenizer and its implementation within the architecture."><meta name=author content><link rel=canonical href=https://yugajmera.github.io/posts/06-gpt-tokenizer/post/><link crossorigin=anonymous href=https://yugajmera.github.io/assets/css/stylesheet.74991b51f7611c2303d0b5649703d675530589621885eb78236e099c22aa93a5.css integrity="sha256-dJkbUfdhHCMD0LVklwPWdVMFiWIYhet4I24JnCKqk6U=" rel="preload stylesheet" as=style><link rel=icon href=https://yugajmera.github.io/assets/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yugajmera.github.io/assets/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yugajmera.github.io/assets/favicon-32x32.png><link rel=apple-touch-icon href=https://yugajmera.github.io/assets/apple-touch-icon.png><link rel=mask-icon href=https://yugajmera.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yugajmera.github.io/posts/06-gpt-tokenizer/post/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>mjx-container[display=true]{margin:1.5em 0!important}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S759YBMKJE",{anonymize_ip:!1})}</script><meta property="og:title" content="GPT Series Part 2: Implementing BPE Tokenizer"><meta property="og:description" content="In this post, we will look at tokenization, one of the most crucial preprocessing steps in LLMs. Tokenization breaks raw text into smaller units called tokens, which can be words, subwords, or special characters. These unique tokens form a vocabulary, each mapped to a unique token ID. The model then uses an embedding table to convert token IDs into dense vector representations, which are fed into the neural network.
Unlike other GPT models, OpenAI has released the code for GPT-2 [1], giving us the opportunity to explore the model&rsquo;s inner workings, including the tokenizer and its implementation within the architecture."><meta property="og:type" content="article"><meta property="og:url" content="https://yugajmera.github.io/posts/06-gpt-tokenizer/post/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-22T00:00:00+00:00"><meta property="article:modified_time" content="2025-02-22T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="GPT Series Part 2: Implementing BPE Tokenizer"><meta name=twitter:description content="In this post, we will look at tokenization, one of the most crucial preprocessing steps in LLMs. Tokenization breaks raw text into smaller units called tokens, which can be words, subwords, or special characters. These unique tokens form a vocabulary, each mapped to a unique token ID. The model then uses an embedding table to convert token IDs into dense vector representations, which are fed into the neural network.
Unlike other GPT models, OpenAI has released the code for GPT-2 [1], giving us the opportunity to explore the model&rsquo;s inner workings, including the tokenizer and its implementation within the architecture."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yugajmera.github.io/posts/"},{"@type":"ListItem","position":2,"name":"GPT Series Part 2: Implementing BPE Tokenizer","item":"https://yugajmera.github.io/posts/06-gpt-tokenizer/post/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GPT Series Part 2: Implementing BPE Tokenizer","name":"GPT Series Part 2: Implementing BPE Tokenizer","description":"In this post, we will look at tokenization, one of the most crucial preprocessing steps in LLMs. Tokenization breaks raw text into smaller units called tokens, which can be words, subwords, or special characters. These unique tokens form a vocabulary, each mapped to a unique token ID. The model then uses an embedding table to convert token IDs into dense vector representations, which are fed into the neural network.\nUnlike other GPT models, OpenAI has released the code for GPT-2 [1], giving us the opportunity to explore the model\u0026rsquo;s inner workings, including the tokenizer and its implementation within the architecture.","keywords":[],"articleBody":"In this post, we will look at tokenization, one of the most crucial preprocessing steps in LLMs. Tokenization breaks raw text into smaller units called tokens, which can be words, subwords, or special characters. These unique tokens form a vocabulary, each mapped to a unique token ID. The model then uses an embedding table to convert token IDs into dense vector representations, which are fed into the neural network.\nUnlike other GPT models, OpenAI has released the code for GPT-2 [1], giving us the opportunity to explore the model‚Äôs inner workings, including the tokenizer and its implementation within the architecture. You can find the official repository here: https://github.com/openai/gpt-2/tree/master\nWant to see how different tokenizers work across various LLMs? Try interactive web app to explore the GPT-2 tokenizer:Tiktoken web app.\nTokenization Different levels of tokenization.\nIn the last post, we used a character-level tokenizer, which splits input text into individual characters. While this approach may seem simple, when dealing with large training data, the number of tokens can grow significantly, making it time-consuming to process.\nA more intuitive approach is word-level tokenization, which splits text into words. This aligns well with human language processing but comes with a major drawback‚Äîit relies on a predefined vocabulary. What happens if a user enters a word that isn‚Äôt in the vocabulary? Since that word does not exist in our vocabulary, it has no corresponding token ID, the tokenizer throws a KeyError.\nA naive way to handle this is by introducing an (unknown) token to represent all out-of-vocabulary (OOV) words. However, this approach isn‚Äôt ideal because it loses information.\nA better solution is subword tokenization, which strikes a balance between character-level and word-level tokenization. It treats frequent words as whole tokens while breaking rare words into smaller subword units.\nOne widely used subword tokenization method is Byte Pair Encoding (BPE) [2], which was employed in GPT models. BPE efficiently decomposes words into subword units and individual characters, eliminating the need for a special unknown word token. Let‚Äôs dive into the BPE algorithm and see how it works in detail.\nByte pair encoding (BPE) The Byte Pair Encoding (BPE) algorithm builds a vocabulary by iteratively merging the most frequent character pairs into subwords and then merging frequent subwords into larger units (words). This process reduces the number of tokens required to represent an input sequence, thereby decreasing computational costs.\nSince transformer-based models like GPTs have a fixed context length, using BPE helps the model attend to longer sequences as they are now represented with a smaller number of tokens.\nThe Byte pair encoding algorithm builds vocabulary by iteratively merging frequent characters into subwords and frequent subwords into words. Merging repeatedly reduces the number of tokens with which an input sequence is represented with, reducing the computation.\nHow BPE works Initialize with single characters - The vocabulary starts with all individual characters (e.g., a, b, c, ‚Ä¶).\nMerge the most frequent pair - The algorithm identifies the most frequently occurring adjacent character pair in the dataset and merges it into a subword.\nFor example, t and h may frequently appear together in common words like ‚Äúthe‚Äù, ‚Äúthen‚Äù, or ‚Äútherefore‚Äù, BPE merges them into the subword th. Create a new token - The newly merged subword (e.g., th) is added to the vocabulary and assigned a new token ID.\nRepeat - This process continues iteratively, merging the most common adjacent token pairs at each step. Over time, subwords like ‚Äúing‚Äù, ‚Äúly‚Äù, ‚Äútion‚Äù, etc., may emerge, allowing the vocabulary to represent common word segments efficiently.\nEach iteration compresses the dataset, representing it with fewer tokens while simultaneously expanding the vocabulary size by introducing new subwords.\nTrade-Off: Vocabulary Size vs. Sequence Length Vocabulary size is an important hyperparameter to consider, as it determines the size of the token embedding table and the final language modeling head. Increasing the vocabulary size has the following effects:\nModel Size: A larger vocabulary increases the number of parameters in the model, leading to higher memory and compute requirements.\nRisk of Underfitting: As the vocabulary grows, it may contain many rare or infrequent tokens that appear only a few times in the training data, making it difficult for the model to learn meaningful representations for these tokens.\nThus, a balance must be struck between sequence length and vocabulary size. The number of merges is a key hyperparameter that controls vocabulary size. For example, GPT-1 used 40,000 merges, while GPT-2 used 50,000 merges.\nCoding BPE Tokenizer Before we dive into coding the BPE tokenizer, it‚Äôs essential to understand that a tokenizer is typically trained separately from the LLM (Large Language Model). The tokenizer is usually trained on a diverse and extensive dataset to learn an efficient subword vocabulary, enabling it to generalize effectively across various domains and languages.\nUnfortunately, the training code for the GPT-2 tokenizer hasn‚Äôt been released, and we only have the inference code for encoding and decoding text, which is available here: https://github.com/openai/gpt-2/blob/master/src/encoder.py\nTo run inference, we just need two files: vocab.bpe and encoder.json.\n#!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe #!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json import os, json with open('encoder.json', 'r') as f: encoder = json.load(f) with open('vocab.bpe', 'r', encoding=\"utf-8\") as f: bpe_data = f.read() bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]] Now, let‚Äôs explore how we can train our own BPE tokenizer and generate similar files that can be stored and later used for inference.\nBits and Bytes In Python, text is represented as a string containing multiple characters. Each character is represented by a Unicode code point, which is an integer value. The ord() function eturns the Unicode code point of a character, and chr() converts a code point back to a character.\ntext = \"Hello üëã\" for t in text: print(f\"{t} -\u003e {ord(t)}\") H -\u003e 72 e -\u003e 101 l -\u003e 108 l -\u003e 108 o -\u003e 111 -\u003e 32 üëã -\u003e 128075 However, Unicode code points are not stored directly in memory as characters. Computers store text as bytes, and these Unicode characters must be encoded into byte streams before they can be processed. We use UTF-8 encoding, which converts each code point into a sequence of 1 to 4 bytes.\nUTF-8 Encoding Rules:\nCode points from 0‚Äì127 (Basic ASCII) ‚Üí Stored in 1 byte. Code points from 128‚Äì2047 ‚Üí Stored in 2 bytes. Code points from 2048‚Äì65535 ‚Üí Stored in 3 bytes Code points 65536+ ‚Üí Stored in 4 bytes. # Convert each character in the string to byte stream text = \"Hello üëã\" byte_arr = text.encode(\"utf-8\") print(byte_arr) print(list(byte_arr)) b'Hello \\xf0\\x9f\\x91\\x8b' [72, 101, 108, 108, 111, 32, 240, 159, 145, 139] Hello remains [72, 101, 108, 108, 111, 32] (same as ASCII) üëã becomes four bytes: [240, 159, 152, 138] (UTF-8 encoding). This would be a valid way to convert text directly into token IDs for the embedding layer of an LLM. However, this approach would represent each character with one to four integers, which would significantly increase the sequence length (10 token IDs for this small text in the example above).\nWhy not directly use Unicode code points (e.g., 128075 for üëã) directly instead of byte streams? This would require us to use the entire space of Unicode symbols to model all Unicode strings, resulting in a base vocabulary of over 130,000, before even adding any multi-symbol tokens.\nTo decode these tokens back to text, we use the decode function.\ntokens = [72, 101, 108, 108, 111, 32, 240, 159, 145, 139] text = bytes(tokens).decode('utf-8') print(text) Hello üëã GPT-2 uses BPE on UTF-8 byte sequences. Since a byte consists of 8 bits, there are $2^8 = 256$ possible values that a single byte can represent, ranging from 0 to 255. UTF-8 can use up to 4 bytes for one character, supporting a wide range of characters from different languages and symbols.\nThus, we can represent any text using this byte vocabulary, which consists of one to four tokens for each character in the text. As a result, the byte-level version of BPE only requires a base vocabulary of 256 tokens.\nThe BPE tokenizer starts with the first 256 byte values (single-byte tokens) in the initial vocabulary.\nvocab = {idx:bytes([idx]) for idx in range(256)} # token ids:utf-8 bytes Our vocabulary is a dictionary that maps token IDs to UTF-8 bytes, which will be helpful for decoding tokens directly back to text.\nHowever, GPT-2‚Äôs approach is slightly more complex. The encoder.json file contains a dictionary that maps Unicode characters to token IDs. For decoding, it flips the key-value mapping and uses a special function, bytes_to_unicode(), to map Unicode characters to UTF-8 bytes. This process is quite cumbersome, and I prefer to handle it all in one go.\nBigram shrinking The next step is to identify the most frequent pairs of integers (called bigrams) in the entire byte stream, and replace those pairs with a new token ID (e.g., 256), appending it to our vocabulary.\n# Convert text to utf-8 byte tokens text = \"the cat in the hat\" tokens = list(text.encode(\"utf-8\")) print(tokens) # Compute frequency of each bigram occuring in tokens list def get_pairs(tokens): counts = {} for i in range(len(tokens) - 1): counts[(tokens[i], tokens[i+1])] = counts.get((tokens[i], tokens[i+1]), 0) + 1 return counts # Replace the bigram with a new id \u0026 return the modified tokens list def merge(tokens, bigram, new_id): new_tokens = [] i = 0 while i \u003c len(tokens): if i \u003c len(tokens)-1 and (tokens[i], tokens[i+1]) == bigram: new_tokens.append(new_id) i += 2 else: new_tokens.append(tokens[i]) i += 1 return new_tokens [116, 104, 101, 32, 99, 97, 116, 32, 105, 110, 32, 116, 104, 101, 32, 104, 97, 116] Performing the merge operation once gives us:\n# Run this cell multiple times with new token IDs pairs = get_pairs(tokens) most_freq_bigram = max(pairs, key=pairs.get) print(\"Most frequent bigram:\", most_freq_bigram) tokens = merge(tokens, most_freq_bigram, 256) print(tokens) Iteration 1\nMost frequent bigram: (116, 104) [256, 101, 32, 99, 97, 116, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116] Input text: the cat in the hat Most frequent bigram (116, 104) corresponds to characters th Replace it with a new token ID that is already not is use 256 New byte stream: \u003c256\u003ee cat in \u003c256\u003ee hat Iteration 2\nMost frequent bigram: (256, 101) [257, 32, 99, 97, 116, 32, 105, 110, 32, 257, 32, 104, 97, 116] Input text: \u003c256\u003ee cat in \u003c256\u003ee hat Most frequent bigram (256, 101) corresponds to characters \u003c256\u003ee. Replace it with a new token ID that is already not is use 257 New byte stream: \u003c257\u003e cat in \u003c257\u003e hat Iteration 3\nMost frequent bigram: (257, 32) [258, 99, 97, 116, 32, 105, 110, 32, 258, 104, 97, 116] Input text: \u003c257\u003e cat in \u003c257\u003e hat Most frequent bigram (256, 32) corresponds to characters \u003c256\u003e (and space). Replace it with a new token ID that is already not is use 258 New byte stream: \u003c258\u003ecat in \u003c258\u003ehat The updated vocabulary might look something like this:\n... 256: \"th\" 257: \"\u003c256\u003ee\" 258: \"\u003c257\u003e \" Our vocabulary begins with 256 single-character tokens and grows by one with each merge. We repeat this process for multiple iterations until we achieve a manageable vocabulary size.\nTraining the tokenizer Now that we understand the BPE algorithm, let‚Äôs train it on the Tiny Shakespeare dataset (from the last post) to obtain a vocabulary set.\n# Convert text to byte stream with open('input.txt', 'r', encoding='utf-8') as f: text = f.read() tokens = list(text.encode(\"utf-8\")) initial_length = len(tokens) # Hyperparameter vocab_size = 276 # desired final vocabulary size num_merges = vocab_size - 256 vocab = {idx:bytes([idx]) for idx in range(256)} # token ids:utf-8 bytes bpe_merges = [] # keeps a track of all merges for i in range(num_merges): pairs = get_pairs(tokens) most_freq_bigram = max(pairs, key=pairs.get) new_token_id = i + 256 # Merge the most frequent bigram tokens = merge(tokens, most_freq_bigram, new_token_id) bpe_merges.append(most_freq_bigram) # Update the vocabulary with the new merged token vocab[new_token_id] = vocab[most_freq_bigram[0]] + vocab[most_freq_bigram[1]] final_length = len(tokens) print(f\"Compression ratio after merging: {initial_length / final_length:.2f}X\") Compression ratio after merging: 1.26X Running this cell trains our tokenizer. We were able to achieve a compression ratio of 1.26 of our text after only 20 merges.\nThe two main variables needed for inference are vocab and bpe_merges. You can store them if you wish for future use.\nvocab contains the final vocabulary set, with token IDs mapped to byte sequences. bpe_merges is a list that stores UTF-8 byte token bigrams. This is equivalent to the vocab.bpe file used in GPT-2 code, which contains Unicode character bigrams. After the merges, we have the updated vocabulary, which can be leveraged to tokenize text during inference efficiently. Let‚Äôs take a look at our updated vocabulary:\n... 272: b'ar' 273: b' th' 274: b'on' 275: b'll' Encoding and Decoding To decode input tokens, we can directly reference them using our vocab dictionary and obtain the corresponding UTF-8 byte sequences. After this, we use the .decode() function to convert the byte sequence back into raw text.\n# Convert token ids back to string (text) def decode(tokens): byte_arr = b\"\".join(vocab[idx] for idx in tokens) text = byte_arr.decode('utf-8', errors=\"replace\") return text The errors=\"replace\" parameter ensures that if the byte sequence contains invalid or unknown byte values that can‚Äôt be decoded, Python will replace those bytes with a replacement character (usually ‚ÄòÔøΩ‚Äô) instead of raising an error. This avoids interrupting the decoding process.\nAs mentioned earlier, Unicode code points from 0‚Äì127 (ASCII range) are stored in a single byte. However, code points beyond 127 are represented by multi-byte sequences, where each byte can range from 0‚Äì255. Thus, bytes between 128 and 255 are never stored alone‚Äîthey must be part of a multi-byte sequence.\nIf we attempt to decode a byte like 128 in isolation, it would throw an error because it‚Äôs an invalid byte when considered alone in UTF-8 encoding. Python will handle this gracefully by replacing such invalid byte sequences with the replacement character ‚ÄòÔøΩ‚Äô rather than raising an error.\nLet‚Äôs test it:\nprint(decode([65, 275, 270, 128])) AllenÔøΩ For encoding the input text, we first convert the characters into UTF-8 byte tokens. The bpe_merges list contains all the merge operations we need to perform, and it should be in the same order as when the list was created. Therefore, we create a bpe_ranks dictionary that ranks each merge based on their priority.\nWe then search for pairs in our text, merging the pair with the lowest rank first. We continue merging until we either reduce the token length to less than 2 or exhaust all matching pairs.\n# Convert text to token ids def encode(text): tokens = list(text.encode(\"utf-8\")) bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges)))) while len(tokens) \u003e 1: # Ensure we have at least 2 tokens to merge pairs = get_pairs(tokens) # Get all pairs # Choose the bigram that matches in bpe_merges with the lowest rank bigram = min(pairs, key=lambda pair: bpe_ranks.get(pair, float('inf'))) # When there are no bigrams that exists in bpe_merges, it returns the first pair by default if bigram not in bpe_ranks: break # Merge the selected bigram and create a new merged token ID merged_token_id = bpe_ranks[bigram] + 256 tokens = merge(tokens, bigram, merged_token_id) return tokens Testing our encoding function:\ntokens = encode(\"Hello üëã\") print(tokens) print(decode(tokens)) [72, 101, 275, 269, 240, 159, 145, 139] Hello üëã Regex Patterns for improved Tokenization The authors of GPT-2 observed that directly applying BPE to byte sequences resulted in suboptimal merges due to the greedy frequency-based merging algorithm. For example, they noticed that multiple versions of common words (e.g., dog., dog?, and dog! for the word dog) were merged, which results in inefficient usage of limited vocabulary slots.\nTo overcome this, they employed a forced split of the text according to specific patterns using regular expressions (regex). This ensures that merges occur only within specific text chunks, thus avoiding unwanted merges across character categories (e.g., merging punctuation with words).\nHere‚Äôs how we can use regex to split the text:\nimport regex as re gpt2_pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\") print(re.findall(gpt2_pat, \"Hello world 123, how're you!?\")) ['Hello', ' world', ' 123', ',', ' how', \"'re\", ' you', '!?'] Instead of applying the BPE algorithm to the entire string at once, we first split the text into chunks according to the regular expression pattern. Then, we run the BPE algorithm within each individual chunk, ensuring that we achieve better tokenization without suboptimal merges.\nUsing the Tiktoken library In practice, I highly recommend using the tiktoken library from OpenAI for tokenization inference as it is optimized for efficiency. Although it does not include a training loop, has already been trained by OpenAI, we can directly use it for tokenization tasks.\nHere‚Äôs how you can use it for encoding a string:\nimport tiktoken enc = tiktoken.get_encoding(\"gpt2\") print(enc.encode(\"Hello World!\")) [15496, 2159, 0] The output shows the token IDs corresponding to the text ‚ÄúHello World!‚Äù according to the GPT-2 tokenizer.\nNext, let‚Äôs encode the entire Shakespeare dataset to examine the compression ratio of the trained GPT-2 tokenizer.\n# Convert text to byte stream with open('input.txt', 'r', encoding='utf-8') as f: text = f.read() tokens = list(text.encode(\"utf-8\")) initial_length = len(tokens) tokens = enc.encode(text) final_length = len(tokens) print(f\"Compression ratio of GPT-2: {initial_length / final_length:.2f}X\") Compression ratio of GPT-2: 3.30X Special Context Tokens In GPT-like large language models, it‚Äôs common to use special tokens to help manage the flow and structure of the data.\nEnd-of-Text token in GPT-2 vocabulary One such token is the \u003c|endoftext|\u003e token, which is used to separate two unrelated texts, especially when multiple independent documents or books are concatenated for training. This helps the model understand that even though the texts are joined together, they are separate and distinct.\nGPT-2‚Äôs vocabulary includes:\n256 raw byte tokens, 50,000 merged tokens, and 1 \u003c|endoftext|\u003e token. This brings the total vocabulary size to 50,257 tokens.\nHere‚Äôs an example from the tiktoken web app:\nScreenshot from tiktoken webapp. Note that the \u003c|endoftext|\u003e token is the last token in the vocabulary corresponding to token id 50256.\nFine-Tuned Conversational Models Additionally, fine-tuned models, especially those used for conversation-based tasks, often introduce new special tokens to track the flow of conversation between a user and an assistant. These tokens help to distinguish different parts of the dialogue and maintain coherence. Some of these tokens include:\n\u003c|im_start|\u003e: Marks the start of an imaginary monologue. user/assistant: Used to denote the speaker (either the user or the assistant). \u003c|im_sep|\u003e: A separator token used to separate different sections or turns in the conversation. \u003c|im_end|\u003e: Marks the end of the monologue or conversation. These tokens are introduced during the fine-tuning phase when the model is trained on a conversational dataset. They can appear in sequences like this:\n\u003c|im_start|\u003e # marks the start of an assistant's monologue \u003c|user|\u003e How are you? \u003c|im_sep|\u003e \u003c|assistant|\u003e I'm doing great! How can I assist you today? \u003c|im_end|\u003e # marks the end of the assistant's monologue Here‚Äôs another example from the tiktoken web app showing these tokens:\nScreenshot from tiktoken webapp showing special tokens used in gpt3.5-turbo conversational model.\nIn practice, these tokens are essential for managing structured dialogues, especially in use cases like chatbots or virtual assistants, where knowing the flow of conversation and who is ‚Äúspeaking‚Äù is crucial.\nReferences Sebastian Rashcka‚Äôs blog: Implementing A Byte Pair Encoding (BPE) Tokenizer From Scratch Andrej Karpathy‚Äôs video: Let‚Äôs build the GPT Tokenizer [1] Radford et al., ‚ÄúLanguage Models are Unsupervised Multitask Learners‚Äù, OpenAI 2019. [2] Sennrich et al., ‚ÄúNeural Machine Translation of Rare Words with Subword Units‚Äù, ACL 2016. ","wordCount":"3213","inLanguage":"en","datePublished":"2025-02-22T00:00:00Z","dateModified":"2025-02-22T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://yugajmera.github.io/posts/06-gpt-tokenizer/post/"},"publisher":{"@type":"Organization","name":"YA's Almanac","logo":{"@type":"ImageObject","url":"https://yugajmera.github.io/assets/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yugajmera.github.io/ accesskey=h title="YA's Almanac (Alt + H)">YA's Almanac</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yugajmera.github.io/ title=Home><span>Home</span></a></li><li><a href=https://yugajmera.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://yugajmera.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">GPT Series Part 2: Implementing BPE Tokenizer</h1><div class=post-meta><span title='2025-02-22 00:00:00 +0000 UTC'>February 22, 2025</span>&nbsp;¬∑&nbsp;16 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#tokenization aria-label=Tokenization>Tokenization</a><ul><li><a href=#byte-pair-encoding-bpe aria-label="Byte pair encoding (BPE)">Byte pair encoding (BPE)</a><ul><li><a href=#how-bpe-works aria-label="How BPE works">How BPE works</a></li><li><a href=#trade-off-vocabulary-size-vs-sequence-length aria-label="Trade-Off: Vocabulary Size vs. Sequence Length">Trade-Off: Vocabulary Size vs. Sequence Length</a></li></ul></li><li><a href=#coding-bpe-tokenizer aria-label="Coding BPE Tokenizer">Coding BPE Tokenizer</a><ul><li><a href=#bits-and-bytes aria-label="Bits and Bytes">Bits and Bytes</a></li><li><a href=#bigram-shrinking aria-label="Bigram shrinking">Bigram shrinking</a></li><li><a href=#training-the-tokenizer aria-label="Training the tokenizer">Training the tokenizer</a></li><li><a href=#encoding-and-decoding aria-label="Encoding and Decoding">Encoding and Decoding</a></li><li><a href=#regex-patterns-for-improved-tokenization aria-label="Regex Patterns for improved Tokenization">Regex Patterns for improved Tokenization</a></li></ul></li><li><a href=#using-the-tiktoken-library aria-label="Using the Tiktoken library">Using the Tiktoken library</a></li><li><a href=#special-context-tokens aria-label="Special Context Tokens">Special Context Tokens</a><ul><li><a href=#end-of-text-token-in-gpt-2-vocabulary aria-label="End-of-Text token in GPT-2 vocabulary">End-of-Text token in GPT-2 vocabulary</a></li><li><a href=#fine-tuned-conversational-models aria-label="Fine-Tuned Conversational Models">Fine-Tuned Conversational Models</a></li></ul></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>In this post, we will look at tokenization, one of the most crucial preprocessing steps in LLMs. Tokenization breaks raw text into smaller units called tokens, which can be words, subwords, or special characters. These unique tokens form a vocabulary, each mapped to a unique token ID. The model then uses an embedding table to convert token IDs into dense vector representations, which are fed into the neural network.</p><p>Unlike other GPT models, OpenAI has released the code for GPT-2 <a href=#references>[1]</a>, giving us the opportunity to explore the model&rsquo;s inner workings, including the tokenizer and its implementation within the architecture. You can find the official repository here: <a href=https://github.com/openai/gpt-2/tree/master>https://github.com/openai/gpt-2/tree/master</a></p><p>Want to see how different tokenizers work across various LLMs? Try interactive web app to explore the GPT-2 tokenizer:<a href="https://tiktokenizer.vercel.app/?model=gpt2">Tiktoken web app</a>.</p><h2 id=tokenization>Tokenization<a hidden class=anchor aria-hidden=true href=#tokenization>#</a></h2><figure class=align-center><img loading=lazy src=../subword.png#center alt="Different levels of tokenization."><figcaption><p>Different levels of tokenization.</p></figcaption></figure><p>In the last post, we used a character-level tokenizer, which splits input text into individual characters. While this approach may seem simple, when dealing with large training data, the number of tokens can grow significantly, making it time-consuming to process.</p><p>A more intuitive approach is word-level tokenization, which splits text into words. This aligns well with human language processing but comes with a major drawback‚Äîit relies on a predefined vocabulary. What happens if a user enters a word that isn&rsquo;t in the vocabulary? Since that word does not exist in our vocabulary, it has no corresponding token ID, the tokenizer throws a KeyError.</p><p>A naive way to handle this is by introducing an <code>&lt;unk></code> (unknown) token to represent all out-of-vocabulary (OOV) words. However, this approach isn&rsquo;t ideal because it loses information.</p><p>A better solution is subword tokenization, which strikes a balance between character-level and word-level tokenization. It treats frequent words as whole tokens while breaking rare words into smaller subword units.</p><p>One widely used subword tokenization method is Byte Pair Encoding (BPE) <a href=#references>[2]</a>,
which was employed in GPT models. BPE efficiently decomposes words into subword units and individual characters, eliminating the need for a special unknown word token. Let&rsquo;s dive into the BPE algorithm and see how it works in detail.</p><h3 id=byte-pair-encoding-bpe>Byte pair encoding (BPE)<a hidden class=anchor aria-hidden=true href=#byte-pair-encoding-bpe>#</a></h3><p>The Byte Pair Encoding (BPE) algorithm builds a vocabulary by iteratively merging the most frequent character pairs into subwords and then merging frequent subwords into larger units (words). This process reduces the number of tokens required to represent an input sequence, thereby decreasing computational costs.</p><p>Since transformer-based models like GPTs have a fixed context length, using BPE helps the model attend to longer sequences as they are now represented with a smaller number of tokens.</p><p>The Byte pair encoding algorithm builds vocabulary by iteratively merging frequent characters into subwords and frequent subwords into words. Merging repeatedly reduces the number of tokens with which an input sequence is represented with, reducing the computation.</p><h4 id=how-bpe-works>How BPE works<a hidden class=anchor aria-hidden=true href=#how-bpe-works>#</a></h4><ol><li><p>Initialize with single characters - The vocabulary starts with all individual characters (e.g., <code>a</code>, <code>b</code>, <code>c</code>, &mldr;).</p></li><li><p>Merge the most frequent pair - The algorithm identifies the most frequently occurring adjacent character pair in the dataset and merges it into a subword.</p><ul><li>For example, <code>t</code> and <code>h</code> may frequently appear together in common words like &ldquo;the&rdquo;, &ldquo;then&rdquo;, or &ldquo;therefore&rdquo;, BPE merges them into the subword <code>th</code>.</li></ul></li><li><p>Create a new token - The newly merged subword (e.g., <code>th</code>) is added to the vocabulary and assigned a new token ID.</p></li><li><p>Repeat - This process continues iteratively, merging the most common adjacent token pairs at each step. Over time, subwords like &ldquo;ing&rdquo;, &ldquo;ly&rdquo;, &ldquo;tion&rdquo;, etc., may emerge, allowing the vocabulary to represent common word segments efficiently.</p></li></ol><p>Each iteration compresses the dataset, representing it with fewer tokens while simultaneously expanding the vocabulary size by introducing new subwords.</p><h4 id=trade-off-vocabulary-size-vs-sequence-length>Trade-Off: Vocabulary Size vs. Sequence Length<a hidden class=anchor aria-hidden=true href=#trade-off-vocabulary-size-vs-sequence-length>#</a></h4><p>Vocabulary size is an important hyperparameter to consider, as it determines the size of the token embedding table and the final language modeling head. Increasing the vocabulary size has the following effects:</p><ul><li><p>Model Size: A larger vocabulary increases the number of parameters in the model, leading to higher memory and compute requirements.</p></li><li><p>Risk of Underfitting: As the vocabulary grows, it may contain many rare or infrequent tokens that appear only a few times in the training data, making it difficult for the model to learn meaningful representations for these tokens.</p></li></ul><p>Thus, a balance must be struck between sequence length and vocabulary size. The number of merges is a key hyperparameter that controls vocabulary size. For example, GPT-1 used 40,000 merges, while GPT-2 used 50,000 merges.</p><h3 id=coding-bpe-tokenizer>Coding BPE Tokenizer<a hidden class=anchor aria-hidden=true href=#coding-bpe-tokenizer>#</a></h3><p>Before we dive into coding the BPE tokenizer, it‚Äôs essential to understand that a tokenizer is typically trained separately from the LLM (Large Language Model). The tokenizer is usually trained on a diverse and extensive dataset to learn an efficient subword vocabulary, enabling it to generalize effectively across various domains and languages.</p><p>Unfortunately, the training code for the GPT-2 tokenizer hasn&rsquo;t been released, and we only have the inference code for encoding and decoding text, which is available here: <a href=https://github.com/openai/gpt-2/blob/master/src/encoder.py>https://github.com/openai/gpt-2/blob/master/src/encoder.py</a></p><p>To run inference, we just need two files: <code>vocab.bpe</code> and <code>encoder.json</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=ch>#!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe</span>
</span></span><span class=line><span class=cl><span class=c1>#!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span><span class=o>,</span> <span class=nn>json</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;encoder.json&#39;</span><span class=p>,</span> <span class=s1>&#39;r&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>encoder</span> <span class=o>=</span> <span class=n>json</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>f</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;vocab.bpe&#39;</span><span class=p>,</span> <span class=s1>&#39;r&#39;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>bpe_data</span> <span class=o>=</span> <span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>bpe_merges</span> <span class=o>=</span> <span class=p>[</span><span class=nb>tuple</span><span class=p>(</span><span class=n>merge_str</span><span class=o>.</span><span class=n>split</span><span class=p>())</span> <span class=k>for</span> <span class=n>merge_str</span> <span class=ow>in</span> <span class=n>bpe_data</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39;</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]]</span> 
</span></span></code></pre></div><p>Now, let‚Äôs explore how we can train our own BPE tokenizer and generate similar files that can be stored and later used for inference.</p><h4 id=bits-and-bytes>Bits and Bytes<a hidden class=anchor aria-hidden=true href=#bits-and-bytes>#</a></h4><p>In Python, text is represented as a string containing multiple characters. Each character is represented by a Unicode code point, which is an integer value. The <code>ord()</code> function eturns the Unicode code point of a character, and <code>chr()</code> converts a code point back to a character.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;Hello üëã&#34;</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=n>text</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>t</span><span class=si>}</span><span class=s2> -&gt; </span><span class=si>{</span><span class=nb>ord</span><span class=p>(</span><span class=n>t</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>H</span> <span class=p>-&gt;</span> <span class=m>72</span>
</span></span><span class=line><span class=cl><span class=n>e</span> <span class=p>-&gt;</span> <span class=m>101</span>
</span></span><span class=line><span class=cl><span class=n>l</span> <span class=p>-&gt;</span> <span class=m>108</span>
</span></span><span class=line><span class=cl><span class=n>l</span> <span class=p>-&gt;</span> <span class=m>108</span>
</span></span><span class=line><span class=cl><span class=n>o</span> <span class=p>-&gt;</span> <span class=m>111</span>
</span></span><span class=line><span class=cl>  <span class=p>-&gt;</span> <span class=m>32</span>
</span></span><span class=line><span class=cl><span class=err>üëã</span> <span class=p>-&gt;</span> <span class=m>128075</span>
</span></span></code></pre></div><p>However, Unicode code points are not stored directly in memory as characters. Computers store text as bytes, and these Unicode characters must be encoded into byte streams before they can be processed. We use UTF-8 encoding, which converts each code point into a sequence of 1 to 4 bytes.</p><p>UTF-8 Encoding Rules:</p><ul><li>Code points from 0‚Äì127 (Basic ASCII) ‚Üí Stored in 1 byte.</li><li>Code points from 128‚Äì2047 ‚Üí Stored in 2 bytes.</li><li>Code points from 2048‚Äì65535 ‚Üí Stored in 3 bytes</li><li>Code points 65536+ ‚Üí Stored in 4 bytes.</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Convert each character in the string to byte stream</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;Hello üëã&#34;</span>
</span></span><span class=line><span class=cl><span class=n>byte_arr</span> <span class=o>=</span> <span class=n>text</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>byte_arr</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=n>byte_arr</span><span class=p>))</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>b</span><span class=err>&#39;</span><span class=n>Hello</span> <span class=err>\</span><span class=n>xf0</span><span class=err>\</span><span class=n>x9f</span><span class=err>\</span><span class=n>x91</span><span class=err>\</span><span class=n>x8b</span><span class=err>&#39;</span>
</span></span><span class=line><span class=cl><span class=na>[72, 101, 108, 108, 111, 32, 240, 159, 145, 139]</span>
</span></span></code></pre></div><ul><li><code>Hello </code>remains <code>[72, 101, 108, 108, 111, 32]</code> (same as ASCII)</li><li><code>üëã</code> becomes four bytes: <code>[240, 159, 152, 138]</code> (UTF-8 encoding).</li></ul><p>This would be a valid way to convert text directly into token IDs for the embedding layer of an LLM. However, this approach would represent each character with one to four integers,
which would significantly increase the sequence length (10 token IDs for this small text in the example above).</p><p>Why not directly use Unicode code points (e.g., <code>128075</code> for <code>üëã</code>) directly instead of byte streams? This would require us to use the entire space of Unicode symbols to model all Unicode strings, resulting in a base vocabulary of over 130,000, before even adding any multi-symbol tokens.</p><p>To decode these tokens back to text, we use the <code>decode</code> function.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=p>[</span><span class=mi>72</span><span class=p>,</span> <span class=mi>101</span><span class=p>,</span> <span class=mi>108</span><span class=p>,</span> <span class=mi>108</span><span class=p>,</span> <span class=mi>111</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>240</span><span class=p>,</span> <span class=mi>159</span><span class=p>,</span> <span class=mi>145</span><span class=p>,</span> <span class=mi>139</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=nb>bytes</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>Hello</span> <span class=err>üëã</span>
</span></span></code></pre></div><p>GPT-2 uses BPE on UTF-8 byte sequences. Since a byte consists of 8 bits, there are $2^8 = 256$ possible values that a single byte can represent, ranging from 0 to 255. UTF-8 can use up to 4 bytes for one character, supporting a wide range of characters from different languages and symbols.</p><p>Thus, we can represent any text using this byte vocabulary, which consists of one to four tokens for each character in the text. As a result, the byte-level version of BPE only requires a base vocabulary of 256 tokens.</p><p>The BPE tokenizer starts with the first 256 byte values (single-byte tokens) in the initial vocabulary.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>vocab</span> <span class=o>=</span> <span class=p>{</span><span class=n>idx</span><span class=p>:</span><span class=nb>bytes</span><span class=p>([</span><span class=n>idx</span><span class=p>])</span> <span class=k>for</span> <span class=n>idx</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>256</span><span class=p>)}</span>  <span class=c1># token ids:utf-8 bytes</span>
</span></span></code></pre></div><p>Our vocabulary is a dictionary that maps token IDs to UTF-8 bytes, which will be helpful for decoding tokens directly back to text.</p><p>However, GPT-2&rsquo;s approach is slightly more complex. The <code>encoder.json</code> file contains a dictionary that maps Unicode characters to token IDs. For decoding, it flips the key-value mapping and uses a special function, <code>bytes_to_unicode()</code>, to map Unicode characters to UTF-8 bytes. This process is quite cumbersome, and I prefer to handle it all in one go.</p><h4 id=bigram-shrinking>Bigram shrinking<a hidden class=anchor aria-hidden=true href=#bigram-shrinking>#</a></h4><p>The next step is to identify the most frequent pairs of integers (called bigrams) in the entire byte stream, and replace those pairs with a new token ID (e.g., <code>256</code>), appending it to our vocabulary.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Convert text to utf-8 byte tokens</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=s2>&#34;the cat in the hat&#34;</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>text</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Compute frequency of each bigram occuring in tokens list</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_pairs</span><span class=p>(</span><span class=n>tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>counts</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span> <span class=o>-</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>counts</span><span class=p>[(</span><span class=n>tokens</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>tokens</span><span class=p>[</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>])]</span> <span class=o>=</span> <span class=n>counts</span><span class=o>.</span><span class=n>get</span><span class=p>((</span><span class=n>tokens</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>tokens</span><span class=p>[</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>]),</span> <span class=mi>0</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>counts</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Replace the bigram with a new id &amp; return the modified tokens list</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>merge</span><span class=p>(</span><span class=n>tokens</span><span class=p>,</span> <span class=n>bigram</span><span class=p>,</span> <span class=n>new_id</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>new_tokens</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span><span class=o>-</span><span class=mi>1</span> <span class=ow>and</span> <span class=p>(</span><span class=n>tokens</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>tokens</span><span class=p>[</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>])</span> <span class=o>==</span> <span class=n>bigram</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>new_tokens</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>new_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>i</span> <span class=o>+=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>new_tokens</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>tokens</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>            <span class=n>i</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>new_tokens</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=na>[116, 104, 101, 32, 99, 97, 116, 32, 105, 110, 32, 116, 104, 101, 32, 104, 97, 116]</span>
</span></span></code></pre></div><p>Performing the merge operation once gives us:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Run this cell multiple times with new token IDs</span>
</span></span><span class=line><span class=cl><span class=n>pairs</span> <span class=o>=</span> <span class=n>get_pairs</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>most_freq_bigram</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>pairs</span><span class=p>,</span> <span class=n>key</span><span class=o>=</span><span class=n>pairs</span><span class=o>.</span><span class=n>get</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Most frequent bigram:&#34;</span><span class=p>,</span> <span class=n>most_freq_bigram</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>merge</span><span class=p>(</span><span class=n>tokens</span><span class=p>,</span> <span class=n>most_freq_bigram</span><span class=p>,</span> <span class=mi>256</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Iteration 1</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>Most</span> <span class=n>frequent</span> <span class=n>bigram</span><span class=p>:</span> <span class=p>(</span><span class=m>116</span><span class=p>,</span> <span class=m>104</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=na>[256, 101, 32, 99, 97, 116, 32, 105, 110, 32, 256, 101, 32, 104, 97, 116]</span>
</span></span></code></pre></div><ul><li>Input text: <code>the cat in the hat</code></li><li>Most frequent bigram <code>(116, 104)</code> corresponds to characters <code>th</code></li><li>Replace it with a new token ID that is already not is use <code>256</code></li><li>New byte stream: <code>&lt;256>e cat in &lt;256>e hat</code></li></ul><p><strong>Iteration 2</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>Most</span> <span class=n>frequent</span> <span class=n>bigram</span><span class=p>:</span> <span class=p>(</span><span class=m>256</span><span class=p>,</span> <span class=m>101</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=na>[257, 32, 99, 97, 116, 32, 105, 110, 32, 257, 32, 104, 97, 116]</span>
</span></span></code></pre></div><ul><li>Input text: <code>&lt;256>e cat in &lt;256>e hat</code></li><li>Most frequent bigram <code>(256, 101)</code> corresponds to characters <code>&lt;256>e</code>.</li><li>Replace it with a new token ID that is already not is use <code>257</code></li><li>New byte stream: <code>&lt;257> cat in &lt;257> hat</code></li></ul><p><strong>Iteration 3</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>Most</span> <span class=n>frequent</span> <span class=n>bigram</span><span class=p>:</span> <span class=p>(</span><span class=m>257</span><span class=p>,</span> <span class=m>32</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=na>[258, 99, 97, 116, 32, 105, 110, 32, 258, 104, 97, 116]</span>
</span></span></code></pre></div><ul><li>Input text: <code>&lt;257> cat in &lt;257> hat</code></li><li>Most frequent bigram <code>(256, 32)</code> corresponds to characters <code>&lt;256> </code>(and space).</li><li>Replace it with a new token ID that is already not is use <code>258</code></li><li>New byte stream: <code>&lt;258>cat in &lt;258>hat</code></li></ul><p>The updated vocabulary might look something like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=p>...</span>
</span></span><span class=line><span class=cl><span class=m>256</span><span class=p>:</span> <span class=s>&#34;th&#34;</span>
</span></span><span class=line><span class=cl><span class=m>257</span><span class=p>:</span> <span class=s>&#34;&lt;256&gt;e&#34;</span>
</span></span><span class=line><span class=cl><span class=m>258</span><span class=p>:</span> <span class=s>&#34;&lt;257&gt; &#34;</span>
</span></span></code></pre></div><p>Our vocabulary begins with 256 single-character tokens and grows by one with each merge. We repeat this process for multiple iterations until we achieve a manageable vocabulary size.</p><h4 id=training-the-tokenizer>Training the tokenizer<a hidden class=anchor aria-hidden=true href=#training-the-tokenizer>#</a></h4><p>Now that we understand the BPE algorithm, let&rsquo;s train it on the Tiny Shakespeare dataset (from the last post) to obtain a vocabulary set.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Convert text to byte stream</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;input.txt&#39;</span><span class=p>,</span> <span class=s1>&#39;r&#39;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>text</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>initial_length</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Hyperparameter</span>
</span></span><span class=line><span class=cl><span class=n>vocab_size</span> <span class=o>=</span> <span class=mi>276</span>                                   <span class=c1># desired final vocabulary size</span>
</span></span><span class=line><span class=cl><span class=n>num_merges</span> <span class=o>=</span> <span class=n>vocab_size</span> <span class=o>-</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>vocab</span> <span class=o>=</span> <span class=p>{</span><span class=n>idx</span><span class=p>:</span><span class=nb>bytes</span><span class=p>([</span><span class=n>idx</span><span class=p>])</span> <span class=k>for</span> <span class=n>idx</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>256</span><span class=p>)}</span>  <span class=c1># token ids:utf-8 bytes</span>
</span></span><span class=line><span class=cl><span class=n>bpe_merges</span> <span class=o>=</span> <span class=p>[]</span>                                   <span class=c1># keeps a track of all merges</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_merges</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>pairs</span> <span class=o>=</span> <span class=n>get_pairs</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>most_freq_bigram</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>pairs</span><span class=p>,</span> <span class=n>key</span><span class=o>=</span><span class=n>pairs</span><span class=o>.</span><span class=n>get</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>new_token_id</span> <span class=o>=</span> <span class=n>i</span> <span class=o>+</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Merge the most frequent bigram</span>
</span></span><span class=line><span class=cl>    <span class=n>tokens</span> <span class=o>=</span> <span class=n>merge</span><span class=p>(</span><span class=n>tokens</span><span class=p>,</span> <span class=n>most_freq_bigram</span><span class=p>,</span> <span class=n>new_token_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>bpe_merges</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>most_freq_bigram</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Update the vocabulary with the new merged token</span>
</span></span><span class=line><span class=cl>    <span class=n>vocab</span><span class=p>[</span><span class=n>new_token_id</span><span class=p>]</span> <span class=o>=</span> <span class=n>vocab</span><span class=p>[</span><span class=n>most_freq_bigram</span><span class=p>[</span><span class=mi>0</span><span class=p>]]</span> <span class=o>+</span> <span class=n>vocab</span><span class=p>[</span><span class=n>most_freq_bigram</span><span class=p>[</span><span class=mi>1</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>final_length</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Compression ratio after merging: </span><span class=si>{</span><span class=n>initial_length</span> <span class=o>/</span> <span class=n>final_length</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>X&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>Compression ratio after merging: 1.26X
</code></pre><p>Running this cell trains our tokenizer. We were able to achieve a compression ratio of 1.26 of our text after only 20 merges.</p><p>The two main variables needed for inference are <code>vocab</code> and <code>bpe_merges</code>. You can store them if you wish for future use.</p><ul><li><code>vocab</code> contains the final vocabulary set, with token IDs mapped to byte sequences.</li><li><code>bpe_merges</code> is a list that stores UTF-8 byte token bigrams. This is equivalent to the
<code>vocab.bpe</code> file used in GPT-2 code, which contains Unicode character bigrams.</li></ul><p>After the merges, we have the updated vocabulary, which can be leveraged to tokenize text during inference efficiently. Let‚Äôs take a look at our updated vocabulary:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=p>...</span>
</span></span><span class=line><span class=cl><span class=m>272</span><span class=p>:</span> <span class=n>b</span><span class=err>&#39;</span><span class=n>ar</span><span class=err>&#39;</span>
</span></span><span class=line><span class=cl><span class=m>273</span><span class=p>:</span> <span class=n>b</span><span class=err>&#39;</span> <span class=n>th</span><span class=err>&#39;</span>
</span></span><span class=line><span class=cl><span class=m>274</span><span class=p>:</span> <span class=n>b</span><span class=err>&#39;</span><span class=k>on</span><span class=err>&#39;</span>
</span></span><span class=line><span class=cl><span class=m>275</span><span class=p>:</span> <span class=n>b</span><span class=err>&#39;</span><span class=n>ll</span><span class=err>&#39;</span>
</span></span></code></pre></div><h4 id=encoding-and-decoding>Encoding and Decoding<a hidden class=anchor aria-hidden=true href=#encoding-and-decoding>#</a></h4><p>To decode input tokens, we can directly reference them using our <code>vocab</code> dictionary and obtain the corresponding UTF-8 byte sequences. After this, we use the <code>.decode()</code> function to convert the byte sequence back into raw text.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Convert token ids back to string (text)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>decode</span><span class=p>(</span><span class=n>tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>byte_arr</span> <span class=o>=</span> <span class=sa>b</span><span class=s2>&#34;&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>vocab</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span> <span class=k>for</span> <span class=n>idx</span> <span class=ow>in</span> <span class=n>tokens</span><span class=p>)</span>   
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>byte_arr</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s1>&#39;utf-8&#39;</span><span class=p>,</span> <span class=n>errors</span><span class=o>=</span><span class=s2>&#34;replace&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>text</span>
</span></span></code></pre></div><p>The <code>errors="replace"</code> parameter ensures that if the byte sequence contains invalid or unknown byte values that can&rsquo;t be decoded, Python will replace those bytes with a replacement character (usually &lsquo;ÔøΩ&rsquo;) instead of raising an error. This avoids interrupting the decoding process.</p><p>As mentioned earlier, Unicode code points from 0‚Äì127 (ASCII range) are stored in a single byte. However, code points beyond 127 are represented by multi-byte sequences, where each byte can range from 0‚Äì255. Thus, bytes between 128 and 255 are never stored alone‚Äîthey must be part of a multi-byte sequence.</p><p>If we attempt to decode a byte like 128 in isolation, it would throw an error because it&rsquo;s an invalid byte when considered alone in UTF-8 encoding. Python will handle this gracefully by replacing such invalid byte sequences with the replacement character &lsquo;ÔøΩ&rsquo; rather than raising an error.</p><p>Let&rsquo;s test it:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>decode</span><span class=p>([</span><span class=mi>65</span><span class=p>,</span> <span class=mi>275</span><span class=p>,</span> <span class=mi>270</span><span class=p>,</span> <span class=mi>128</span><span class=p>]))</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=n>Allen</span><span class=err>ÔøΩ</span>
</span></span></code></pre></div><p>For encoding the input text, we first convert the characters into UTF-8 byte tokens. The <code>bpe_merges</code> list contains all the merge operations we need to perform, and it should be in the same order as when the list was created. Therefore, we create a <code>bpe_ranks</code> dictionary that ranks each merge based on their priority.</p><p>We then search for pairs in our text, merging the pair with the lowest rank first. We continue merging until we either reduce the token length to less than 2 or exhaust all matching pairs.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Convert text to token ids </span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>encode</span><span class=p>(</span><span class=n>text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>tokens</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>text</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>bpe_ranks</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span><span class=nb>zip</span><span class=p>(</span><span class=n>bpe_merges</span><span class=p>,</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>bpe_merges</span><span class=p>))))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>:</span>              <span class=c1># Ensure we have at least 2 tokens to merge</span>
</span></span><span class=line><span class=cl>        <span class=n>pairs</span> <span class=o>=</span> <span class=n>get_pairs</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>       <span class=c1># Get all pairs</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># Choose the bigram that matches in bpe_merges with the lowest rank</span>
</span></span><span class=line><span class=cl>        <span class=n>bigram</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span><span class=n>pairs</span><span class=p>,</span> <span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>pair</span><span class=p>:</span> <span class=n>bpe_ranks</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>pair</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># When there are no bigrams that exists in bpe_merges, it returns the first pair by default</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>bigram</span> <span class=ow>not</span> <span class=ow>in</span> <span class=n>bpe_ranks</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>break</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Merge the selected bigram and create a new merged token ID</span>
</span></span><span class=line><span class=cl>        <span class=n>merged_token_id</span> <span class=o>=</span> <span class=n>bpe_ranks</span><span class=p>[</span><span class=n>bigram</span><span class=p>]</span> <span class=o>+</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl>        <span class=n>tokens</span> <span class=o>=</span> <span class=n>merge</span><span class=p>(</span><span class=n>tokens</span><span class=p>,</span> <span class=n>bigram</span><span class=p>,</span> <span class=n>merged_token_id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>tokens</span>
</span></span></code></pre></div><p>Testing our encoding function:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>encode</span><span class=p>(</span><span class=s2>&#34;Hello üëã&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>decode</span><span class=p>(</span><span class=n>tokens</span><span class=p>))</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=na>[72, 101, 275, 269, 240, 159, 145, 139]</span>
</span></span><span class=line><span class=cl><span class=n>Hello</span> <span class=err>üëã</span>
</span></span></code></pre></div><h4 id=regex-patterns-for-improved-tokenization>Regex Patterns for improved Tokenization<a hidden class=anchor aria-hidden=true href=#regex-patterns-for-improved-tokenization>#</a></h4><p>The authors of GPT-2 observed that directly applying BPE to byte sequences resulted in suboptimal merges due to the greedy frequency-based merging algorithm. For example, they noticed that multiple versions of common words (e.g., <code>dog.</code>, <code>dog?</code>, and <code>dog!</code> for the word <code>dog</code>) were merged, which results in inefficient usage of limited vocabulary slots.</p><p>To overcome this, they employed a forced split of the text according to specific patterns using regular expressions (regex). This ensures that merges occur only within specific text chunks, thus avoiding unwanted merges across character categories (e.g., merging punctuation with words).</p><p>Here‚Äôs how we can use regex to split the text:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>regex</span> <span class=k>as</span> <span class=nn>re</span>
</span></span><span class=line><span class=cl><span class=n>gpt2_pat</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=sa>r</span><span class=s2>&#34;&#34;&#34;&#39;s|&#39;t|&#39;re|&#39;ve|&#39;m|&#39;ll|&#39;d| ?\p</span><span class=si>{L}</span><span class=s2>+| ?\p</span><span class=si>{N}</span><span class=s2>+| ?[^\s\p</span><span class=si>{L}</span><span class=s2>\p</span><span class=si>{N}</span><span class=s2>]+|\s+(?!\S)|\s+&#34;&#34;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>re</span><span class=o>.</span><span class=n>findall</span><span class=p>(</span><span class=n>gpt2_pat</span><span class=p>,</span> <span class=s2>&#34;Hello world 123, how&#39;re you!?&#34;</span><span class=p>))</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=na>[&#39;Hello&#39;, &#39; world&#39;, &#39; 123&#39;, &#39;,&#39;, &#39; how&#39;, &#34;&#39;re&#34;, &#39; you&#39;, &#39;!?&#39;]</span>
</span></span></code></pre></div><p>Instead of applying the BPE algorithm to the entire string at once, we first split the text into chunks according to the regular expression pattern. Then, we run the BPE algorithm within each individual chunk, ensuring that we achieve better tokenization without suboptimal merges.</p><h3 id=using-the-tiktoken-library>Using the Tiktoken library<a hidden class=anchor aria-hidden=true href=#using-the-tiktoken-library>#</a></h3><p>In practice, I highly recommend using the <a href=https://github.com/openai/tiktoken>tiktoken</a> library from OpenAI for tokenization inference as it is optimized for efficiency. Although it does not include a training loop, has already been trained by OpenAI, we can directly use it for tokenization tasks.</p><p>Here‚Äôs how you can use it for encoding a string:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tiktoken</span>
</span></span><span class=line><span class=cl><span class=n>enc</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>get_encoding</span><span class=p>(</span><span class=s2>&#34;gpt2&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>enc</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;Hello World!&#34;</span><span class=p>))</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-csharp data-lang=csharp><span class=line><span class=cl><span class=na>[15496, 2159, 0]</span>
</span></span></code></pre></div><p>The output shows the token IDs corresponding to the text &ldquo;Hello World!&rdquo; according to the GPT-2 tokenizer.</p><p>Next, let&rsquo;s encode the entire Shakespeare dataset to examine the compression ratio of the trained GPT-2 tokenizer.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Convert text to byte stream</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;input.txt&#39;</span><span class=p>,</span> <span class=s1>&#39;r&#39;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span> <span class=o>=</span> <span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>text</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>initial_length</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>enc</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>final_length</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Compression ratio of GPT-2: </span><span class=si>{</span><span class=n>initial_length</span> <span class=o>/</span> <span class=n>final_length</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>X&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre tabindex=0><code>Compression ratio of GPT-2: 3.30X
</code></pre><h3 id=special-context-tokens>Special Context Tokens<a hidden class=anchor aria-hidden=true href=#special-context-tokens>#</a></h3><p>In GPT-like large language models, it&rsquo;s common to use special tokens to help manage the flow and structure of the data.</p><h4 id=end-of-text-token-in-gpt-2-vocabulary>End-of-Text token in GPT-2 vocabulary<a hidden class=anchor aria-hidden=true href=#end-of-text-token-in-gpt-2-vocabulary>#</a></h4><p>One such token is the <code>&lt;|endoftext|></code> token, which is used to separate two unrelated texts, especially when multiple independent documents or books are concatenated for training. This helps the model understand that even though the texts are joined together, they are separate and distinct.</p><p>GPT-2‚Äôs vocabulary includes:</p><ul><li>256 raw byte tokens,</li><li>50,000 merged tokens, and</li><li>1 <code>&lt;|endoftext|></code> token.</li></ul><p>This brings the total vocabulary size to <strong>50,257</strong> tokens.</p><p>Here‚Äôs an example from the tiktoken web app:</p><figure class=align-center><img loading=lazy src=../special.png#center alt="Screenshot from tiktoken webapp. Note that the &amp;lt;|endoftext|&amp;gt; token is the last token in the vocabulary corresponding to token id 50256."><figcaption><p>Screenshot from tiktoken webapp. Note that the <code>&lt;|endoftext|></code> token is the last token in the vocabulary corresponding to token id <code>50256</code>.</p></figcaption></figure><h4 id=fine-tuned-conversational-models>Fine-Tuned Conversational Models<a hidden class=anchor aria-hidden=true href=#fine-tuned-conversational-models>#</a></h4><p>Additionally, fine-tuned models, especially those used for conversation-based tasks, often introduce new special tokens to track the flow of conversation between a user and an assistant. These tokens help to distinguish different parts of the dialogue and maintain coherence. Some of these tokens include:</p><ol><li><code>&lt;|im_start|></code>: Marks the start of an imaginary monologue.</li><li><code>user/assistant</code>: Used to denote the speaker (either the user or the assistant).</li><li><code>&lt;|im_sep|></code>: A separator token used to separate different sections or turns in the conversation.</li><li><code>&lt;|im_end|></code>: Marks the end of the monologue or conversation.</li></ol><p>These tokens are introduced during the fine-tuning phase when the model is trained on a conversational dataset. They can appear in sequences like this:</p><pre tabindex=0><code>&lt;|im_start|&gt;  # marks the start of an assistant&#39;s monologue
&lt;|user|&gt; How are you?
&lt;|im_sep|&gt; 
&lt;|assistant|&gt; I&#39;m doing great! How can I assist you today?
&lt;|im_end|&gt;  # marks the end of the assistant&#39;s monologue
</code></pre><p>Here‚Äôs another example from the tiktoken web app showing these tokens:</p><figure class=align-center><img loading=lazy src=../conversation.png#center alt="Screenshot from tiktoken webapp showing special tokens used in gpt3.5-turbo conversational model."><figcaption><p>Screenshot from tiktoken webapp showing special tokens used in gpt3.5-turbo conversational model.</p></figcaption></figure><p>In practice, these tokens are essential for managing structured dialogues, especially in use cases like chatbots or virtual assistants, where knowing the flow of conversation and who is &ldquo;speaking&rdquo; is crucial.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li><a href=https://sebastianraschka.com/blog/2025/bpe-from-scratch.html>Sebastian Rashcka&rsquo;s blog: Implementing A Byte Pair Encoding (BPE) Tokenizer From Scratch</a></li><li><a href="https://www.youtube.com/watch?v=zduSFxRajkE">Andrej Karpathy&rsquo;s video: Let&rsquo;s build the GPT Tokenizer</a></li><li>[1] Radford et al., &ldquo;<a href=https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>Language Models are Unsupervised Multitask Learners</a>&rdquo;, OpenAI 2019.</li><li>[2] Sennrich et al., &ldquo;<a href=https://arxiv.org/abs/1508.07909>Neural Machine Translation of Rare Words with Subword Units</a>&rdquo;, ACL 2016.</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://yugajmera.github.io/posts/06-gpt2/post/><span class=title>¬´ Prev</span><br><span>GPT Series Part 3: Building GPT-2 & Sampling Techniques</span>
</a><a class=next href=https://yugajmera.github.io/posts/06-gpt/post/><span class=title>Next ¬ª</span><br><span>GPT Series Part 1: Understanding LLMs & Coding GPT-1 from scratch</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://yugajmera.github.io/>YA's Almanac</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>