<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.124.1"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>YA's Almanac</title>
<meta name=description content><meta name=author content><link rel=canonical href=https://yugajmera.github.io/><link crossorigin=anonymous href=https://yugajmera.github.io/assets/css/stylesheet.74991b51f7611c2303d0b5649703d675530589621885eb78236e099c22aa93a5.css integrity="sha256-dJkbUfdhHCMD0LVklwPWdVMFiWIYhet4I24JnCKqk6U=" rel="preload stylesheet" as=style><link rel=icon href=https://yugajmera.github.io/assets/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yugajmera.github.io/assets/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yugajmera.github.io/assets/favicon-32x32.png><link rel=apple-touch-icon href=https://yugajmera.github.io/assets/apple-touch-icon.png><link rel=mask-icon href=https://yugajmera.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://yugajmera.github.io/index.xml><link rel=alternate type=application/json href=https://yugajmera.github.io/index.json><link rel=alternate hreflang=en href=https://yugajmera.github.io/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-S759YBMKJE"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-S759YBMKJE",{anonymize_ip:!1})}</script><meta property="og:title" content="YA's Almanac"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://yugajmera.github.io/"><meta name=twitter:card content="summary"><meta name=twitter:title content="YA's Almanac"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"YA's Almanac","url":"https://yugajmera.github.io/","description":"","thumbnailUrl":"https://yugajmera.github.io/assets/favicon.ico","sameAs":["mailto:yugajmera@gmail.com","https://github.com/YugAjmera","https://linkedin.com/in/yug-ajmera","https://instagram.com/yug_ajmera"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yugajmera.github.io/ accesskey=h title="YA's Almanac (Alt + H)">YA's Almanac</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yugajmera.github.io/ title=Home><span class=active>Home</span></a></li><li><a href=https://yugajmera.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://yugajmera.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class="first-entry home-info"><header class=entry-header><h1>üëã Welcome to my blog</h1></header><div class=entry-content>Hi there! I&rsquo;m <strong>Yug Ajmera</strong>, and this is where I document my learning notes. I&rsquo;m currently a research engineer at NEC Labs America, working in the field of <a href=https://www.nec-labs.com/research/media-analytics/projects/autonomous-driving/>Autonomous Driving</a>. After work, you can find me at the gym üí™, cooking something at home üç≤, or catching sunsets ‚õÖ around the beautiful Bay Area!</div><footer class=entry-footer><div class=social-icons><a href=mailto:yugajmera@gmail.com target=_blank rel="noopener noreferrer me" title=Email><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg>
</a><a href=https://github.com/YugAjmera target=_blank rel="noopener noreferrer me" title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a href=https://linkedin.com/in/yug-ajmera target=_blank rel="noopener noreferrer me" title=Linkedin><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
</a><a href=https://instagram.com/yug_ajmera target=_blank rel="noopener noreferrer me" title=Instagram><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"/><path d="M16 11.37A4 4 0 1112.63 8 4 4 0 0116 11.37z"/><line x1="17.5" y1="6.5" x2="17.5" y2="6.5"/></svg></a></div></footer></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Transformers for Image Classification: ViT and CLIP</h2></header><div class=entry-content><p>Modelling in computer vision has long been dominated by convolutional neural networks (CNNs). We‚Äôve already discussed famous architectures like VGGNet and ResNet in previous posts, which have served as the primary backbones for a variety of vision tasks.
In contrast, network architectures in natural language processing (NLP) have evolved along a different trajectory. The dominant architecture in NLP today is the Transformer, designed for sequence modeling. Models like GPT-3 have achieved remarkable success, scaling to over 100 billion parameters thanks to their computational efficiency and scalability....</p></div><footer class=entry-footer><span title='2025-03-17 00:00:00 +0000 UTC'>March 17, 2025</span>&nbsp;¬∑&nbsp;20 min</footer><a class=entry-link aria-label="post link to Transformers for Image Classification: ViT and CLIP" href=https://yugajmera.github.io/posts/07-vit/post/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>GPT Series Part 3: Building GPT-2 & Sampling Techniques</h2></header><div class=entry-content><p>Building on our previous exploration of GPT-1, let‚Äôs now modify its architecture to recreate the GPT-2 [1] small model, which has 124 million parameters. While the original paper refers to this as 117M, OpenAI later clarified the actual count.
One major advantage of GPT-2 is that OpenAI has released its pre-trained weights, allowing us to load them into our implementation. This not only serves as a sanity check for our model but also provides a strong foundation for fine-tuning....</p></div><footer class=entry-footer><span title='2025-03-01 00:00:00 +0000 UTC'>March 1, 2025</span>&nbsp;¬∑&nbsp;18 min</footer><a class=entry-link aria-label="post link to GPT Series Part 3: Building GPT-2 & Sampling Techniques" href=https://yugajmera.github.io/posts/06-gpt2/post/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>GPT Series Part 2: Implementing BPE Tokenizer</h2></header><div class=entry-content><p>In this post, we will look at tokenization, one of the most crucial preprocessing steps in LLMs. Tokenization breaks raw text into smaller units called tokens, which can be words, subwords, or special characters. These unique tokens form a vocabulary, each mapped to a unique token ID. The model then uses an embedding table to convert token IDs into dense vector representations, which are fed into the neural network.
Unlike other GPT models, OpenAI has released the code for GPT-2 [1], giving us the opportunity to explore the model‚Äôs inner workings, including the tokenizer and its implementation within the architecture....</p></div><footer class=entry-footer><span title='2025-02-22 00:00:00 +0000 UTC'>February 22, 2025</span>&nbsp;¬∑&nbsp;16 min</footer><a class=entry-link aria-label="post link to GPT Series Part 2: Implementing BPE Tokenizer" href=https://yugajmera.github.io/posts/06-gpt-tokenizer/post/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>GPT Series Part 1: Understanding LLMs & Coding GPT-1 from scratch</h2></header><div class=entry-content><p>By now, you‚Äôve probably used OpenAI‚Äôs ChatGPT‚Äîa chatbot that has taken the AI community by storm and transformed the way we work. First released in 2022 with GPT-3.5 (Generative Pre-trained Transformer 3.5) as its backend model, it reached one million users in just five days and a staggering 100 million in two months.
ChatGPT interface
The unprecedented success of ChatGPT fueled further research into the technology behind it‚ÄîLarge Language Models (LLMs)....</p></div><footer class=entry-footer><span title='2025-01-27 00:00:00 +0000 UTC'>January 27, 2025</span>&nbsp;¬∑&nbsp;22 min</footer><a class=entry-link aria-label="post link to GPT Series Part 1: Understanding LLMs & Coding GPT-1 from scratch" href=https://yugajmera.github.io/posts/06-gpt/post/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Generalizing Attention with Transformers</h2></header><div class=entry-content><p>In the previous post, we explored sequence modeling using an encoder-decoder architecture connected through an attention mechanism. This mechanism the decoder to ‚Äúattend‚Äù to different parts of the input at each time step while generating the output sequence.
Attention can also be applied to a variety of tasks, such as image captioning. In this case, the decoder RNN focuses on different regions of the input image as it generates each word of the output caption....</p></div><footer class=entry-footer><span title='2024-11-18 00:00:00 +0000 UTC'>November 18, 2024</span>&nbsp;¬∑&nbsp;19 min</footer><a class=entry-link aria-label="post link to Generalizing Attention with Transformers" href=https://yugajmera.github.io/posts/05-transformer/post/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Sequence Modeling with Recurrent Neural Networks and Attention</h2></header><div class=entry-content><p>In previous discussions, we focused on feedforward neural networks, which take a single image as input, process it through multiple layers of convolution, normalization, or fully connected layers, and output a single label for image classification tasks. This is a one-to-one relationship: a single image maps to a single output label.
However, there are other types of problems we want to solve using deep learning that involve variable-length sequences as both inputs and outputs....</p></div><footer class=entry-footer><span title='2024-11-08 00:00:00 +0000 UTC'>November 8, 2024</span>&nbsp;¬∑&nbsp;13 min</footer><a class=entry-link aria-label="post link to Sequence Modeling with Recurrent Neural Networks and Attention" href=https://yugajmera.github.io/posts/05-attention/post/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Generative Image Models Part 1: VAE and GANs</h2></header><div class=entry-content><p>Machine learning can broadly be categorized into three types:
Supervised Learning: The model is trained to learn a mapping from input data $\mathbf{x}$ to output labels $\mathbf{y}$. This includes tasks like image classification, object detection, and image captioning. However, supervised learning depends heavily on large, labeled datasets‚Äîtypically created through extensive human annotation.
Unsupervised Learning: The model is trained to discover patterns or underlying structures within unlabelled data. A key objective here is to learn the data distribution $p_{data}(\mathbf{x})$, enabling us to sample from it to generate new data....</p></div><footer class=entry-footer><span title='2022-11-16 00:00:00 +0000 UTC'>November 16, 2022</span>&nbsp;¬∑&nbsp;12 min</footer><a class=entry-link aria-label="post link to Generative Image Models Part 1: VAE and GANs" href=https://yugajmera.github.io/posts/04-gen1/post/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>ImageNet Challenge: The Olympics of Deep Learning</h2></header><div class=entry-content><p>The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was an annual competition that took place from 2010 to 2017, attracting teams from around the world to showcase their best-performing image classification models. This challenge became a crucial benchmark in the field, with its winners significantly influencing the landscape of image recognition and deep learning research.
The competition used a subset of the ImageNet dataset, containing 1.3M training examples across 1000 different classes, with 50k validation and 100k test examples....</p></div><footer class=entry-footer><span title='2022-10-18 00:00:00 +0000 UTC'>October 18, 2022</span>&nbsp;¬∑&nbsp;16 min</footer><a class=entry-link aria-label="post link to ImageNet Challenge: The Olympics of Deep Learning" href=https://yugajmera.github.io/posts/03-imagenet/post/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Convolutional Neural Networks: Deep Learning for Image Recognition</h2></header><div class=entry-content><p>Linear classifiers or MLPs that we have discussed so far don‚Äôt respect the 2D spatial structure of input images. These images are flattened into a 1D vector before passing them through the network which destroys the spatial structure of the image.
This creates a need for a new computational model that can operate on images while preserving spatial relationships ‚Äî Convolutional Neural Networks (CNNs). Let‚Äôs understand the components of this CNN model....</p></div><footer class=entry-footer><span title='2022-09-18 00:00:00 +0000 UTC'>September 18, 2022</span>&nbsp;¬∑&nbsp;15 min</footer><a class=entry-link aria-label="post link to Convolutional Neural Networks: Deep Learning for Image Recognition" href=https://yugajmera.github.io/posts/02-cnn/post/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Deep Learning Basics Part 3: The Cherry on Top</h2></header><div class=entry-content><p>We‚Äôve already coded our first neural network architecture from scratch and learned how to train it. Our deep learning cake is almost ready‚Äîbut we still need the toppings to make it more appealing. In this part, we‚Äôre going to discuss the available toppings‚Äîconcepts that enhance optimization and help us reach a better final solution for the model‚Äôs weights. The most important topping among them is Regularization.
Regularization When optimizing, our goal is to find the specific set of weights that minimize loss our training data, aiming for the highest possible accuracy on the test set....</p></div><footer class=entry-footer><span title='2022-08-21 00:00:00 +0000 UTC'>August 21, 2022</span>&nbsp;¬∑&nbsp;15 min</footer><a class=entry-link aria-label="post link to Deep Learning Basics Part 3: The Cherry on Top" href=https://yugajmera.github.io/posts/01-dl3/post/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://yugajmera.github.io/page/2/>Next&nbsp;&nbsp;¬ª</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://yugajmera.github.io/>YA's Almanac</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>